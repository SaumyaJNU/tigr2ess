<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Anim Cogn</journal-id><journal-id journal-id-type="iso-abbrev">Anim Cogn</journal-id><journal-title-group><journal-title>Animal Cognition</journal-title></journal-title-group><issn pub-type="ppub">1435-9448</issn><issn pub-type="epub">1435-9456</issn><publisher><publisher-name>Springer Berlin Heidelberg</publisher-name><publisher-loc>Berlin/Heidelberg</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">5818571</article-id><article-id pub-id-type="publisher-id">1165</article-id><article-id pub-id-type="doi">10.1007/s10071-018-1165-3</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Paper</subject></subj-group></article-categories><title-group><article-title>Mechanisms underlying speech sound discrimination and categorization in humans and zebra finches</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Burgering</surname><given-names>Merel A.</given-names></name><address><phone>+31 134662994</phone><email>m.a.burgering@tilburguniversity.edu</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>ten Cate</surname><given-names>Carel</given-names></name><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Vroomen</surname><given-names>Jean</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0943 3265</institution-id><institution-id institution-id-type="GRID">grid.12295.3d</institution-id><institution>Department of Cognitive Neuropsychology, </institution><institution>Tilburg University, </institution></institution-wrap>Warandelaan 2, P.O. Box 90153, 5000 LE Tilburg, The Netherlands </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2312 1970</institution-id><institution-id institution-id-type="GRID">grid.5132.5</institution-id><institution>Institute Biology Leiden (IBL) Leiden University, </institution></institution-wrap>P.O. Box 9505, 2300 RA Leiden, The Netherlands </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2312 1970</institution-id><institution-id institution-id-type="GRID">grid.5132.5</institution-id><institution>Leiden Institute for Brain and Cognition (LIBC), </institution><institution>Leiden University, </institution></institution-wrap>Leiden, The Netherlands </aff></contrib-group><pub-date pub-type="epub"><day>12</day><month>2</month><year>2018</year></pub-date><pub-date pub-type="pmc-release"><day>12</day><month>2</month><year>2018</year></pub-date><pub-date pub-type="ppub"><year>2018</year></pub-date><volume>21</volume><issue>2</issue><fpage>285</fpage><lpage>299</lpage><history><date date-type="received"><day>26</day><month>10</month><year>2017</year></date><date date-type="rev-recd"><day>29</day><month>1</month><year>2018</year></date><date date-type="accepted"><day>1</day><month>2</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2018</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Speech sound categorization in birds seems in many ways comparable to that by humans, but it is unclear what mechanisms underlie such categorization. To examine this, we trained zebra finches and humans to discriminate two pairs of edited speech sounds that varied either along one dimension (vowel or speaker sex) or along two dimensions (vowel and speaker sex). Sounds could be memorized individually or categorized based on one dimension or by integrating or combining both dimensions. Once training was completed, we tested generalization to new speech sounds that were either more extreme, more ambiguous (i.e., close to the category boundary), or within-category intermediate between the trained sounds. Both humans and zebra finches learned the one-dimensional stimulus&#x02013;response mappings faster than the two-dimensional mappings. Humans performed higher on the trained, extreme and within-category intermediate test-sounds than on the ambiguous ones. Some individual birds also did so, but most performed higher on the trained exemplars than on the extreme, within-category intermediate and ambiguous test-sounds. These results suggest that humans rely on rule learning to form categories and show poor performance when they cannot apply a rule. Birds rely mostly on exemplar-based memory with weak evidence for rule learning.</p><sec><title>Electronic supplementary material</title><p>The online version of this article (10.1007/s10071-018-1165-3) contains supplementary material, which is available to authorized users.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Categorization</kwd><kwd>Speech perception</kwd><kwd>Comparative cognition</kwd><kwd>Songbirds</kwd><kwd>Zebra finch</kwd><kwd>Human</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>024.001.006</award-id><principal-award-recipient><name><surname>Vroomen</surname><given-names>Jean</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer-Verlag GmbH Germany, part of Springer Nature 2018</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Many studies have demonstrated that nonhuman animals (hereafter: animals) can be taught to discriminate human speech sounds. For example, speech discrimination in Japanese quail (Kluender et al. <xref ref-type="bibr" rid="CR26">1987</xref>), pigeons and blackbirds (Hienz et al. <xref ref-type="bibr" rid="CR20">1981</xref>), rats (Eriksson and Villa <xref ref-type="bibr" rid="CR12">2006</xref>), cats, monkeys (Dewson <xref ref-type="bibr" rid="CR7">1964</xref>), budgerigars (Dooling and Brown <xref ref-type="bibr" rid="CR9">1990</xref>), ferrets (Bizley et al. <xref ref-type="bibr" rid="CR6">2013</xref>), baboons (Hienz and Brady <xref ref-type="bibr" rid="CR19">1988</xref>), chinchillas and macaques (Kuhl and Miller <xref ref-type="bibr" rid="CR31">1975</xref>; Kuhl and Padden <xref ref-type="bibr" rid="CR32">1982</xref>) seems in many ways comparable to that of humans with respect to forming speech sound categories. Recent studies demonstrated that also zebra finches can discriminate isolated vowels and natural or synthetic syllables that differ in vowel (Kriengwatana et al. <xref ref-type="bibr" rid="CR28">2015a</xref>; Ohms et al. <xref ref-type="bibr" rid="CR37">2010</xref>, <xref ref-type="bibr" rid="CR38">2012</xref>). Furthermore, the birds were able to maintain this discrimination when the syllables were pronounced by new speakers of the same sex or the other sex, which reveals the ability to generalize perceptually learned sounds to other speakers (Kriengwatana et al. <xref ref-type="bibr" rid="CR28">2015a</xref>; Ohms et al. <xref ref-type="bibr" rid="CR37">2010</xref>). However, what type of cognitive mechanisms underlie this discrimination and generalization and to what extent zebra finches can show categorization is yet unknown. Comparative studies can reveal more about the cognitive mechanisms used by birds and humans (Mercado et al. <xref ref-type="bibr" rid="CR35">2005</xref>). Here, we compare speech sound categorization of zebra finches and humans using two one-dimensional stimulus&#x02013;response (SR) mappings in which subjects had to discriminate either &#x02018;<italic>wet&#x02019;</italic> from &#x02018;<italic>wit&#x02019;</italic> or male from female speakers, and two two-dimensional SR-mappings in which subjects were required to use both dimensions. After subjects had learned to accurately categorize the trained sounds, we tested generalization to more and less extreme versions of the stimuli. Different theories on the mechanisms underlying categorization predict differences in learning speed between one- and two-dimensional mappings as well as in generalization to novel stimuli (Smith <xref ref-type="bibr" rid="CR43">2014</xref>; Smith et al. <xref ref-type="bibr" rid="CR45">2011</xref>, <xref ref-type="bibr" rid="CR46">2012</xref>, <xref ref-type="bibr" rid="CR47">2016</xref>).</p><p id="Par3">Auditory categorization is a cognitive mechanism crucial for speech perception (Erickson and Kruschke <xref ref-type="bibr" rid="CR11">1998</xref>; Francis and Nusbaum <xref ref-type="bibr" rid="CR13">2002</xref>; Goudbeek et al. <xref ref-type="bibr" rid="CR17">2009</xref>; Holt and Lotto <xref ref-type="bibr" rid="CR23">2010</xref>), facilitating both first language acquisition in infants (Eimas et al. <xref ref-type="bibr" rid="CR10">1971</xref>) and second language acquisition in adults (Holt and Lotto <xref ref-type="bibr" rid="CR22">2006</xref>; Kuhl <xref ref-type="bibr" rid="CR30">2004</xref>). It allows humans to categorize sounds as being a particular vowel or from a male or female speaker. Categorization involves within-category generalization and between-category discrimination. Categorization also implies mapping of these sounds to an auditory category in a multi-dimensional space (Erickson and Kruschke <xref ref-type="bibr" rid="CR11">1998</xref>; Hazan and Barrett <xref ref-type="bibr" rid="CR18">2000</xref>). This mechanism is remarkable since categories may overlap and variability within categories may be high (Goudbeek et al. <xref ref-type="bibr" rid="CR17">2009</xref>; Hillenbrand et al. <xref ref-type="bibr" rid="CR21">1995</xref>). An example of such overlapping categorizations is that for vowels and speaker sex. Both vowel categorization and speaker sex categorization (often described as gender categorization) have been demonstrated in humans (Fuller et al. <xref ref-type="bibr" rid="CR14">2014</xref>; Goudbeek et al. <xref ref-type="bibr" rid="CR17">2009</xref>; Holt and Lotto <xref ref-type="bibr" rid="CR23">2010</xref>; Massida et al. <xref ref-type="bibr" rid="CR34">2013</xref>; Skuk et al. <xref ref-type="bibr" rid="CR42">2015</xref>). Vowel perception requires both speaker normalization and categorization based on segmental information, mostly determined by the ratio between the two lowest formant frequencies: F1/F2 (Johnson <xref ref-type="bibr" rid="CR24">1990</xref>; Kriengwatana et al. <xref ref-type="bibr" rid="CR29">2015b</xref>; Polka and Bohn <xref ref-type="bibr" rid="CR39">2003</xref>). For categorization based on speaker sex, human listeners mostly rely on the pitch (fundamental frequency&#x02014;F0) (Fuller et al. <xref ref-type="bibr" rid="CR14">2014</xref>; Skuk et al. <xref ref-type="bibr" rid="CR42">2015</xref>). Whether and how birds can categorize speech sounds by speaker sex is, to the best of our knowledge, unknown.</p><p id="Par4">The formation of human vowel categories is affected by learning (Kuhl <xref ref-type="bibr" rid="CR30">2004</xref>). The exposure to individual sounds results in an abstract representation beyond the exemplars. Different mechanisms may underlie such categorization, such as prototype learning, rule-based learning, or information-integration (Ashby and Maddox <xref ref-type="bibr" rid="CR3">2005</xref>; Erickson and Kruschke <xref ref-type="bibr" rid="CR11">1998</xref>; Maddox and Ashby <xref ref-type="bibr" rid="CR33">2004</xref>; Minda and Smith <xref ref-type="bibr" rid="CR36">2001</xref>; Smith et al. <xref ref-type="bibr" rid="CR45">2011</xref>, <xref ref-type="bibr" rid="CR46">2012</xref>, <xref ref-type="bibr" rid="CR47">2016</xref>; Smith and Minda <xref ref-type="bibr" rid="CR44">1999</xref>). Such learning mechanisms contrast with exemplar-based memorization, in which sounds in a stimulus set are discriminated based on learning the individual training stimuli. This can be seen as a nonanalytic way of learning (Smith et al. <xref ref-type="bibr" rid="CR46">2012</xref>). Generalization to new sounds is then based on the similarity to any of the trained stimuli. With prototype learning, some features of training sounds belonging to the same category are &#x02018;averaged&#x02019; to form a prototype. The response to new stimuli depends on the characteristics shared with the category prototypes. Rule-based learning involves the learning of a one-dimensional rule (vowel or speaker sex) or conjunction rule (e.g., press left if stimulus is &#x02018;0&#x02019; on dimension <italic>x</italic> and &#x02018;1&#x02019; on dimension <italic>y</italic> (&#x02018;01&#x02019;) vs. press right if stimulus is &#x02018;1&#x02019; on dimension <italic>x</italic> and &#x02018;0&#x02019; on dimension <italic>y</italic> (&#x02018;10&#x02019;)) (Ashby and Maddox <xref ref-type="bibr" rid="CR3">2005</xref>). Here, the subjects identify the dimension or combination of dimensions on which stimuli can be distinguished. This analytical learning result in learning a rule that humans can describe verbally. This will lead to optimal categorization if, for example, the pitch of a sound is above or below a certain value (Smith et al. <xref ref-type="bibr" rid="CR45">2011</xref>). Information-integration concerns an implicit mechanism that is used when only the integration of two or more dimensions enables correct classification (Gottwald and Garner <xref ref-type="bibr" rid="CR15">1972</xref>; Goudbeek et al. <xref ref-type="bibr" rid="CR17">2009</xref>; Posner and Keele <xref ref-type="bibr" rid="CR40">1968</xref>). Previous studies on visual and auditory categorization showed that humans use a rule-based mechanism, when possible (Goudbeek et al. <xref ref-type="bibr" rid="CR16">2007</xref>, <xref ref-type="bibr" rid="CR17">2009</xref>; Smith et al. <xref ref-type="bibr" rid="CR46">2012</xref>, <xref ref-type="bibr" rid="CR47">2016</xref>).</p><p id="Par5">In the current study, we examined the occurrence of processes resulting in category-level knowledge for both humans and zebra finches. We used a 2-alternative forced-choice (2-AFC) task to compare performance on four different speech sound mappings. In one-dimensional mappings, subjects were trained to categorize four training sounds either based on vowel or speaker sex. In addition, there were two two-dimensional mappings, which required the use of both dimensions to classify the stimuli. In one of these mappings, the optimal boundary was diagonal; therefore, we used the descriptive term diagonal mapping (experiment 2) (Ashby et al. <xref ref-type="bibr" rid="CR5">2002</xref>). Here, category formation is possible by integrating both dimensions. The other two-dimensional mapping, an exclusive-or (XOR) mapping (Anderson et al. <xref ref-type="bibr" rid="CR2">2006</xref>; Ashby et al. <xref ref-type="bibr" rid="CR5">2002</xref>; Smith <xref ref-type="bibr" rid="CR43">2014</xref>; Smith et al. <xref ref-type="bibr" rid="CR47">2016</xref>), also required the combination of vowel and speaker sex, but there is no straight forward rule that could define categories, and subjects had to remember that male <italic>wit</italic> and female <italic>wet</italic> were one category, and female <italic>wit</italic> and male <italic>wet</italic> the other (experiment 3) (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>). Training continued until criterion was reached. In a subsequent test phase, we examined categorization of the trained sounds by examining generalization to new test-sounds that were either further away from the hypothetical category boundary (extreme test-sounds), closer to the category boundary (ambiguous test-sounds), or in-between the trained sounds (within-category intermediate test-sounds).<fig id="Fig1"><label>Fig.&#x000a0;1</label><caption><p>The panels display a stimulus matrix in which the vowel continuum is represented on the <italic>X</italic>-axis, from <italic>wet</italic> to <italic>wit</italic> from left to right, and the speaker sex continuum is represented on the <italic>Y</italic>-axis, from female to male from top to bottom. The distances from the hypothetical category boundary (dashed line) to the trained sounds (Tr), the more extreme (Ext) and more ambiguous (Amb) test-sounds are represented with arrows. If a subject uses exemplar memorization, it will perform best on Tr during the test. If a subject forms categories, generalization to new test-sounds may depend on the distance of these sounds from the category boundary, but one might expect similar responses to Tr, intermediate (Int) and Ext stimuli and possibly a lesser response to Amb. Left panel&#x02014;example of a one-dimensional mapping (the <italic>wet</italic>&#x02013;<italic>wit</italic> distinction). Right panel&#x02014;example of a diagonal mapping. Gray boxes on the dotted lines represent sounds (between categories) not used for the analyses described here</p></caption><graphic xlink:href="10071_2018_1165_Fig1_HTML" id="MO1"/></fig>
</p><p id="Par6">We expected humans to have no difficulty with one-dimensional mappings as these may fit already fine-tuned categories for vowels and for speaker sex (Goudbeek et al. <xref ref-type="bibr" rid="CR17">2009</xref>). Furthermore, we expected generalization to new test-sounds to depend on the distance of these sounds from the category boundary (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>). When the categories are well established, the extreme and within-category intermediate test-sounds should be easy to categorize because they are away from the boundary, whereas the ambiguous test-sounds may be harder to categorize because they are close to the boundary. Although zebra finches showed that acoustic differences between different vowels can be more salient than the differences between same vowels produced by different (male) speakers (Dooling <xref ref-type="bibr" rid="CR8">1992</xref>), zebra finches obviously do not already possess the human categories for vowels and speaker sex. As a result they might learn to respond to individual training stimuli by exemplar-based memorization. However, during training they might discover acoustical similarities between stimuli and hence also categorize these in a rule-based way. If they use exemplar-based memorization, we then expected performance at test to be best on the trained sounds; generalization to new sounds should depend on the acoustical distance from the trained sounds (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>). A previous study that demonstrated vowel categorization in European starlings suggests that extensive training on dense vowel distributions with many exemplars is a prerequisite for category learning (Kluender et al. <xref ref-type="bibr" rid="CR27">1998</xref>). However, recent studies suggested that zebra finches might acquire categories during training on a small set of stimuli (Kriengwatana et al. <xref ref-type="bibr" rid="CR28">2015a</xref>; Ohms et al. <xref ref-type="bibr" rid="CR37">2010</xref>). If this is rule-based, then zebra finches&#x02019; performance in the test phase should show a human-like pattern of generalization: high performance on the trained, extreme and within-category intermediate test-sounds, but low performance on the ambiguous ones.</p><p id="Par7">In the two-dimensional SR-mappings, subjects were trained to categorize the four training sounds along two dimensions rather than one. We expected humans to have more difficulty learning and maintaining these two-dimensional mappings than the one-dimensional mappings (Goudbeek et al. <xref ref-type="bibr" rid="CR16">2007</xref>), in particular for the XOR mapping for which the categories are heterogeneous and allow no generalization. If zebra finches are able to acquire similar dimensions during training to humans, then they might also have more difficulty with the two-dimensional than one-dimensional mappings. Alternatively, if zebra finches are purely relying on exemplar-based categorization, it may not matter if the training sounds vary in one or two dimensions, so performances in test is expected to be similar for all four mappings.</p></sec><sec id="Sec2"><title>Methods</title><sec id="Sec3"><title>Subjects</title><p id="Par8"><italic>Birds</italic>&#x02014;We used thirty-six adult zebra finches, (<italic>Taeniopygia guttata)</italic> (18 males and 18 females) from the Leiden University breeding colony. All birds were between 120 and 563&#x000a0;days post-hatching at the start of the experiment. Prior to the experiment, birds were housed in single-sex groups of no more than fifteen animals and they were kept on a 13.5 L:10.5 D schedule at 20&#x02013;22&#x000a0;&#x000b0;C. The birds always had access to a seed mixture (42% yellow millet, 22% canary seed, 16% yellow panis, 12% white millet, 6% red millet and 2% red panis). Twice a week, the birds received some egg food (mashed boiled eggs) and vegetables and fruits (grated carrots and apple). During the experiment, drinking water, cuttlebone, and grit were available ad libitum. The birds had no previous experience with similar behavioral experiments. All animal procedures were approved by the Leiden Committee for animal experimentation (DEC) (DEC number 14178).</p><p id="Par9"><italic>Humans</italic>&#x02014;Sixty students from Tilburg University (39 women, 21 men) with mean age of 21 (standard deviation (SD)&#x000a0;=&#x000a0;3&#x000a0;years) participated after having given written informed consent. Participants reported normal hearing and were na&#x000ef;ve to sounds used in the experiment and research question. All participants received course credits for participation. The study was conducted in accordance with the ethical standards of the 2013 Declaration of Helsinki.</p></sec><sec id="Sec4"><title>Apparatus</title><p id="Par10"><italic>Birds</italic>&#x02014;Zebra finches were individually housed in an operant conditioning chamber (Skinnerbox) (70 (l)&#x000a0;&#x000d7;&#x000a0;30 (d)&#x000a0;&#x000d7;&#x000a0;45 (h)&#x000a0;cm), constructed of wire mesh front and side walls and a foamed PVC back wall. The cage was placed in a sound-attenuated chamber. A fluorescent lamp (Phillips Master TL-D 90 DeLuxe 18&#x000a0;W/965, The Netherlands) served as the light source and was placed on top of the Skinnerbox. The same light/dark schedule as in the breeding colony was applied. The back wall of the cage contained three horizontally aligned gray round pecking keys (hereafter: sensors) with a red LED light at the top of each sensor. Sound stimuli were played at approximately 70&#x000a0;dB (SPL meter, RION NL 15, RION) through a speaker (Vifa MG10SD09-08) 1&#x000a0;m above the cage. The three pecking sensors, the fluorescent lamp, the food hatch and speaker were connected to an operant conditioning controller that also registered all sensor pecks of the bird (supplement Fig.&#x000a0;1). Pecking the middle sensor elicited a sound stimulus and illuminated the LED light of the left and right sensor. Depending on the sound, the bird had to the peck left or right sensor. A correct response resulted in access to food for 8&#x02013;10&#x000a0;s and an incorrect response led to 1&#x02013;15&#x000a0;s darkness depending on the experimental phase.</p><p id="Par11"><italic>Humans</italic>&#x02014;The experiment took place in a dimly lit sound-attenuated room. Instructions were presented on a 19-in monitor positioned at eye-level, 70&#x000a0;cm from the participant&#x02019;s head. The sound was presented through Sennheiser HD-203 headphones with a peak intensity of 60&#x000a0;dB. The participant responded by pressing one of two buttons on a response box standing in front of the monitor.</p></sec><sec id="Sec5"><title>Stimulus material</title><p id="Par12">We created three versions for all sounds in the stimulus matrix in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> (hereafter: stimulus matrices). In order to create the three stimulus matrices of morphed speech sounds, recordings of <italic>wet</italic> and <italic>wit</italic> from six speakers (three male, three female) from an earlier study were selected (Ohms et al. <xref ref-type="bibr" rid="CR37">2010</xref>). The sound <italic>wet</italic> was pronounced as <italic>wet</italic> in General American English (the open-middle front unrounded vowel in /w&#x003b5;t/ in International Phonetic Alphabet (IPA) and the sound <italic>wit</italic> was pronounced as /<italic>wit</italic>/&#x000a0;in General American English (the near-close near-front unrounded vowel in /wIt/ in IPA). The vowels were chosen based on canonical Dutch F1/F2 values for each sex (Adank et al. <xref ref-type="bibr" rid="CR1">2004</xref>). Three stimulus matrices were constructed with Tandem-STRAIGHT (Kawahara et al. <xref ref-type="bibr" rid="CR25">2008</xref>; Skuk and Schweinberger <xref ref-type="bibr" rid="CR41">2014</xref>), each based on four different natural speech recordings: <italic>wet</italic> and <italic>wit</italic> spoken by one male and <italic>wet</italic> and <italic>wit</italic> spoken by one female. All recordings were selected based on little noise and few fluctuations in the formant frequencies and for each stimulus matrix, recordings were matched based on duration and formants.</p><p id="Par13">Stimulus creation started by creating two male&#x02013;female continua, one for <italic>wet</italic> and the other for <italic>wit.</italic> From these two male&#x02013;female morphs, the vowel morphs were constructed following the same procedure. Discriminability of the morphed sounds to humans was tested in a pilot study (<italic>N</italic>&#x000a0;=&#x000a0;7). Based on Spearman&#x02013;Karber curves for all individuals, a step-size of 14% on the <italic>wet</italic>&#x02013;<italic>wit</italic> continuum and a step-size of 10% on the female&#x02013;male continuum were chosen in order to balance the salience on both dimensions. Sounds on the <italic>wet</italic>&#x02013;<italic>wit</italic> continuum went from 8 to 92%, and sounds on the male&#x02013;female continuum were going from 20 to 80%. We excluded natural end-points in order to keep the acoustic manipulation for all sounds the same. Four training stimuli (supplement Table&#x000a0;1 and Figs.&#x000a0;2&#x02013;4) and twelve test stimuli, including more extreme and more ambiguous test-sounds were used for all experiments (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>, left and right).<fig id="Fig2"><label>Fig.&#x000a0;2</label><caption><p>All subjects were trained to categorize four training sounds (Tr1, Tr2, Tr3, and Tr4 for the vowel-, speaker sex-or XOR mapping, and Tr5, Tr6, Tr7 and Tr8 for the diagonal mapping) into two categories. All sounds come from the same set of stimuli but are labeled differently for the vowel-, speaker sex-and XOR mapping versus the diagonal mapping. Upon reaching criterion they were tested on nonreinforced trained and test-sounds. Left panel&#x02014;The sound labeling for subjects assigned to the vowel-, speaker sex-or XOR mapping. The within-category intermediate sounds for the vowel mapping (Int) and for the speaker sex mapping (<italic>Int</italic>). Right panel&#x02014;The sound labeling for subject assigned to the diagonal mapping</p></caption><graphic xlink:href="10071_2018_1165_Fig2_HTML" id="MO2"/></fig>
</p></sec><sec id="Sec6"><title>Design</title><p id="Par14">The subjects were randomly assigned to one of the different mappings (mapping was between-subjects). Every mapping was completed by 15 humans and nine birds. Per mapping, each of the three versions of the stimulus matrix was used for five humans and for three birds.</p></sec><sec id="Sec7"><title>Procedure</title><p id="Par15">All subjects were trained to categorize four training sounds into two categories. Upon reaching criterion, they were tested on the trained and nonreinforced test-sounds. In the vowel mapping, Tr1 and Tr3 (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>, left panel) were assigned to one category and Tr2 and Tr4 to the other category (Tr1&#x02013;Tr3 vs. Tr2&#x02013;Tr4). In the speaker sex mapping, Tr1 and Tr2 were assigned to one category and Tr3 and Tr4 to the other category (Tr1&#x02013;Tr2 vs. Tr3&#x02013;Tr4). In the XOR mapping, Tr1 and Tr4 were assigned to one category, and Tr2 and Tr3 to the other category (Tr1&#x02013;Tr4 vs. Tr2&#x02013;Tr3). In the diagonal mapping (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>, right panel), Tr5 and Tr7 were assigned to one category and Tr6 and Tr8 to the other category (Tr5&#x02013;Tr7 vs. Tr6&#x02013;Tr8).</p><p id="Par16"><italic>Birds</italic>&#x02014;At the start of the experiment, every animal was physically examined to allow monitoring of welfare. During the experiment, the birds were closely monitored. If, for some reason, a bird had not been able to obtain food for 18&#x000a0;h, the food hatch opened automatically. Each experiment consisted of a shaping, a training, a transition and a test phase.</p><p id="Par17">Prior to the experiment, the bird needed to acclimate to the cage and learn where to find food. The food hatch was open and the three LEDs on the pecking sensors were switched on. After a few hours up to overnight, the shaping phase was started by closing the food hatch. During the first shaping phase, the bird had to learn to peck all three sensors. Pecking the middle sensor elicited one of the two unfamiliar zebra finch songs (song A of 58&#x000a0;ms or song B of 94&#x000a0;ms), pecking the left sensor or right sensor elicited song A, respectively, song B and led to opening of the food hatch for 10&#x000a0;s. Birds that did not start pecking spontaneously were trained in sessions by flickering the LEDs on the sensors. Once the bird started pecking all sensors, the second shaping phase was started. In this phase, the bird had to learn to initiate its own trial by pecking the middle sensor first and then respond to the played sound by pecking the left or right sensor. When song A was played, pecking the left sensor resulted in food access whereas pecking the right sensor resulted in a preset time of darkness and vice versa for song B. The birds had a response time of 25&#x000a0;s and a trial ended automatically in case the bird did not respond within this time window. An initial darkness of one second built up to 3&#x000a0;s and ultimately 15&#x000a0;s darkness and 8&#x000a0;s food access time. The inter-trial interval was 2&#x000a0;s.</p><p id="Par18">For every day, the discrimination between the stimuli by each bird was calculated as the proportion of correct responses out of all sounds that birds responded to. After 3&#x000a0;days performing at&#x000a0;&#x0003e;&#x000a0;0.75, the bird was transferred to the training phase, during which the bird was trained on four training sounds (Tr1-to-Tr4, or Tr5-to-Tr8) according to the relevant SR-schema the bird was assigned to.</p><p id="Par19">After a bird had learned to associate the four training sounds to the correct sensor (overall discrimination score&#x000a0;&#x0003e;&#x000a0;0.75, and a score of&#x000a0;&#x0003e;&#x000a0;0.60 for each sensor for three consecutive days), the bird was transferred to the transition phase, during which these four stimuli were not reinforced in 20% of the trials for 1&#x000a0;day. By doing so, the bird was prepared for the test phase. During the test phase, 12 new sounds (other morphs out of the same stimuli set) were introduced. Test-sounds were never reinforced and were randomly interspersed between training sounds. Of all trials, 20% were test-sounds and 80% were training sounds. After 40 repeats of all test-sounds, the experiment was finished and the bird was returned to the aviary.</p><p id="Par20"><italic>Humans</italic>&#x02014;The human participants were instructed to sort the sounds into two different groups. They were left na&#x000ef;ve to the relevant SR-assignment. The experiment consisted of three phases: a familiarization phase, a training phase, and a test phase. In the familiarization phase, all four training sounds were played two times in random order in order to familiarize subjects with the sounds. Hereafter, the training phase followed wherein the participants learned to assign the four training sounds into two categories based on visual feedback (&#x02018;correct&#x02019; and &#x02018;incorrect&#x02019;) after each response. In the training phase, all four training sounds were repeated five times (20 sounds per training block) in a random order at 100% reinforcement. The participants were promoted to the next phase if accuracy was on average&#x000a0;&#x0003e;&#x000a0;0.75 and&#x000a0;&#x0003e;&#x000a0;0.60 per category. If the participant did not reach the criteria, the block was repeated until a maximum of 15 blocks (300 trials). The test phase consisted of four blocks of 80 nonreinforced trials each (five&#x000a0;&#x000d7;&#x000a0;four training sounds and five&#x000a0;&#x000d7;&#x000a0;12 new speech sounds in a random order). After each block, the four training sounds were all two times randomly repeated and reinforced. In a short post-experimental questionnaire, humans were asked to explain how they sorted the sounds.</p></sec><sec id="Sec8"><title>Analyses</title><p id="Par21">Both for humans and zebra finches, the response data were recorded as binomial measurements (number of left (&#x02018;0&#x02019;) and right (&#x02018;1&#x02019;) responses). For both species, a proportion &#x02018;correct&#x02019; for the different sound types was calculated by taking the average scores of the proportion of correct responses to a particular sound type on each side of the midline between the differentially reinforced stimuli (e.g., taking the average of the proportion of correct pecks to &#x02018;extreme <italic>wit</italic>&#x02019; and proportion of correct rejections to &#x02018;extreme <italic>wet</italic>&#x02019; for the vowel test). For the birds, the proportions correct for the trained sounds included nonreinforced trials only.</p><p id="Par22"><italic>Training</italic>&#x02014;We measured the number of training trials (birds) or training blocks (humans) required before reaching the overall proportion correct of&#x000a0;&#x0003e;&#x000a0;0.75 as well as discrimination for both left and right of&#x000a0;&#x0003e;&#x000a0;0.60 on three consecutive days (birds) or one training block (humans). For both species, the distribution for the number of training trials or training blocks of the four different experimental conditions were checked for normality. Because the datasets were not normally distributed, we submitted both datasets (humans and birds separately) to separate Kruskal&#x02013;Wallis tests, wherein mapping type was the fixed factor. In order to test whether subjects learned one-dimensional mappings (vowel and speaker sex combined) faster than two-dimensional mappings (diagonal and XOR combined), we ran a separate GLM/Mann&#x02013;Whitney test wherein dimensionality was the fixed factor.</p><p id="Par23"><italic>Test</italic>&#x02014;For the analysis of each experimental condition, we calculated the proportion of correct responses per sound type, i.e., for each group of trained, extreme, ambiguous and within-category intermediate sounds (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>). For each sound type, distributions of all proportions correct were checked for normality. For the one-dimensional mappings, the proportion correct for the four extreme sounds, four trained sounds, four ambiguous sounds and the two within-category intermediate sounds (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>, left) were submitted to two separate 2 (species: human/bird)&#x000a0;&#x000d7;&#x000a0;4 (sound type: trained, extreme, ambiguous, within-category intermediate) ANOVA&#x02019;s. For the diagonal mappings, the proportion correct for the four extreme sounds, four trained sounds and four within-category intermediate sounds (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>, right) were submitted to a 2 (species: human/bird)&#x000a0;&#x000d7;&#x000a0;3 (sound type: trained, extreme, within-category intermediate) ANOVA. For the XOR mappings, the proportion correct for the four extreme sounds, four trained sounds and four ambiguous sounds (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>, left) were submitted to a 2 (species: human/bird)&#x000a0;&#x000d7;&#x000a0;3 (sound type: trained, extreme, ambiguous) ANOVA. Post hoc analyses were performed when the main analyses revealed significant effects.</p></sec></sec><sec id="Sec9"><title>Results</title><sec id="Sec10"><title>Training phase</title><p id="Par24"><italic>Birds</italic>&#x02014;In order to reach criterion, birds required on average 5507 (SD&#x000a0;=&#x000a0;2291) trials in the vowel mapping, 4884 (SD&#x000a0;=&#x000a0;2890) trials in the speaker sex mapping, 7534 (SD&#x000a0;=&#x000a0;4185) trials in the diagonal mapping, and 9012 (SD&#x000a0;=&#x000a0;5330) trials in the XOR mapping (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>). In order to test whether the number of trials before reaching criterion was different between the four experimental conditions, a Kruskal&#x02013;Wallis test was performed which rendered a Chi-square value of 5238 that was not significant (<italic>p</italic>&#x000a0;=&#x000a0;0.155). Additionally, we ran a Mann&#x02013;Whitney test to compare the number of trials in the one-dimensional mappings (vowel and speaker sex) to the two-dimensional mappings (diagonal and XOR), that indicated birds were slower on the two-dimensional (median&#x000a0;=&#x000a0;7576) than on the one-dimensional (median&#x000a0;=&#x000a0;4601) mappings (<italic>U</italic>&#x000a0;=&#x000a0;97.5 and <italic>p</italic>&#x000a0;=&#x000a0;0.040).<fig id="Fig3"><label>Fig.&#x000a0;3</label><caption><p>The number of trials for four experimental conditions for the zebra finches. Boxplots display median, interquartile range and full range</p></caption><graphic xlink:href="10071_2018_1165_Fig3_HTML" id="MO3"/></fig>
</p><p id="Par25"><italic>Humans</italic>&#x02014;Humans required on average 5.13 (SD&#x000a0;=&#x000a0;4.64) training blocks to reach criterion in the vowel task, 1.07 (SD&#x000a0;=&#x000a0;0.26) training blocks to reach criterion in the speaker sex mapping, 2.33 (SD&#x000a0;=&#x000a0;0.98) training blocks to reach criterion in the diagonal mapping, and 9.53 (SD&#x000a0;=&#x000a0;3.56) training blocks to reach criterion in the XOR mapping (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>). In order to test whether the number of training blocks before reaching criterion was different between the four experimental conditions, a Kruskal&#x02013;Wallis test was performed which rendered a Chi-square value of 36.301 (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001). Post hoc analysis with Mann&#x02013;Whitney tests with a Bonferroni correction showed that the number of training blocks in the speaker sex mapping (median&#x000a0;=&#x000a0;1) was significantly faster than in the vowel mapping (median&#x000a0;=&#x000a0;3) (<italic>U</italic>&#x000a0;=&#x000a0;41, <italic>p</italic>&#x000a0;=&#x000a0;0.001), the XOR mapping (median&#x000a0;=&#x000a0;8) (<italic>U</italic>&#x000a0;=&#x000a0;0.000, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001) and the diagonal mapping (median&#x000a0;=&#x000a0;2) (<italic>U</italic>&#x000a0;=&#x000a0;27, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001). The number of training blocks before reaching criterion in the vowel mapping (median&#x000a0;=&#x000a0;3) was lower than in the XOR mapping (median&#x000a0;=&#x000a0;8) (<italic>U</italic>&#x000a0;=&#x000a0;53, <italic>p</italic>&#x000a0;=&#x000a0;0.013) but not significantly lower than in the diagonal mapping (median&#x000a0;=&#x000a0;2) (<italic>U</italic>&#x000a0;=&#x000a0;90.5, <italic>p</italic>&#x000a0;=&#x000a0;0.351). The number of training blocks before reaching criterion was significantly lower in the diagonal mapping (median&#x000a0;=&#x000a0;2) than in the XOR mapping (median&#x000a0;=&#x000a0;8) (<italic>U</italic>&#x000a0;=&#x000a0;0.000, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001). Thus, humans learn the speaker sex mapping the fastest and the XOR mapping the slowest. To compare the number of training blocks before reaching criterion on the one-dimensional with the two-dimensional mappings, a Mann&#x02013;Whitney test was performed that indicated that birds were significantly slower on two-dimensional (median&#x000a0;=&#x000a0;4, 5) than on one-dimensional mappings (median&#x000a0;=&#x000a0;1) (<italic>U</italic>&#x000a0;=&#x000a0;214.5 and <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001).<fig id="Fig4"><label>Fig.&#x000a0;4</label><caption><p>The number of training blocks for four experimental conditions for humans. Boxplots display median, interquartile range and full range. Significant differences (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.05) are indicated at the top</p></caption><graphic xlink:href="10071_2018_1165_Fig4_HTML" id="MO4"/></fig>
</p></sec><sec id="Sec11"><title>Test phase</title><p id="Par26">Figures&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>, <xref rid="Fig6" ref-type="fig">6</xref>, <xref rid="Fig7" ref-type="fig">7</xref> and <xref rid="Fig8" ref-type="fig">8</xref> display boxplots with median, interquartile range and full range of the proportions correct (supplement Tables&#x000a0;2 and 3 for average proportions correct and SD&#x02019;s). The within-category intermediate sound type is indicated with &#x02018;intermediate.&#x02019; We applied an arcsine transformation on the proportion correct because not all were normally distributed. In addition, the supplemental data (Tables&#x000a0;4&#x02013;7 and Figs.&#x000a0;5&#x02013;7) contain individual data which suggest the use of different mechanisms among individual birds.
</p><sec id="Sec12"><title>Vowel mapping</title><p id="Par27">For individuals trained with the vowel mapping, birds performed much higher on the trained sounds than on the extreme, ambiguous, and within-category intermediate sounds, whereas humans performed about equally high on all sounds. This generalization was supported by a significant main effect for sound type (<italic>F</italic>(3,88)&#x000a0;=&#x000a0;9.386, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.242), species (<italic>F</italic>(1,88)&#x000a0;=&#x000a0;72.250, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.451) and a significant interaction effect for sound type&#x000a0;&#x000d7;&#x000a0;species (<italic>F</italic>(3,88)&#x000a0;=&#x000a0;4.804, <italic>p</italic>&#x000a0;=&#x000a0;0.004) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.141). A one-way ANOVA for birds showed that there was a significant difference between the sound type (<italic>F</italic>(3,32)&#x000a0;=&#x000a0;25.497, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.705). Post hoc analyses showed that proportions correct for trained sounds (0.87&#x000a0;&#x000b1;&#x000a0;0.09) were significantly higher than extreme (0.61&#x000a0;&#x000b1;&#x000a0;0.06) (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001), ambiguous (0.58&#x000a0;&#x000b1;&#x000a0;0.04) (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001) and within-category intermediate sounds (0.62&#x000a0;&#x000b1;&#x000a0;0.10) (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001). For humans, we found a significant difference for sound type (<italic>F</italic>(3,56)&#x000a0;=&#x000a0;2.885, <italic>p</italic>&#x000a0;=&#x000a0;0.044) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.134), but the paired differences were not significantly different from each other in the post hoc analyses. In addition to the analysis at group level, we noted there were individual differences among the birds: one bird out of nine showed generalization to the extreme and within-category intermediate sounds (proportion correct&#x000a0;&#x0003e;&#x000a0;0.75), whereas this bird showed a lower proportion correct for the ambiguous sounds (0.62) (supplement Table&#x000a0;4 and Fig.&#x000a0;5). Humans readily generalized to the more and less extreme test tokens. All humans, except one, reported the correct strategy. This person performed on chance level for all new sounds.<fig id="Fig5"><label>Fig.&#x000a0;5</label><caption><p>Proportions correct for the four sound type in the vowel mapping. Birds are represented in white, humans in gray. The boxplots represent the median, interquartile range, and full range of the proportions correct for the different sound types. Significant differences (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.05) are indicated at the top. The horizontal line marks chance level of 0.5 correct</p></caption><graphic xlink:href="10071_2018_1165_Fig5_HTML" id="MO5"/></fig></p></sec><sec id="Sec13"><title>Speaker sex mapping</title><p id="Par28">For individuals trained with the speaker sex mapping, similar analyses were run as in the vowel mapping. In the 2 (species)&#x000a0;&#x000d7;&#x000a0;4 (sound type) ANOVA, there was a main effect of sound type (<italic>F</italic>(3,88)&#x000a0;=&#x000a0;8.264, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.220) indicating that both birds and humans performed relatively low on the ambiguous sounds (birds 0.62&#x000a0;&#x000b1;&#x000a0;0.05 vs. humans 0.72&#x000a0;&#x000b1;&#x000a0;0.11) compared to the extreme (birds 0.78&#x000a0;&#x000b1;&#x000a0;0.08 vs. humans 0.86&#x000a0;&#x000b1;&#x000a0;0.16), within-category intermediate (birds 0.80&#x000a0;&#x000b1;&#x000a0;0.05 vs. humans 0.83&#x000a0;&#x000b1;&#x000a0;0.17) and trained sounds (birds 0.89&#x000a0;&#x000b1;&#x000a0;0.04 vs. humans 0.83&#x000a0;&#x000b1;&#x000a0;0.16). There was no effect of species (<italic>F</italic>(1,88)&#x000a0;=&#x000a0;3.582, <italic>p</italic>&#x000a0;=&#x000a0;0.062) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.039) and no interaction effect of sound type&#x000a0;&#x000d7;&#x000a0;species (<italic>F</italic>(3,88)&#x000a0;=&#x000a0;1.637, <italic>p</italic>&#x000a0;=&#x000a0;0.187) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.053). In a separate ANOVA for birds, we found a significant difference between sound type (<italic>F</italic>(3,32)&#x000a0;=&#x000a0;26.844, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.716). Post hoc analyses showed that proportions correct for trained sounds were significantly higher than extreme sounds (<italic>p</italic>&#x000a0;=&#x000a0;0.001), ambiguous sounds (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001), and intermediate sounds (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.004). The extreme versus within-category intermediate sounds did not differ significantly from each other (<italic>p</italic>&#x000a0;=&#x000a0;1.000). Proportions correct for ambiguous sounds were significantly lower than extreme sounds (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001) and within-category intermediate sounds (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001). For humans, we found a significant difference between sound types (<italic>F</italic>(3,56)&#x000a0;=&#x000a0;3.073, <italic>p</italic>&#x000a0;=&#x000a0;0.035) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.141) but post hoc analyses were all nonsignificant between any combination of two sound types. Individual data showed that five out of nine birds showed generalization to extreme and within-category intermediate sounds (proportion correct&#x000a0;&#x0003e;&#x000a0;0.75) (supplement Table&#x000a0;5 and Fig.&#x000a0;6). These birds performed higher on new extreme and within-category intermediate sounds than on ambiguous sounds. Humans readily generalized to the more and less extreme test tokens. Three humans in the speaker sex mapping reported that they categorized the sounds based on vowels instead of speaker sex. The others reported the correct strategy. Individual data showed that four humans performed low (&#x0003c;&#x000a0;0.70 proportion correct) and close to chance level on all sound types.<fig id="Fig6"><label>Fig.&#x000a0;6</label><caption><p>Proportions correct for the four sound types in the speaker sex mapping. Birds are represented in white, humans in gray. The boxplots represent the median, interquartile range, and full range of the proportions correct for the different sound types. Significant differences (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.05) are indicated at the top. The horizontal line marks chance level of 0.5</p></caption><graphic xlink:href="10071_2018_1165_Fig6_HTML" id="MO6"/></fig></p></sec><sec id="Sec14"><title>Discussion one-dimensional mappings</title><p id="Par29">In the vowel mapping, zebra finches showed limited generalization to new test-sounds and always performed best on the trained exemplars, a pattern that is indicative of exemplar-based categorization. In the speaker sex mapping, birds also performed best on the trained exemplars, but at the individual level five out of nine birds also showed considerable generalization to extreme and within-category intermediate test-sounds, suggestive of rule-based categorization. Humans showed a generalization pattern that indicates rule-based categorization both for vowels as for speaker sex with higher performance on extreme than ambiguous test-sounds. Both one-dimensional-mappings are easy to solve and verbalize with a simple rule (/e/vs./I/, or male vs. female) and match with human categories for vowel and speaker sex.</p></sec></sec><sec id="Sec15"><title>Diagonal mapping</title><p id="Par30">For individuals trained with the diagonal mapping, the 2 (species)&#x000a0;&#x000d7;&#x000a0;3 (sound type) ANOVA showed that the main effect of sound type was not significant (<italic>F</italic>(2,66)&#x000a0;=&#x000a0;0.921, <italic>p</italic>&#x000a0;=&#x000a0;0.403) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.027) and the main effect of species was also not significant (<italic>F</italic>(1,66)&#x000a0;=&#x000a0;0.844, <italic>p</italic>&#x000a0;=&#x000a0;0.361) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.013) (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>). There was a significant interaction effect for sound type&#x000a0;&#x000d7;&#x000a0;species (<italic>F</italic>(1,66)&#x000a0;=&#x000a0;11.705, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.262). Birds showed relatively high performance on the trained sounds (birds 0.87&#x000a0;&#x000b1;&#x000a0;0.05 vs. humans 0.73&#x000a0;&#x000b1;&#x000a0;0.09) whereas humans performed higher on the extreme (birds 0.74&#x000a0;&#x000b1;&#x000a0;0.06 vs. humans 0.92&#x000a0;&#x000b1;&#x000a0;0.09) and within-category intermediate sounds (birds 0.68&#x000a0;&#x000b1;&#x000a0;0.08 vs. humans 0.82&#x000a0;&#x000b1;&#x000a0;0.10). In a separate ANOVA for birds, we found a significant difference between sound types (<italic>F</italic>(2,24)&#x000a0;=&#x000a0;17.125, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.588). Post hoc tests showed that performance on the trained sounds was significantly higher than on extreme sounds (<italic>p</italic>&#x000a0;=&#x000a0;0.001) and within-category intermediate sounds (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001). Individual data showed that four out of nine birds showed generalization to extreme sounds (proportion correct&#x000a0;&#x0003e;&#x000a0;0.75) and two birds showed generalization to within-category intermediate sounds (proportion correct&#x000a0;&#x0003e;&#x000a0;0.75) (supplement Table&#x000a0;6 and Fig.&#x000a0;7). The separate ANOVA for humans only found a marginally significant effect of sound type (<italic>F</italic>(2,42)&#x000a0;=&#x000a0;3.157, <italic>p</italic>&#x000a0;=&#x000a0;0.053) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.131), revealing that humans performed slightly higher on the extreme and within-category intermediate sounds than on the trained sounds. Five out of fifteen human participants reported that they used the speaker sex dimension to categorize the new sounds and six participants reported that they used the vowel dimension. The others did not report a strategy.<fig id="Fig7"><label>Fig.&#x000a0;7</label><caption><p>Proportions correct for three sound types in the diagonal mapping. Birds are represented in white, humans in gray. The boxplots represent the median, interquartile range, and full range of the proportions correct for the different sound types. Significant differences (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.05) are indicated at the top. The horizontal line marks chance level of 0.5</p></caption><graphic xlink:href="10071_2018_1165_Fig7_HTML" id="MO7"/></fig></p><sec id="Sec16"><title>Discussion diagonal mapping</title><p id="Par31">Birds showed relatively high performance on the trained sounds, which indicates that they show mostly exemplar-based categorization, with some evidence for use of information-integration in a few birds. Humans performed higher on the extreme and within-category intermediate sounds (although not significant) than on the trained sounds. This outcome may suggest that this mapping induced humans to integrate both dimensions. In the post-experimental questionnaire, 11 out of the 15 humans reported that they categorized the sounds on one dimension, either vowel or speaker sex, thus indicating that they were not able to describe their categorization performance explicitly.</p></sec><sec id="Sec17"><title>XOR mapping</title><p id="Par32">For individuals trained with the XOR mapping, the 2 (species)&#x000a0;&#x000d7;&#x000a0;3 (sound type) ANOVA showed a significant main effect for sound type (<italic>F</italic>(2,66)&#x000a0;=&#x000a0;9.895, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.231) and a nonsignificant main effect for species (<italic>F</italic>(1,66)&#x000a0;=&#x000a0;0.245, <italic>p</italic>&#x000a0;=&#x000a0;0.622) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.004) (Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>). The interaction between sound type&#x000a0;&#x000d7;&#x000a0;species was significant (<italic>F</italic>(2,66)&#x000a0;=&#x000a0;7.129, <italic>p</italic>&#x000a0;=&#x000a0;0.002) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.178). Birds performed higher on the trained sounds (0.86&#x000a0;&#x000b1;&#x000a0;0.06) than new sounds (ambiguous: 0.55&#x000a0;&#x000b1;&#x000a0;0.03 and extreme 0.54&#x000a0;&#x000b1;&#x000a0;0.05), whereas for humans this difference between sound types was nonexistent. A separate ANOVA for birds confirmed that there was a significant difference between sound types (<italic>F</italic>(2,24)&#x000a0;=&#x000a0;87.011, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.879). Post hoc tests demonstrated that performance on trained sounds was significantly higher than on extreme (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001) and ambiguous sounds (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001). Individual data showed that all birds performed high on trained sounds, pointing toward strong exemplar-based memorization, but they showed a less distinctive pattern for the new test-sounds (supplement Table&#x000a0;7).</p><p id="Par33">The same ANOVA for humans showed that there was no significant difference between the sound types (<italic>F</italic>(2,42)&#x000a0;=&#x000a0;1.706, <italic>p</italic>&#x000a0;=&#x000a0;0.194) (<italic>&#x003b7;</italic><sup>2</sup>&#x000a0;=&#x000a0;0.075). Among humans, there was more variation in the proportions correct for all sound types. Individual data showed that six out of fifteen humans performed high (&#x0003e;&#x000a0;0.75 proportion correct) on the trained and extreme sounds whereas five humans performed around chance level. Two participants reported that they used a one-dimensional rule (either vowel or speaker sex) and the others could not describe their strategy.<fig id="Fig8"><label>Fig.&#x000a0;8</label><caption><p>Proportions correct for three sound types in the XOR mapping. Birds are represented in white, humans in gray. The boxplots represent the median, interquartile range, and full range of the proportions correct for the different sound types. Significant differences (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.05) are indicated at the top. The horizontal line marks chance level of 0.5</p></caption><graphic xlink:href="10071_2018_1165_Fig8_HTML" id="MO8"/></fig></p></sec></sec><sec id="Sec18"><title>Discussion XOR mapping</title><p id="Par34">In the XOR mapping, birds showed much higher performance on the trained sounds than on the extreme or ambiguous sounds. The proportions correct on those sounds are close to chance level. This suggest that zebra finches had formed an exemplar-based memory of the training sounds. Humans had great difficulty with the XOR mapping, presumably because they easily confused the SR-assignment.</p></sec></sec><sec id="Sec19"><title>General discussion</title><p id="Par35">Humans and birds were trained to categorize four speech sounds that differed in vowel and speaker sex into different functional categories according to various SR-mappings. Birds showed no significant overall differences in learning the different SR-mappings, whereas humans showed fastest learning in the speaker sex mapping, and slowest learning in the XOR mapping. However, zebra finches did show significantly faster learning when both one-dimensional mappings were taken together and compared with the two-dimensional mappings combined. For humans, this finding fits the hypothesis that in one-dimensional mappings they preferentially rely on preexisting categories that are rule-based, whereas they need to employ a different learning strategy in two-dimensional mappings because they cannot apply a simple rule. For zebra finches, the effect is weaker, but might indicate that they may also be able to use similarities between the stimuli to enhance their learning, and that nonanalytic, exemplar-based processing is not the only system through which birds categorize sounds (Smith et al. <xref ref-type="bibr" rid="CR47">2016</xref>). Nevertheless, the birds seem to rely more on exemplar-based memorization than humans. The responses of humans and birds to the test stimuli support these conclusions. Below, we will first discuss the results obtained by the birds in more detail, next those by humans and end with a comparison of both.</p><p id="Par36"><italic>Birds&#x02014;</italic>Based on previous studies (Ohms et al. <xref ref-type="bibr" rid="CR37">2010</xref>, <xref ref-type="bibr" rid="CR38">2012</xref>), we expected birds to be able to categorize speech sounds based on vowels, even after training on a small stimulus set. However, generalization in the one-dimensional vowel test was limited, suggesting that exemplar memorization was dominant in the present experiment. An explanation for the discrepancy between the earlier results of Ohms et al. (<xref ref-type="bibr" rid="CR37">2010</xref>) and the current ones may be the different number of training stimuli. We trained the birds on four-to-two mappings whereas Ohms et al. (<xref ref-type="bibr" rid="CR37">2010</xref>) trained the birds in the first phase on a two-to-two mapping and in the next phase on ten-to-two mappings (<italic>wet</italic> vs. <italic>wit</italic>, spoken by either five male or five female voices) using a Go-nogo paradigm. During the test phase, we presented the birds with new stimuli, whereas Ohms et al. (<xref ref-type="bibr" rid="CR37">2010</xref>) used an incremental test setup with a transfer training in which the five voices were replaced by novel voices of either the same or the other sex. Possibly, the more extensive training exposure to different voices by Ohms et al. (<xref ref-type="bibr" rid="CR37">2010</xref>) might have enhanced vowel category formation. It indicates that our design may not have used sufficient variation with respect to the number of training stimuli per category to induce categorization, and hence may have been biased against obtaining categorization.</p><p id="Par37">In the speaker sex test, birds had higher proportions correct for trained tokens compared to the ambiguous, extreme, and within-category intermediate sounds, suggesting that their memorization was again mainly based on exemplar learning. Nevertheless, extreme and within-category intermediate sounds were better categorized than ambiguous sounds. Also individual data showed that some birds showed clear generalization in the speaker sex mapping, despite the limited number of training stimuli. To our best knowledge, generalization on a speaker sex dimension has not yet been demonstrated in animal research. This indicates the presence of some sort of rule-like learning process, although higher performance on extreme and within-category intermediate sounds compared to ambiguous sounds in a unidimensional mapping does not necessarily imply analytic processing (Wills et al. <xref ref-type="bibr" rid="CR51">2009</xref>). It is hard to assess whether high performance on within-category intermediate and extreme sounds is really due to rule learning or normal generalization.</p><p id="Par38">Our findings raise the question which acoustic cues (sound parameters) the birds used in learning the SR-mapping in the one-dimensional tests. The generalization across speakers of both sexes shown in the experiments by Ohms et al. (<xref ref-type="bibr" rid="CR37">2010</xref>) was ascribed to the birds generalizing on the basis of the formant ratios only, as neither the absolute frequency of the formants nor that of the underlying pitch of the voices (F0) could be used to that end. The bird that generalized in the vowel mapping in this experiment might also have used this feature. In contrast, the generalization in the speaker sex test cannot be based on formant ratios, as the training stimuli for both male and female voices contained the same vowels (/e/&#x000a0;and&#x000a0;/I/). In this case, the birds might, similar to what is known from humans, have used the pitch as the most salient factor distinguishing male from female voices. This would imply that zebra finches attend to both and have the flexibility to use either absolute or relative frequency features and to single out one dimension for making generalizations. However, due to our holistic morphing of natural speech sounds, the question which parameters birds used in their discrimination between the sounds remains open.</p><p id="Par39">In the two-dimensional mappings, the birds showed strong memorization of the trained sounds. The generalization for extreme sounds in the diagonal mapping, displayed by four out of nine individuals, suggests that these zebra finches displayed some implicit categorization possibly by integrating information of both dimensions. These results suggest that there is not a single mechanism used by all birds. Individual data of the XOR mapping showed that all birds performed high on trained sounds, pointing toward strong memorization, but they performed just above chance level for the new test-sounds. For this SR-mapping, the birds thus seemed to rely strongly on exemplar-based memorization.</p><p id="Par40">To summarize the results of the zebra finches, it seems that the most prominent mechanism that zebra finches use to generalize to novel speech items is exemplar-based learning. Nevertheless the results of some individuals in the one-dimensional mappings strongly suggest that zebra finches also have the ability to categorize stimuli in a more rule-like manner, while the two-dimensional diagonal mapping shows evidence of information-integration based learning.</p><p id="Par41"><italic>Humans</italic>&#x02014;Given the evidence for vowel categorization and speaker sex categorization by humans (Goudbeek et al. <xref ref-type="bibr" rid="CR17">2009</xref>), as well as their propensity for rule learning (Smith et al. <xref ref-type="bibr" rid="CR46">2012</xref>, <xref ref-type="bibr" rid="CR47">2016</xref>), we expected humans to have no difficulty with categorization in the one-dimensional SR-mappings. Nevertheless, humans were significantly faster in the speaker sex training. Possibly, humans approach the training at first as a multi-talker environment wherein they try to identify the different speakers (Fuller et al. <xref ref-type="bibr" rid="CR14">2014</xref>) before they focus on the (in this case irrelevant) content. Humans showed no clear difference between the SR-mappings in how they generalized to new test-sounds. Nevertheless, three participants in the speaker sex test reported that they categorized the sounds based on vowels. Their strategy in the test phase may be attributed to the fact that feedback was very occasional (Goudbeek et al. <xref ref-type="bibr" rid="CR17">2009</xref>), but the fact that we only found a shift from using the speaker dimension toward the vowel dimension and not in the other direction may suggest a bias toward vowel categorization rather than speaker sex categorization.</p><p id="Par42">Since humans are known to initially use a one-dimensional solution in a multi-dimensional SR-mapping they tend to find multi-dimensional mapping harder (Ashby et al. <xref ref-type="bibr" rid="CR4">1999</xref>; Goudbeek et al. <xref ref-type="bibr" rid="CR17">2009</xref>). We therefore expected that the two-dimensional SR-mappings would be harder to learn than the one-dimensional mappings. Indeed, learning in both the speaker sex and the vowel mapping was faster than in the XOR mapping. Also, the speaker sex mapping was learned faster than the diagonal mapping. The faster learning in the one-dimensional mappings fits the hypothesis that humans use their preexisting categories in these mappings.</p><p id="Par43">In the tests, humans readily generalized in the one-dimensional mappings, as expected based on their preexisting categories. The post-experimental self-reports confirm this inference. Humans also show generalization in the diagonal mapping. They reported often that their categorization was based on a simple rule (vowel or speaker sex) but the high proportions correct for extreme sounds suggested that some people used both dimensions suggesting that they used an implicit information-integration approach that they could not verbalize (Goudbeek et al. <xref ref-type="bibr" rid="CR17">2009</xref>). Humans had great difficulty with categorization in the XOR mapping. One participant reported that he approached the XOR mapping as a one-dimensional vowel mapping and another participant reported that she approached the task as a one-dimensional speaker sex mapping, i.e., here also they attempted to apply a unidimensional solution in a multi-dimensional mapping, as has also been reported in other studies (Ashby et al. <xref ref-type="bibr" rid="CR4">1999</xref>).</p><p id="Par44">To summarize the data for humans: they demonstrate clear evidence of rule-based categorization in the one-dimensional mappings and the ability to use either the vowel or the speaker sex dimension to categorize speech sounds. When such a rule-based strategy is not possible, humans struggle with categorizing test stimuli although implicit information-integration learning seems present.</p><p id="Par45"><italic>Birds versus humans</italic>&#x02014;While our findings show that birds seem capable of a limited degree of rule learning and categorization based on an information-integration mechanism, it is also clear that they rely primarily on exemplar-based memorization. There is a considerable gap between their performance and that of humans. For humans, the sharp contrast between the high performance on one-dimensional mappings and the low performance on, in particular, the XOR mapping showed that rule-based categorization is much more developed in humans than in birds. In contrast, the birds are much better at discriminating the training sounds from the test-sounds in the XOR mapping than humans are&#x02014;a result that indicates that birds can readily use an exemplar-based categorization mechanism, while humans struggle by trying to solve the mapping in a more analytical, rule-based way. While this may reflect a genuine and fundamental species difference in categorization mechanisms, it cannot be excluded that humans&#x02019; lifetime exposure to the variety of speech sounds may contribute to the species difference. Also, training the birds with a more extensive set of stimuli in the one-dimensional mappings might have resulted in a clearer evidence of rule-based categorization.</p><p id="Par46">Our findings fit, at least to some extent, visual categorization experiments. In these experiments, involving categorizations somewhat comparable to our auditory experiments, macaques, capuchin monkeys and humans learned a one-dimensional SR-mapping faster than a two-dimensional information-integration mapping (Smith et al. <xref ref-type="bibr" rid="CR46">2012</xref>). Pigeons, however, learned these mappings equally quickly (Smith et al. <xref ref-type="bibr" rid="CR46">2012</xref>), presumably by using a nonanalytic exemplar-based learning mechanism. This led Smith et al. (<xref ref-type="bibr" rid="CR46">2012</xref>) to conclude that monkeys, but not pigeons, are capable of more analytical, rule-based like learning. From this they suggest that pigeons may be representative of an ancestral vertebrate categorization system dominated by an integral, holistic and nonanalytic learning mechanism (Smith et al. <xref ref-type="bibr" rid="CR47">2016</xref>). However, our zebra finch data provide some evidence that a more analytical and integrative learning can also be present in some bird species. Among birds, species like corvids and some parrots show cognitive abilities at a level comparable to that of primate species (ten Cate and Healy <xref ref-type="bibr" rid="CR49">2017</xref>). Also, budgerigars show more evidence of abstraction in an auditory rule learning mapping than zebra finches (Spierings and ten Cate <xref ref-type="bibr" rid="CR48">2016</xref>), which in turn seem capable of detecting more regularities in auditory signals than pigeons (ten Cate et al. <xref ref-type="bibr" rid="CR50">2016</xref>). For this reason, we suggest that further comparative studies are needed to reveal the phylogenetic distribution and evolution of different types of categorization systems and how and why the differences between various species, including humans, evolved.</p></sec><sec sec-type="supplementary-material"><title>Electronic supplementary material</title><sec id="Sec20"><p>Below is the link to the electronic supplementary material.
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="10071_2018_1165_MOESM1_ESM.docx"><caption><p>Supplementary material 1 (DOCX 4323&#x000a0;kb)</p></caption></media></supplementary-material></p></sec></sec></body><back><fn-group><fn><p><bold>Electronic supplementary material</bold></p><p>The online version of this article (10.1007/s10071-018-1165-3) contains supplementary material, which is available to authorized users.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This research was supported by Gravitation Grant 024.001.006 of the Language in Interaction Consortium from Netherlands Organization for Scientific Research. The authors declare that they have no conflict of interest.</p></ack><notes notes-type="funding-information"><title>Funding</title><p>This research was supported by Gravitation Grant 024.001.006 of the Language in Interaction Consortium from Netherlands Organization for Scientific Research.</p></notes><notes notes-type="COI-statement"><title>Compliance with Ethical Standards</title><sec id="FPar1"><title>Ethical approval (zebra finches)</title><p id="Par47">All applicable international, national, and/or institutional guidelines for the care and use of animals were followed.</p></sec><sec id="FPar2"><title>Ethical approval (humans)</title><p id="Par48">All procedures performed in studies involving human participants were in accordance with the ethical standards of the institutional and/or national research committee and with the 1964 Helsinki Declaration and its later amendments or comparable ethical standards.</p></sec><sec id="FPar3"><title>Informed consent (humans)</title><p id="Par49">Informed consent was obtained from all individual participants included in the study.</p></sec></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adank</surname><given-names>P</given-names></name><name><surname>van Hout</surname><given-names>R</given-names></name><name><surname>Smits</surname><given-names>R</given-names></name></person-group><article-title>An acoustic description of the vowels of Northern and Southern Standard Dutch</article-title><source>J Acoust Soc Am</source><year>2004</year><volume>116</volume><fpage>1729</fpage><lpage>1738</lpage><pub-id pub-id-type="doi">10.1121/1.1779271</pub-id><?supplied-pmid 15478440?><pub-id pub-id-type="pmid">15478440</pub-id></element-citation></ref><ref id="CR2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>B</given-names></name><name><surname>Peissig</surname><given-names>JJ</given-names></name><name><surname>Singer</surname><given-names>J</given-names></name><name><surname>Sheinberg</surname><given-names>DL</given-names></name></person-group><article-title>XOR style tasks for testing visual object processing in monkeys</article-title><source>Vis Res</source><year>2006</year><volume>46</volume><fpage>1804</fpage><lpage>1815</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2005.11.023</pub-id><?supplied-pmid 16406468?><pub-id pub-id-type="pmid">16406468</pub-id></element-citation></ref><ref id="CR3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>FG</given-names></name><name><surname>Maddox</surname><given-names>WT</given-names></name></person-group><article-title>Human category learning</article-title><source>Annu Rev Psychol</source><year>2005</year><volume>56</volume><fpage>149</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.56.091103.070217</pub-id><?supplied-pmid 15709932?><pub-id pub-id-type="pmid">15709932</pub-id></element-citation></ref><ref id="CR4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>FG</given-names></name><name><surname>Queller</surname><given-names>S</given-names></name><name><surname>Berretty</surname><given-names>PM</given-names></name></person-group><article-title>On the dominance of unidimensional rules in unsupervised categorization</article-title><source>Percept Psychophys</source><year>1999</year><volume>61</volume><fpage>1178</fpage><lpage>1199</lpage><pub-id pub-id-type="doi">10.3758/BF03207622</pub-id><?supplied-pmid 10497436?><pub-id pub-id-type="pmid">10497436</pub-id></element-citation></ref><ref id="CR5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>FG</given-names></name><name><surname>Maddox</surname><given-names>WT</given-names></name><name><surname>Bohil</surname><given-names>CJ</given-names></name></person-group><article-title>Observational versus feedback training in rule-based and information-integration category learning</article-title><source>Mem Cogn</source><year>2002</year><volume>30</volume><fpage>666</fpage><lpage>677</lpage><pub-id pub-id-type="doi">10.3758/BF03196423</pub-id></element-citation></ref><ref id="CR6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizley</surname><given-names>JK</given-names></name><name><surname>Walker</surname><given-names>KMM</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Schnupp</surname><given-names>JWH</given-names></name></person-group><article-title>Spectral timbre perception in ferrets: discrimination of artificial vowels under different listening conditions</article-title><source>J Acoust Soc Am</source><year>2013</year><volume>133</volume><fpage>365</fpage><lpage>376</lpage><pub-id pub-id-type="doi">10.1121/1.4768798</pub-id><?supplied-pmid 23297909?><pub-id pub-id-type="pmid">23297909</pub-id></element-citation></ref><ref id="CR7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dewson</surname><given-names>JH</given-names><suffix>3rd</suffix></name></person-group><article-title>Speech sound discrimination by cats</article-title><source>Science (New York, NY)</source><year>1964</year><volume>144</volume><fpage>555</fpage><lpage>556</lpage><pub-id pub-id-type="doi">10.1126/science.144.3618.555</pub-id></element-citation></ref><ref id="CR8"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dooling</surname><given-names>RJ</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Cazals</surname><given-names>Y</given-names></name><name><surname>Demany</surname><given-names>L</given-names></name><name><surname>Horner</surname><given-names>K</given-names></name></person-group><article-title>Perception of speech sounds by birds</article-title><source>The 9th international symposium on hearing: auditory physiology and perception, Carcens, France</source><year>1992</year><publisher-loc>Oxford</publisher-loc><publisher-name>Pergamon Press</publisher-name><fpage>407</fpage><lpage>413</lpage></element-citation></ref><ref id="CR9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dooling</surname><given-names>RJ</given-names></name><name><surname>Brown</surname><given-names>SD</given-names></name></person-group><article-title>Speech-perception by budgerigars (<italic>Melopsittacus undulatus</italic>)&#x02014;spoken vowels</article-title><source>Percept Psychophys</source><year>1990</year><volume>47</volume><fpage>568</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.3758/BF03203109</pub-id><?supplied-pmid 2367177?><pub-id pub-id-type="pmid">2367177</pub-id></element-citation></ref><ref id="CR10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eimas</surname><given-names>PD</given-names></name><name><surname>Siqueland</surname><given-names>ER</given-names></name><name><surname>Jusczyk</surname><given-names>P</given-names></name><name><surname>Vigorito</surname><given-names>J</given-names></name></person-group><article-title>Speech perception in infants</article-title><source>Science</source><year>1971</year><volume>171</volume><fpage>303</fpage><pub-id pub-id-type="doi">10.1126/science.171.3968.303</pub-id><?supplied-pmid 5538846?><pub-id pub-id-type="pmid">5538846</pub-id></element-citation></ref><ref id="CR11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erickson</surname><given-names>MA</given-names></name><name><surname>Kruschke</surname><given-names>JK</given-names></name></person-group><article-title>Rules and exemplars in category learning</article-title><source>J Math Psychol</source><year>1998</year><volume>42</volume><fpage>483</fpage><lpage>484</lpage></element-citation></ref><ref id="CR12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eriksson</surname><given-names>JL</given-names></name><name><surname>Villa</surname><given-names>AEP</given-names></name></person-group><article-title>Learning of auditory equivalence classes for vowels by rats</article-title><source>Behav Proc</source><year>2006</year><volume>73</volume><fpage>348</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1016/j.beproc.2006.08.005</pub-id></element-citation></ref><ref id="CR13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Francis</surname><given-names>AL</given-names></name><name><surname>Nusbaum</surname><given-names>HC</given-names></name></person-group><article-title>Selective attention and the acquisition of new phonetic categories</article-title><source>J Exp Psychol Hum Percept Perform</source><year>2002</year><volume>28</volume><fpage>349</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.28.2.349</pub-id><?supplied-pmid 11999859?><pub-id pub-id-type="pmid">11999859</pub-id></element-citation></ref><ref id="CR14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuller</surname><given-names>CD</given-names></name><name><surname>Gaudrain</surname><given-names>E</given-names></name><name><surname>Clarke</surname><given-names>JN</given-names></name><name><surname>Galvin</surname><given-names>JJ</given-names></name><name><surname>Fu</surname><given-names>QJ</given-names></name><name><surname>Free</surname><given-names>RH</given-names></name><name><surname>Baskent</surname><given-names>D</given-names></name></person-group><article-title>Gender categorization is abnormal in cochlear implant users</article-title><source>JARO J Assoc Res Otolaryngol</source><year>2014</year><volume>15</volume><fpage>1037</fpage><lpage>1048</lpage><pub-id pub-id-type="doi">10.1007/s10162-014-0483-7</pub-id><?supplied-pmid 25172111?><pub-id pub-id-type="pmid">25172111</pub-id></element-citation></ref><ref id="CR15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gottwald</surname><given-names>RL</given-names></name><name><surname>Garner</surname><given-names>WR</given-names></name></person-group><article-title>Effects of focusing strategy on speeded classification with grouping, filtering, and condensation tasks</article-title><source>Percept Psychophys</source><year>1972</year><volume>11</volume><fpage>179</fpage><pub-id pub-id-type="doi">10.3758/BF03210371</pub-id></element-citation></ref><ref id="CR16"><mixed-citation publication-type="other">Goudbeek M, Swingley D, Kluender KR, ISCA (2007) The limits of multidimensional category learning. In: Interspeech 2007: 8th annual conference of the international speech communication association, vol 1&#x02013;4, pp 1301&#x02013;1304</mixed-citation></ref><ref id="CR17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goudbeek</surname><given-names>M</given-names></name><name><surname>Swingley</surname><given-names>D</given-names></name><name><surname>Smits</surname><given-names>R</given-names></name></person-group><article-title>Supervised and unsupervised learning of multidimensional acoustic categories</article-title><source>J Exp Psychol Hum Percept Perform</source><year>2009</year><volume>35</volume><fpage>1913</fpage><lpage>1933</lpage><pub-id pub-id-type="doi">10.1037/a0015781</pub-id><?supplied-pmid 19968443?><pub-id pub-id-type="pmid">19968443</pub-id></element-citation></ref><ref id="CR18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hazan</surname><given-names>V</given-names></name><name><surname>Barrett</surname><given-names>S</given-names></name></person-group><article-title>The development of phonemic categorization in children aged 6&#x02013;12</article-title><source>J Phon</source><year>2000</year><volume>28</volume><fpage>377</fpage><lpage>396</lpage><pub-id pub-id-type="doi">10.1006/jpho.2000.0121</pub-id></element-citation></ref><ref id="CR19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hienz</surname><given-names>RD</given-names></name><name><surname>Brady</surname><given-names>JV</given-names></name></person-group><article-title>The acquisition of vowel discriminations by nonhuman-primates</article-title><source>J Acoust Soc Am</source><year>1988</year><volume>84</volume><fpage>186</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1121/1.396963</pub-id><?supplied-pmid 3411047?><pub-id pub-id-type="pmid">3411047</pub-id></element-citation></ref><ref id="CR20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hienz</surname><given-names>RD</given-names></name><name><surname>Sachs</surname><given-names>MB</given-names></name><name><surname>Sinnott</surname><given-names>JM</given-names></name></person-group><article-title>Discrimination of steady-state vowels by blackbirds and pigeons</article-title><source>J Acoust Soc Am</source><year>1981</year><volume>70</volume><fpage>699</fpage><lpage>706</lpage><pub-id pub-id-type="doi">10.1121/1.386933</pub-id></element-citation></ref><ref id="CR21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hillenbrand</surname><given-names>J</given-names></name><name><surname>Getty</surname><given-names>LA</given-names></name><name><surname>Clark</surname><given-names>MJ</given-names></name><name><surname>Wheeler</surname><given-names>K</given-names></name></person-group><article-title>Acoustic characteristics of American&#x02013;English vowels</article-title><source>J Acoust Soc Am</source><year>1995</year><volume>97</volume><fpage>3099</fpage><lpage>3111</lpage><pub-id pub-id-type="doi">10.1121/1.411872</pub-id><?supplied-pmid 7759650?><pub-id pub-id-type="pmid">7759650</pub-id></element-citation></ref><ref id="CR22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holt</surname><given-names>LL</given-names></name><name><surname>Lotto</surname><given-names>AJ</given-names></name></person-group><article-title>Cue weighting in auditory categorization: implications for first and second language acquisition</article-title><source>J Acoust Soc Am</source><year>2006</year><volume>119</volume><fpage>3059</fpage><lpage>3071</lpage><pub-id pub-id-type="doi">10.1121/1.2188377</pub-id><?supplied-pmid 16708961?><pub-id pub-id-type="pmid">16708961</pub-id></element-citation></ref><ref id="CR23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holt</surname><given-names>LL</given-names></name><name><surname>Lotto</surname><given-names>AJ</given-names></name></person-group><article-title>Speech perception as categorization attention</article-title><source>Percept Psychophys</source><year>2010</year><volume>72</volume><fpage>1218</fpage><lpage>1227</lpage><pub-id pub-id-type="doi">10.3758/APP.72.5.1218</pub-id></element-citation></ref><ref id="CR24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>K</given-names></name></person-group><article-title>The role of perceived speaker identity in F0 normalization of vowels</article-title><source>J Acoust Soc Am</source><year>1990</year><volume>88</volume><fpage>642</fpage><lpage>654</lpage><pub-id pub-id-type="doi">10.1121/1.399767</pub-id><?supplied-pmid 2212287?><pub-id pub-id-type="pmid">2212287</pub-id></element-citation></ref><ref id="CR25"><mixed-citation publication-type="other">Kawahara H, Morise M, Takahashi T, Nisimura R, Irino T, Banno H, IEEE (2008) Tandem-straight: a temporally stable power spectral representation for periodic signals and applications to interference-free spectrum, <italic>F</italic>0, and aperiodicity estimation. In: 33rd IEEE international conference on acoustics, speech and signal processing, Las Vegas, NV, Mar 30&#x02013;Apr 04 2008. International Conference on Acoustics Speech and Signal Processing (ICASSP), IEEE, New York, pp 3933&#x02013;3936. 10.1109/icassp.2008.4518514</mixed-citation></ref><ref id="CR26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kluender</surname><given-names>KR</given-names></name><name><surname>Diehl</surname><given-names>RL</given-names></name><name><surname>Killeen</surname><given-names>PR</given-names></name></person-group><article-title>Japanese quail can learn phonetic categories</article-title><source>Science</source><year>1987</year><volume>237</volume><fpage>1195</fpage><lpage>1197</lpage><pub-id pub-id-type="doi">10.1126/science.3629235</pub-id><?supplied-pmid 3629235?><pub-id pub-id-type="pmid">3629235</pub-id></element-citation></ref><ref id="CR27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kluender</surname><given-names>KR</given-names></name><name><surname>Lotto</surname><given-names>AJ</given-names></name><name><surname>Holt</surname><given-names>LL</given-names></name><name><surname>Bloedel</surname><given-names>SL</given-names></name></person-group><article-title>Role of experience for language-specific functional mappings of vowel sounds</article-title><source>J Acoust Soc Am</source><year>1998</year><volume>104</volume><fpage>3568</fpage><lpage>3582</lpage><pub-id pub-id-type="doi">10.1121/1.423939</pub-id><?supplied-pmid 9857515?><pub-id pub-id-type="pmid">9857515</pub-id></element-citation></ref><ref id="CR28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriengwatana</surname><given-names>B</given-names></name><name><surname>Escudero</surname><given-names>P</given-names></name><name><surname>Kerkhoven</surname><given-names>AH</given-names></name><name><surname>ten Cate</surname><given-names>C</given-names></name></person-group><article-title>A general auditory bias for handling speaker variability in speech? Evidence in humans and songbirds</article-title><source>Front Psychol</source><year>2015</year><volume>6</volume><fpage>14</fpage><pub-id pub-id-type="doi">10.3389/fpsyg.2015.01243</pub-id><pub-id pub-id-type="pmid">25698987</pub-id></element-citation></ref><ref id="CR29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriengwatana</surname><given-names>B</given-names></name><name><surname>Escudero</surname><given-names>P</given-names></name><name><surname>ten Cate</surname><given-names>C</given-names></name></person-group><article-title>Revisiting vocal perception in non-human animals: a review of vowel discrimination, speaker voice recognition, and speaker normalization</article-title><source>Front Psychol</source><year>2015</year><volume>5</volume><fpage>13</fpage><pub-id pub-id-type="doi">10.3389/fpsyg.2014.01543</pub-id></element-citation></ref><ref id="CR30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhl</surname><given-names>PK</given-names></name></person-group><article-title>Early language acquisition: cracking the speech code</article-title><source>Nat Rev Neurosci</source><year>2004</year><volume>5</volume><fpage>831</fpage><lpage>843</lpage><pub-id pub-id-type="doi">10.1038/nrn1533</pub-id><?supplied-pmid 15496861?><pub-id pub-id-type="pmid">15496861</pub-id></element-citation></ref><ref id="CR31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhl</surname><given-names>PK</given-names></name><name><surname>Miller</surname><given-names>JD</given-names></name></person-group><article-title>Speech perception by the chinchilla: voiced&#x02013;voiceless distinction in alveolar plosive consonants</article-title><source>Science (New York, NY)</source><year>1975</year><volume>190</volume><fpage>69</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1126/science.1166301</pub-id></element-citation></ref><ref id="CR32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhl</surname><given-names>PK</given-names></name><name><surname>Padden</surname><given-names>DM</given-names></name></person-group><article-title>Enhanced discriminability at the phonetic boundaries for the voicing feature in macaques</article-title><source>Percept Psychophys</source><year>1982</year><volume>32</volume><fpage>542</fpage><lpage>550</lpage><pub-id pub-id-type="doi">10.3758/BF03204208</pub-id><?supplied-pmid 7167352?><pub-id pub-id-type="pmid">7167352</pub-id></element-citation></ref><ref id="CR33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maddox</surname><given-names>WT</given-names></name><name><surname>Ashby</surname><given-names>FG</given-names></name></person-group><article-title>Dissociating explicit and procedural-learning based systems of perceptual category learning</article-title><source>Behav Proc</source><year>2004</year><volume>66</volume><fpage>309</fpage><lpage>332</lpage><pub-id pub-id-type="doi">10.1016/j.beproc.2004.03.011</pub-id></element-citation></ref><ref id="CR34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Massida</surname><given-names>Z</given-names></name><name><surname>Marx</surname><given-names>M</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>James</surname><given-names>C</given-names></name><name><surname>Fraysse</surname><given-names>B</given-names></name><name><surname>Barone</surname><given-names>P</given-names></name><name><surname>Deguine</surname><given-names>O</given-names></name></person-group><article-title>Gender categorization in cochlear implant users</article-title><source>J Speech Lang Hear Res</source><year>2013</year><volume>56</volume><fpage>1389</fpage><lpage>1401</lpage><pub-id pub-id-type="doi">10.1044/1092-4388(2013/12-0132)</pub-id><?supplied-pmid 24023381?><pub-id pub-id-type="pmid">24023381</pub-id></element-citation></ref><ref id="CR35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mercado</surname><given-names>E</given-names></name><name><surname>Orduna</surname><given-names>I</given-names></name><name><surname>Nowak</surname><given-names>JM</given-names></name></person-group><article-title>Auditory categorization of complex sounds by rats (<italic>Rattus norvegicus</italic>)</article-title><source>J Comput Psychol</source><year>2005</year><volume>119</volume><fpage>90</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1037/0735-7036.119.1.90</pub-id></element-citation></ref><ref id="CR36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Minda</surname><given-names>JP</given-names></name><name><surname>Smith</surname><given-names>JD</given-names></name></person-group><article-title>Prototypes in category learning: the effects of category size, category structure, and stimulus complexity</article-title><source>J Exp Psychol Learn Mem Cogn</source><year>2001</year><volume>27</volume><fpage>775</fpage><lpage>799</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.27.3.775</pub-id><?supplied-pmid 11394680?><pub-id pub-id-type="pmid">11394680</pub-id></element-citation></ref><ref id="CR37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohms</surname><given-names>VR</given-names></name><name><surname>Gill</surname><given-names>A</given-names></name><name><surname>Van Heijningen</surname><given-names>CAA</given-names></name><name><surname>Beckers</surname><given-names>GJL</given-names></name><name><surname>ten Cate</surname><given-names>C</given-names></name></person-group><article-title>Zebra finches exhibit speaker-independent phonetic perception of human speech</article-title><source>Proc R Soc B Biol Sci</source><year>2010</year><volume>277</volume><fpage>1003</fpage><lpage>1009</lpage><pub-id pub-id-type="doi">10.1098/rspb.2009.1788</pub-id></element-citation></ref><ref id="CR38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohms</surname><given-names>VR</given-names></name><name><surname>Escudero</surname><given-names>P</given-names></name><name><surname>Lammers</surname><given-names>K</given-names></name><name><surname>ten Cate</surname><given-names>C</given-names></name></person-group><article-title>Zebra finches and Dutch adults exhibit the same cue weighting bias in vowel perception</article-title><source>Anim Cogn</source><year>2012</year><volume>15</volume><fpage>155</fpage><lpage>161</lpage><pub-id pub-id-type="doi">10.1007/s10071-011-0441-2</pub-id><?supplied-pmid 21761144?><pub-id pub-id-type="pmid">21761144</pub-id></element-citation></ref><ref id="CR39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polka</surname><given-names>L</given-names></name><name><surname>Bohn</surname><given-names>OS</given-names></name></person-group><article-title>Asymmetries in vowel perception</article-title><source>Speech Commun</source><year>2003</year><volume>41</volume><fpage>221</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1016/S0167-6393(02)00105-X</pub-id></element-citation></ref><ref id="CR40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Posner</surname><given-names>MI</given-names></name><name><surname>Keele</surname><given-names>SW</given-names></name></person-group><article-title>On genesis of abstract ideas</article-title><source>J Exp Psychol</source><year>1968</year><volume>77</volume><fpage>353</fpage><pub-id pub-id-type="doi">10.1037/h0025953</pub-id><?supplied-pmid 5665566?><pub-id pub-id-type="pmid">5665566</pub-id></element-citation></ref><ref id="CR41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skuk</surname><given-names>VG</given-names></name><name><surname>Schweinberger</surname><given-names>SR</given-names></name></person-group><article-title>Influences of fundamental frequency, formant frequencies, aperiodicity, and spectrum level on the perception of voice gender</article-title><source>J Speech Lang Hear Res</source><year>2014</year><volume>57</volume><fpage>285</fpage><lpage>296</lpage><pub-id pub-id-type="doi">10.1044/1092-4388(2013/12-0314)</pub-id><?supplied-pmid 23882002?><pub-id pub-id-type="pmid">23882002</pub-id></element-citation></ref><ref id="CR42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skuk</surname><given-names>VG</given-names></name><name><surname>Dammann</surname><given-names>LM</given-names></name><name><surname>Schweinberger</surname><given-names>SR</given-names></name></person-group><article-title>Role of timbre and fundamental frequency in voice gender adaptation</article-title><source>J Acoust Soc Am</source><year>2015</year><volume>138</volume><fpage>1180</fpage><lpage>1193</lpage><pub-id pub-id-type="doi">10.1121/1.4927696</pub-id><?supplied-pmid 26328731?><pub-id pub-id-type="pmid">26328731</pub-id></element-citation></ref><ref id="CR43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>JD</given-names></name></person-group><article-title>Prototypes, exemplars, and the natural history of categorization</article-title><source>Psychon Bull Rev</source><year>2014</year><volume>21</volume><fpage>312</fpage><lpage>331</lpage><pub-id pub-id-type="doi">10.3758/s13423-013-0506-0</pub-id><?supplied-pmid 24005828?><pub-id pub-id-type="pmid">24005828</pub-id></element-citation></ref><ref id="CR44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>JD</given-names></name><name><surname>Minda</surname><given-names>JP</given-names></name></person-group><article-title>Prototypes in the mist: the early epochs of category learning (vol 24, pg 1411, 1998)</article-title><source>J Exp Psychol Learn Mem Cogn</source><year>1999</year><volume>25</volume><fpage>69</fpage><pub-id pub-id-type="doi">10.1037/h0090333</pub-id></element-citation></ref><ref id="CR45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>JD</given-names></name><name><surname>Ashby</surname><given-names>FG</given-names></name><name><surname>Berg</surname><given-names>ME</given-names></name><name><surname>Murphy</surname><given-names>MS</given-names></name><name><surname>Spiering</surname><given-names>B</given-names></name><name><surname>Cook</surname><given-names>RG</given-names></name><name><surname>Grace</surname><given-names>RC</given-names></name></person-group><article-title>Pigeons&#x02019; categorization may be exclusively nonanalytic</article-title><source>Psychon Bull Rev</source><year>2011</year><volume>18</volume><fpage>414</fpage><lpage>421</lpage><pub-id pub-id-type="doi">10.3758/s13423-010-0047-8</pub-id><?supplied-pmid 21327382?><pub-id pub-id-type="pmid">21327382</pub-id></element-citation></ref><ref id="CR46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>JD</given-names></name><etal/></person-group><article-title>Implicit and explicit categorization: a tale of four species</article-title><source>Neurosci Biobehav Rev</source><year>2012</year><volume>36</volume><fpage>2355</fpage><lpage>2369</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2012.09.003</pub-id><?supplied-pmid 22981878?><pub-id pub-id-type="pmid">22981878</pub-id></element-citation></ref><ref id="CR47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>JD</given-names></name><name><surname>Zakrzewski</surname><given-names>AC</given-names></name><name><surname>Johnson</surname><given-names>JM</given-names></name><name><surname>Valleau</surname><given-names>JC</given-names></name><name><surname>Church</surname><given-names>BA</given-names></name></person-group><article-title>Categorization: the view from animal cognition</article-title><source>Behav Sci</source><year>2016</year><volume>6</volume><fpage>24</fpage><pub-id pub-id-type="doi">10.3390/bs6020012</pub-id></element-citation></ref><ref id="CR48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spierings</surname><given-names>MJ</given-names></name><name><surname>ten Cate</surname><given-names>C</given-names></name></person-group><article-title>Zebra finches as a model species to understand the roots of rhythm</article-title><source>Front Neurosci</source><year>2016</year><volume>10</volume><fpage>3</fpage><pub-id pub-id-type="doi">10.3389/fnins.2016.00345</pub-id><pub-id pub-id-type="pmid">26834532</pub-id></element-citation></ref><ref id="CR49"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>ten Cate</surname><given-names>C</given-names></name><name><surname>Healy</surname><given-names>SDIP</given-names></name></person-group><source>Avian cognition</source><year>2017</year><publisher-loc>Cambridge</publisher-loc><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="CR50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>ten Cate</surname><given-names>C</given-names></name><name><surname>Spierings</surname><given-names>M</given-names></name><name><surname>Hubert</surname><given-names>J</given-names></name><name><surname>Honing</surname><given-names>H</given-names></name></person-group><article-title>Can birds perceive rhythmic patterns? A review and experiments on a songbird and a parrot species</article-title><source>Front Psychol</source><year>2016</year><volume>7</volume><fpage>14</fpage><pub-id pub-id-type="pmid">26869945</pub-id></element-citation></ref><ref id="CR51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wills</surname><given-names>AJ</given-names></name><etal/></person-group><article-title>A comparative analysis of the categorization of multidimensional stimuli: I. Unidimensional classification does not necessarily imply analytic processing; evidence from pigeons (Columba livia), squirrels (Sciurus carolinensis), and humans (Homo sapiens)</article-title><source>J Comp Psychol</source><year>2009</year><volume>123</volume><fpage>391</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.1037/a0016216</pub-id><?supplied-pmid 19929108?><pub-id pub-id-type="pmid">19929108</pub-id></element-citation></ref></ref-list></back></article>