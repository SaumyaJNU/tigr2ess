<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Plant Methods</journal-id><journal-id journal-id-type="iso-abbrev">Plant Methods</journal-id><journal-title-group><journal-title>Plant Methods</journal-title></journal-title-group><issn pub-type="epub">1746-4811</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6236889</article-id><article-id pub-id-type="publisher-id">366</article-id><article-id pub-id-type="doi">10.1186/s13007-018-0366-8</article-id><article-categories><subj-group subj-group-type="heading"><subject>Methodology</subject></subj-group></article-categories><title-group><article-title>Detection and analysis of wheat spikes using Convolutional Neural Networks</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4326-4332</contrib-id><name><surname>Hasan</surname><given-names>Md Mehedi</given-names></name><address><email>mmhasan.unsw@gmail.com</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Chopin</surname><given-names>Joshua P.</given-names></name><address><email>Joshua.Chopin@unisa.edu.au</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Laga</surname><given-names>Hamid</given-names></name><address><email>H.Laga@murdoch.edu.au</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Miklavcic</surname><given-names>Stanley J.</given-names></name><address><email>Stan.Miklavcic@unisa.edu.au</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 8994 5086</institution-id><institution-id institution-id-type="GRID">grid.1026.5</institution-id><institution>Phenomics and Bioinformatics Research Centre, </institution><institution>University of South Australia, </institution></institution-wrap>Mawson Lakes, Adelaide, 5095 Australia </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0436 6763</institution-id><institution-id institution-id-type="GRID">grid.1025.6</institution-id><institution>School of Engineering and Information Technology, </institution><institution>Murdoch University, </institution></institution-wrap>Perth, Western Australia 6150 Australia </aff></contrib-group><pub-date pub-type="epub"><day>15</day><month>11</month><year>2018</year></pub-date><pub-date pub-type="pmc-release"><day>15</day><month>11</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>14</volume><elocation-id>100</elocation-id><history><date date-type="received"><day>29</day><month>6</month><year>2018</year></date><date date-type="accepted"><day>1</day><month>11</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2018</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p></license></permissions><abstract id="Abs1"><sec><title>Background</title><p id="Par1">Field phenotyping by remote sensing has received increased interest in recent years with the possibility of achieving high-throughput analysis of crop fields. Along with the various technological developments, the application of machine learning methods for image analysis has enhanced the potential for quantitative assessment of a multitude of crop traits. For wheat breeding purposes, assessing the production of wheat spikes, as the grain-bearing organ, is a useful proxy measure of grain production. Thus, being able to detect and characterize spikes from images of wheat fields is an essential component in a wheat breeding pipeline for the selection of high yielding varieties.
</p></sec><sec><title>Results</title><p id="Par2">We have applied a deep learning approach to accurately detect, count and analyze wheat spikes for yield estimation. We have tested the approach on a set of images of wheat field trial comprising 10 varieties subjected to three fertilizer treatments. The images have been captured over one season, using high definition RGB cameras mounted on a land-based imaging platform, and viewing the wheat plots from an oblique angle. A subset of in-field images has been accurately labeled by manually annotating all the spike regions. This annotated dataset, called SPIKE, is then used to train four region-based Convolutional Neural Networks (R-CNN) which take, as input, images of wheat plots, and accurately detect and count spike regions in each plot. The CNNs also output the spike density and a classification probability for each plot. Using the same R-CNN architecture, four different models were generated based on four different datasets of training and testing images captured at various growth stages. Despite the challenging field imaging conditions, e.g., variable illumination conditions, high spike occlusion, and complex background, the four R-CNN models achieve an average detection accuracy ranging from 88 to <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$94\%$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mn>94</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq1.gif"/></alternatives></inline-formula> across different sets of test images. The most robust R-CNN model, which achieved the highest accuracy, is then selected to study the variation in spike production over 10 wheat varieties and three treatments. The SPIKE dataset and the trained CNN are the main contributions of this paper.</p></sec><sec><title>Conclusion</title><p id="Par3">With the availability of good training datasets such us the SPIKE dataset proposed in this article, deep learning techniques can achieve high accuracy in detecting and counting spikes from complex wheat field images. The proposed robust R-CNN model, which has been trained on spike images captured during different growth stages, is optimized for application to a wider variety of field scenarios. It accurately quantifies the differences in yield produced by the 10 varieties we have studied, and their respective responses to fertilizer treatment. We have also observed that the other R-CNN models exhibit more specialized performances. The data set and the R-CNN model, which we make publicly available, have the potential to greatly benefit plant breeders by facilitating the high throughput selection of high yielding varieties.</p></sec><sec><title>Electronic supplementary material</title><p>The online version of this article (10.1186/s13007-018-0366-8) contains supplementary material, which is available to authorized users.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Plant phenotyping</kwd><kwd>Spike detection</kwd><kwd>Deep learning</kwd><kwd>Field imaging</kwd><kwd>Statistical analysis</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000923</institution-id><institution>Australian Research Council</institution></institution-wrap></funding-source><award-id>LP140100347</award-id><award-id>LP150100055</award-id><principal-award-recipient><name><surname>Miklavcic</surname><given-names>Stanley J.</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2018</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background</title><p id="Par4">Wheat is one of the most globally significant crop species with an annual worldwide grain production of 700 million tonnes&#x000a0;[<xref ref-type="bibr" rid="CR1">1</xref>]. In recent years, however, there is an increasing demand for grain. At the same time, the seasonal fluctuations, the extreme weather events and the altering climate in various regions of the world, increase the risk of inconsistent supply. This points to the need to identify hardier and higher yielding plant varieties to both increase crop production and improve plant tolerance to biotic and abiotic stresses.</p><p id="Par5">To discover higher-yielding and more stress-tolerant varieties, biologists and breeders rely more and more on high-throughput phenotyping techniques to measure various plant traits, which in turn are used to understand plant&#x02019;s response to various environmental conditions and treatments, with the hope to improve grain yield.</p><p id="Par6">Early works on high-throughput image-based phenotyping focused on controlled environments such as purpose-built chambers and automated glasshouses. Li et al.&#x000a0;[<xref ref-type="bibr" rid="CR2">2</xref>], for example, proposed an approach that detects, counts and measures the geometric properties of spikes of a single plant grown in a controlled environment. Bi et al.&#x000a0;[<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR4">4</xref>] and Pound et al.&#x000a0;[<xref ref-type="bibr" rid="CR5">5</xref>], on the other hand, measured more detailed morphological properties, such as the numbers of awns and spikelets, of plants imaged in small purpose-built chambers with uniform backgrounds. Unfortunately, in such experiments plants are confined to small pots, which no doubt affect root development, nutrient uptake and, ultimately, yield. Some experiments have been carried out using plants grown in large (120 cm <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times $$\end{document}</tex-math><mml:math id="M4"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq2.gif"/></alternatives></inline-formula> 80 cm) indoor bins, which are capable of housing almost 100 plants in competition&#x000a0;[<xref ref-type="bibr" rid="CR6">6</xref>&#x02013;<xref ref-type="bibr" rid="CR8">8</xref>]. Spike detection was not attempted in these latter studies, but their more critical limitation was that the plants, although grown closer to field-like conditions and not individually in pots, were not subject to realistic environmental conditions. The challenge to providing quantitative plant breeder support is yield estimation under true field conditions, relying on the ability to accurately and automatically detect and count the ears of wheat in the field.</p><p id="Par7">A range of different phenotyping platforms exist for capturing images in the field&#x000a0;[<xref ref-type="bibr" rid="CR9">9</xref>&#x02013;<xref ref-type="bibr" rid="CR11">11</xref>]. However, due to the large scale nature of such studies, many researchers have turned to aerial imaging systems such as unmanned aerial vehicles&#x000a0;[<xref ref-type="bibr" rid="CR12">12</xref>&#x02013;<xref ref-type="bibr" rid="CR15">15</xref>] and satellite imagery&#x000a0;[<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR17">17</xref>]. While these approaches are capable of capturing information about a large number of plants across a large area of land within a short period of time, only coarse level information, such as mean canopy coverage and mean canopy color, has thus far been reported. It should also be kept in mind that the nature of the uncontrolled field environment poses significant challenges for both image acquisition and image analysis algorithms, which should ideally be robust to changing conditions and applied autonomously. The challenges indeed often result in images being analyzed manually or semi-automatically, and often qualitatively.</p><p id="Par8">In this study we utilize a land-based vehicle and a single RGB camera to acquire images of a field. The proximity of the camera to the plants allows for high-resolution data capture. The simplicity of the imaging set-up makes it affordable and easy to implement, thus accessible to any potential user. The remaining challenge, on which we focus attention here, is of analyzing these high resolution images to extract quantitative information such as the number and density of wheat spikes. To go some way to meeting this challenge we have chosen to image plots from an oblique perspective as opposed to the more common nadir perspective. In an oblique view a significant number of spike features such as texture, color, shape etc. can be discerned easily. These features can be more readily extracted for the purposes of various plant phenotyping applications such as spike counting (which is the focus of this paper), spike shape measurement, spike texture, disease detection, grain yield estimation etc. We note that we are not unique in taking this more advantageous perspective [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR19">19</xref>].</p><p id="Par9">There are some computer vision approaches for detecting spikes in field images obtained using land-based imaging techniques, which have been reported in the literature. Fernandez-Gallego et al.&#x000a0;[<xref ref-type="bibr" rid="CR20">20</xref>] used RGB cameras manually held at approximately one meter above the center of the plant canopy to gain images from a nadir perspective. The authors then apply the Laplacian and the median filter to produce a transformed image where the local maxima can be detected and classified as wheat spikes. This approach achieved a recognition rates of up to 92%, but failed when observing plants in different developmental stages (32%). Alharbi et al.&#x000a0;[<xref ref-type="bibr" rid="CR18">18</xref>], which used Gabor filters, principal component analysis and k-means clustering, were able to achieve an average accuracy of 90.7%. The approach, however, places constraints on image content such as the density of spikes, color and texture differences between spikes and shoots and the angle of spikes in the image. Zhou et al.&#x000a0;[<xref ref-type="bibr" rid="CR21">21</xref>] proposed an image fusion method by using multi-sensor data and an improved maximum entropy segmentation algorithm to detect wheat spikes in the field. However, the method required the use of a multi-spectral camera and was validated on images where canopy and spikes rarely overlap or occlude one another.</p><p id="Par10">Machine learning has been adopted as the method of choice in many recent image analysis applications to address a number of plant phenotyping problems. These include the study of wheat spikes in controlled environments&#x000a0;[<xref ref-type="bibr" rid="CR2">2</xref>], the classification of leaf species and leaf venation&#x000a0;[<xref ref-type="bibr" rid="CR22">22</xref>], the analysis of the architecture of root systems&#x000a0;[<xref ref-type="bibr" rid="CR23">23</xref>, <xref ref-type="bibr" rid="CR24">24</xref>], the measurement of plant stress levels&#x000a0;[<xref ref-type="bibr" rid="CR25">25</xref>] and the determination of wheat growth stages&#x000a0;[<xref ref-type="bibr" rid="CR26">26</xref>]. More recently, deep learning has begun to outperform previous image analysis and machine learning approaches and promises a step-change in the performance of image-based phenotyping. In particular, the use of Convolutional Neural Networks (CNNs) for image analysis tasks has seen a rapid increase in popularity. For instance, CNNs have been used to improve the performance of the approach of Wilf et al.&#x000a0;[<xref ref-type="bibr" rid="CR22">22</xref>] for identifying and counting leaf species&#x000a0;[<xref ref-type="bibr" rid="CR27">27</xref>], to quantitatively phenotype <italic>Arabidopsis thaliana</italic> plants grown in controlled environments&#x000a0;[<xref ref-type="bibr" rid="CR28">28</xref>, <xref ref-type="bibr" rid="CR29">29</xref>], and to provide detailed quantitative characterization of wheat spikes on plants grown in controlled environments&#x000a0;[<xref ref-type="bibr" rid="CR3">3</xref>&#x02013;<xref ref-type="bibr" rid="CR5">5</xref>].</p><p id="Par11">In this study we present the first deep learning model designed specifically to detect and characterize wheat spikes present in wheat field images. We adapt, train and apply a variant of CNN, hereinafter referred to as <italic>Region-based Convolutional Neural Networks (R-CNN)</italic>, to accurately count wheat spikes in images acquired with our land-based RGB imaging platform. The approach relies on a training data set of images containing spikes that have been labeled manually with rectangular boxes; The procedure produces a complete list of locations and dimensions of bounding boxes identifying plant spikes detected in images unseen during the training stage. A successful deep learning analysis requires thorough training using large data sets of high quality&#x000a0;[<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR30">30</xref>]. As such, a second major contribution of this work is the release of the SPIKE data set, made up of hundreds of high quality images containing over 20,000 labeled wheat spikes.</p><p id="Par12">The outline of this article is as follows. In the "<xref rid="Sec2" ref-type="sec">Methods</xref>" section we describe the field trial we have studied and the image acquisition system. The images from this field trial form the SPIKE data set which is then described in detail and used for training and testing of our R-CNN model. Finally, we also present the metrics used for the validation of the proposed CNN model. In the "<xref rid="Sec7" ref-type="sec">Results and discussion</xref>" section we analyze the performance of the model both on the main data set and on subsets containing images of field plots at different growth stages. We also provide an analysis of the density of spikes detected in images of plots of different wheat varieties treated with fertilizer at different times. In brief, we found that early treatment resulted in significantly higher yields (spike densities) for nearly all the varieties tested, than what were produced by the same varieties either untreated or treated later in the season.</p></sec><sec id="Sec2"><title>Methods</title><p id="Par13">Figure&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> shows the overall work-flow of the in-field wheat spike detection system. The goal is to develop a fast and accurate system which can detect spikes from field images. The output is a list of bounding boxes enclosing wheat spikes, as well as the confidence level for each box, along with a count of the total number of spikes. The model has been developed in two main stages: the training stage, used to train the R-CNN for spike detection, and the testing stage, in which the trained CNN model is applied to test images.<fig id="Fig1"><label>Fig. 1</label><caption><p>The general work-flow diagram of the proposed system. The top diagram shows the training procedure of the Convoluted Neural Network (CNN) model implemented in this article. The bottom diagram depicts the testing procedure that is followed to obtain our results</p></caption><graphic xlink:href="13007_2018_366_Fig1_HTML" id="MO2"/></fig></p><sec id="Sec3"><title>Experimental setup</title><p id="Par14">The field trial was conducted at Mallala (&#x02212; 34.457062, 138.481487), South Australia, in a randomized complete block design with a total of 90 plots, 18 rows and 5 columns, consisting of ten spring wheat (<italic>Triticum aestivum</italic> L.) varieties (Drysdale, Excalibur, Gladius, Gregory, Kukri, Mace, Magenta, RAC875, Scout, Yitpi) and nine replicates of each, all of which were sown on July 3, 2017. To mitigate the boundary effects, an additional plot (not included in the analysis) was planted at the beginning and at the end of each row of plots. The plots were 1.2 m wide, with an inter-row spacing of approximately 0.2 m, and 4 m long with a gap of approximately 2 m between plot rows and 0.3 m between columns. To explore the impact of fertilizer on wheat spike production, each variety was subject to three fertilizer treatments: no treatment, early treatment, and late treatment. Each combination of variety <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times $$\end{document}</tex-math><mml:math id="M6"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq3.gif"/></alternatives></inline-formula> treatment is replicated three times. Two thirds of the replicates were treated at a standard rate of 80 kg nitrogen, 40 kg phosphorus and 40 kg potassium per hectare (referred to as <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$16-8-16 \; N-P-K$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mn>16</mml:mn><mml:mo>-</mml:mo><mml:mn>8</mml:mn><mml:mo>-</mml:mo><mml:mn>16</mml:mn><mml:mspace width="0.277778em"/><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mi>P</mml:mi><mml:mo>-</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq4.gif"/></alternatives></inline-formula>), while the other 30 plots received no treatment at all. For the early treatment, the macronutrients nitrogen, phosphorus and potassium were applied on July 14. Urea was then applied on July 18 to the same 30 plots. For the late stage treatment, both fertilizers were applied together on September 26. The imaging of the plots took place approximately twice a week during the period of July 21&#x02013;November 22, 2017.</p><p id="Par15">The land-based vehicle used for image capture is shown in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>. This wagon is comprised of a steel frame and four wheels with a central overhead rail for mounting imaging sensors. While capable of housing a stereo pair of cameras for orthogonal viewing, only the camera mounted at one end at an angle oblique to the plots was used for this study. Viewed from directly above, many spikes, primarily those near the viewing axis, appear in images small and circular, making them difficult to detect (see the comparison of images of the same plot taken from the two perspectives in Additional file <xref rid="MOESM1" ref-type="media">1</xref>). Although not pursued in this paper, a perspective view also admits the possibility of a more detailed analysis of spikes (for, say, grain number estimation) with a greater fraction of their length visible, although the problem of partial occlusion of some spikes may complicate the estimation process. Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>b (inset, top right) shows an image captured with this imaging platform. The images were acquired using an 18.1 megapixel Canon EOS 60D digital camera, shown in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>a, surrounded with a waterproof casing. Manual focus was used during all the imaging sessions with the camera focused at 2.2 m and 1.8 m during early and late plant growth stages, respectively. Following some experimentation, a viewing angle of <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$55^{\circ }$$\end{document}</tex-math><mml:math id="M10"><mml:msup><mml:mn>55</mml:mn><mml:mo>&#x02218;</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq5.gif"/></alternatives></inline-formula> from the horizontal overhead rail was chosen to capture a maximum plot area with minimal the area from overlapping regions. The camera sensor is located 190&#x000a0;cm above the ground level. The camera settings were as follows;<list list-type="bullet"><list-item><p id="Par16">Focal length&#x02014;18 mm,</p></list-item><list-item><p id="Par17">Aperture&#x02014;f/9.0,</p></list-item><list-item><p id="Par18">ISO&#x02014;automatic and</p></list-item><list-item><p id="Par19">Exposure time&#x02014;1/500 s.</p></list-item></list>Finally, the resolution of images was 5184 <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times $$\end{document}</tex-math><mml:math id="M12"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq6.gif"/></alternatives></inline-formula> 3456 pixels, resulting in an image resolution of approximately 0.04 cm per pixel.<fig id="Fig2"><label>Fig. 2</label><caption><p>The ground-based vehicle for imaging in the field. <bold>a</bold> A camera, angled for oblique viewing, is placed at the top of an imaging frame mounted on a four-wheel base (the wagon). The frame also supports two stereo cameras, angled vertically, placed in the center of the top section. These have not been used in this article. <bold>b</bold> A schematic of the wagon from a side-view. A sample image taken with the oblique-view camera is shown in the inset, top right</p></caption><graphic xlink:href="13007_2018_366_Fig2_HTML" id="MO3"/></fig>
</p></sec><sec id="Sec4"><title>The SPIKE dataset</title><p id="Par20">The high quality in-field images from this field trial are used to construct the SPIKE data set, a key contribution of this study. The SPIKE data set has three main components:<list list-type="bullet"><list-item><p id="Par21">Over 300 images of ten wheat varieties at three different growth stages.</p></list-item><list-item><p id="Par22">Annotations for each image denoting the bounding boxes of spikes.</p></list-item><list-item><p id="Par23">Deep learning models trained on these images and labels.</p></list-item></list>A diagram illustrating each of these components is shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>. First, images are acquired in the field. These are then automatically cropped so that only the region of interest (ROI) is kept. The captured in-field images contain other objects including neighbor plots, plot gaps, vehicle and color-chart which are not required in our approach. So, a significant SPIKE region from the plot is selected as ROI and cropped automatically for all images in the experiment. Next, the images are manually annotated with bounding boxes highlighting all the spikes present in the images. The images and annotations are then fed to the Convolutional Neural Network (CNN) for training.<fig id="Fig3"><label>Fig. 3</label><caption><p>Steps for training the model. Images are acquired in the field before being automatically cropped to a region of interest. Training images were then manually annotated with bounding boxes. Finally, both the cropped and annotated images were passed to the R-CNN model for training</p></caption><graphic xlink:href="13007_2018_366_Fig3_HTML" id="MO4"/></fig></p><p id="Par24"><italic>Images</italic> While the original images capture the majority of the 4&#x000a0;m <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times $$\end{document}</tex-math><mml:math id="M14"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq7.gif"/></alternatives></inline-formula> 1.2&#x000a0;m plot area, they also contain parts of the neighboring plots, inter-plot weeds and parts of the wagon. These background objects can confound the testing phase; a particular issue is spikes of neighboring plots appearing in an image and thus included in the density estimation. To overcome this issue, images were automatically cropped to a 0.8&#x000a0;m <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times $$\end{document}</tex-math><mml:math id="M16"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq8.gif"/></alternatives></inline-formula> 0.8&#x000a0;m region of interest as shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>. In total, 335 images containing a total of approximately 25,000 wheat spikes have been captured. With our camera image resolution, the spike size [width, height] ranged from [10 px, 80 px] to [50 px, 300 px].</p><p id="Par25">We found that the most convenient situation for detecting wheat spikes in images is when there is considerable color contrast between the spikes and other parts of the canopy. As such, the majority of the images in the SPIKE data set contains images where the spikes are approximately green in color while the canopy has already senesced to a more yellow color. However, in order to fully test the capabilities of deep learning techniques for spike detection in the field, the SPIKE data set also includes a number of images taken at two other growth stages, where spike detection spikes is more difficult. Hereafter we denote the three different situations, shown in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>, as:<list list-type="bullet"><list-item><p id="Par26">Green Spike and Green Canopy (GSGC)</p></list-item><list-item><p id="Par27">Green Spike and Yellow Canopy (GSYC)</p></list-item><list-item><p id="Par28">Yellow Spike and Yellow Canopy (YSYC).</p></list-item></list>The GSGC, GSYC, and YSYC images were acquired on the 26/10/2017, 9/11/2017 and 16/11/2017, respectively. Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> shows the number of images acquired for each of the three classes. Although the data set contains 255 GSYC images, only 235 were used for training while the remaining 20 were reserved for testing. Each of the GSGC and YSYC data sets comprise 40 images, of which 35 have been used for training and 5 for testing. The second half of the table, which indicates how many images were used in the different models, will be explained in more detail at the end of this section.<fig id="Fig4"><label>Fig. 4</label><caption><p>Examples of training images captured at three different growth stages. From left to right: the GSGC images contain green spikes and a green canopy, the GSYC images contain green spikes and a yellow canopy, and the YSYC images contain yellow spikes and a yellow canopy.</p></caption><graphic xlink:href="13007_2018_366_Fig4_HTML" id="MO5"/></fig><table-wrap id="Tab1"><label>Table 1</label><caption><p>Number of images from each growth stage used for training and testing</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Images</th><th align="left">GSYC</th><th align="left">GSGC</th><th align="left" colspan="2">YSYC</th></tr></thead><tbody><tr><td align="left" colspan="5">Data</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;Training</td><td char="." align="char">235</td><td char="." align="char">35</td><td char="." align="char" colspan="2">35</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;Test</td><td char="." align="char">20</td><td char="." align="char">5</td><td char="." align="char" colspan="2">5</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;Total</td><td char="." align="char">255</td><td char="." align="char">40</td><td char="." align="char" colspan="2">40</td></tr></tbody></table><table frame="hsides" rules="groups"><thead><tr><th align="left"> Images</th><th align="left">GSYC</th><th align="left">+ GSGC</th><th align="left">+ YSYC</th><th align="left">GSYC++</th></tr></thead><tbody><tr><td align="left" colspan="5">Models</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;Training</td><td char="." align="char">235</td><td char="." align="char">270</td><td char="." align="char">270</td><td char="." align="char">305</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;Test</td><td char="." align="char">20</td><td char="." align="char">25</td><td char="." align="char">25</td><td char="." align="char">30</td></tr></tbody></table></table-wrap>
</p><p id="Par29"><italic>Annotations</italic> The images have been labeled by multiple experts at the resolution of <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2000\times 1500$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:mn>2000</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1500</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq9.gif"/></alternatives></inline-formula> pixels. For the annotation of images, we used the publicly available Video Object Tagging Tool provided by Microsoft. Each labelled image has an additional text file containing the coordinates of the annotated bounding boxes, see Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>. In this file the boxes are saved as a 4-tuple <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(x_b,y_b,w_b,h_b)$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq10.gif"/></alternatives></inline-formula> where <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(x_b,y_b)$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq11.gif"/></alternatives></inline-formula> denotes the top-left corner of the box while the pair <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(w_b,h_b)$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq12.gif"/></alternatives></inline-formula> denotes the width and height of the bounding box. Each image contains approximately 70&#x02013;80 spikes. Therefore, in total, the 335 images contain approximately 25,000 annotated spikes.<fig id="Fig5"><label>Fig. 5</label><caption><p>Representation and annotation of the SPIKE data set. The data set is split into three subsets: the positive, negative and test images. Image labeling classes, locations and their annotations are saved in a separate text file</p></caption><graphic xlink:href="13007_2018_366_Fig5_HTML" id="MO6"/></fig></p><p id="Par30"><italic>Model development</italic> The SPIKE data set of 335 images in total was split into 305 training images and 30 testing images. This split was performed at the image level, not at the spike level, to ensure that no spikes from the same image could be seen in both training and testing sets. We found that the GSYC class of images, which exhibit a high color contrast, were the most suitable for spike detection in the field. For this reason the main model used in this study was trained and tested only on the set of GSYC images. However, in order to better understand the effect of spike and canopy color on deep learning models we trained three additional models using the two other classes. The reader is referred to the bottom half of Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> for a summary of the number of training and testing images used in each of the four models. The + GSGC and + YSYC models were trained using the original 235 images as well as the 35 GSGC images and 35 YSYC images, respectively. They also have a set of test images made up of combinations of the test images from their corresponding classes. Finally, a fourth model, &#x02018;GSYC++&#x02019;, was based on the 305 training images from all three classes and had a test set comprised of all the 30 designated test images.</p></sec><sec id="Sec5"><title>R-CNN model</title><p id="Par31">Region-based Convolutional Neural Network (R-CNN) was introduced by Girshick et al.&#x000a0;[<xref ref-type="bibr" rid="CR31">31</xref>] for object detection using a selective search to detect regions of interest and a CNN to classify them. Later, Fast R-CNN by ROI pooling [<xref ref-type="bibr" rid="CR32">32</xref>] was used after final convolution to extract a fixed length feature vector from the feature map along with the training of all network weights with back-propagation. Later, Faster R-CNN was developed by Ren et al.&#x000a0; [<xref ref-type="bibr" rid="CR33">33</xref>]. This model consists of two networks: a region proposal network (RPN) for generating region proposals, and a convolutional network which takes the proposed regions to detect objects almost in real-time. The main difference between the two region-based methods is that, to generate region proposals, Fast R-CNN uses selective search whereas Faster R-CNN uses high-speed RPN and shares the bulk of the computation time with object detection. Briefly, RPN ranks the region boxes (called anchors) and proposes the ones that are most likely to contain the desired objects. Due to its fast processing capability and high recognition rate, Faster R-CNN is used in this article for wheat spike detection. Python implementation of the Faster-RCNN is publicly available and can be accessed online [<xref ref-type="bibr" rid="CR34">34</xref>]. The implementation is modified somewhat and hyper-parameters have been optimized for better classification of the spike regions and overall detection performance. A detailed description of R-CNN, the specific architecture of the model, and the image processing techniques used in this article can be found in Additional file <xref rid="MOESM2" ref-type="media">2</xref>.</p><p id="Par32">For each box detected, the R-CNN provides as output a corresponding confidence level, <inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C\in [0,1]$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq13.gif"/></alternatives></inline-formula>, where 0 represents the lowest level of confidence that a detected object is a spike and 1 represents the highest level of confidence. When a detected box proposed by the CNN has a confidence value <italic>C</italic> that is larger than a predefined threshold, then the proposal is classified as a spike. Otherwise, it is classified as a background. Higher values of <italic>C</italic> will result in fewer boxes being incorrectly labeled as spikes, but will also result in more spikes being incorrectly labeled as background. Conversely, low values of <italic>C</italic> will correspond to incorrectly captured (background) regions but will rarely miss plant spikes. In this study we have chosen to use a confidence value of <inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C=0.5$$\end{document}</tex-math><mml:math id="M28"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq14.gif"/></alternatives></inline-formula> as it provided a desirable trade-off between the two scenarios.</p></sec><sec id="Sec6"><title>Validation</title><p id="Par33">The output of the R-CNN used in this study is a list of bounding boxes which will ideally contain all of the wheat spikes in an image. The goal of this study is for the number of boxes to accurately match the number of spikes in an image. Denoting boxes as spike or non-spike can yield three potential results, with the latter two being sources of error: <italic>true positive (TP)</italic>&#x02014;correctly classifying a region as a spike; <italic>false positive (FP)</italic>&#x02014;incorrectly classifying a background region as a spike as well as multiple detection of the same spike; and <italic>false negative (FN)</italic>&#x02014;incorrectly classifying a spike as a background region. In contrast, <italic>true negative (TN)</italic>&#x02014;correct classification of background is always &#x02019;zero&#x02019; and is not required in this binary classification problem where foreground is always determined for object detection. In order to quantify our errors, the validation metrics are based on the concepts of precision, recall, accuracy and the F1 score, which are defined as follows:<list list-type="bullet"><list-item><p id="Par34"><inline-formula id="IEq15"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {Precision} = \displaystyle \frac{TP}{TP+FP}$$\end{document}</tex-math><mml:math id="M30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>Precision</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq15.gif"/></alternatives></inline-formula> measures how many of the detected regions are actually spikes.</p></list-item><list-item><p id="Par35"><inline-formula id="IEq16"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {Recall} = \displaystyle \frac{TP}{TP+FN}$$\end{document}</tex-math><mml:math id="M32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>Recall</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq16.gif"/></alternatives></inline-formula> measures how many of the spikes in the image have been captured.</p></list-item><list-item><p id="Par36"><inline-formula id="IEq17"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {Accuracy} = \displaystyle \frac{TP+TN}{TP+TN+FP+FN}$$\end{document}</tex-math><mml:math id="M34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>Accuracy</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq17.gif"/></alternatives></inline-formula> implies the models performance</p></list-item><list-item><p id="Par37"><inline-formula id="IEq18"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {F1 Score} = \displaystyle 2 \frac{Precision\cdot Recall}{Precision + Recall}$$\end{document}</tex-math><mml:math id="M36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>F1 Score</mml:mtext><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq18.gif"/></alternatives></inline-formula> is the harmonic mean of Precision and Recall. It is a useful measure to observe a model&#x02019;s robustness.</p></list-item><list-item><p id="Par38">The mean Average Precision (mAP)&#x000a0;[<xref ref-type="bibr" rid="CR35">35</xref>], which quantifies how precise the method is at varying levels of Recall. It can be expressed as follows: <disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \begin{aligned} mAP = \frac{1}{11}\sum _{r_{i}\in \left\{ 0,0.1,...,1\right\} }^{}\max _{r_{i}:r_{i}\ge r}p(r_{i}). \end{aligned} \end{aligned}$$\end{document}</tex-math><mml:math id="M38" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>m</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>11</mml:mn></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mfenced close="}" open="{" separators=""><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mfenced></mml:mrow><mml:mrow/></mml:munderover><mml:munder><mml:mo movablelimits="true">max</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02265;</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="13007_2018_366_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>
</p><p id="Par39">In other words, it is defined as the mean precision of a set of eleven equally spaced Recall levels <inline-formula id="IEq19"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[0,0.1,\ldots ,1]$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq19.gif"/></alternatives></inline-formula>. Here, <inline-formula id="IEq20"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p(r_i)$$\end{document}</tex-math><mml:math id="M42"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq20.gif"/></alternatives></inline-formula> is the measured Precision at Recall <inline-formula id="IEq21"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r_i$$\end{document}</tex-math><mml:math id="M44"><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq21.gif"/></alternatives></inline-formula>. The Precision at each Recall level <inline-formula id="IEq22"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r_i$$\end{document}</tex-math><mml:math id="M46"><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq22.gif"/></alternatives></inline-formula> is interpolated by taking the maximum Precision measured for which the corresponding Recall exceeds <italic>r</italic>.</p></list-item></list>All the experiments in this article were conducted using a high-performance computer with Intel Xeon 3.50 GHz processor and 128 GB of memory. Also, a NVIDIA GeForce graphics processing unit(GPU) has 12 GB memory which is used along with the CPU to accelerate the training of the CNN.</p></sec></sec><sec id="Sec7"><title>Results and discussion</title><p id="Par40">The performance of the proposed model was measured in terms of detection accuracy and mean precision defined in the Validation Section. To demonstrate the robustness of deep learning for spike detection, we analyzed the degrees to which the different training and testing data sets, captured at different growth stages, affect the model performance. Finally, we analyze the differences in spike density across the different varieties grown under the three different treatments in the field trial.</p><sec id="Sec8"><title>Performance</title><p id="Par41">For each test image the R-CNN program returns the locations of the detected spikes, the total number of spikes, and a classification probability (confidence) for each detected spike, see Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>. The GSYC class of images was chosen to train the main model proposed in this study. For 20 test images, the model achieved a mAP of 0.6653 and an average accuracy of <inline-formula id="IEq23"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$93.3\%$$\end{document}</tex-math><mml:math id="M48"><mml:mrow><mml:mn>93.3</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq23.gif"/></alternatives></inline-formula> based on the 1463 spikes detected among the 1570 manually counted spikes. For each test image, the following statistics are provided in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>; the number of spikes in the ground truth image, the number of spikes detected by the proposed approach, the number of true positives, the number of false positives, the number of false negatives, the precision, the mAP, the accuracy, and the F1 score.
The output images of this table are included in the supplementary material (Additional file <xref rid="MOESM2" ref-type="media">2</xref>).<table-wrap id="Tab2"><label>Table 2</label><caption><p>Evaluation and validation of spike detection using the GSYC image model applied to the GSYC image data set</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Image no</th><th align="left">GT-count</th><th align="left">Detected</th><th align="left">TP</th><th align="left">FP</th><th align="left">FN</th><th align="left">Precision</th><th align="left">mAP</th><th align="left">Accuracy</th><th align="left">F1-score</th></tr></thead><tbody><tr><td align="left">Test_001.jpg</td><td align="left">73</td><td align="left">71</td><td align="left">70</td><td align="left">1</td><td align="left">3</td><td char="." align="char">0.98</td><td char="." align="char">
<italic>0.7289</italic>
</td><td align="left"><italic>96</italic>%</td><td char="." align="char">0.97</td></tr><tr><td align="left">Test_012.jpg</td><td align="left">75</td><td align="left">68</td><td align="left">68</td><td align="left">0</td><td align="left">7</td><td char="." align="char">1.00</td><td char="." align="char">
<italic>0.6002</italic>
</td><td align="left"><italic>91</italic>%</td><td char="." align="char">0.95</td></tr><tr><td align="left">Test_025.jpg</td><td align="left">87</td><td align="left">85</td><td align="left">84</td><td align="left">1</td><td align="left">3</td><td char="." align="char">0.98</td><td char="." align="char">
<italic>0.7324</italic>
</td><td align="left"><italic>96</italic>%</td><td char="." align="char">0.97</td></tr><tr><td align="left">Test_032.jpg</td><td align="left">80</td><td align="left">76</td><td align="left">76</td><td align="left">0</td><td align="left">4</td><td char="." align="char">1.00</td><td char="." align="char">
<italic>0.7286</italic>
</td><td align="left"><italic>95</italic>%</td><td char="." align="char">0.97</td></tr><tr><td align="left">Test_118.jpg</td><td align="left">76</td><td align="left">73</td><td align="left">70</td><td align="left">3</td><td align="left">6</td><td char="." align="char">0.95</td><td char="." align="char">
<italic>0.6126</italic>
</td><td align="left"><italic>92</italic>%</td><td char="." align="char">0.93</td></tr><tr><td align="left">Test_141.jpg</td><td align="left">66</td><td align="left">61</td><td align="left">58</td><td align="left">3</td><td align="left">8</td><td char="." align="char">0.95</td><td char="." align="char">
<italic>0.5835</italic>
</td><td align="left"><italic>88</italic>%</td><td char="." align="char">0.91</td></tr><tr><td align="left">Test_185.jpg</td><td align="left">69</td><td align="left">68</td><td align="left">65</td><td align="left">3</td><td align="left">4</td><td char="." align="char">0.95</td><td char="." align="char">
<italic>0.7105</italic>
</td><td align="left"><italic>94</italic>%</td><td char="." align="char">0.94</td></tr><tr><td align="left">Test_199.jpg</td><td align="left">72</td><td align="left">69</td><td align="left">68</td><td align="left">1</td><td align="left">4</td><td char="." align="char">0.98</td><td char="." align="char">
<italic>0.7184</italic>
</td><td align="left"><italic>94</italic>%</td><td char="." align="char">0.96</td></tr><tr><td align="left">Test_220.jpg</td><td align="left">80</td><td align="left">79</td><td align="left">76</td><td align="left">3</td><td align="left">4</td><td char="." align="char">0.96</td><td char="." align="char">
<italic>0.7229</italic>
</td><td align="left"><italic>95</italic>%</td><td char="." align="char">0.95</td></tr><tr><td align="left">Test_242.jpg</td><td align="left">70</td><td align="left">64</td><td align="left">63</td><td align="left">1</td><td align="left">7</td><td char="." align="char">0.90</td><td char="." align="char">
<italic>0.5926</italic>
</td><td align="left"><italic>90</italic>%</td><td char="." align="char">0.94</td></tr><tr><td align="left">Test_254.jpg</td><td align="left">83</td><td align="left">77</td><td align="left">76</td><td align="left">1</td><td align="left">7</td><td char="." align="char">0.98</td><td char="." align="char">
<italic>0.6085</italic>
</td><td align="left"><italic>91</italic>%</td><td char="." align="char">0.95</td></tr><tr><td align="left">Test_320.jpg</td><td align="left">80</td><td align="left">77</td><td align="left">74</td><td align="left">3</td><td align="left">6</td><td char="." align="char">0.96</td><td char="." align="char">
<italic>0.6213</italic>
</td><td align="left"><italic>92</italic>%</td><td char="." align="char">0.94</td></tr><tr><td align="left">Test_383.jpg</td><td align="left">87</td><td align="left">84</td><td align="left">78</td><td align="left">6</td><td align="left">9</td><td char="." align="char">0.92</td><td char="." align="char">
<italic>0.5947</italic>
</td><td align="left"><italic>90</italic>%</td><td char="." align="char">0.91</td></tr><tr><td align="left">Test_399.jpg</td><td align="left">80</td><td align="left">78</td><td align="left">77</td><td align="left">1</td><td align="left">3</td><td char="." align="char">0.98</td><td char="." align="char">
<italic>0.7301</italic>
</td><td align="left"><italic>96</italic>%</td><td char="." align="char">0.97</td></tr><tr><td align="left">Test_417.jpg</td><td align="left">96</td><td align="left">93</td><td align="left">89</td><td align="left">4</td><td align="left">7</td><td char="." align="char">0.95</td><td char="." align="char">
<italic>0.6573</italic>
</td><td align="left"><italic>93</italic>%</td><td char="." align="char">0.94</td></tr><tr><td align="left">Test_421.jpg</td><td align="left">71</td><td align="left">73</td><td align="left">70</td><td align="left">3</td><td align="left">1</td><td char="." align="char">0.95</td><td char="." align="char">
<italic>0.7552</italic>
</td><td align="left"><italic>98</italic>%</td><td char="." align="char">0.97</td></tr><tr><td align="left">Test_422.jpg</td><td align="left">82</td><td align="left">79</td><td align="left">78</td><td align="left">1</td><td align="left">4</td><td char="." align="char">0.98</td><td char="." align="char">
<italic>0.7211</italic>
</td><td align="left"><italic>95</italic>%</td><td char="." align="char">0.96</td></tr><tr><td align="left">Test_432.jpg</td><td align="left">85</td><td align="left">81</td><td align="left">79</td><td align="left">2</td><td align="left">6</td><td char="." align="char">0.97</td><td char="." align="char">
<italic>0.6502</italic>
</td><td align="left"><italic>93</italic>%</td><td char="." align="char">0.95</td></tr><tr><td align="left">Test_437.jpg</td><td align="left">70</td><td align="left">64</td><td align="left">62</td><td align="left">2</td><td align="left">8</td><td char="." align="char">0.96</td><td char="." align="char">
<italic>0.5924</italic>
</td><td align="left"><italic>88</italic>%</td><td char="." align="char">0.92</td></tr><tr><td align="left">Test_480.jpg</td><td align="left">88</td><td align="left">84</td><td align="left">82</td><td align="left">2</td><td align="left">6</td><td char="." align="char">0.97</td><td char="." align="char">
<italic>0.6441</italic>
</td><td align="left"><italic>93</italic>%</td><td char="." align="char">0.95</td></tr><tr><td align="left">Total</td><td align="left">1570</td><td align="left">1504</td><td align="left">1463</td><td align="left">41</td><td align="left">107</td><td char="." align="char">&#x02212;</td><td char="." align="char">&#x02212;</td><td align="left">&#x02212;</td><td char="." align="char">&#x02212;</td></tr><tr><td align="left">Average</td><td align="left">&#x02212;</td><td align="left">&#x02212;</td><td align="left">&#x02212;</td><td align="left">&#x02212;</td><td align="left">&#x02212;</td><td char="." align="char">
<italic>0.97</italic>
</td><td char="." align="char">
<italic>0.6653</italic>
</td><td align="left"><italic>93.4</italic>%</td><td char="." align="char">
<italic>0.95</italic>
</td></tr><tr><td align="left">Standard dev.</td><td align="left">7.82</td><td align="left">8.17</td><td align="left">7.86</td><td align="left">1.46</td><td align="left">1.11</td><td char="." align="char">0.02</td><td char="." align="char">0.06</td><td align="left">0.03</td><td char="." align="char">0.02</td></tr></tbody></table></table-wrap><fig id="Fig6"><label>Fig. 6</label><caption><p>An example of a generated output image (<bold>b</bold>) from a test image (<bold>a</bold>). Detected spikes are indicated using bounding boxes along with their respective classification confidences. Among 82, 78 spikes were detected with a mAP of 0.7211 and an accuracy of 95.18%</p></caption><graphic xlink:href="13007_2018_366_Fig6_HTML" id="MO7"/></fig></p></sec><sec id="Sec9"><title>Testing the supplementary models</title><p id="Par42">In this section, the results of the base GSYC model are compared with those of the other three models. The comparative analysis for different testing sample combinations is presented in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref> in terms of the average detection accuracy (ADA) and in Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref> in terms of mean Average Precision.<table-wrap id="Tab3"><label>Table 3</label><caption><p>average detection accuracy (ADA) (<inline-formula id="IEq24"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\%$$\end{document}</tex-math><mml:math id="M50"><mml:mo>%</mml:mo></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq24.gif"/></alternatives></inline-formula>) of the Faster R-CNN on different SPIKE dataset models</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Test images</th><th align="left">Base</th><th align="left" colspan="3">Extended model</th></tr><tr><th align="left">GSYC (%)</th><th align="left">+ GSGC (%)</th><th align="left">+ YSYC (%)</th><th align="left">GSYC++ (%)</th></tr></thead><tbody><tr><td align="left">GSYC (20)</td><td char="." align="char">93.4</td><td char="." align="char">92.2</td><td char="." align="char">91.7</td><td char="." align="char">92.9</td></tr><tr><td align="left">GSGC (5)</td><td char="." align="char">89.6</td><td char="." align="char">94.5</td><td char="." align="char">87.4</td><td char="." align="char">93.7</td></tr><tr><td align="left">YSYC (5)</td><td char="." align="char">84.8</td><td char="." align="char">86.5</td><td char="." align="char">93.1</td><td char="." align="char">92.3</td></tr><tr><td align="left">GSYC + GSGC + YSYC (30)</td><td char="." align="char">89.8</td><td char="." align="char">90.7</td><td char="." align="char">91.9</td><td char="." align="char">93.2</td></tr></tbody></table></table-wrap>
</p><p id="Par43">From Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>, one can see that the spike detection accuracy is always within the range of 88&#x02013;98 for the 20 images tested. This is quite satisfactory considering the challenges associated with in-field imaging, e.g., complex backgrounds, various illumination conditions, shadow effects and self occlusion. Also, the high mAP of 0.6653 shows the proficiency of our R-CNN, trained on the SPIKE data set. This is to be compared with the mAP performance of other CNN&#x02019;s applied to prominent data sets such as PASCAL VOC &#x000a0;[<xref ref-type="bibr" rid="CR35">35</xref>] and COCO&#x000a0;[<xref ref-type="bibr" rid="CR33">33</xref>], for the detection of 21 and 80, respectively, regular object classes such as, men, car, horse, dog, cat, bicycle, etc. Figure&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref> shows the relationship between ground truth number of spikes and the estimated number of spikes, for each of the 20 images. The R-CNN approach provides a near one-to-one estimate of the number of spikes per image (the line slope is 1.0086), with an intercept value of <inline-formula id="IEq25"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-\,3.95$$\end{document}</tex-math><mml:math id="M52"><mml:mrow><mml:mo>-</mml:mo><mml:mspace width="0.166667em"/><mml:mn>3.95</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq25.gif"/></alternatives></inline-formula> indicating an intrinsic error of just four spikes. The model produces a high <inline-formula id="IEq26"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R^2$$\end{document}</tex-math><mml:math id="M54"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq26.gif"/></alternatives></inline-formula> value of 0.93, proving a strong linear relationship between the ground truth and the results of our approach.<fig id="Fig7"><label>Fig. 7</label><caption><p>Ground truth versus estimated number of spikes per plot. The horizontal axis refers to the number of spikes estimated by the proposed approach and the vertical axis refers to the number of spikes that have been manually counted</p></caption><graphic xlink:href="13007_2018_366_Fig7_HTML" id="MO8"/></fig></p><p id="Par44">The efficiency of a training model can also be analyzed by observing the training loss and error rates while the model is learning. An epoch is defined as one full pass forwards and backwards through the network during the learning stage. While the model weights are initialized randomly, after a number of epochs they become closer to their final values, progressively reducing rates of error and training loss. Figure&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref> shows that the loss metric (described in full detail in Additional file <xref rid="MOESM2" ref-type="media">2</xref>) is decreasing over subsequent epochs of training. Although the loss and error rate is initially high, after each training epoch the reduced rate of error is accompanied by a higher detection accuracy; the loss and error rates become almost constant after 200 epochs indicating that no further improvement is possible. Based on several trials the number of epochs was fixed at 400 to avoid overfitting. This choice produced the high accuracy results presented in this article.<fig id="Fig8"><label>Fig. 8</label><caption><p>Number of epochs versus training loss. While training loss begins large it steadily decreases over the training epochs until eventually, at around 200 epochs, the benefit of further training appears to be negligible</p></caption><graphic xlink:href="13007_2018_366_Fig8_HTML" id="MO9"/></fig></p><p id="Par45">When limited to GSYC images, the GSYC model proved to return the highest accuracy in terms of ADA, valued as a percentage of spikes detected, as testing and training images covered plants at the same growth. When applied to GSGC or YSYC testing images, however, while still achieving a high accuracy, the performance had declined. Including GSGC and YSYC images in the test image set reduced the accuracy from 93.4 to 91.8% and 88.7%, respectively. Clearly, detection accuracy deteriorates when testing with images that are unknown to the trained model. Note also that the lower detection accuracy following inclusion of YSYC images in the GSYC data set points to the increased difficulty of differentiating yellow spikes from yellow canopy. The ADA comparison reflects the anticipated and indeed intuitive fact that a model can perform best when applied to similar types of images as those used for training. The consistent mAP results confirm the ADA finding.</p><p id="Par46">The same situation is reflected by the + GSGC and + YSYC models. These models work well when applied to image types that are included in the respective training sets. Not surprisingly, the GSYC++ model performs consistently better, in terms of both ADA and mAP, for all types of testing samples. It is not clear what factors are responsible for the highest degree of accuracy found for the GSYC&#x000a0;+&#x000a0;YSYC&#x000a0;+&#x000a0;GSGC image set. In light of the superior accuracy of the GSYC++ model it can be concluded that a model is particularly robust if trained with all types of spike-versus-canopy scenarios. With no a priori knowledge of samples, this model will perform better than the other training models. In fact, in the other models, the mAP for spike detection is reduced wherein GSYC++ model it is higher while maintaining the higher accuracy of <inline-formula id="IEq28"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$93.2\%$$\end{document}</tex-math><mml:math id="M56"><mml:mrow><mml:mn>93.2</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq28.gif"/></alternatives></inline-formula>. Considering that we are dealing with in-field imaging complexities and we are seeking to detect hundreds of spikes in an image, the mAP value of 0.6763 leading to a <inline-formula id="IEq29"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$93.2\%$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:mn>93.2</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq29.gif"/></alternatives></inline-formula> detection accuracy with the extended GSYC++ model is significantly better than the performance exhibited with the conventional VOC07 or COCO data sets &#x000a0;[<xref ref-type="bibr" rid="CR33">33</xref>], with values ranging from 64 to <inline-formula id="IEq30"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$78\%$$\end{document}</tex-math><mml:math id="M60"><mml:mrow><mml:mn>78</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq30.gif"/></alternatives></inline-formula>.</p><p id="Par47">From Tables <xref rid="Tab3" ref-type="table">3</xref> and <xref rid="Tab4" ref-type="table">4</xref>, it can be concluded that if a model is trained properly, Faster R-CNN can detect with high accuracy spikes in images that were acquired at the same growth stage and in an equivalent category. The precision of a model may drop but its scalability and robustness will depend on how well it is trained, particularly by including all different types of complex scenarios. Based on the performances of the different CNN models and considering the ADA and mAP metrics for bounding box regression described in Additional file <xref rid="MOESM2" ref-type="media">2</xref>, the <inline-formula id="IEq31"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$GSYC++$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mi>G</mml:mi><mml:mi>S</mml:mi><mml:mi>Y</mml:mi><mml:mi>C</mml:mi><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq31.gif"/></alternatives></inline-formula> model was chosen to analyze the spike density variation across the different treatments applied to the different wheat varieties. For this latter investigation we selected an imaging date that is different from the dates used for data acquisition and training of the CNN models.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Detection mAP of Faster R-CNN on different SPIKE dataset models</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Test images</th><th align="left">Base</th><th align="left" colspan="3">Extended model</th></tr><tr><th align="left">GSYC</th><th align="left">+ GSGC</th><th align="left">+ YSYC</th><th align="left">GSYC++</th></tr></thead><tbody><tr><td align="left">GSYC (20)</td><td char="." align="char">0.6653</td><td char="." align="char">0.6462</td><td char="." align="char">0.6435</td><td char="." align="char">0.6575</td></tr><tr><td align="left">GSGC (5)</td><td char="." align="char">0.6570</td><td char="." align="char">0.7077</td><td char="." align="char">0.6405</td><td char="." align="char">0.6857</td></tr><tr><td align="left">YSYC (5)</td><td char="." align="char">0.6546</td><td char="." align="char">0.6590</td><td char="." align="char">0.7163</td><td char="." align="char">0.7085</td></tr><tr><td align="left">GSYC + GSGC + YSYC (30)</td><td char="." align="char">0.6050</td><td char="." align="char">0.6413</td><td char="." align="char">0.6520</td><td char="." align="char">0.6763</td></tr></tbody></table></table-wrap>
</p></sec><sec id="Sec10"><title>Spike density analysis</title><p id="Par48">A third contribution of this paper is a comparative analysis of spike density for the different wheat varieties under the different treatments. The 10 varieties underwent three different fertilizer treatments: no treatment, early treatment, and late treatment. Determining spike density as a function of genotype and treatment should provide some insight into their relative contribution to yield. The latter is based on the total number of detected spikes within the ROI within each plot, resulting in an estimate of spike density (number per square meter). Since the ROI is uniformly cropped and consistently defined, edge effects are minimized. To quantify spike density, we have constructed another test set different from the set of images used in training and from the previous testing analysis. The image set is derived from the imaging session conducted on 7/11/2017. This test set contains 90 images of the 10 different varieties subject to the three treatments, with three replicates for each case. We remark in passing that the spike densities found in this study were consistent with the conditions for the region and standard sowing rate (45 g of seed per plot). The densities thus are not as high as found in other parts of Australia or elsewhere in the world.</p><p id="Par49">Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref> shows the number of spikes detected using the GSYC++ model. For the different categories of variety <inline-formula id="IEq32"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times $$\end{document}</tex-math><mml:math id="M64"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq32.gif"/></alternatives></inline-formula> treatment, the average values show the mean number of spikes detected in the three replicated plots. It is clear that the untreated wheat plants generally produced fewer spikes per square meter compared with either of the other two treatments. In the case of early fertilization, the varieties Excalibur, Drysdale and Gladius produced significantly more spikes (and hence greater spike densities) than the other varieties (see Fig.&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref>). The effect of an early treatment was more moderate for Kukri, Mace and Scout, whose densities increased by just over 15 spikes per square meter. In complete contrast, the effect of fertilizer application on RAC875, at either time point, was negligible.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Spike density (per square meter) of wheat varieties for different types of treatments. The detection is performed using the GSYC++ model</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Wheat</th><th align="left" colspan="4">No treatment</th><th align="left" colspan="4">Early treatment</th><th align="left" colspan="4">Late treatment</th></tr><tr><th align="left">Varieties</th><th align="left">Rep1</th><th align="left">Rep2</th><th align="left">Rep3</th><th align="left">Avg</th><th align="left">Rep1</th><th align="left">Rep1</th><th align="left">Rep3</th><th align="left">Avg</th><th align="left">Rep1</th><th align="left">Rep2</th><th align="left">Rep3</th><th align="left">Avg</th></tr></thead><tbody><tr><td align="left">Kukri</td><td char="." align="char">136</td><td char="." align="char">146</td><td char="." align="char">141</td><td char="." align="char">141</td><td char="." align="char">165</td><td char="." align="char">160</td><td char="." align="char">167</td><td char="." align="char">164</td><td char="." align="char">148</td><td char="." align="char">162</td><td char="." align="char">152</td><td char="." align="char">154</td></tr><tr><td align="left">RAC875</td><td char="." align="char">130</td><td char="." align="char">148</td><td char="." align="char">142</td><td char="." align="char">140</td><td char="." align="char">154</td><td char="." align="char">145</td><td char="." align="char">145</td><td char="." align="char">148</td><td char="." align="char">135</td><td char="." align="char">146</td><td char="." align="char">142</td><td char="." align="char">141</td></tr><tr><td align="left">Excalibur</td><td char="." align="char">145</td><td char="." align="char">129</td><td char="." align="char">146</td><td char="." align="char">140</td><td char="." align="char">173</td><td char="." align="char">180</td><td char="." align="char">172</td><td char="." align="char">175</td><td char="." align="char">145</td><td char="." align="char">159</td><td char="." align="char">146</td><td char="." align="char">150</td></tr><tr><td align="left">Gladius</td><td char="." align="char">142</td><td char="." align="char">142</td><td char="." align="char">136</td><td char="." align="char">140</td><td char="." align="char">169</td><td char="." align="char">177</td><td char="." align="char">176</td><td char="." align="char">174</td><td char="." align="char">140</td><td char="." align="char">156</td><td char="." align="char">160</td><td char="." align="char">152</td></tr><tr><td align="left">Drysdale</td><td char="." align="char">152</td><td char="." align="char">141</td><td char="." align="char">145</td><td char="." align="char">146</td><td char="." align="char">170</td><td char="." align="char">178</td><td char="." align="char">180</td><td char="." align="char">176</td><td char="." align="char">159</td><td char="." align="char">146</td><td char="." align="char">163</td><td char="." align="char">156</td></tr><tr><td align="left">Mace</td><td char="." align="char">158</td><td char="." align="char">143</td><td char="." align="char">149</td><td char="." align="char">150</td><td char="." align="char">170</td><td char="." align="char">170</td><td char="." align="char">164</td><td char="." align="char">168</td><td char="." align="char">173</td><td char="." align="char">167</td><td char="." align="char">149</td><td char="." align="char">163</td></tr><tr><td align="left">Yitpi</td><td char="." align="char">146</td><td char="." align="char">129</td><td char="." align="char">142</td><td char="." align="char">139</td><td char="." align="char">160</td><td char="." align="char">161</td><td char="." align="char">165</td><td char="." align="char">162</td><td char="." align="char">150</td><td char="." align="char">160</td><td char="." align="char">161</td><td char="." align="char">157</td></tr><tr><td align="left">Scout</td><td char="." align="char">135</td><td char="." align="char">152</td><td char="." align="char">154</td><td char="." align="char">147</td><td char="." align="char">163</td><td char="." align="char">164</td><td char="." align="char">168</td><td char="." align="char">165</td><td char="." align="char">139</td><td char="." align="char">158</td><td char="." align="char">183</td><td char="." align="char">160</td></tr><tr><td align="left">Magenta</td><td char="." align="char">133</td><td char="." align="char">160</td><td char="." align="char">160</td><td char="." align="char">151</td><td char="." align="char">166</td><td char="." align="char">163</td><td char="." align="char">166</td><td char="." align="char">165</td><td char="." align="char">160</td><td char="." align="char">147</td><td char="." align="char">155</td><td char="." align="char">154</td></tr><tr><td align="left">Gregory</td><td char="." align="char">149</td><td char="." align="char">158</td><td char="." align="char">143</td><td char="." align="char">150</td><td char="." align="char">161</td><td char="." align="char">164</td><td char="." align="char">155</td><td char="." align="char">160</td><td char="." align="char">149</td><td char="." align="char">160</td><td char="." align="char">153</td><td char="." align="char">154</td></tr></tbody></table></table-wrap><fig id="Fig9"><label>Fig. 9</label><caption><p>Spike density for different varieties and treatments. Varieties are sorted by highest spike density under early treatment conditions. Blue bars represent spike density averaged over plots with no treatment, orange bars are averaged over plots with early treatment and grey bars are averaged over plots with late treatment. The bottom graph shows the standard deviation of mean values of different wheat varieties for various types of treatments. Red dotted, orange solid and green solid bars show, respectively, the standard deviation for late, early and no treatment of varieties</p></caption><graphic xlink:href="13007_2018_366_Fig9_HTML" id="MO10"/></fig></p><p id="Par50">Regarding the timing of treatment, the early stage treatment resulted in significantly higher yields for nearly all varieties than what was produced by the same variety treated later in the season. We speculate that this was due at least in part to the longer exposure time of the fertilized soil to rainfall, which facilitated greater uptake of nutrients than possibly occurred with the plants treated later in the season. On the other hand, it is also possible that the comparison is simply consistent with established findings [<xref ref-type="bibr" rid="CR36">36</xref>] that an early treatment results in greater biomass, while a later treatment can instead result in increased grain nitrogen content. Unfortunately, no analysis of the grain was conducted in this field trial to confirm such an outcome. Further studies are underway to assess the importance of timing on the question of grain filling versus biomass production.</p><p id="Par51">Shown also in Fig.&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref> is the degree of variation between replicates of the 10 cultivars under different treatments. In the majority of cases, adding fertilizer early in the season reduced the degree of variation across replicates: no treatment resulted in a deviation of between 3 and 15 <inline-formula id="IEq33"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {spikes/m}^2$$\end{document}</tex-math><mml:math id="M66"><mml:msup><mml:mtext>spikes/m</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq33.gif"/></alternatives></inline-formula> over the 10 varieties, while for the plots treated early, the spread reduced to between 1 and 5 <inline-formula id="IEq34"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hbox {spikes/m}^2$$\end{document}</tex-math><mml:math id="M68"><mml:msup><mml:mtext>spikes/m</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq34.gif"/></alternatives></inline-formula>. The greater consistency possibly highlights another aspect of fertilizer treatment. Applying fertilizer later in the season did little to improve consistency, with only 2 out of 3 replicates showing similar results, the third differing significantly, as found in the case of no treatment. Indeed, if one removes the outliers then one could conclude that, as in the case of RAC875, there is little difference between the untreated plots and the late treated plots of Gregory, Excalibur and Magenta.
</p></sec></sec><sec id="Sec11"><title>Conclusion</title><p id="Par52">Estimating the yield of cereal crops grown in the field is a challenging task, yet it is an essential focus of plant breeders for wheat variety selection and improved crop productivity. Most of the previous works involving image analysis of wheat spikes have been conducted in laboratory conditions and controlled environments. Here, we have presented the first deep learning models for spike detection, trained on wheat images taken in the field. The models are capable of accurately detecting wheat spikes within a complex and changing imaging environment. The best performing model produced an average accuracy and F1 score of <inline-formula id="IEq35"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$93.4\%$$\end{document}</tex-math><mml:math id="M70"><mml:mrow><mml:mn>93.4</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2018_366_Article_IEq35.gif"/></alternatives></inline-formula> and 0.95, respectively, when tested on 20 images containing 1570 spikes in total. Although we have not applied the model to oblique-view images of higher spike density field plots, due to the lack of access to such images, we expect the model to perform well at higher densities notwithstanding partial occlusion. Improvement is nevertheless possible by complementing the SPIKE data set with further training images of partial spike objects. The ability to count spikes in the field, a trait closely related to crop yield, to such a degree of accuracy, without destructive sampling or time consuming manual effort, is a significant step forward in field-based plant phenotyping.
</p></sec><sec sec-type="supplementary-material"><title>Additional file</title><sec id="Sec12"><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="13007_2018_366_MOESM1_ESM.pdf"><caption><p><bold>Additional file 1.</bold> View Comparison and Spike Detection Results Comparison between images captured from the top and oblique view angle. Additional spike detection results which contain the original image and corresponding spike detected output image for GSGC, GSYC and YSYC test images.</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM2"><media xlink:href="13007_2018_366_MOESM2_ESM.pdf"><caption><p><bold>Additional file 2.</bold> CNN for Spike Detection. Technical details of the overall Faster R-CNN architecture and step-wise description to train the model for spike detection.</p></caption></media></supplementary-material>
</p></sec></sec></body><back><ack><title>Authors' contributions</title><p>SJM was responsible for research program conceptualization and experimental design. The image data collection was undertaken by JC. Image annotation of the SPIKE data set was performed by MH and JC. MH was responsible for implementing and executing the deep learning analysis. MH prepared the figures and wrote the first draft of the manuscript. JC, HL and SJM revised the manuscript. All authors contributed equally to the final editing of the manuscript. All authors read and approved the final manuscript.</p><sec id="FPar1"><title>Acknowledgements</title><p id="Par54">We thank Jinhai Cai for discussions and for providing feedback on the manuscript.</p></sec><sec id="FPar3"><title>Competing interests</title><p id="Par56">The authors declare no conflict of interest. The funding sponsors had no role in the design of the study; in the collection, analysis or interpretation of data; in the writing of the manuscript; nor in the decision to publish the results.</p></sec><sec id="FPar2"><title>Funding</title><p id="Par55">The authors are grateful for funding support from the Australian Research Council under its Linkage funding scheme (Projects LP140100347 and LP150100055).</p></sec><sec id="FPar4"><title>Publisher&#x02019;s Note</title><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></sec></ack><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">FAOSTAT. <ext-link ext-link-type="uri" xlink:href="http://faostat3.fao.org/faostat-gateway/go/to/browse/Q/QC/E">http://faostat3.fao.org/faostat-gateway/go/to/browse/Q/QC/E</ext-link></mixed-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Cai</surname><given-names>J</given-names></name><name><surname>Berger</surname><given-names>B</given-names></name><name><surname>Okamoto</surname><given-names>M</given-names></name><name><surname>Miklavcic</surname><given-names>SJ</given-names></name></person-group><article-title>Detecting spikes of wheat plants using neural networks with laws texture energy</article-title><source>Plant Methods</source><year>2017</year><volume>13</volume><issue>29046709</issue><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="pmid">28053646</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bi</surname><given-names>K</given-names></name><name><surname>Jiang</surname><given-names>P</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Shi</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>C</given-names></name></person-group><article-title>Non-destructive measurement of wheat spike characteristics based on morphological image processing</article-title><source>Trans Chin Soc Agric Eng</source><year>2010</year><volume>26</volume><issue>12</issue><fpage>212</fpage><lpage>216</lpage></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bi</surname><given-names>K</given-names></name><name><surname>Jiang</surname><given-names>P</given-names></name><name><surname>Wei</surname><given-names>T</given-names></name><name><surname>Huang</surname><given-names>F</given-names></name><name><surname>Wang</surname><given-names>C</given-names></name></person-group><article-title>The design of wheat variety bp classifier based on wheat ear feature</article-title><source>Chin Agric Sci Bull</source><year>2011</year><volume>28</volume><issue>6</issue><fpage>464</fpage><lpage>468</lpage></element-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Pound MP, Atkinson JA, Wells DM, Pridmore TP, French AP. Deep learning for multi-task plant phenotyping. In: IEEE international conference on computer vision workshop (ICCVW); 2017. p. 2055&#x02013;63</mixed-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kovalchuk</surname><given-names>N</given-names></name><name><surname>Laga</surname><given-names>H</given-names></name><name><surname>Cai</surname><given-names>J</given-names></name><name><surname>Kumar</surname><given-names>P</given-names></name><name><surname>Parent</surname><given-names>B</given-names></name><name><surname>Lu</surname><given-names>Z</given-names></name><name><surname>Miklavcic</surname><given-names>SJ</given-names></name><name><surname>Haefele</surname><given-names>SM</given-names></name></person-group><article-title>Phenotyping of plants in competitive but controlled environments: a study of drought response in transgenic wheat</article-title><source>Funct Plant Biol</source><year>2017</year><volume>44</volume><issue>3</issue><fpage>290</fpage><lpage>301</lpage><pub-id pub-id-type="doi">10.1071/FP16202</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>H</given-names></name><name><surname>Cao</surname><given-names>Z</given-names></name><name><surname>Xiao</surname><given-names>Y</given-names></name><name><surname>Zhuang</surname><given-names>B</given-names></name><name><surname>Shen</surname><given-names>C</given-names></name></person-group><article-title>Tasselnet: counting maize tassels in the wild via local counts regression network</article-title><source>Plant Methods</source><year>2017</year><volume>13</volume><issue>1</issue><fpage>79</fpage><pub-id pub-id-type="doi">10.1186/s13007-017-0224-0</pub-id><pub-id pub-id-type="pmid">29118821</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ubbens</surname><given-names>J</given-names></name><name><surname>Cieslak</surname><given-names>M</given-names></name><name><surname>Prusinkiewicz</surname><given-names>P</given-names></name><name><surname>Stavness</surname><given-names>I</given-names></name></person-group><article-title>The use of plant models in deep learning: an application to leaf counting in rosette plants</article-title><source>Plant Methods</source><year>2018</year><volume>14</volume><issue>1</issue><fpage>6</fpage><pub-id pub-id-type="doi">10.1186/s13007-018-0273-z</pub-id><pub-id pub-id-type="pmid">29375647</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perez-Sanz</surname><given-names>F</given-names></name><name><surname>Navarro</surname><given-names>PJ</given-names></name><name><surname>Egea-Cortines</surname><given-names>M</given-names></name></person-group><article-title>Plant phenomics: an overview of image acquisition technologies and image data analysis algorithms</article-title><source>GigaScience</source><year>2017</year><volume>6</volume><issue>11</issue><fpage>1</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1093/gigascience/gix092</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Araus</surname><given-names>JL</given-names></name><name><surname>Cairns</surname><given-names>JE</given-names></name></person-group><article-title>Field high-throughput phenotyping: the new crop breeding frontier</article-title><source>Trends Plant Sci</source><year>2014</year><volume>19</volume><issue>1</issue><fpage>52</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1016/j.tplants.2013.09.008</pub-id><pub-id pub-id-type="pmid">24139902</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montes</surname><given-names>JM</given-names></name><name><surname>Melchinger</surname><given-names>AE</given-names></name><name><surname>Reif</surname><given-names>JC</given-names></name></person-group><article-title>Novel throughput phenotyping platforms in plant genetic studies</article-title><source>Trends Plant Sci</source><year>2007</year><volume>12</volume><issue>10</issue><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1016/j.tplants.2007.08.006</pub-id><pub-id pub-id-type="pmid">17719833</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holman</surname><given-names>FH</given-names></name><name><surname>Riche</surname><given-names>AB</given-names></name><name><surname>Michalski</surname><given-names>A</given-names></name><name><surname>Castle</surname><given-names>M</given-names></name><name><surname>Wooster</surname><given-names>MJ</given-names></name><name><surname>Hawkesford</surname><given-names>MJ</given-names></name></person-group><article-title>High throughput field phenotyping of wheat plant height and growth rate in field plot trials using UAV based remote sensing</article-title><source>Remote Sens.</source><year>2016</year><volume>8</volume><issue>12</issue><fpage>1031</fpage><pub-id pub-id-type="doi">10.3390/rs8121031</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khan</surname><given-names>Z</given-names></name><name><surname>Rahimi-Eichi</surname><given-names>V</given-names></name><name><surname>Haefele</surname><given-names>S</given-names></name><name><surname>Garnett</surname><given-names>T</given-names></name><name><surname>Miklavcic</surname><given-names>SJ</given-names></name></person-group><article-title>Estimation of vegetation indices for high-throughput phenotyping of wheat using aerial imaging</article-title><source>Plant Methods</source><year>2018</year><volume>14</volume><issue>1</issue><fpage>20</fpage><pub-id pub-id-type="doi">10.1186/s13007-018-0287-6</pub-id><pub-id pub-id-type="pmid">29563961</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>Y</given-names></name><name><surname>Thomasson</surname><given-names>JA</given-names></name><name><surname>Murray</surname><given-names>SC</given-names></name><name><surname>Pugh</surname><given-names>NA</given-names></name><name><surname>Rooney</surname><given-names>WL</given-names></name><name><surname>Shafian</surname><given-names>S</given-names></name><name><surname>Rajan</surname><given-names>N</given-names></name><name><surname>Rouze</surname><given-names>G</given-names></name><name><surname>Morgan</surname><given-names>CLS</given-names></name><name><surname>Neely</surname><given-names>HL</given-names></name></person-group><article-title>Others: unmanned aerial vehicles for high-throughput phenotyping and agronomic research</article-title><source>PLoS ONE</source><year>2016</year><volume>11</volume><issue>7</issue><fpage>1</fpage><lpage>26</lpage></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madec</surname><given-names>S</given-names></name><name><surname>Baret</surname><given-names>F</given-names></name><name><surname>deSolan</surname><given-names>B</given-names></name><name><surname>Thomas</surname><given-names>S</given-names></name><name><surname>Dutartre</surname><given-names>D</given-names></name><name><surname>Jezequel</surname><given-names>S</given-names></name><name><surname>Hemmerle</surname><given-names>M</given-names></name><name><surname>Colombeau</surname><given-names>G</given-names></name><name><surname>Comar</surname><given-names>A</given-names></name></person-group><article-title>High-throughput phenotyping of plant height: comparing unmanned aerial vehicles and ground lidar estimates</article-title><source>Front Plant Sci</source><year>2017</year><volume>8</volume><fpage>2002</fpage><pub-id pub-id-type="doi">10.3389/fpls.2017.02002</pub-id><pub-id pub-id-type="pmid">29230229</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Azzari G, Lobell DB. Satellite estimates of crop area and maize yield in zambia&#x02019;s agricultural districts. In: Proceedings of the AGU fall meeting; 2015</mixed-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lobell</surname><given-names>DB</given-names></name><name><surname>Thau</surname><given-names>D</given-names></name><name><surname>Seifert</surname><given-names>C</given-names></name><name><surname>Engle</surname><given-names>E</given-names></name><name><surname>Little</surname><given-names>B</given-names></name></person-group><article-title>A scalable satellite-based crop yield mapper</article-title><source>Remote Sens Environ</source><year>2015</year><volume>164</volume><fpage>324</fpage><lpage>333</lpage><pub-id pub-id-type="doi">10.1016/j.rse.2015.04.021</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Alharbi N, Zhou J, Wang W. Automatic counting of wheat spikes from wheat growth images. In: 7th international conference on pattern recognition applications and methods; 2018. p. 346&#x02013;55</mixed-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>C</given-names></name><name><surname>Liang</surname><given-names>D</given-names></name><name><surname>Yang</surname><given-names>X</given-names></name><name><surname>Yang</surname><given-names>H</given-names></name><name><surname>Yue</surname><given-names>J</given-names></name><name><surname>Yang</surname><given-names>G</given-names></name></person-group><article-title>Wheat ears counting in field conditions based on multi-feature optimization and TWSVM</article-title><source>Front Plant Sci</source><year>2018</year><volume>9</volume><fpage>1024</fpage><pub-id pub-id-type="doi">10.3389/fpls.2018.01024</pub-id><pub-id pub-id-type="pmid">30057587</pub-id></element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernandez-Gallego</surname><given-names>JA</given-names></name><name><surname>Kefauver</surname><given-names>SC</given-names></name><name><surname>Guti&#x000e9;rrez</surname><given-names>NA</given-names></name><name><surname>Nieto-Taladriz</surname><given-names>MT</given-names></name><name><surname>Araus</surname><given-names>JL</given-names></name></person-group><article-title>Wheat ear counting in-field conditions: high throughput and low-cost approach using RGB images</article-title><source>Plant Methods</source><year>2018</year><volume>14</volume><issue>1</issue><fpage>22</fpage><pub-id pub-id-type="doi">10.1186/s13007-018-0289-4</pub-id><pub-id pub-id-type="pmid">29568319</pub-id></element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>C</given-names></name><name><surname>Liang</surname><given-names>D</given-names></name><name><surname>Yang</surname><given-names>X</given-names></name><name><surname>Xu</surname><given-names>B</given-names></name><name><surname>Yang</surname><given-names>G</given-names></name></person-group><article-title>Recognition of wheat spike from field based phenotype platform using multi-sensor fusion and improved maximum entropy segmentation algorithms</article-title><source>Remote Sens</source><year>2018</year><volume>10</volume><issue>2</issue><fpage>246</fpage><pub-id pub-id-type="doi">10.3390/rs10020246</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilf</surname><given-names>P</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Chikkerur</surname><given-names>S</given-names></name><name><surname>Little</surname><given-names>SA</given-names></name><name><surname>Wing</surname><given-names>SL</given-names></name><name><surname>Terre</surname><given-names>T</given-names></name></person-group><article-title>Computer vision cracks the leaf code</article-title><source>Proc Natl Acad Sci</source><year>2016</year><volume>113</volume><fpage>3305</fpage><lpage>3310</lpage><pub-id pub-id-type="doi">10.1073/pnas.1524473113</pub-id><pub-id pub-id-type="pmid">26951664</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>P</given-names></name><name><surname>Huang</surname><given-names>C</given-names></name><name><surname>Cai</surname><given-names>J</given-names></name><name><surname>Miklavcic</surname><given-names>SJ</given-names></name></person-group><article-title>Root phenotyping by root tip detection and classification through statistical learning</article-title><source>Plant Soil</source><year>2014</year><volume>380</volume><issue>1</issue><fpage>193</fpage><lpage>209</lpage><pub-id pub-id-type="doi">10.1007/s11104-014-2071-3</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>P</given-names></name><name><surname>Cai</surname><given-names>J</given-names></name><name><surname>Miklavcic</surname><given-names>SJ</given-names></name></person-group><article-title>A complete system for 3D reconstruction of roots for phenotypic analysis</article-title><source>Adv Exp Med Biol</source><year>2015</year><volume>823</volume><fpage>249</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-10984-8_14</pub-id><pub-id pub-id-type="pmid">25381112</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>A</given-names></name><name><surname>Ganapathysubramanian</surname><given-names>B</given-names></name><name><surname>Singh</surname><given-names>AK</given-names></name><name><surname>Sarkar</surname><given-names>S</given-names></name></person-group><article-title>Machine learning for high-throughput stress phenotyping in plants</article-title><source>Trends Plant Sci</source><year>2016</year><volume>21</volume><issue>2</issue><fpage>110</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1016/j.tplants.2015.10.015</pub-id><pub-id pub-id-type="pmid">26651918</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadeghi-Tehran</surname><given-names>P</given-names></name><name><surname>Sabermanesh</surname><given-names>K</given-names></name><name><surname>Virlet</surname><given-names>N</given-names></name><name><surname>Hawkesford</surname><given-names>MJ</given-names></name></person-group><article-title>Automated method to determine two critical growth stages of wheat: heading and flowering</article-title><source>Front Plant Sci</source><year>2017</year><volume>8</volume><issue>252</issue><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="pmid">28220127</pub-id></element-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barre</surname><given-names>P</given-names></name><name><surname>Stover</surname><given-names>BC</given-names></name><name><surname>Muller</surname><given-names>KF</given-names></name><name><surname>Steinhage</surname><given-names>V</given-names></name></person-group><article-title>LeafNet: a computer vision system for automatic plant species identification</article-title><source>Ecol Inform</source><year>2017</year><volume>40</volume><fpage>50</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1016/j.ecoinf.2017.05.005</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Namin ST, Esmaeilzadeh M, Najafi M, Brown TB, Borevitz JO. Deep phenotyping: deep learning for temporal phenotype/genotype classification. bioRxiv; 2017. p. 1&#x02013;29</mixed-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ubbens</surname><given-names>JR</given-names></name><name><surname>Stavness</surname><given-names>I</given-names></name></person-group><article-title>Deep plant phenomics: a deep learning platform for complex plant phenotyping tasks</article-title><source>Front Plant Sci</source><year>2017</year><volume>8</volume><fpage>1190</fpage><pub-id pub-id-type="doi">10.3389/fpls.2017.01190</pub-id><pub-id pub-id-type="pmid">28736569</pub-id></element-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsaftaris</surname><given-names>SA</given-names></name><name><surname>Minervini</surname><given-names>M</given-names></name><name><surname>Scharr</surname><given-names>H</given-names></name></person-group><article-title>Machine learning for plant phenotyping needs image processing</article-title><source>Trends Plant Sci</source><year>2016</year><volume>21</volume><issue>12</issue><fpage>989</fpage><lpage>991</lpage><pub-id pub-id-type="doi">10.1016/j.tplants.2016.10.002</pub-id><pub-id pub-id-type="pmid">27810146</pub-id></element-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Girshick R, Donahue J, Darrell T, Malik T. Rich feature hierarchies for accurate object detection and semantic segmentation. In: 2014 IEEE conference on computer vision and pattern recognition; 2014. p. 580&#x02013;7</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Girshick R. Fast R-CNN. In: 2015 IEEE international conference on computer vision (ICCV); 2015. p. 1440&#x02013;48</mixed-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>He</surname><given-names>K</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><article-title>Faster R-CNN: towards real-time object detection with region proposal networks</article-title><source>IEEE Trans Pattern Anal Mach Intell</source><year>2017</year><volume>39</volume><issue>6</issue><fpage>1137</fpage><lpage>1149</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2577031</pub-id><pub-id pub-id-type="pmid">27295650</pub-id></element-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Faster-RCNN. <ext-link ext-link-type="uri" xlink:href="https://github.com/rbgirshick/py-faster-rcnn">https://github.com/rbgirshick/py-faster-rcnn</ext-link></mixed-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Everingham</surname><given-names>M</given-names></name><name><surname>Gool</surname><given-names>LV</given-names></name><name><surname>Williams</surname><given-names>CKI</given-names></name><name><surname>Winn</surname><given-names>J</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><article-title>The pascal visual object classes (VOC) challenge</article-title><source>Int J Comput Vis</source><year>2010</year><volume>88</volume><issue>2</issue><fpage>303</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1007/s11263-009-0275-4</pub-id></element-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liao</surname><given-names>H</given-names></name><name><surname>Yan</surname><given-names>X</given-names></name><name><surname>Rubio</surname><given-names>G</given-names></name><name><surname>Beebe</surname><given-names>S</given-names></name><name><surname>Blair</surname><given-names>M</given-names></name><name><surname>Lynch</surname><given-names>JP</given-names></name></person-group><article-title>Genetic mapping of basal root gravitropism and phosphorus acquisition efficiency in common bean</article-title><source>Funct Plant Biol</source><year>2004</year><volume>31</volume><issue>10</issue><fpage>959</fpage><lpage>970</lpage><pub-id pub-id-type="doi">10.1071/FP03255</pub-id></element-citation></ref></ref-list></back></article>