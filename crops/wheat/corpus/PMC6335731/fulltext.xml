<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.0 20120330//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.0?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">J Anal Methods Chem</journal-id><journal-id journal-id-type="iso-abbrev">J Anal Methods Chem</journal-id><journal-id journal-id-type="publisher-id">JAMC</journal-id><journal-title-group><journal-title>Journal of Analytical Methods in Chemistry</journal-title></journal-title-group><issn pub-type="ppub">2090-8865</issn><issn pub-type="epub">2090-8873</issn><publisher><publisher-name>Hindawi</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6335731</article-id><article-id pub-id-type="doi">10.1155/2019/1537568</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Tracing Geographical Origins of Teas Based on FT-NIR Spectroscopy: Introduction of Model Updating and Imbalanced Data Handling Approaches</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Hong</surname><given-names>Xue-Zhen</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref><xref ref-type="aff" rid="I2">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Fu</surname><given-names>Xian-Shu</given-names></name><xref ref-type="aff" rid="I3">
<sup>3</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Zheng-Liang</given-names></name><xref ref-type="aff" rid="I3">
<sup>3</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Li</given-names></name><xref ref-type="aff" rid="I4">
<sup>4</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Yu</surname><given-names>Xiao-Ping</given-names></name><xref ref-type="aff" rid="I3">
<sup>3</sup>
</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-5827-3699</contrib-id><name><surname>Ye</surname><given-names>Zi-Hong</given-names></name><email>zhye@cjlu.edu.cn</email><xref ref-type="aff" rid="I3">
<sup>3</sup>
</xref></contrib></contrib-group><aff id="I1">
<sup>1</sup>College of Quality &#x00026; Safety Engineering, China Jiliang University, Xueyuan Street, Xiasha Higher Education District, Hangzhou 310018, China</aff><aff id="I2">
<sup>2</sup>BioCircuits Institute, University of California, La Jolla, San Diego, CA 92093, USA</aff><aff id="I3">
<sup>3</sup>Zhejiang Provincial Key Laboratory of Biometrology and Inspection &#x00026; Quarantine, College of Life Sciences, China Jiliang University, Xueyuan Street, Xiasha Higher Education District, Hangzhou 310018, China</aff><aff id="I4">
<sup>4</sup>Department of Computer Science, Zhejiang University, Hangzhou 310027, China</aff><author-notes><fn fn-type="other"><p>Guest Editor: Andrey Bogomolov</p></fn></author-notes><pub-date pub-type="collection"><year>2019</year></pub-date><pub-date pub-type="epub"><day>3</day><month>1</month><year>2019</year></pub-date><volume>2019</volume><elocation-id>1537568</elocation-id><history><date date-type="received"><day>3</day><month>8</month><year>2018</year></date><date date-type="accepted"><day>29</day><month>11</month><year>2018</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2019 Xue-Zhen Hong et al.</copyright-statement><copyright-year>2019</copyright-year><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><abstract><p>This work presents a reliable approach to trace teas' geographical origins despite changes in teas caused by different harvest years. A total of 1447 tea samples collected from various areas in 2014 (660 samples) and 2015 (787 samples) were detected by FT-NIR. Seven classifiers trained on the 2014 dataset all succeeded to trace origins of samples collected in 2014; however, they all failed to predict origins for the 2015 samples due to different data distributions and imbalanced dataset. Three outlier detection based undersampling approaches&#x02014;one-class SVM (OC-SVM), isolation forest and elliptic envelope&#x02014;were then proposed; as a result, the highest macro average recall (MAR) for the 2015 dataset was improved from 56.86% to 73.95% (by SVM). A model updating approach was also applied, and the prediction MAR was significantly improved with increase in the updating rate. The best MAR (90.31%) was first achieved by the OC-SVM combined SVM classifier at a 50% rate.</p></abstract><funding-group><award-group><funding-source>Key Research and Development Program of Zhejiang Province</funding-source><award-id>2018C02041</award-id></award-group><award-group><funding-source>National Key Research and Development Program of China</funding-source><award-id>2016YFF0202305</award-id><award-id>2017YFF0210200</award-id></award-group><award-group><funding-source>Science and Technology Development of AQSIQ</funding-source><award-id>2016QK190</award-id></award-group></funding-group></article-meta></front><body><sec id="sec1"><title>1. Introduction</title><p>Tea is one of the most widely consumed beverages in the world because of its pleasurable taste, aroma, and healthy effects [<xref rid="B1" ref-type="bibr">1</xref>]. Freshly harvested tea leaves are processed differently to produce specific types of tea, such as unfermented green tea, fully fermented black tea and semifermented oolong tea. Of all the types, oolong has been proven to be able to reduce obesity and control diabetes [<xref rid="B2" ref-type="bibr">2</xref>].</p><p>Teas from different geographical origins vary in their quality due to different climate conditions (e.g., temperature, sun exposure time, and rainfall), soil, growth altitude, and so on. These factors would affect chemical compositions [<xref rid="B3" ref-type="bibr">3</xref>] of teas and thus determine their special aroma and taste&#x02014;eventually their market prices. Many countries have published relevant regulations such as geographical indication (GI) certification to protect valuable products originated from certain areas. For example, as a GI product, the famous Wuyi rock-essence (WY) tea only grows in certain areas of Wuyishan city, Fuijan province of China (GB/T 18745&#x02013;2006) [<xref rid="B4" ref-type="bibr">4</xref>]. However, driven by profits, special local teas are often replaced by inferior teas from other origins in the tea market. Therefore, it is of significant importance to develop reliable geographical origin tracing approaches for teas.</p><p>Among numerous techniques employed for origin tracing in the last decade, near-infrared (NIR) spectroscopy combined with pattern recognition methods has proved to be a valuable tool. Based on vibrational responses of chemical bonds to NIR radiation, this technique could provide different spectral fingerprints for products from different geographical origins. Many researchers have successfully applied NIR for origin tracing of honeys [<xref rid="B5" ref-type="bibr">5</xref>], olive oils [<xref rid="B6" ref-type="bibr">6</xref>], nuts [<xref rid="B7" ref-type="bibr">7</xref>], fruits [<xref rid="B8" ref-type="bibr">8</xref>], wines [<xref rid="B9" ref-type="bibr">9</xref>], wheats [<xref rid="B10" ref-type="bibr">10</xref>], herbal medicines [<xref rid="B11" ref-type="bibr">11</xref>], teas [<xref rid="B12" ref-type="bibr">12</xref>], and so on. In the case of teas, research studies can be subdivided into identifying teas from different nations [<xref rid="B13" ref-type="bibr">13</xref>], different areas within one nation [<xref rid="B14" ref-type="bibr">14</xref>], and different locations within the same geographical origin [<xref rid="B15" ref-type="bibr">15</xref>].</p><p>However, there exist some problems: First, in most cases, 60% to 75% of data from each group were selected for training. This kind of data-splitting strategy tends to raise the risk of over-fitting especially when each group consists of replicate samples; therefore, classifiers built based on this splitting strategy often give over-optimistic performances but would probably fail to predict data with different distributions.</p><p>Second, apart from the origin factor, chemical composition of tea leaves also varies depending on the &#x0201c;time&#x0201d; factor, such as harvesting timing, storage time, etc. [<xref rid="B16" ref-type="bibr">16</xref>]. In fact, NIR has also been reported to successfully discriminate teas with different storage periods [<xref rid="B17" ref-type="bibr">17</xref>] and harvest timing [<xref rid="B18" ref-type="bibr">18</xref>]. This raises the following question: how do the time factor and its interaction with the origin factor affect teas' NIR fingerprints for geographical traceability? Or is it possible to build a reliable NIR-based origin tracing model, despite changes in teas caused by other factors?</p><p>As far as we know, few research studies have been reported on this issue. Though there was a similar research on wheat, the result was not satisfying: Zhao et al. [<xref rid="B19" ref-type="bibr">19</xref>] tried to discriminate origins of wheats harvested from 2007/2008 and 2008/2009 using NIR; as a result, tracing models trained by data from one certain year could successfully predict samples from the same year, but totally failed to predict samples from the other year (only 39.2% accuracy).</p><p>In this research, a rapid and reliable geographical origin tracing approach based on Fourier transform near-infrared (FT-NIR) spectroscopy was proposed. A total of 1447 oolong tea samples from various origins were collected in 2014 and 2015 for the experiment. Seven classification algorithms were employed, and origin tracing models built based on them were compared. Three undersampling techniques as well as a model updating approach were also proposed to improve reliability of these models. The main objectives of this research were (1) to explore how the geographical origin factor, harvest time factor, and their interaction would affect teas' NIR fingerprints for geographical traceability; (2) to find solutions for classification tasks involved with imbalanced dataset and different data distributions; and (3) to build a reliable and robust origin tracing model for teas despite the changes in teas due to different harvest years.</p></sec><sec id="sec2"><title>2. Materials and Methods</title><sec id="sec2.1"><title>2.1. Sample Preparation</title><p>The famous WY tea was selected as a representative of oolong for the geographical origin tracing research. To train a practical origin tracing model and avoid over-fitting, we tried to include as much diversity within each group as possible, i.e., tea samples for each origin (group) were collected from various areas (tea gardens) instead of just a few areas (with multiple replicates).</p><p>The whole research includes a two-year experiment. In 2014, 660 tea samples were collected to build a fast origin tracing model. Out of the 660 samples, 495 were authentic WY samples collected from 33 areas within the protected geographical origin, and 165 were Non-WY (NWY) oolong teas collected from 11 areas outside of the protected origin. In 2015, 787 tea samples were collected to validate if the origin tracing model trained by data from 2014 was still reliable and practical for tea samples from 2015. Out of the 787 samples, 687 were authentic WY samples collected from 74 areas within the protected origin, and 100 were NWY samples collected from 10 areas outside of the protected origin.</p><p>All the samples were spring teas (purchased before June) and were preserved in cold storage (4&#x000b0;C) before measuring. A description of the four tea groups was presented in <xref rid="tab1" ref-type="table">Table 1</xref>.</p></sec><sec id="sec2.2"><title>2.2. NIR Sampling Procedure</title><p>A TENSOR37 FT-NIR spectrometer (Bruker, Ettlingen, Germany) was employed for the research. Each tea sample was packed in a quartz cuvette and detected with a PbS detector. Average values of 64 scanning spectra ranging from 4000&#x02009;cm<sup>&#x02212;1</sup> to 12000&#x02009;cm<sup>&#x02212;1</sup> were considered as the raw NIR data. Since the resolution was set as 8&#x02009;cm<sup>&#x02212;1</sup> and the scanning interval was set as 1.928&#x02009;cm<sup>&#x02212;1</sup>, there were in total 4148 data points in the raw spectrum for each tea sample. Thus, the size of the raw dataset (2014 and 2015) was 1447 samples &#x000d7; 4148 variables. All the measurements were carried out at a room temperature of 25 &#x000b1; 1&#x000b0;C.</p></sec><sec id="sec2.3"><title>2.3. Statistical Analysis and Pattern Recognition Methods</title><sec id="sec2.3.1"><title>2.3.1. Supervised Classifiers</title><p>Many classifiers have been widely applied for the analysis of NIR data. However, there exists no classifier that would always outperform others in every case. Thus, it would be better to try several common classifiers. In this paper, seven classifiers including both parameter and nonparameter approaches were applied to build geographical origin tracing models, and performances of these classifiers were compared. The classifiers employed here were as follows: linear discriminant analysis (LDA), support vector machine (SVM), stochastic gradient descent- (SGD-) based classifier, decision tree (DT), adaptive boosting- (AdaBoost-) based classifier, random forest (RF), and multilayer perceptron (MLP).</p><p>Developed by Fisher, LDA is a supervised approach that constructs discriminant functions through linear combination of labeled data. LDA consists of two stages: separation and allocation. The former stage is to find discriminant functions that can separate the groups well, and the later stage is to assign an unknown object to one of the groups using the discriminant functions.</p><p>SVM, which has been one of the most successful machine learning technique for the past decade, is a machine working in the high dimensional feature space formed by mapping of <italic>n</italic>-dimensional input vector into a <italic>K</italic>-dimensional feature space (<italic>K</italic> &#x0003e; <italic>n</italic>) through the use of a kernel function. The kernel function used here was radial basis function (RBF), which is able to reduce the computational complexity of the training procedure and to give a good performance under general smoothness assumptions [<xref rid="B20" ref-type="bibr">20</xref>]. The optimum parameter combination of gamma and <italic>C</italic> were found to be gamma = 0.001, <italic>C</italic> = 1.</p><p>SGD, which has been widely employed in solving large-scale machine learning problems, is a simple yet very efficient approach to discriminative learning of classifiers under convex loss functions such as linear SVM and logistic regression [<xref rid="B21" ref-type="bibr">21</xref>]. The loss function used here was linear SVM.</p><p>DT, which is a nonparametric learning method, predicts the value of a target variable by learning simple decision rules inferred from the data features. There are two common issues for the construction of decision trees: (a) the growth of the tree to enable it to accurately categorize the training dataset and (b) the pruning stage, whereby superfluous nodes and branches are removed in order to improve classification accuracy [<xref rid="B22" ref-type="bibr">22</xref>].</p><p>An AdaBoost classifier begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted [<xref rid="B23" ref-type="bibr">23</xref>]. In this paper, the base estimator from which the boosted ensemble was built was DTs, and the boosting algorithm used was the SAMME.R [<xref rid="B24" ref-type="bibr">24</xref>].</p><p>RF is a metaestimator that fits a number of DT classifiers on various subsamples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. Each tree gives a classification, and the majority vote of trees in the forest is used to determine the final class [<xref rid="B25" ref-type="bibr">25</xref>]. In this research, the number of trees in the forest was set as 15.</p><p>MLP is composed of one input layer with <italic>&#x003c1;</italic> inputs, one or more hidden layers with <italic>n</italic> hidden neurons, and one output layer with <italic>q</italic> outputs. Selection of the optimum number of hidden layers as well as neurons in the hidden layers is important but also complicated. If an inadequate number of neurons are used, the network will be unable to model complex data, and the resulting fit will be poor. If too many neurons are used, the training time may become excessively long, and, worse, the network may over fit the data. The MLP employed here contained one hidden layer with 15 neurons, and error back propagation learning algorithm based on the Levenberg&#x02013;Marquardt algorithm was used to train the neural network.</p></sec><sec id="sec2.3.2"><title>2.3.2. Outlier Detection Based Undersampling Approaches for Imbalanced Classification</title><p>As described before, the WY and NWY classes were not represented equally: the ratio of 2014IN to 2014OUT was 695 to 165 (that is, 3&#x02009;:&#x02009;1), and that of 2015IN to 2015OUT was 687 to 100 (that is, 6.87&#x02009;:&#x02009;1). Imbalanced data like this could cause frustration because classifiers tend to favor the larger class, e.g., the classifiers might classify most of the tea samples as WY teas. Thus, it is important to balance classes in the training data.</p><p>The main objective of balancing classes is to either increasing the frequency of the minority class (over-sampling) or decreasing the frequency of the majority class (undersampling). In this research, three outlier detection based undersampling approaches&#x02014;one-class SVM (OC-SVM), isolation forest (IF), and elliptic envelope (EE)&#x02014;were proposed, and contamination of the lager class was set (among 0.38 to 0.55) with the goal of balancing the size of the WY and NWY classes.</p><p>Developed by Sch&#x000f6;lkopf et al. [<xref rid="B26" ref-type="bibr">26</xref>], the OC-SVM tries to find a hyper sphere which has maximal distance from the origin in feature space <italic>F</italic> and separates all the data points from the origin. This results in a binary function which captures regions in the input space where the probability density of the data lives. In general, the OC-SVM gives useful results in situations such as outlier detection in high dimension, or without any assumptions on the distribution of the inlying data.</p><p>IF builds an ensemble of trees for a given dataset; then anomalies are those instances which have short average path lengths on the trees [<xref rid="B27" ref-type="bibr">27</xref>]. This approach utilizes no distance or density measures to detect anomalies, which eliminates major computational cost of distance calculation in all distance-based and density-based methods.</p><p>EE is a common outlier detection approach that assumes data come from a known distribution (e.g., Gaussian distributed). From this assumption, this approach fits a robust covariance estimate to the data, and thus fits an ellipse to the central data points, ignoring points outside the central mode.</p></sec><sec id="sec2.3.3"><title>2.3.3. Performance Measures for Imbalanced Classification</title><p>Accuracy is a popular performance measure index for classification tasks. However, as mentioned before, datasets for this research were not balanced. Thus, even if we build a completely useless classifier that classifies all the samples from 2015 as WY teas, we could still get an accuracy of 87.3% (687/787).</p><p>In such cases, recall and macro average recall (MAR) are more important. The recall for a class is the number of true positives (i.e., the number of items correctly labeled as belonging to the positive class) divided by the total number of elements that actually belong to the positive class (i.e., the sum of true positives and false negatives), and the MAR is the average of recall values for all classes (gives equal weight to each class).</p></sec><sec id="sec2.3.4"><title>2.3.4. Model Updating</title><p>For a model to predict accurately, the data which are making predictions on must have a similar distribution as the data on which the model was trained. Thus, it is often a good practice to continuously monitor the incoming data and retrain the model on newer data when the new data distribution has deviated significantly from the original training data distribution.</p><p>In this research, because data distributions of the 2014 and 2015 groups were different, a model updating approach with different updating ratios&#x02014;10% to 60%&#x02014;was applied. During the model updating process, the 2015 dataset was randomly divided into 10 parts: part <italic>X</italic> was added to retrain the model, and the last four parts (parts 7&#x02013;10) were considered as the testing set. <italic>X</italic> here could be any positive integers in the range of 1 to 6, representing the updating rate of 10% to 60%. This random-split process was repeated 100 times, and the average testing results were recorded for later analysis.</p></sec><sec id="sec2.3.5"><title>2.3.5. Other Approaches Employed and Software Implemented</title><p>Principle component analysis (PCA) was employed to extract features from the original variables. One-way and two-ways analysis of variance (ANOVA) were performed to determine if there were differences among different groups, and Tukey's multiple comparison was performed to separate the means at <italic>P</italic> &#x0003c; 0.05.</p><p>All data analysis procedures were performed using Python 3, mostly the scikit-learn tool, which is a simple and efficient tool for data mining and data analysis.</p></sec></sec></sec><sec id="sec3"><title>3. Results and Discussion</title><sec id="sec3.1"><title>3.1. NIR Responses to the Four Tea Groups</title><p>NIR responses (average values) to the four tea groups collected from different geographical origins and harvest years (marked as 2014IN, 2014OUT, 2015IN, and 2015OUT) are presented in <xref ref-type="fig" rid="fig1">Figure 1</xref>, where the <italic>x</italic>-axis represents the spectra scanning range, and the <italic>y</italic>-axis represents the NIR response. The wavenumbers from 9000 to 12000&#x02009;cm<sup>&#x02212;1</sup> were excluded due to their lower sensitivity and signal-to-noise ratio [<xref rid="B14" ref-type="bibr">14</xref>].</p><p>As observed in <xref ref-type="fig" rid="fig1">Figure 1</xref>, there are some intensive spectral peaks in the region of 9000&#x02013;4000&#x02009;cm<sup>&#x02212;1</sup>. These peaks were mainly generated by the stretch or deformation vibration of C-H, N-H, O-H, and C=O bonds, which are the primary structural components of organic molecules [<xref rid="B28" ref-type="bibr">28</xref>]. For example, the peak around 8330&#x02009;cm<sup>&#x02212;1</sup> was induced by the second overtone of -C-H stretching, the peaks in 6000 to 7000&#x02009;cm<sup>&#x02212;1</sup> were mainly induced by the O-H and N-H stretching, the peaks in 5500 to 6000&#x02009;cm<sup>&#x02212;1</sup> were mainly induced by the fundamental stretching of -C-H, the -CH<sub>2</sub>, and the -CH<sub>3</sub> overtone, the peaks around 5200&#x02009;cm<sup>&#x02212;1</sup> were induced by the combination of O-H and C-O stretching, the peaks around 4700&#x02009;cm<sup>&#x02212;1</sup> were induced by the combination of O-H bending and C-O stretching, and the peaks around 4300&#x02009;cm<sup>&#x02212;1</sup> were induced by the combination of C-H stretching and -CH<sub>2</sub> deformation. These vibrations were mainly caused by ingredients in teas such as catechins, polyphenols, alkaloids, volatile and nonvolatile acid, and some aroma compounds [<xref rid="B29" ref-type="bibr">29</xref>].</p><p>It is interesting to notice that the teas from the same harvest year (e.g., 2014IN and 2014OUT groups or 2015IN and 2015OUT groups) have much closer NIR responding curves than those from the same geographical origin (e.g., 2014IN and 2015IN groups or 2014OUT and 2015OUT groups) in the scanning range of 9000 to 5000&#x02009;cm<sup>&#x02212;1</sup>, suggesting the &#x0201c;harvest year&#x0201d; factor played a much bigger role than the &#x0201c;geographical origin&#x0201d; factor in that region. What is more interesting is, in the range of 9000 to 7250&#x02009;cm<sup>&#x02212;1</sup>, responding curves of the 2015IN and 2015OUT groups are separable while those of the 2014IN and 2014OUT groups are mostly overlapped; however, a totally reversed case is found in the range of 7250 to 5200&#x02009;cm<sup>&#x02212;1</sup>, where the responding curves of the 2015IN and 2015OUT groups are mostly overlapped while the 2014IN and 2014OUT groups are separable. Meanwhile, in the scanning range of 5000 to 4000&#x02009;cm<sup>&#x02212;1</sup>, the four groups seem to have relatively close responding curves. This does not seem great for our origin tracing tasks.</p><p>In summary, the NIR responses to teas were affected by both the &#x0201c;harvest year&#x0201d; and &#x0201c;geographical origin&#x0201d; factors, and the &#x0201c;harvest year&#x0201d; factor had a greater weight on most of the original spectral variables. This indicates that origin tracing models trained by data from one certain year might not be suitable for tea samples from the other year. Thus, it is important to eliminate the influence of the &#x0201c;harvest year&#x0201d; factor and find a feature subset that could better describe the differences in teas contributed by the &#x0201c;geographical origin&#x0201d; factor.</p></sec><sec id="sec3.2"><title>3.2. Preprocessing and Transformation of the Original NIR Data</title><p>PCA was applied for feature extraction. Meanwhile, since teas collected from different years have different data distribution centers (i.e., different group means), a standard scaling preprocessing approach was also employed to eliminate the influence of the &#x0201c;harvest year&#x0201d; factor, and data features from different years were all scaled to the [0, 1] range.</p></sec><sec id="sec3.3"><title>3.3. Discrimination of Teas Collected from Different Geographical Origins in 2014</title><p>The 660 tea samples from 2014 (495 in 2014IN and 165 in 2014OUT) were used to train and test origin classification models. Seven classifiers&#x02014;LDA, SVM, SGD, DT, RF, AdaBoost, and MLP&#x02014;were applied. To evaluate performances of these classifiers for teas collected from 2014, 70% of the dataset (462 sample) was randomly chosen as the training set, and the rest 30% was considered as the testing set. This random-split process was repeated 100 times, and the average training and testing results were recorded in <xref rid="tab2" ref-type="table">Table 2</xref>, where recall 1 and recall 2 were recall values for the WY and the NWY groups, respectively.</p><p>As observed in <xref rid="tab2" ref-type="table">Table 2</xref>, even though the LDA, SVM, DT, AdaBoost, and MLP classifiers were significantly better (alpha = 0.01) than the SGD and the RF classifiers in the case of the training set, all seven classifiers achieved high recall values with a minimum MAR value (equally weighted average of recall 1 and recall 2) of 98.95%. In the case of the testing set, all seven classifiers were able to discriminate the WY tea samples properly (recall 1 &#x02265; 91.32%); yet none of them could reach a recall2 value higher than 90%. This might be explained by the following two reasons: (1) the ratio of 2014IN to 2014OUT was 695 to 165; therefore, these classifiers tended to favor the larger group (WY group) by classifying more NWY samples as WY samples. (2) Data distributions of the 2014IN and 2014OUT groups were not far away from each other, and there might also be outliers around the potential decision boundaries, making it hard to draw clear decision boundaries that could fully discriminate the two groups. After calculating the prediction MAR for the testing set, the LDA and MLP classifiers were found to be the best (MAR &#x02265; 93%), followed by three equally good classifiers&#x02014;the SVM (MAR = 89.65%), SGD (MAR = 88.51%), and AdaBoost (MAR = 88.27%) classifiers. Meanwhile, even though the DT classifier got the lowest MAR value (80.89%), it was still higher than 80%.</p><p>In general, all seven classifiers succeeded to predict geographical origins for the tea samples from 2014. This coincides with other researchers' works, where classifiers trained and tested by data from a one-time experiment always have good prediction accuracy. Now, our next question is, Are these origin tracing models reliable and robust? Or will they still be able to correctly classify tea samples from the next year?</p></sec><sec id="sec3.4"><title>3.4. Validation of the Origin Tracing Models Using the 2015 Tea Samples</title><p>To validate the aforementioned origin classification models, 787 new tea samples (687 WY teas and 100 NWY teas) were collected in 2015, and predictions of these new samples based on the seven classifiers were given in the last three columns of <xref rid="tab2" ref-type="table">Table 2</xref>.</p><p>As observed in <xref rid="tab2" ref-type="table">Table 2</xref>, the results are far from satisfactory. For all seven classifiers, there were significant descending in recall values: the prediction recall for the 687 new WY samples (recall 1) ranged from 58.42% to 90.74%, with the highest value achieved by the SVM (90.74%) and the lowest value provided by the MLP (58.42%); the prediction recall for the 100 new NWY samples was much worse&#x02014;the recall2 ranged from 16.38% to 26.22%. In general, all seven classifiers failed to correctly predict the origins of the new tea samples from 2015. Even though the SVM classifier was significantly better than the other six classifiers at the 0.01 level, it only got a MAR of 56.86%, suggesting the origin tracing models trained by the 2014 dataset could not be directly applied to the 2015 dataset.</p><p>This phenomenon might be explained by the following two reasons: (1) as we mentioned before, the 2014 dataset was slightly imbalanced, and there were outliers around the decision boundary. Therefore, classifiers built based on the 2014 dataset tended to favor the WY tea group. (2) underlying data distributions of the 2014 and the 2015 datasets were not exactly the same. Compared to the 2014OUT group, data distribution of the 2015OUT group was closer to the decision boundaries. Therefore, most of data points from the 2015OUT group were distributed on the wrong side (2014IN group) of the already biased decision boundaries.</p><p>It is also worth noticing that the LDA and MLP classifiers, which were proven to be the best for the prediction of the 2014 dataset, are now significantly worse than the other approaches. This is because LDA usually generates good power with strong assumptions of multivariate normality of the explanatory variables and equal covariance among different populations; yet these assumptions are rare in practice. In this case, the discriminant functions that could separate the 2014 dataset well were not suitable for the 2015 dataset. As to the MLP, though it does not make any assumption regarding the underlying probability density functions or other probabilistic information about the pattern classes in comparison to other probability based models, it could easily get over-fitting especially when it was trained by a relatively small dataset.</p></sec><sec id="sec3.5"><title>3.5. Model Improving and Updating</title><p>In order to improve the origin identification models, three issues&#x02014;imbalanced dataset, outliers, and different data distributions from the 2014 and 2015 datasets&#x02014;must be solved.</p><sec id="sec3.5.1"><title>3.5.1. Dealing with Imbalanced Classification and Outliers</title><p>Three outlier detection based undersampling approaches&#x02014;IF, OC-SVM, and EE&#x02014;were employed to solve the issues of &#x0201c;imbalanced classification&#x0201d; and &#x0201c;outliers&#x0201d; in the 2014 dataset, and the results are presented in <xref rid="tab3" ref-type="table">Table 3</xref>.</p><p>As observed in <xref rid="tab3" ref-type="table">Table 3</xref>, no matter which of the three undersampling approaches was used, prediction MAR for the 2015 dataset based on the seven classifiers (especially SVM) were all improved. This improvement was achieved because of the significant increase in the prediction recall for the 100 NWY tea samples&#x02014;the recall2 now ranges from 37% to as high as 76%, in contrast to the original range of 16.38% to 26.22%. However, it is noticeable that the prediction recall for the 687 WY tea samples was actually descending. This is in favor of our assumption that both the 2015IN and 2015OUT groups were closer to the decision boundaries than the 2014 groups. Therefore, when the issues of imbalanced dataset and outliers were taken care of, changes in new decision boundaries that favored the 2015OUT group would inevitably affect the 2015IN group.</p><p>In general, the OC-SVM and IF approaches, which are able to recover reasonable approximations even when the inlier distribution is strongly non-Gaussian, were better than the EE approach. Meanwhile, the highest prediction MAR for the 2015 dataset was improved from 56.86% to 73.95% by the SVM approach.</p></sec><sec id="sec3.5.2"><title>3.5.2. Dealing with Different Data Distributions</title><p>A model updating approach with updating ratio of 10% to 60% was applied to further improve both the recall 1 and recall 2 for the tea samples from 2015. The idea of this updating approach is really simple: by adding some data from 2015 into the training process, the retrained classifier could find a more accurate discriminate boundary. Since the SVM classifier was better than the other classifiers in general, only the SVM classifiers combined with the OC-SVM and IF approaches were chosen for the model updating analysis.</p><p>
<xref ref-type="fig" rid="fig2">Figure 2</xref> visually displays how the prediction MAR for the testing set changed at different updating ratios under the two undersampling approaches.</p><p>As observed in <xref ref-type="fig" rid="fig2">Figure 2</xref>, for both the IF and OC-SVM combined SVM classifiers, prediction MAR increased&#x02014;but the increasing rate decreased&#x02014;with increase in the updating ratio. In general, the OC-SVM was better than the IF except at the ratio of 10%, where the IF seemed to have a slightly higher MAR value. Two-way ANOVA and post hoc test were then applied to analyze the results, and the interaction effect between the undersampling approach and updating ratio was found significant at 0.01 level (<xref rid="tab4" ref-type="table">Table 4</xref>).</p><p>As observed in <xref rid="tab4" ref-type="table">Table 4</xref>, for both the IF and OC-SVM combined SVM classifiers, identification performance for the WY (recall 1) and NWY (recall 2) teas got better with increase in the updating ratio up to 50%&#x02014;there was no significant difference between the updating rate of 50% and 60% at 0.01 level. The best prediction performance for the tea samples from 2015 was first achieved by the OC-SVM combined SVM classifier at an updating rate of 50% (MAR = 90.31%). Meanwhile, even though the IF seemed to have a slightly higher MAR value than the OC-SVM at 10%, there was no significant difference between these two undersampling approaches at this updating rate.</p></sec></sec></sec><sec id="sec4"><title>4. Conclusions</title><p>It could be concluded from this work that both the harvest time and geographical origin factors had significant effect on NIR fingerprints of teas, and the harvest time factor seemed to have a bigger weight. Meanwhile, classification algorithms tended to favor the larger group for imbalanced datasets. Thus, if applied directly without solving the issues of different data distributions, outliers and imbalanced datasets, the origin tracing models trained by tea samples from one certain year would fail to predict tea samples collected from the other years.</p><p>This work also demonstrates that OC-SVM is a good outlier detection approach, and the OC-SVM based undersampling approach could be a powerful tool for imbalanced classification. The proposed model updating and outlier detection based undersampling approaches make it possible to build a reliable and robust origin tracing model that could be applied for teas collected from different years.</p><p>In summary, data distributions can be expected to drift over time, deploying a model is not a one-time exercise but rather a continuous process. Inspired by this research, our next plan is to adjust this model for teas with different storage times on a monthly basis.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors are grateful to the financial support from the Key Research and Development Program of Zhejiang Province (no. 2018C02041), the National Key Research and Development Program of China (nos. 2016YFF0202305 and 2017YFF0210200), and the Science and Technology Development of AQSIQ (no. 2016QK190).</p></ack><sec sec-type="data-availability"><title>Data Availability</title><p>The data used to support the findings of this study are available from the corresponding author upon request.</p></sec><sec><title>Conflicts of Interest</title><p>The authors declare that they have no conflicts of interest.</p></sec><ref-list><ref id="B1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rashidi</surname><given-names>B.</given-names></name><name><surname>Malekzadeh</surname><given-names>M.</given-names></name><name><surname>Goodarzi</surname><given-names>M.</given-names></name><name><surname>Masoudifar</surname><given-names>A.</given-names></name><name><surname>Mirzaei</surname><given-names>H.</given-names></name></person-group><article-title>Green tea and its anti-angiogenesis effects</article-title><source><italic toggle="yes">Biomedicine and Pharmacotherapy</italic></source><year>2017</year><volume>89</volume><fpage>949</fpage><lpage>956</lpage><pub-id pub-id-type="doi">10.1016/j.biopha.2017.01.161</pub-id><pub-id pub-id-type="other">2-s2.0-85014850827</pub-id><pub-id pub-id-type="pmid">28292023</pub-id></element-citation></ref><ref id="B2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ng</surname><given-names>K. W.</given-names></name><name><surname>Cao</surname><given-names>Z. J.</given-names></name><name><surname>Chen</surname><given-names>H. B.</given-names></name><name><surname>Zhao</surname><given-names>Z. Z.</given-names></name><name><surname>Zhu</surname><given-names>L.</given-names></name><name><surname>Yi</surname><given-names>T.</given-names></name></person-group><article-title>Oolong tea: a critical review of processing methods, chemical composition, health effects and risk</article-title><source><italic toggle="yes">Critical Reviews in Food Science and Nutrition</italic></source><year>2017</year><fpage>1</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1080/10408398.2017.1347556</pub-id></element-citation></ref><ref id="B3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasquini</surname><given-names>B.</given-names></name><name><surname>Orlandini</surname><given-names>S.</given-names></name><name><surname>Goodarzi</surname><given-names>M.</given-names></name><name><surname>Caprini</surname><given-names>C.</given-names></name><name><surname>Gotti</surname><given-names>R.</given-names></name><name><surname>Furlanetto</surname><given-names>S.</given-names></name></person-group><article-title>Chiral cyclodextrin-modified micellar electrokinetic chromatography and chemometric techniques for green tea samples origin discrimination</article-title><source><italic toggle="yes">Talanta</italic></source><year>2016</year><volume>150</volume><fpage>7</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1016/j.talanta.2015.12.003</pub-id><pub-id pub-id-type="other">2-s2.0-84949937740</pub-id><pub-id pub-id-type="pmid">26838375</pub-id></element-citation></ref><ref id="B4"><label>4</label><element-citation publication-type="book"><collab>GB/T 18745-2006</collab><source><italic toggle="yes">Product of Geographical Indication. Wuyi Rock-Essence Tea</italic></source><year>2006</year><publisher-loc>Beijing, China</publisher-loc><publisher-name>National Standard of the People&#x02019;s Republic of China</publisher-name></element-citation></ref><ref id="B5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>L.</given-names></name><name><surname>Liu</surname><given-names>H.</given-names></name><name><surname>Zhang</surname><given-names>B.</given-names></name><name><surname>Wu</surname><given-names>D.</given-names></name></person-group><article-title>Application of electronic nose with multivariate analysis and sensor selection for botanical origin identification and quality determination of honey</article-title><source><italic toggle="yes">Food and Bioprocess Technology</italic></source><year>2014</year><volume>8</volume><issue>2</issue><fpage>359</fpage><lpage>370</lpage><pub-id pub-id-type="doi">10.1007/s11947-014-1407-6</pub-id><pub-id pub-id-type="other">2-s2.0-84926621003</pub-id></element-citation></ref><ref id="B6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>P.</given-names></name><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>He</surname><given-names>Y.</given-names></name></person-group><article-title>Identification of geographical origin of olive oil using visible and near-infrared spectroscopy technique combined with chemometrics</article-title><source><italic toggle="yes">Food and Bioprocess Technology</italic></source><year>2009</year><volume>5</volume><issue>1</issue><fpage>235</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1007/s11947-009-0302-z</pub-id><pub-id pub-id-type="other">2-s2.0-84856226020</pub-id></element-citation></ref><ref id="B7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vitale</surname><given-names>R.</given-names></name><name><surname>Bevilacqua</surname><given-names>M.</given-names></name><name><surname>Bucci</surname><given-names>R.</given-names></name><name><surname>Magr&#x000ec;</surname><given-names>A. D.</given-names></name><name><surname>Magr&#x000ec;</surname><given-names>A. L.</given-names></name><name><surname>Marini</surname><given-names>F.</given-names></name></person-group><article-title>A rapid and non-invasive method for authenticating the origin of pistachio samples by NIR spectroscopy and chemometrics</article-title><source><italic toggle="yes">Chemometrics and Intelligent Laboratory Systems</italic></source><year>2012</year><volume>121</volume><fpage>90</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1016/j.chemolab.2012.11.019</pub-id><pub-id pub-id-type="other">2-s2.0-84872871457</pub-id></element-citation></ref><ref id="B8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lyu</surname><given-names>Q.</given-names></name><name><surname>Liao</surname><given-names>Q.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Lan</surname><given-names>Y.</given-names></name></person-group><article-title>Feasibility of SSC prediction for navel orange based on origin recognition using NIR spectroscopy</article-title><source><italic toggle="yes">Intelligent Automation and Soft Computing</italic></source><year>2015</year><volume>21</volume><issue>3</issue><fpage>305</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1080/10798587.2015.1015775</pub-id><pub-id pub-id-type="other">2-s2.0-84928420572</pub-id></element-citation></ref><ref id="B9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>F.</given-names></name><name><surname>Yang</surname><given-names>D.</given-names></name><name><surname>Ying</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>B.</given-names></name><name><surname>Zheng</surname><given-names>Y.</given-names></name><name><surname>Jiang</surname><given-names>T.</given-names></name></person-group><article-title>Discrimination between Shaoxing wines and other Chinese rice wines by near-infrared spectroscopy and chemometrics</article-title><source><italic toggle="yes">Food and Bioprocess Technology</italic></source><year>2010</year><volume>5</volume><issue>2</issue><fpage>786</fpage><lpage>795</lpage><pub-id pub-id-type="doi">10.1007/s11947-010-0347-z</pub-id><pub-id pub-id-type="other">2-s2.0-84856582006</pub-id></element-citation></ref><ref id="B10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>H.</given-names></name><name><surname>Guo</surname><given-names>B.</given-names></name><name><surname>Wei</surname><given-names>Y.</given-names></name><name><surname>Zhang</surname><given-names>B.</given-names></name></person-group><article-title>Effects of grown origin, genotype, harvest year, and their interactions of wheat kernels on near infrared spectral fingerprints for geographical traceability</article-title><source><italic toggle="yes">Food Chemistry</italic></source><year>2014</year><volume>152</volume><fpage>316</fpage><lpage>322</lpage><pub-id pub-id-type="doi">10.1016/j.foodchem.2013.11.122</pub-id><pub-id pub-id-type="other">2-s2.0-84890812751</pub-id><pub-id pub-id-type="pmid">24444943</pub-id></element-citation></ref><ref id="B11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>P.</given-names></name><name><surname>Yu</surname><given-names>Z.</given-names></name></person-group><article-title>Species authentication and geographical origin discrimination of herbal medicines by near infrared spectroscopy: a review</article-title><source><italic toggle="yes">Journal of Pharmaceutical Analysis</italic></source><year>2015</year><volume>5</volume><issue>5</issue><fpage>277</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1016/j.jpha.2015.04.001</pub-id><pub-id pub-id-type="other">2-s2.0-84944278139</pub-id><pub-id pub-id-type="pmid">29403941</pub-id></element-citation></ref><ref id="B12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ouyang</surname><given-names>Q.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Chen</surname><given-names>Q.</given-names></name><etal/></person-group><article-title>Intelligent evaluation of color sensory quality of black tea by visible-near infrared spectroscopy technology: a comparison of spectra and color data information</article-title><source><italic toggle="yes">Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy</italic></source><year>2017</year><volume>180</volume><fpage>91</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1016/j.saa.2017.03.009</pub-id><pub-id pub-id-type="other">2-s2.0-85014417723</pub-id></element-citation></ref><ref id="B13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>G.</given-names></name><name><surname>Wang</surname><given-names>S.</given-names></name><name><surname>Ning</surname><given-names>J.</given-names></name><etal/></person-group><article-title>Quantitative analysis and geographical traceability of black tea using Fourier transform near-infrared spectroscopy (FT-NIRS)</article-title><source><italic toggle="yes">Food Research International</italic></source><year>2013</year><volume>53</volume><issue>2</issue><fpage>822</fpage><lpage>826</lpage><pub-id pub-id-type="doi">10.1016/j.foodres.2012.10.032</pub-id><pub-id pub-id-type="other">2-s2.0-84882846991</pub-id></element-citation></ref><ref id="B14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meng</surname><given-names>W.</given-names></name><name><surname>Xu</surname><given-names>X.</given-names></name><name><surname>Cheng</surname><given-names>K. K.</given-names></name><etal/></person-group><article-title>Geographical origin discrimination of oolong tea (TieGuanYin, <italic>Camellia sinensis</italic> (L.) O. Kuntze) using proton nuclear magnetic resonance spectroscopy and near-infrared spectroscopy</article-title><source><italic toggle="yes">Food Analytical Methods</italic></source><year>2017</year><volume>10</volume><issue>11</issue><fpage>3508</fpage><lpage>3522</lpage><pub-id pub-id-type="doi">10.1007/s12161-017-0920-4</pub-id><pub-id pub-id-type="other">2-s2.0-85018754848</pub-id></element-citation></ref><ref id="B15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>H. Y.</given-names></name><name><surname>Yin</surname><given-names>Q. B.</given-names></name><name><surname>Xu</surname><given-names>L.</given-names></name><etal/></person-group><article-title>Challenges of large-class-number classification (LCNC): a novel ensemble strategy (ES) and its application to discriminating the geographical origins of 25 green teas</article-title><source><italic toggle="yes">Chemometrics and Intelligent Laboratory Systems</italic></source><year>2016</year><volume>157</volume><fpage>43</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.chemolab.2016.06.018</pub-id><pub-id pub-id-type="other">2-s2.0-84976639670</pub-id></element-citation></ref><ref id="B16"><label>16</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Belitz</surname><given-names>H. D.</given-names></name><name><surname>Grosch</surname><given-names>W.</given-names></name><name><surname>Schieberle</surname><given-names>P.</given-names></name></person-group><article-title>Coffee, tea, cocoa</article-title><source><italic toggle="yes">Food Chemistry</italic></source><year>2009</year><publisher-loc>Berlin, Germany</publisher-loc><publisher-name>Springer</publisher-name><fpage>938</fpage><lpage>970</lpage></element-citation></ref><ref id="B17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiong</surname><given-names>C.</given-names></name><name><surname>Liu</surname><given-names>C.</given-names></name><name><surname>Pan</surname><given-names>W.</given-names></name><etal/></person-group><article-title>Non-destructive determination of total polyphenols content and classification of storage periods of Iron Buddha tea using multispectral imaging system</article-title><source><italic toggle="yes">Food Chemistry</italic></source><year>2015</year><volume>176</volume><fpage>130</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1016/j.foodchem.2014.12.057</pub-id><pub-id pub-id-type="other">2-s2.0-84919932999</pub-id><pub-id pub-id-type="pmid">25624215</pub-id></element-citation></ref><ref id="B18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arai</surname><given-names>K.</given-names></name><name><surname>Graduate School of Science and Engineering, Saga University, Japan</surname></name><name><surname>Sasaki</surname><given-names>Y.</given-names></name><name><surname>Kasuya</surname><given-names>S.</given-names></name><name><surname>Matusura</surname><given-names>H.</given-names></name></person-group><article-title>Appropriate tealeaf harvest timing determination based on NIR images</article-title><source><italic toggle="yes">International Journal of Information Technology and Computer Science</italic></source><year>2015</year><volume>7</volume><issue>7</issue><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.5815/ijitcs.2015.07.01</pub-id></element-citation></ref><ref id="B19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>H.</given-names></name><name><surname>Guo</surname><given-names>B.</given-names></name><name><surname>Wei</surname><given-names>Y.</given-names></name><name><surname>Zhang</surname><given-names>B.</given-names></name></person-group><article-title>Near infrared reflectance spectroscopy for determination of the geographical origin of wheat</article-title><source><italic toggle="yes">Food Chemistry</italic></source><year>2013</year><volume>138</volume><issue>2-3</issue><fpage>1902</fpage><lpage>1907</lpage><pub-id pub-id-type="doi">10.1016/j.foodchem.2012.11.037</pub-id><pub-id pub-id-type="other">2-s2.0-84873706359</pub-id><pub-id pub-id-type="pmid">23411323</pub-id></element-citation></ref><ref id="B20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name></person-group><article-title>Use of electronic nose and tongue to track freshness of cherry tomatoes squeezed for juice consumption: comparison of different sensor fusion approaches</article-title><source><italic toggle="yes">Food and Bioprocess Technology</italic></source><year>2014</year><volume>8</volume><issue>1</issue><fpage>158</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1007/s11947-014-1390-y</pub-id><pub-id pub-id-type="other">2-s2.0-84920707916</pub-id></element-citation></ref><ref id="B21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Crammer</surname><given-names>K.</given-names></name><name><surname>Vucetic</surname><given-names>S.</given-names></name></person-group><article-title>Breaking the curse of kernelization: budgeted stochastic gradient descent for large-scale SVM training</article-title><source><italic toggle="yes">Journal of Machine Learning Research</italic></source><year>2012</year><volume>13</volume><fpage>3103</fpage><lpage>3131</lpage></element-citation></ref><ref id="B22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farid</surname><given-names>D. M.</given-names></name><name><surname>Zhang</surname><given-names>L.</given-names></name><name><surname>Rahman</surname><given-names>C. M.</given-names></name><name><surname>Hossain</surname><given-names>M. A.</given-names></name><name><surname>Strachan</surname><given-names>R.</given-names></name></person-group><article-title>Hybrid decision tree and na&#x000ef;ve Bayes classifiers for multi-class classification tasks</article-title><source><italic toggle="yes">Expert Systems with Applications</italic></source><year>2014</year><volume>41</volume><issue>4</issue><fpage>1937</fpage><lpage>1946</lpage><pub-id pub-id-type="doi">10.1016/j.eswa.2013.08.089</pub-id><pub-id pub-id-type="other">2-s2.0-84888373332</pub-id></element-citation></ref><ref id="B23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>S. Y.</given-names></name><name><surname>Upneja</surname><given-names>A.</given-names></name></person-group><article-title>Predicting restaurant financial distress using decision tree and AdaBoosted decision tree models</article-title><source><italic toggle="yes">Economic Modelling</italic></source><year>2014</year><volume>36</volume><fpage>354</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1016/j.econmod.2013.10.005</pub-id><pub-id pub-id-type="other">2-s2.0-84886437226</pub-id></element-citation></ref><ref id="B24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reiss</surname><given-names>A.</given-names></name><name><surname>Hendeby</surname><given-names>G.</given-names></name><name><surname>Stricker</surname><given-names>D.</given-names></name></person-group><article-title>A novel confidence-based multiclass boosting algorithm for mobile physical activity monitoring</article-title><source><italic toggle="yes">Personal and Ubiquitous Computing</italic></source><year>2014</year><volume>19</volume><issue>1</issue><fpage>105</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.1007/s00779-014-0816-x</pub-id></element-citation></ref><ref id="B25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ai</surname><given-names>F. F.</given-names></name><name><surname>Bin</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>Z. M.</given-names></name><etal/></person-group><article-title>Application of random forests to select premium quality vegetable oils by their fatty acid composition</article-title><source><italic toggle="yes">Food Chemistry</italic></source><year>2014</year><volume>143</volume><fpage>472</fpage><lpage>478</lpage><pub-id pub-id-type="doi">10.1016/j.foodchem.2013.08.013</pub-id><pub-id pub-id-type="other">2-s2.0-84884579129</pub-id><pub-id pub-id-type="pmid">24054269</pub-id></element-citation></ref><ref id="B26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sch&#x000f6;lkopf</surname><given-names>B.</given-names></name><name><surname>Platt</surname><given-names>J. C.</given-names></name><name><surname>Shawe-Taylor</surname><given-names>J.</given-names></name><name><surname>Smola</surname><given-names>A. J.</given-names></name><name><surname>Williamson</surname><given-names>R. C.</given-names></name></person-group><article-title>Estimating the support of a high-dimensional distribution</article-title><source><italic toggle="yes">Neural Computation</italic></source><year>2001</year><volume>13</volume><issue>7</issue><fpage>1443</fpage><lpage>1471</lpage><pub-id pub-id-type="doi">10.1162/089976601750264965</pub-id><pub-id pub-id-type="other">2-s2.0-0000487102</pub-id><pub-id pub-id-type="pmid">11440593</pub-id></element-citation></ref><ref id="B27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>F. T.</given-names></name><name><surname>Ting</surname><given-names>K. M.</given-names></name><name><surname>Zhou</surname><given-names>Z. H.</given-names></name></person-group><article-title>Isolation-based anomaly detection</article-title><source><italic toggle="yes">ACM Transactions on Knowledge Discovery from Data (TKDD)</italic></source><year>2012</year><volume>6</volume><issue>1</issue><fpage>1</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1145/2133360.2133363</pub-id><pub-id pub-id-type="other">2-s2.0-84859412430</pub-id></element-citation></ref><ref id="B28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>L.</given-names></name><name><surname>Ye</surname><given-names>Z. H.</given-names></name><name><surname>Yan</surname><given-names>S. M.</given-names></name><etal/></person-group><article-title>Combining local wavelength information and ensemble learning to enhance the specificity of class modeling techniques: identification of food geographical origins and adulteration</article-title><source><italic toggle="yes">Analytica Chimica Acta</italic></source><year>2012</year><volume>754</volume><fpage>31</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1016/j.aca.2012.10.011</pub-id><pub-id pub-id-type="other">2-s2.0-84868688510</pub-id><pub-id pub-id-type="pmid">23140951</pub-id></element-citation></ref><ref id="B29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Sun</surname><given-names>C.</given-names></name><name><surname>Luo</surname><given-names>L.</given-names></name><name><surname>He</surname><given-names>Y.</given-names></name></person-group><article-title>Determination of tea polyphenols content by infrared spectroscopy coupled with iPLS and random frog techniques</article-title><source><italic toggle="yes">Computers and Electronics in Agriculture</italic></source><year>2015</year><volume>112</volume><fpage>28</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2015.01.005</pub-id><pub-id pub-id-type="other">2-s2.0-84939950294</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="fig1" orientation="portrait" position="float"><label>Figure 1</label><caption><p>NIR responses (average values) to the four tea groups collected from different geographical origins in different years.</p></caption><graphic xlink:href="JAMC2019-1537568.001"/></fig><fig id="fig2" orientation="portrait" position="float"><label>Figure 2</label><caption><p>Prediction of tea samples from 2015 based on OC-SVM and IF combined SVM classifiers at different updating ratios.</p></caption><graphic xlink:href="JAMC2019-1537568.002"/></fig><table-wrap id="tab1" orientation="portrait" position="float"><label>Table 1</label><caption><p>Description of tea samples.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Year</th><th align="center" rowspan="1" colspan="1">Group ID</th><th align="center" rowspan="1" colspan="1">Type</th><th align="center" rowspan="1" colspan="1">Group size</th><th align="center" rowspan="1" colspan="1">Tea gardens per group</th><th align="center" rowspan="1" colspan="1">Raw variables per sample</th></tr></thead><tbody><tr><td align="left" rowspan="2" colspan="1">2014</td><td align="center" rowspan="1" colspan="1">2014IN</td><td align="center" rowspan="1" colspan="1">WY<sup>a</sup></td><td align="center" rowspan="1" colspan="1">495</td><td align="center" rowspan="1" colspan="1">33</td><td align="center" rowspan="1" colspan="1">4148</td></tr><tr><td align="center" rowspan="1" colspan="1">2014OUT</td><td align="center" rowspan="1" colspan="1">NWY<sup>b</sup></td><td align="center" rowspan="1" colspan="1">165</td><td align="center" rowspan="1" colspan="1">11</td><td align="center" rowspan="1" colspan="1">4148</td></tr><tr><td align="left" colspan="6" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="2" colspan="1">2015</td><td align="center" rowspan="1" colspan="1">2015IN</td><td align="center" rowspan="1" colspan="1">WY</td><td align="center" rowspan="1" colspan="1">687</td><td align="center" rowspan="1" colspan="1">74</td><td align="center" rowspan="1" colspan="1">4148</td></tr><tr><td align="center" rowspan="1" colspan="1">2015OUT</td><td align="center" rowspan="1" colspan="1">NWY</td><td align="center" rowspan="1" colspan="1">100</td><td align="center" rowspan="1" colspan="1">10</td><td align="center" rowspan="1" colspan="1">4148</td></tr></tbody></table><table-wrap-foot><fn><p>
<sup>a</sup>WY: authentic Wuyi rock-essence tea with a protected geographical indication (GI); <sup>b</sup>NWY: Non-Wuyi tea.</p></fn></table-wrap-foot></table-wrap><table-wrap id="tab2" orientation="portrait" position="float"><label>Table 2</label><caption><p>Comparative classification of tea samples based on seven different classifiers.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2" colspan="1">Classifier</th><th align="center" colspan="3" rowspan="1">Training set (70% of 14 dataset)</th><th align="center" colspan="3" rowspan="1">Testing set (30% of 14 dataset)</th><th align="center" colspan="3" rowspan="1">15 dataset</th></tr><tr><th align="center" rowspan="1" colspan="1">Recall 1</th><th align="center" rowspan="1" colspan="1">Recall 2</th><th align="center" rowspan="1" colspan="1">MAR<sup>a</sup></th><th align="center" rowspan="1" colspan="1">Recall 1</th><th align="center" rowspan="1" colspan="1">Recall 2</th><th align="center" rowspan="1" colspan="1">MAR</th><th align="center" rowspan="1" colspan="1">Recall 1</th><th align="center" rowspan="1" colspan="1">Recall 2</th><th align="center" rowspan="1" colspan="1">MAR</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">LDA</td><td align="center" rowspan="1" colspan="1">0.9994</td><td align="center" rowspan="1" colspan="1">0.9983</td><td align="center" rowspan="1" colspan="1">
<bold>0.9989 A</bold>
</td><td align="center" rowspan="1" colspan="1">0.9826</td><td align="center" rowspan="1" colspan="1">0.8925</td><td align="center" rowspan="1" colspan="1">
<bold>0.9376 A</bold>
</td><td align="center" rowspan="1" colspan="1">0.6199</td><td align="center" rowspan="1" colspan="1">0.2347</td><td align="center" rowspan="1" colspan="1">
<bold>0.4273 C</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">SVM</td><td align="center" rowspan="1" colspan="1">1.0000</td><td align="center" rowspan="1" colspan="1">1.0000</td><td align="center" rowspan="1" colspan="1">
<bold>1.0000 A</bold>
</td><td align="center" rowspan="1" colspan="1">0.9639</td><td align="center" rowspan="1" colspan="1">0.8291</td><td align="center" rowspan="1" colspan="1">
<bold>0.8965 B</bold>
</td><td align="center" rowspan="1" colspan="1">0.9074</td><td align="center" rowspan="1" colspan="1">0.2298</td><td align="center" rowspan="1" colspan="1">
<bold>0.5686 A</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">SGD<sup>b</sup></td><td align="center" rowspan="1" colspan="1">0.9987</td><td align="center" rowspan="1" colspan="1">0.9804</td><td align="center" rowspan="1" colspan="1">
<bold>0.9895 C</bold>
</td><td align="center" rowspan="1" colspan="1">0.9740</td><td align="center" rowspan="1" colspan="1">0.7961</td><td align="center" rowspan="1" colspan="1">
<bold>0.8851 B</bold>
</td><td align="center" rowspan="1" colspan="1">0.6621</td><td align="center" rowspan="1" colspan="1">0.1638</td><td align="center" rowspan="1" colspan="1">
<bold>0.4130 C</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Decision tree</td><td align="center" rowspan="1" colspan="1">1.0000</td><td align="center" rowspan="1" colspan="1">1.0000</td><td align="center" rowspan="1" colspan="1">
<bold>1.0000 A</bold>
</td><td align="center" rowspan="1" colspan="1">0.9132</td><td align="center" rowspan="1" colspan="1">0.7046</td><td align="center" rowspan="1" colspan="1">
<bold>0.8089 D</bold>
</td><td align="center" rowspan="1" colspan="1">0.8461</td><td align="center" rowspan="1" colspan="1">0.2461</td><td align="center" rowspan="1" colspan="1">
<bold>0.5461 B</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Random forest</td><td align="center" rowspan="1" colspan="1">0.9996</td><td align="center" rowspan="1" colspan="1">0.9904</td><td align="center" rowspan="1" colspan="1">
<bold>0.9950 B</bold>
</td><td align="center" rowspan="1" colspan="1">0.9703</td><td align="center" rowspan="1" colspan="1">0.7046</td><td align="center" rowspan="1" colspan="1">
<bold>0.8374 C</bold>
</td><td align="center" rowspan="1" colspan="1">0.8723</td><td align="center" rowspan="1" colspan="1">0.2195</td><td align="center" rowspan="1" colspan="1">
<bold>0.5459 B</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">AdaBoost<sup>c</sup></td><td align="center" rowspan="1" colspan="1">1.0000</td><td align="center" rowspan="1" colspan="1">1.0000</td><td align="center" rowspan="1" colspan="1">
<bold>1.0000 A</bold>
</td><td align="center" rowspan="1" colspan="1">0.9691</td><td align="center" rowspan="1" colspan="1">0.7963</td><td align="center" rowspan="1" colspan="1">
<bold>0.8827 B</bold>
</td><td align="center" rowspan="1" colspan="1">0.8370</td><td align="center" rowspan="1" colspan="1">0.2622</td><td align="center" rowspan="1" colspan="1">
<bold>0.5496 B</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">MLP<sup>d</sup></td><td align="center" rowspan="1" colspan="1">1.0000</td><td align="center" rowspan="1" colspan="1">1.0000</td><td align="center" rowspan="1" colspan="1">
<bold>1.0000 A</bold>
</td><td align="center" rowspan="1" colspan="1">0.9670</td><td align="center" rowspan="1" colspan="1">0.8957</td><td align="center" rowspan="1" colspan="1">
<bold>0.9314 A</bold>
</td><td align="center" rowspan="1" colspan="1">0.5842</td><td align="center" rowspan="1" colspan="1">0.2576</td><td align="center" rowspan="1" colspan="1">
<bold>0.4209 C</bold>
</td></tr></tbody></table><table-wrap-foot><fn><p>
<sup>a</sup>MAR: macro average recall; <sup>b</sup>SGD: stochastic gradient descent; <sup>c</sup>AdaBoost: adaptive boosting; <sup>d</sup>MLP: multilayer perceptron. Means with the same letter(s) are not significantly different at 0.01 level.</p></fn></table-wrap-foot></table-wrap><table-wrap id="tab3" orientation="portrait" position="float"><label>Table 3</label><caption><p>Improving the prediction of 2015 tea samples using three outlier detection based undersampling approaches.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2" colspan="1">Classifier</th><th align="center" colspan="3" rowspan="1">One Class SVM (OC-SVM)</th><th align="center" colspan="3" rowspan="1">Isolation forest (IF)</th><th align="center" colspan="3" rowspan="1">Elliptic envelope (EE)</th></tr><tr><th align="center" rowspan="1" colspan="1">Recall 1</th><th align="center" rowspan="1" colspan="1">Recall 2</th><th align="center" rowspan="1" colspan="1">MAR<sup>a</sup></th><th align="center" rowspan="1" colspan="1">Recall 1</th><th align="center" rowspan="1" colspan="1">Recall 2</th><th align="center" rowspan="1" colspan="1">MAR</th><th align="center" rowspan="1" colspan="1">Recall 1</th><th align="center" rowspan="1" colspan="1">Recall 2</th><th align="center" rowspan="1" colspan="1">MAR</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">LDA</td><td align="center" rowspan="1" colspan="1">0.4993</td><td align="center" rowspan="1" colspan="1">0.3700</td><td align="center" rowspan="1" colspan="1">
<bold>0.4346</bold>
</td><td align="center" rowspan="1" colspan="1">0.4643</td><td align="center" rowspan="1" colspan="1">0.4100</td><td align="center" rowspan="1" colspan="1">
<bold>0.4372</bold>
</td><td align="center" rowspan="1" colspan="1">0.4498</td><td align="center" rowspan="1" colspan="1">0.3900</td><td align="center" rowspan="1" colspan="1">
<bold>0.4199</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">SVM</td><td align="center" rowspan="1" colspan="1">0.7409</td><td align="center" rowspan="1" colspan="1">0.7300</td><td align="center" rowspan="1" colspan="1">
<bold>0.7355</bold>
</td><td align="center" rowspan="1" colspan="1">0.7191</td><td align="center" rowspan="1" colspan="1">0.7600</td><td align="center" rowspan="1" colspan="1">
<bold>0.7395</bold>
</td><td align="center" rowspan="1" colspan="1">0.7118</td><td align="center" rowspan="1" colspan="1">0.7500</td><td align="center" rowspan="1" colspan="1">
<bold>0.7309</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">SGD<sup>b</sup></td><td align="center" rowspan="1" colspan="1">0.5997</td><td align="center" rowspan="1" colspan="1">0.5100</td><td align="center" rowspan="1" colspan="1">
<bold>0.5549</bold>
</td><td align="center" rowspan="1" colspan="1">0.5997</td><td align="center" rowspan="1" colspan="1">0.5100</td><td align="center" rowspan="1" colspan="1">
<bold>0.5549</bold>
</td><td align="center" rowspan="1" colspan="1">0.5662</td><td align="center" rowspan="1" colspan="1">0.5100</td><td align="center" rowspan="1" colspan="1">
<bold>0.5381</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Decision tree</td><td align="center" rowspan="1" colspan="1">0.6652</td><td align="center" rowspan="1" colspan="1">0.5700</td><td align="center" rowspan="1" colspan="1">
<bold>0.6176</bold>
</td><td align="center" rowspan="1" colspan="1">0.6958</td><td align="center" rowspan="1" colspan="1">0.5800</td><td align="center" rowspan="1" colspan="1">
<bold>0.6379</bold>
</td><td align="center" rowspan="1" colspan="1">0.6201</td><td align="center" rowspan="1" colspan="1">0.5400</td><td align="center" rowspan="1" colspan="1">
<bold>0.5800</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Random forest</td><td align="center" rowspan="1" colspan="1">0.7089</td><td align="center" rowspan="1" colspan="1">0.6300</td><td align="center" rowspan="1" colspan="1">
<bold>0.6694</bold>
</td><td align="center" rowspan="1" colspan="1">0.6812</td><td align="center" rowspan="1" colspan="1">0.6200</td><td align="center" rowspan="1" colspan="1">
<bold>0.6506</bold>
</td><td align="center" rowspan="1" colspan="1">0.6710</td><td align="center" rowspan="1" colspan="1">0.6100</td><td align="center" rowspan="1" colspan="1">
<bold>0.6405</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">AdaBoost</td><td align="center" rowspan="1" colspan="1">0.6405</td><td align="center" rowspan="1" colspan="1">0.6800</td><td align="center" rowspan="1" colspan="1">
<bold>0.6602</bold>
</td><td align="center" rowspan="1" colspan="1">0.7322</td><td align="center" rowspan="1" colspan="1">0.5400</td><td align="center" rowspan="1" colspan="1">
<bold>0.6361</bold>
</td><td align="center" rowspan="1" colspan="1">0.6667</td><td align="center" rowspan="1" colspan="1">0.5600</td><td align="center" rowspan="1" colspan="1">
<bold>0.6133</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">MLP<sup>c</sup></td><td align="center" rowspan="1" colspan="1">0.6856</td><td align="center" rowspan="1" colspan="1">0.5200</td><td align="center" rowspan="1" colspan="1">
<bold>0.6028</bold>
</td><td align="center" rowspan="1" colspan="1">0.8020</td><td align="center" rowspan="1" colspan="1">0.3800</td><td align="center" rowspan="1" colspan="1">
<bold>0.5910</bold>
</td><td align="center" rowspan="1" colspan="1">0.7205</td><td align="center" rowspan="1" colspan="1">0.3800</td><td align="center" rowspan="1" colspan="1">
<bold>0.5503</bold>
</td></tr></tbody></table><table-wrap-foot><fn><p>
<sup>a</sup>MAR: macro average recall; <sup>b</sup>SGD: stochastic gradient descent; <sup>c</sup>MLP: multilayer perceptron.</p></fn></table-wrap-foot></table-wrap><table-wrap id="tab4" orientation="portrait" position="float"><label>Table 4</label><caption><p>Comparison of prediction results for 2015 dataset based on OC-SVM and IF combined SVM classifiers at different updating ratios.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2" colspan="1">Updating rate</th><th align="center" colspan="3" rowspan="1">One-class SVM (OC-SVM)</th><th align="center" colspan="3" rowspan="1">Isolation forest (IF)</th></tr><tr><th align="center" rowspan="1" colspan="1">Recall 1</th><th align="center" rowspan="1" colspan="1">Recall 2</th><th align="center" rowspan="1" colspan="1">MAR<sup>a</sup></th><th align="center" rowspan="1" colspan="1">Recall 1</th><th align="center" rowspan="1" colspan="1">Recall 2</th><th align="center" rowspan="1" colspan="1">MAR</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">10%</td><td align="center" rowspan="1" colspan="1">0.8275</td><td align="center" rowspan="1" colspan="1">0.7324</td><td align="center" rowspan="1" colspan="1">
<bold>0.7799 G</bold>
</td><td align="center" rowspan="1" colspan="1">0.8414</td><td align="center" rowspan="1" colspan="1">0.7257</td><td align="center" rowspan="1" colspan="1">
<bold>0.7835 G</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">20%</td><td align="center" rowspan="1" colspan="1">0.8482</td><td align="center" rowspan="1" colspan="1">0.8386</td><td align="center" rowspan="1" colspan="1">
<bold>0.8434 E</bold>
</td><td align="center" rowspan="1" colspan="1">0.8617</td><td align="center" rowspan="1" colspan="1">0.7983</td><td align="center" rowspan="1" colspan="1">
<bold>0.8300 F</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">30%</td><td align="center" rowspan="1" colspan="1">0.8612</td><td align="center" rowspan="1" colspan="1">0.8749</td><td align="center" rowspan="1" colspan="1">
<bold>0.8680 D</bold>
</td><td align="center" rowspan="1" colspan="1">0.8717</td><td align="center" rowspan="1" colspan="1">0.8312</td><td align="center" rowspan="1" colspan="1">
<bold>0.8515 E</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">40%</td><td align="center" rowspan="1" colspan="1">0.8739</td><td align="center" rowspan="1" colspan="1">0.9077</td><td align="center" rowspan="1" colspan="1">
<bold>0.8908 B</bold>
</td><td align="center" rowspan="1" colspan="1">0.8767</td><td align="center" rowspan="1" colspan="1">0.8599</td><td align="center" rowspan="1" colspan="1">
<bold>0.8683 D</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">50%</td><td align="center" rowspan="1" colspan="1">0.8824</td><td align="center" rowspan="1" colspan="1">0.9238</td><td align="center" rowspan="1" colspan="1">
<bold>0.9031 A</bold>
</td><td align="center" rowspan="1" colspan="1">0.8802</td><td align="center" rowspan="1" colspan="1">0.8727</td><td align="center" rowspan="1" colspan="1">
<bold>0.8765 C,D</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">60%</td><td align="center" rowspan="1" colspan="1">0.8867</td><td align="center" rowspan="1" colspan="1">0.9379</td><td align="center" rowspan="1" colspan="1">
<bold>0.9123 A</bold>
</td><td align="center" rowspan="1" colspan="1">0.8860</td><td align="center" rowspan="1" colspan="1">0.8770</td><td align="center" rowspan="1" colspan="1">
<bold>0.8815 B,C</bold>
</td></tr></tbody></table><table-wrap-foot><fn><p>
<sup>a</sup>MAR: macro average recall. Means with the same letter(s) are not significantly different at 0.01 level.</p></fn></table-wrap-foot></table-wrap></floats-group></article>