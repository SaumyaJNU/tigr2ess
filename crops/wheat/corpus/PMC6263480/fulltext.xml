<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6263480</article-id><article-id pub-id-type="doi">10.3390/s18113731</article-id><article-id pub-id-type="publisher-id">sensors-18-03731</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Wheat Height Estimation Using LiDAR in Comparison to Ultrasonic Sensor and UAS</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6300-2973</contrib-id><name><surname>Yuan</surname><given-names>Wenan</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03731">1</xref><xref rid="c1-sensors-18-03731" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Jiating</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03731">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-4959-0481</contrib-id><name><surname>Bhatta</surname><given-names>Madhav</given-names></name><xref ref-type="aff" rid="af2-sensors-18-03731">2</xref></contrib><contrib contrib-type="author"><name><surname>Shi</surname><given-names>Yeyin</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03731">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9109-6954</contrib-id><name><surname>Baenziger</surname><given-names>P. Stephen</given-names></name><xref ref-type="aff" rid="af2-sensors-18-03731">2</xref></contrib><contrib contrib-type="author"><name><surname>Ge</surname><given-names>Yufeng</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03731">1</xref></contrib></contrib-group><aff id="af1-sensors-18-03731"><label>1</label>Biological Systems Engineering Department, University of Nebraska&#x02013;Lincoln, Lincoln, NE 68503, USA; <email>jiatingli@huskers.unl.edu</email> (J.L.); <email>yshi18@unl.edu</email> (Y.S.); <email>yge2@unl.edu</email> (Y.G.)</aff><aff id="af2-sensors-18-03731"><label>2</label>Department of Agronomy and Horticulture, University of Nebraska&#x02013;Lincoln, Lincoln, NE 68503, USA; <email>madhav.bhatta@huskers.unl.edu</email> (M.B.); <email>pbaenziger1@unl.edu</email> (P.S.B.)</aff><author-notes><corresp id="c1-sensors-18-03731"><label>*</label>Correspondence: <email>wenan.yuan@huskers.unl.edu</email>; Tel.: +1-402-472-3435</corresp></author-notes><pub-date pub-type="epub"><day>02</day><month>11</month><year>2018</year></pub-date><pub-date pub-type="collection"><month>11</month><year>2018</year></pub-date><volume>18</volume><issue>11</issue><elocation-id>3731</elocation-id><history><date date-type="received"><day>25</day><month>9</month><year>2018</year></date><date date-type="accepted"><day>31</day><month>10</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; 2018 by the authors.</copyright-statement><copyright-year>2018</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>As one of the key crop traits, plant height is traditionally evaluated manually, which can be slow, laborious and prone to error. Rapid development of remote and proximal sensing technologies in recent years allows plant height to be estimated in more objective and efficient fashions, while research regarding direct comparisons between different height measurement methods seems to be lagging. In this study, a ground-based multi-sensor phenotyping system equipped with ultrasonic sensors and light detection and ranging (LiDAR) was developed. Canopy heights of 100 wheat plots were estimated five times during a season by the ground phenotyping system and an unmanned aircraft system (UAS), and the results were compared to manual measurements. Overall, LiDAR provided the best results, with a root-mean-square error (RMSE) of 0.05 m and an R<sup>2</sup> of 0.97. UAS obtained reasonable results with an RMSE of 0.09 m and an R<sup>2</sup> of 0.91. Ultrasonic sensors did not perform well due to our static measurement style. In conclusion, we suggest LiDAR and UAS are reliable alternative methods for wheat height evaluation.</p></abstract><kwd-group><kwd>crop</kwd><kwd>plant breeding</kwd><kwd>phenotyping</kwd><kwd>proximal sensing</kwd><kwd>remote sensing</kwd></kwd-group></article-meta></front><body><sec id="sec1-sensors-18-03731"><title>1. Introduction</title><p>Plant height is one of the most important parameters for crop selection in a breeding program. For wheat, height is associated with grain yield [<xref rid="B1-sensors-18-03731" ref-type="bibr">1</xref>], lodging [<xref rid="B2-sensors-18-03731" ref-type="bibr">2</xref>], biomass [<xref rid="B3-sensors-18-03731" ref-type="bibr">3</xref>], and resistance to certain disease [<xref rid="B4-sensors-18-03731" ref-type="bibr">4</xref>]. Traditionally, plant height is measured manually using a yardstick. This method is labor-intensive and time-consuming when a large number of plants need to be evaluated. In addition, it is prone to error during reading and recording, especially in harsh weather conditions. Alternative but reliable methods for plant height evaluation are needed.</p><p>Field phenotyping has been gaining popularity in recent years due to its ability of sensing various crop traits non-destructively in a high-throughput fashion [<xref rid="B5-sensors-18-03731" ref-type="bibr">5</xref>,<xref rid="B6-sensors-18-03731" ref-type="bibr">6</xref>,<xref rid="B7-sensors-18-03731" ref-type="bibr">7</xref>], and sophisticated multi-sensor phenotyping systems such as &#x0201c;Field Scanalyzer&#x0201d; [<xref rid="B8-sensors-18-03731" ref-type="bibr">8</xref>], &#x0201c;Ladybird&#x0201d; [<xref rid="B9-sensors-18-03731" ref-type="bibr">9</xref>], &#x0201c;Phenomobile&#x0201d; [<xref rid="B10-sensors-18-03731" ref-type="bibr">10</xref>] and &#x0201c;Phenomobile Lite&#x0201d; [<xref rid="B11-sensors-18-03731" ref-type="bibr">11</xref>] have been reported. As for estimating plant height, several techniques have been adopted in previous research, and the basic principles behind most of the techniques are either time-of-flight (ToF) or triangulation. The ultrasonic sensor, ToF camera [<xref rid="B12-sensors-18-03731" ref-type="bibr">12</xref>,<xref rid="B13-sensors-18-03731" ref-type="bibr">13</xref>] and most scanning light detection and ranging (LiDAR) techniques are all based on the ToF principle, whereas the structured-light scanner [<xref rid="B12-sensors-18-03731" ref-type="bibr">12</xref>], stereo camera or stereo vision [<xref rid="B14-sensors-18-03731" ref-type="bibr">14</xref>,<xref rid="B15-sensors-18-03731" ref-type="bibr">15</xref>], and structure from motion, which is a technique commonly used in unmanned aircraft system (UAS) imagery, are based on the triangulation principle.</p><p>As some of the most common methods for plant height estimation at present, ultrasonic sensor, LiDAR and UAS can be favored over one another because of the unique advantages and disadvantages they possess. The ultrasonic sensor is typically inexpensive and user-friendly, and has a long history of being utilized in plant height measurement [<xref rid="B16-sensors-18-03731" ref-type="bibr">16</xref>]. However some of its disadvantages include reduced sensor accuracies when sensors become farther from objects due to the larger field of view (FOV) [<xref rid="B17-sensors-18-03731" ref-type="bibr">17</xref>], sensor&#x02019;s sensitivity to temperature as sound speed changes with temperature [<xref rid="B18-sensors-18-03731" ref-type="bibr">18</xref>], and the susceptibility of sound waves to plant leaf size, angle, and surfaces [<xref rid="B16-sensors-18-03731" ref-type="bibr">16</xref>]. LiDAR and UAS are relatively new methods for estimating various plant traits such as height, biomass and ground cover [<xref rid="B11-sensors-18-03731" ref-type="bibr">11</xref>,<xref rid="B19-sensors-18-03731" ref-type="bibr">19</xref>,<xref rid="B20-sensors-18-03731" ref-type="bibr">20</xref>,<xref rid="B21-sensors-18-03731" ref-type="bibr">21</xref>]. LiDAR is considered a widely-accepted and promising sensor for plant 3D reconstruction because of its high spatial resolution, low beam divergence and versatility regardless of ambient light conditions [<xref rid="B9-sensors-18-03731" ref-type="bibr">9</xref>,<xref rid="B11-sensors-18-03731" ref-type="bibr">11</xref>,<xref rid="B22-sensors-18-03731" ref-type="bibr">22</xref>]. However, LiDAR is also costly, and LiDAR data can be voluminous and challenging to process [<xref rid="B23-sensors-18-03731" ref-type="bibr">23</xref>]. UAS has been increasingly used in crop phenotyping over the past decade. A low flight altitude allows images to be captured with relatively high spatial resolution, and it is flexible in terms of temporal resolution and the types of deployed sensors [<xref rid="B24-sensors-18-03731" ref-type="bibr">24</xref>,<xref rid="B25-sensors-18-03731" ref-type="bibr">25</xref>,<xref rid="B26-sensors-18-03731" ref-type="bibr">26</xref>]. However, UAS has limited payload and flight time [<xref rid="B10-sensors-18-03731" ref-type="bibr">10</xref>], and the pilot needs to have certain level of proficiency to acquire data with optimal quality.</p><p>Ultrasonic sensors, LiDAR and UAS have been exploited for a wide range of crops in the past. However, ultrasonic sensors and UAS were not able to provide consistently accurate height estimations when compared to LiDAR. For example, the ultrasonic sensor has been used to estimate the height of cotton [<xref rid="B27-sensors-18-03731" ref-type="bibr">27</xref>,<xref rid="B28-sensors-18-03731" ref-type="bibr">28</xref>], alfalfa [<xref rid="B29-sensors-18-03731" ref-type="bibr">29</xref>], wild blueberry [<xref rid="B30-sensors-18-03731" ref-type="bibr">30</xref>,<xref rid="B31-sensors-18-03731" ref-type="bibr">31</xref>], legume-grass [<xref rid="B16-sensors-18-03731" ref-type="bibr">16</xref>,<xref rid="B32-sensors-18-03731" ref-type="bibr">32</xref>], Bermuda grass [<xref rid="B29-sensors-18-03731" ref-type="bibr">29</xref>], barley [<xref rid="B33-sensors-18-03731" ref-type="bibr">33</xref>] and wheat [<xref rid="B29-sensors-18-03731" ref-type="bibr">29</xref>,<xref rid="B34-sensors-18-03731" ref-type="bibr">34</xref>,<xref rid="B35-sensors-18-03731" ref-type="bibr">35</xref>], with root-mean-square error (RMSE) from 0.022 to 0.072 and R<sup>2</sup> from 0.44 to 0.90 reported. Similarly, UAS has been applied to various crops including corn [<xref rid="B36-sensors-18-03731" ref-type="bibr">36</xref>,<xref rid="B37-sensors-18-03731" ref-type="bibr">37</xref>,<xref rid="B38-sensors-18-03731" ref-type="bibr">38</xref>], sorghum [<xref rid="B37-sensors-18-03731" ref-type="bibr">37</xref>,<xref rid="B39-sensors-18-03731" ref-type="bibr">39</xref>,<xref rid="B40-sensors-18-03731" ref-type="bibr">40</xref>] and wheat [<xref rid="B20-sensors-18-03731" ref-type="bibr">20</xref>,<xref rid="B41-sensors-18-03731" ref-type="bibr">41</xref>,<xref rid="B42-sensors-18-03731" ref-type="bibr">42</xref>,<xref rid="B43-sensors-18-03731" ref-type="bibr">43</xref>,<xref rid="B44-sensors-18-03731" ref-type="bibr">44</xref>,<xref rid="B45-sensors-18-03731" ref-type="bibr">45</xref>], and the results from different studies varied greatly, with R<sup>2</sup> ranging from 0.27 to 0.99. On the other hand, LiDAR has been employed for crops such as cotton [<xref rid="B17-sensors-18-03731" ref-type="bibr">17</xref>], blueberry [<xref rid="B46-sensors-18-03731" ref-type="bibr">46</xref>] and wheat [<xref rid="B8-sensors-18-03731" ref-type="bibr">8</xref>,<xref rid="B9-sensors-18-03731" ref-type="bibr">9</xref>,<xref rid="B10-sensors-18-03731" ref-type="bibr">10</xref>,<xref rid="B11-sensors-18-03731" ref-type="bibr">11</xref>,<xref rid="B45-sensors-18-03731" ref-type="bibr">45</xref>,<xref rid="B47-sensors-18-03731" ref-type="bibr">47</xref>], and RMSE from 0.017 m to 0.089 m and R<sup>2</sup> from 0.86 to 0.99 were obtained.</p><p>In existing studies of utilizing terrestrial LiDAR, an experimental field is usually scanned by a LiDAR that moves continuously with a constant speed. For a manned multi-sensor system, this might be problematic since sensors such as cameras often require to be stationary to record high quality data, which can cause difficulties for software programming to harness multiple sensor data flows simultaneously, as well as in maintaining the uniform speed during operation. Moreover, despite all the successes and failures of applying ultrasonic sensors, LiDAR and UAS in plant height estimation, a direct comparison between the three methods was missing in previous research. In this study, we aimed to explore a new methodology of processing LiDAR data in the context of a static measurement style, and our ultimate objective was to compare the ultrasonic sensor, LiDAR and UAS in terms of their plant height estimation performance.</p></sec><sec id="sec2-sensors-18-03731"><title>2. Materials and Methods</title><sec id="sec2dot1-sensors-18-03731"><title>2.1. Experiment Arrangement</title><p>The experiment was conducted during the 2018 growing season at Agronomy Research Farm in Lincoln, NE, USA (40.86027&#x000b0; N, 96.61502&#x000b0; W). The experimental field contained 100 wheat plots where an augmented design with 10 checks replicated twice was used. The wheat lines consisted of 80 wheat genotypes produced at University of Nebraska&#x02013;Lincoln, NE, USA. The planting was done on 20 October 2017, and the plots were harvested on 29 June 2018.</p><p>Five data collection campaigns were conducted during the season. On each occasion, the 100 plots were scanned by a ground phenotyping system and a UAS. The plots were also measured by a yardstick using two methods depending on the growth stage (<xref rid="sensors-18-03731-t001" ref-type="table">Table 1</xref>). At vegetative stages plant height was measured from soil surface to the top of stem, or apical bud (method A). At reproductive stages plant height was measured from soil surface to the top of spike excluding awns (method B) [<xref rid="B1-sensors-18-03731" ref-type="bibr">1</xref>]. For each plot three measurements were taken and averaged as the reference height of the plot. </p></sec><sec id="sec2dot2-sensors-18-03731"><title>2.2. Ground Phenotyping System</title><sec id="sec2dot2dot1-sensors-18-03731"><title>2.2.1. Hardware</title><p>The ground phenotyping system was built based on the concept of another system developed by Bai et al. [<xref rid="B48-sensors-18-03731" ref-type="bibr">48</xref>]. In addition to three ultrasonic sensors (ToughSonic 14, Senix Corporation, Hinesburg, VT, USA) mounted on three sensor bars, a LiDAR (VLP-16 Puck, Velodyne LiDAR Inc., San Jose, CA, USA) was also incorporated on the middle sensor bar (<xref ref-type="fig" rid="sensors-18-03731-f001">Figure 1</xref>).</p><p>The ultrasonic sensors have a FOV of 14&#x000b0; and a maximum measurement distance of 4.27 m. The measurement rate was set at 20 Hz. The sensors produce 0 to 10 volts direct current (VDC) signals, which are proportional to the distance between sensors and objects. Voltage signals were measured using a LabJack U6 data acquisition (DAQ) board (LabJack Corporation, Lakewood, CO, USA).</p><p>The LiDAR transfers data via Ethernet. It has 16 near-infrared lasers with a 903 nm wavelength, and it detects distance up to 100 m. The sensor has a vertical FOV of 30&#x000b0; with a resolution of 2&#x000b0;, and a horizontal FOV of 360&#x000b0; with an adjustable resolution between 0.1&#x000b0; and 0.4&#x000b0;. Since only half of the full azimuth range could be possibly useful for our application of scanning crop canopies (<xref ref-type="fig" rid="sensors-18-03731-f002">Figure 2</xref>), the LiDAR&#x02019;s horizontal FOV range was configured as 180&#x000b0;, and a 0.1&#x000b0; horizontal resolution was adopted for higher precision. The sensor was also configured to report the strongest return for each laser firing.</p></sec><sec id="sec2dot2dot2-sensors-18-03731"><title>2.2.2. Software</title><p>A customized program was developed for sensor controlling and data acquisition using LabVIEW 2016 (National Instruments, Austin, TX, USA) (<xref ref-type="fig" rid="sensors-18-03731-f003">Figure 3</xref>) based on the original program from Bai et al. [<xref rid="B48-sensors-18-03731" ref-type="bibr">48</xref>]. The ground phenotyping system adopted a static measurement style [<xref rid="B48-sensors-18-03731" ref-type="bibr">48</xref>]. Instead of collecting data continuously, sensor outputs were saved only when designated buttons were triggered.</p><p>Voltage signals from ultrasonic sensors were converted to distances in the program through an equation calibrated in lab:<disp-formula>D = 29.116V + 11.641<label>(1)</label></disp-formula>
where D is distance in meters and V is sensor signal in volts. Ultrasonic canopy heights were then calculated as:<disp-formula>H<sub>c</sub> = H<sub>s</sub> &#x02212; D,<label>(2)</label></disp-formula>
where H<sub>c</sub> is ultrasonic canopy height and H<sub>s</sub> is ultrasonic sensor height. H<sub>s</sub> was determined by measuring the distance between the sensors and soil surface before data collection, and LiDAR height was determined in the same way.</p><p>A subprogram was developed for LiDAR and incorporated in the main program. The subprogram receives data packets from LiDAR through the user datagram protocol (UDP). Each data packet contains azimuth and distance information of all 16 lasers, and the subprogram extracts and converts the information into a 3D Cartesian coordinate system. The origin of the coordinate system was defined as shown in <xref ref-type="fig" rid="sensors-18-03731-f004">Figure 4</xref>. After acquiring the XYZ coordinates of the points, the subprogram trims the point cloud in the X-dimension using a threshold of &#x000b1;1.5 &#x000d7; &#x0201c;plot width&#x0201d; (<xref ref-type="fig" rid="sensors-18-03731-f002">Figure 2</xref>) to delete points outside the desired range. &#x0201c;Plot width&#x0201d; is defined as the distance between the centers of two adjacent alleyways, and was 1.524 m in this study. The point cloud is finally split by two borders of &#x000b1;0.5 &#x000d7; &#x0201c;plot width&#x0201d; into three parts. <xref ref-type="fig" rid="sensors-18-03731-f005">Figure 5</xref> is an example of a raw point cloud captured by LiDAR.</p></sec><sec id="sec2dot2dot3-sensors-18-03731"><title>2.2.3. Height Extraction from LiDAR Point Clouds</title><p>One issue that we encountered often in the field was the slant of the phenocart and the sensor bars due to the unevenness and slope of the ground (<xref ref-type="fig" rid="sensors-18-03731-f006">Figure 6</xref>). Corresponding LiDAR point clouds thus would show the tilted angle in the Cartesian coordinate system.</p><p>In order to obtain accurate canopy height estimations from LiDAR, pre-processing is necessary for all raw point clouds to correct for this slanting issue before extracting height information. One assumption for pre-processing is that the ground slope variation between the three plots within LiDAR&#x02019;s horizontal FOV can be ignored. LiDAR point clouds were processed using MATLAB R2017a (The MathWorks, Inc., Natick, MA, USA).</p><p>The basic principle of the point cloud pre-processing is that by fitting a linear least-squares curve to the Y-Z plane, the X-Y plane and the X-Z plane of a point cloud, respectively, and converting the slopes of the fitted curves to angles, the tilt of point clouds can be cancelled through rotating point clouds by the magnitude of the angles in reversed direction. For details see <xref ref-type="app" rid="app1-sensors-18-03731">Appendix A</xref>. After pre-processing was performed, cumulative Z value percentiles of a point cloud with 0.5 percentage intervals from 0 to 100 percent were extracted. In total there were 200 height values extracted and investigated for each plot.</p></sec></sec><sec id="sec2dot3-sensors-18-03731"><title>2.3. UAS</title><sec id="sec2dot3dot1-sensors-18-03731"><title>2.3.1. Hardware</title><p>A Zenmuse X5R RGB camera (DJI, Shenzhen, China) was mounted on a rotary-wing unmanned aerial vehicle (UAV), Matrice 600 Pro (M600) (DJI, Shenzhen, China). The RGB camera has an effective pixel resolution of 4608 &#x000d7; 3456. M600 was not available at the 2nd data collection campaign, and was replaced by another rotary-wing UAV, Phantom 3 Pro (P3P) (DJI, Shenzhen, China), with an RGB camera of 4000 &#x000d7; 3000 effective pixel resolution. For both cameras, the capture modes were set as auto, and the white balance was set to Sunny or Cloudy mode based on the specific weather conditions at the data collection campaigns.</p></sec><sec id="sec2dot3dot2-sensors-18-03731"><title>2.3.2. Flight Missions</title><p>The flight altitude was set to 20 m and 15 m above ground level for M600 and P3P, respectively, to achieve comparable ground sampling distance (GSD). The resulting GSD for M600-derived RGB mosaic was 0.47&#x02013;0.48 cm/pixel, and was 0.67 cm/pixel for P3P-derived mosaic. The forward overlap and side overlap were both set as 88 percent. </p><p>Twenty-one black and white cross-centered wooden boards, used as ground control points (GCPs), were evenly distributed over the 1.15-hectare field. Their GPS locations were measured by a GNSS RTK-GPS receiver (Topcon Positioning Systems, Inc., Tokyo, Japan), with sub-centimeter accuracy (less than 1 cm) in the X and Y directions, and centimeter accuracy (less than 2 cm) in the Z direction.</p></sec><sec id="sec2dot3dot3-sensors-18-03731"><title>2.3.3. Image Processing</title><p>RGB images were processed using Pix4Dmapper (Pix4D, Lausanne, Switzerland) to generate a digital surface model (DSM) in three steps: initial processing (step 1), point cloud and mesh (step 2), and DSM, orthomosaic and index (step 3). In step 1, 2D key-points&#x02014;points with common features among several images&#x02014;were matched, and 3D automatic tie points were derived. To further geo-calibrate the images, the geo-coordinates of GCPs&#x02019; centers were imported and marked out in associated images. In step 2, additional tie points were added to generate a densified point cloud based on the automatic tie points. In step 3, Delaunay triangulation was used to interpolate between tie points to generate the DSM, and the output was saved as a GeoTIFF file.</p><p>Since manual measurements represented the average heights of plots, UAS-derived plant heights were calculated on a plot level. The 100 plots were equally delineated in a shapefile in ArcMap (ArcGIS v10.5.1, Environmental System Research Institute Inc., Redlands, CA, USA) as shown in <xref ref-type="fig" rid="sensors-18-03731-f007">Figure 7</xref>. Each black rectangle was matched with the actual wheat plot by a designated ID number.</p></sec><sec id="sec2dot3dot4-sensors-18-03731"><title>2.3.4. Plant Height Extraction</title><p>A plant height map was created by subtracting a digital terrain model (DTM) from the DSM. DTM represents the elevation of bare soil, and it was generated by an interpolation tool, Kriging, in ArcGIS. Roughly 40% of all soil pixels were randomly selected from the DSM map for the interpolation. In order to explore the most representative plant height for each plot, pixel value percentiles within each plot delineation with 1 percentage intervals from 0 to 100 percent were calculated.</p></sec></sec></sec><sec id="sec3-sensors-18-03731"><title>3. Results</title><sec id="sec3dot1-sensors-18-03731"><title>3.1. Raw Point Clouds versus Processed Point Clouds</title><p>To evaluate the effectiveness of LiDAR point cloud pre-processing, plant heights were also extracted from all raw point clouds. With manual measurements being the standard, the minimum RMSE and the corresponding percentile of raw point clouds and processed point clouds at each data collection campaign were compared (<xref rid="sensors-18-03731-t002" ref-type="table">Table 2</xref>).</p><p>The point cloud pre-processing consistently improved the precision of LiDAR&#x02019;s plant height estimation by lowering the minimum RMSE at different data collection campaigns by between 12.85% and 44.95%, which confirmed its effectiveness for reducing the influence of the uneven ground surface on point clouds.</p></sec><sec id="sec3dot2-sensors-18-03731"><title>3.2. LiDAR Height Estimation Performace by Date, Manual Method and Plot Position</title><p>By comparing to manual measurements, RMSE, bias and R<sup>2</sup> of the heights extracted at each of the 200 percentiles of the processed point clouds across five data collection campaigns were investigated (<xref ref-type="fig" rid="sensors-18-03731-f008">Figure 8</xref>).</p><p>For a point cloud, low percentiles of the Z value represent the height of ground, and high percentiles represent the height of vegetation above ground. Since the height of a wheat plot was never measured as the height of the tallest plant, it can be seen why RMSE dropped as percentile increased and rose again when percentile approached 100 percent. At the percentiles of the minimum RMSE, the average bias over five data collections was &#x02212;0.0011 m, which demonstrated LiDAR&#x02019;s accuracy. The percentiles for maximum R<sup>2</sup> fluctuated between 98 and 99 percent, which did not appear to agree with the percentiles of minimum RMSE for the first two data collection campaigns (<xref rid="sensors-18-03731-t002" ref-type="table">Table 2</xref>).</p><p>Considering that the percentile of minimum RMSE could always vary if data were collected at different dates, identifying the optimal percentile for each individual data collection campaign was impractical. Instead of treating all data collection campaigns equally and choosing one universal percentile, we classified the 1st and 2nd data collection campaigns as the method A category, and the 3rd, 4th and 5th data collection campaigns as the method B category (<xref rid="sensors-18-03731-t001" ref-type="table">Table 1</xref>) for more precise height estimations. The RMSE of method A, method B and the all category (meaning all five data collection campaigns were treated as a whole) were compared (<xref rid="sensors-18-03731-t003" ref-type="table">Table 3</xref>).</p><p>The effect of plot position on RMSE was also investigated (<xref rid="sensors-18-03731-t003" ref-type="table">Table 3</xref>). LiDAR had a fixed horizontal resolution, so the closer an object was to LiDAR, the denser the acquired point cloud of that object would be. In our case, the point cloud generated at each measurement included two side plots and one middle plot, with LiDAR positioned above the middle plot; thus, middle plots had denser point clouds than side plots. On average the point clouds of side plots had about 6000 points while those of middle plots had about 8000 points.</p><p>Based on <xref rid="sensors-18-03731-t003" ref-type="table">Table 3</xref>, the manual method affected RMSE substantially as the minimum RMSE of the all category was 37.45% and 65.08% higher than the minimum RMSE of method A and B categories, respectively. Thus, it makes sense to use different optimal percentiles for the two method categories for future work. However, plot position did not seem to affect RMSE in a significant way, with an average RMSE increase of 0.0026 m when plot positions were not differentiated in the two method categories. Hence, the effect of plot position can be ignored in the future as the additional RMSE impact should be minor.</p></sec><sec id="sec3dot3-sensors-18-03731"><title>3.3. Optimal Pixel Value Percentiles of Plant Height Map from UAS</title><p>Using manual measurements as the reference, RMSE, bias and R<sup>2</sup> of plant heights extracted at each of the 100 pixel value percentiles of the plant height map were investigated. With the same reasoning as mentioned in <xref ref-type="sec" rid="sec3dot2-sensors-18-03731">Section 3.2</xref>, method categories were also applied here. For the method A category, the 89th percentile provided the smallest RMSE, of 0.0439 m, with a bias of &#x02212;0.0035 m and an R<sup>2</sup> of 0.897. For the method B category, the 100th percentile achieved the lowest RMSE of 0.1086 m, and the bias and R<sup>2</sup> were &#x02212;0.0702 m and 0.436, respectively.</p></sec><sec id="sec3dot4-sensors-18-03731"><title>3.4. Height Estimation Comparison between LiDAR, Ultrasonic Sensor and UAS</title><p>Over five data collection campaigns, ultrasonic sensor estimated canopy heights, UAS estimated canopy heights where 89th and 100th pixel value percentiles were chosen for method A and B categories, and LiDAR estimated canopy heights where 82nd and 99th Z value percentiles of processed point clouds were chosen for method A and B categories were plotted against manual measurements (<xref ref-type="fig" rid="sensors-18-03731-f009">Figure 9</xref>).</p><p>Among the three methods, LiDAR performed the best, UAS provided reasonable results, and ultrasonic sensors did not achieve suitable height estimates. With a large RMSE of 0.34 m and a low R<sup>2</sup> of 0.05, ultrasonic sensors tended to overestimate wheat canopy heights during the 1st data collection campaign and underestimate heights in the remaining data collection campaigns. As discussed in <xref ref-type="sec" rid="sec4dot1-sensors-18-03731">Section 4.1</xref>, ultrasonic sensors also provided some negative readings. Overall, UAS provided good wheat heights estimates, with an RMSE of 0.09 m and an R<sup>2</sup> of 0.91. However, this method tended to underestimate heights and its estimations tended to scatter more as wheat plants grew taller. LiDAR provided the most precise and accurate height estimations throughout the season, with a low RMSE of 0.05 m, a low bias of &#x02212;0.02 m and a high R<sup>2</sup> of 0.97. In terms of the results, LiDAR and UAS can be considered as alternative plant height evaluation methods.</p></sec></sec><sec id="sec4-sensors-18-03731"><title>4. Discussion</title><sec id="sec4dot1-sensors-18-03731"><title>4.1. Ultrasonic Sensor</title><p>The poor performance of ultrasonic sensors in this study can be explained by sensor limitations, wheat morphology and our measurement style. An ultrasonic sensor generates sound waves to detect distance. When the sound waves are not reflected straight back to the sensor, due to either sensor orientation or object surface orientation, the ultrasonic sensor may not capture the reflected sound waves. In this study, the slanting issue of the phenocart could be a source of such a problem. Further, when the surface of an object is not large enough to create strong echoes, an ultrasonic sensor may not treat the weak echoes as valid signals. A typical wheat plant has narrow leaves and thin spikes, thus making it hard for ultrasonic sensors to detect valid signals reflected from wheat. Moreover, because of our static measurement style, for each plot the ultrasonic sensor was only able to sample a small area (about 0.05 m<sup>2</sup> assuming 1 m distance between sensor and canopy) to represent the whole plot. Due to within-plot variation, the random error from sampling could not be assessed or corrected, which led to the low performance of the ultrasonic sensors. And&#x000fa;jar et al. [<xref rid="B35-sensors-18-03731" ref-type="bibr">35</xref>] also used ultrasonic sensors in a static measurement style to detect weeds among wheat plants, and a low Pearson&#x02019;s correlation of 0.32 between ultrasonic sensor readings and manually measured wheat heights was observed.</p><p>The overestimation and underestimation of wheat height by ultrasonic sensors is illustrated in <xref ref-type="fig" rid="sensors-18-03731-f010">Figure 10</xref>. For a young wheat plant, clustered leaves with natural curvature appeared to reflect sound waves effectively, but the reference height was measured as the height of the stem top instead of the leaf top (method A). As wheat plants grew taller and spikes started to emerge, only the vegetation at the bottom of the plant seemed to have sufficient density to reflect strong echoes, hence resulting in a spike tip height that was lower than that found by manual measurement (method B).</p><p>Near-zero canopy heights can appear when ultrasonic sensors cannot detect any significant echoes except for those reflected from ground. Moreover, if the phenocart is slanted so that the distance between ultrasonic sensors and ground at a given moment is larger than H<sub>s</sub> in Equation (2), negative canopy heights will be recorded.</p><p>To improve plant height estimation of ultrasonic sensors, a continuous measurement style&#x02014;i.e., multiple measurements per plot&#x02014;is preferred. In a previous study by Scotford and Miller [<xref rid="B34-sensors-18-03731" ref-type="bibr">34</xref>], approximately 180 wheat height measurements from ultrasonic sensor were recorded for each plot, and it was found that the 90% percentile of each data set provided the best wheat height estimation, with the lowest RMSE for a wheat variety of 0.046 m. Pittman et al. [<xref rid="B29-sensors-18-03731" ref-type="bibr">29</xref>] extracted 25&#x02013;30 ultrasonic sensor readings per wheat plot, and found a Pearson&#x02019;s R of 0.85 compared to manual measurements. </p><p>The continuous measurement style is superior to static measurement in terms of obtaining better ultrasonic height estimations. In the context of our manned multi-sensor system, however, the phenocart was often required to stop to capture images. Two issues could occur if a continuous measurement style were adopted for the system: first, due to the highly variable phenocart speed on a field with a rough surface, inconsistent numbers of height measurements could be recorded for different plots; second, a large number of repeated measurements will be taken from the same sampling area when the phenocart is stationary. Both issues can bias the data and make them troublesome to process. The static measurement style may, therefore, still be preferable for our system, in which case the ultrasonic sensor is not the best method for wheat height estimation.</p></sec><sec id="sec4dot2-sensors-18-03731"><title>4.2. UAS</title><p>In this study, UAS tended to underestimate wheat canopy heights, and the underestimation became more significant after the 2nd data collection campaign. Other related studies also found similar issues [<xref rid="B41-sensors-18-03731" ref-type="bibr">41</xref>,<xref rid="B43-sensors-18-03731" ref-type="bibr">43</xref>]. One possible explanation is that, due to the resolution limitation of cameras, wheat spikes could not be effectively detected in the images. The plant height map could only represent the heights of wheat leaf tops that were clearly identifiable in the images instead of the heights of wheat spike tips.</p><p>Scattered UAS plant height estimations near the end of the season might be explained by wind during data collection. Wind would not affect manual measurements as plants were held by hand while measuring; however wind could cause large movement of plants during UAS data collection, which became worse when plants were taller. Inconsistent plant positions among images could reduce mosaic quality, thus leading to higher error when generating DSM.</p><p>Generally, UAS-derived plant heights are obtained from the difference between DSM and DTM, and methods of deriving DTM vary among studies. Interpolating soil points segmented from DSM [<xref rid="B43-sensors-18-03731" ref-type="bibr">43</xref>,<xref rid="B45-sensors-18-03731" ref-type="bibr">45</xref>] and scanning bare soil before seedling emergence [<xref rid="B43-sensors-18-03731" ref-type="bibr">43</xref>,<xref rid="B44-sensors-18-03731" ref-type="bibr">44</xref>] have been indicated to provide similar plant height estimations [<xref rid="B43-sensors-18-03731" ref-type="bibr">43</xref>]. Typically, a specific pixel value within each plot delineation on a plant height map is selected to represent plant height of each plot, such as the average [<xref rid="B41-sensors-18-03731" ref-type="bibr">41</xref>,<xref rid="B43-sensors-18-03731" ref-type="bibr">43</xref>,<xref rid="B44-sensors-18-03731" ref-type="bibr">44</xref>], the 99.5th percentile [<xref rid="B45-sensors-18-03731" ref-type="bibr">45</xref>] or the 100th percentile [<xref rid="B49-sensors-18-03731" ref-type="bibr">49</xref>]. However, these values might not necessarily be the most representative for plant height depending on the growth stage, thus 100 different pixel value percentiles from the plant height map were investigated in this study.</p><p>The 0.91 R<sup>2</sup> achieved in this study was not better than those of other relevant studies on wheat, such as an R<sup>2</sup> of 0.92 from Bendig et al. [<xref rid="B41-sensors-18-03731" ref-type="bibr">41</xref>], R of 0.88 to 0.98 from Schirrmann et al. [<xref rid="B44-sensors-18-03731" ref-type="bibr">44</xref>], and an R<sup>2</sup> of 0.99 and an RMSE less than 0.03 m at a single data collection campaign by Holman et al. [<xref rid="B43-sensors-18-03731" ref-type="bibr">43</xref>]. Considering UAS plant height estimations are also affected by factors including image resolution, sample size and plant growth stage, the pixel value percentile selection methodology used in this study can, nonetheless, serve as a reference for future research.</p><p>To improve UAS plant height estimation in future studies, higher spatial resolution of images can be achieved by decreasing flight altitude, which will, however, increase fight time. Also, point clouds can be directly used instead of by extracting plant heights from a rasterized 2D plant height map, thus reserving the greatest quantity of plant 3D information.</p></sec><sec id="sec4dot3-sensors-18-03731"><title>4.3. LiDAR</title><p>The LiDAR point cloud pre-processing proposed in this study effectively reduced the influence from the slanting issue of the phenocart on the field. However, when ground is fully covered by vegetation, LiDAR with strongest return mode might not capture enough ground points, and pre-processing of the point cloud could not be undertaken. Due to the beam divergence of the lasers, a single firing of a laser can hit multiple objects resulting in multiple returns, and, typically, LiDAR can be configured to report multiple returns. A suggested solution is to configure LiDAR in multiple return mode since the last return signal has a higher chance of being reflected by soil, so a sufficient amount of ground points might be collected.</p><p>For processed point clouds, the minimum RMSE and the corresponding percentile increased as wheat grew taller (<xref rid="sensors-18-03731-t002" ref-type="table">Table 2</xref>). As method B was measuring the tip of wheat spikes while method A was measuring the top of wheat stems, it was expected that the optimal percentiles increased with data collection campaigns. Wind was suspected to be the reason for the increasing RMSE. As wheat plants get taller, wind can cause a larger degree of bending in plants, and LiDAR can capture deformed point clouds due to the wind. At the 5th data collection campaign, when the minimum RMSE was the largest, the wind speed on the field was maintained at 8.0 to 8.9 m/s, with gust speeds up to 14.8 m/s.</p><p>Generally, extracting plant heights from point clouds can include the following steps: soil level estimation, noisy point removal, rasterization of the point cloud, and percentile selection. Similar to the purpose of our ground baseline correction (<xref ref-type="app" rid="secAdot4-sensors-18-03731">Appendix A.4</xref>), most studies removed the effect of uneven soil levels by subtracting the corresponding soil height from vegetation points. The peak of the point cloud&#x02019;s Z value histogram [<xref rid="B11-sensors-18-03731" ref-type="bibr">11</xref>,<xref rid="B45-sensors-18-03731" ref-type="bibr">45</xref>], mean height of non-vegetation points [<xref rid="B10-sensors-18-03731" ref-type="bibr">10</xref>], vehicle wheel contact points [<xref rid="B9-sensors-18-03731" ref-type="bibr">9</xref>] and direct soil measurement at the beginning of the season [<xref rid="B47-sensors-18-03731" ref-type="bibr">47</xref>] have all been used to estimate soil level. Some studies have also assumed constant distance between sensor and ground [<xref rid="B17-sensors-18-03731" ref-type="bibr">17</xref>]. LiDAR can detect spurious points in very bright light conditions [<xref rid="B11-sensors-18-03731" ref-type="bibr">11</xref>], and some studies [<xref rid="B11-sensors-18-03731" ref-type="bibr">11</xref>,<xref rid="B45-sensors-18-03731" ref-type="bibr">45</xref>] removed outlier points by the method proposed by Rusu et al. [<xref rid="B50-sensors-18-03731" ref-type="bibr">50</xref>]. We did not perform any noise removal technique, since even if a small number of erroneous points existed, they would not affect our optimal percentile significantly. Point clouds are sometimes rasterized for easier future data analysis, and statistics such as maximum, mean and certain percentiles are calculated for each grid or pixel. We preferred point clouds over 2D height maps because rasterization can cause loss of information. &#x0201c;Percentiles&#x0201d; of point clouds are essentially plant heights, and 95th [<xref rid="B10-sensors-18-03731" ref-type="bibr">10</xref>], 95.5th [<xref rid="B11-sensors-18-03731" ref-type="bibr">11</xref>], 99.5th [<xref rid="B45-sensors-18-03731" ref-type="bibr">45</xref>] and 100th percentiles [<xref rid="B17-sensors-18-03731" ref-type="bibr">17</xref>,<xref rid="B46-sensors-18-03731" ref-type="bibr">46</xref>] have all been adopted in different studies.</p><p>Compared to the results of other relevant studies on wheat height estimation using LiDAR, such as an R<sup>2</sup> of 0.90 and an RMSE of 3.47 cm from Madec et al. [<xref rid="B45-sensors-18-03731" ref-type="bibr">45</xref>], R<sup>2</sup> of 0.88 and 0.95 at two different months from Underwood et al. [<xref rid="B9-sensors-18-03731" ref-type="bibr">9</xref>], an R<sup>2</sup> of 0.993 and an RMSE of 0.017 m from Jimenez-Berni et al. [<xref rid="B11-sensors-18-03731" ref-type="bibr">11</xref>], and an R<sup>2</sup> of 0.86 and an RMSE of 78.93 mm from Deery et al. [<xref rid="B10-sensors-18-03731" ref-type="bibr">10</xref>], this study demonstrated the practicality of obtaining adequate wheat canopy height estimations using LiDAR based only on a section of a plot instead of the whole plot. The advantage here was higher system throughput and easier data processing, but the downside might be lower precision for plant height estimation. In this study, the advantage of 3D LiDAR technology allowed us to adopt a static measurement style, whereas for a 2D LiDAR, the continuous motion of the sensor is a necessity for generating 3D point clouds.</p><p>Compared to an ultrasonic sensor, LiDAR had a much higher spatial resolution, and the laser beams were thin and diverged much less than sound waves. Compared to UAS, LiDAR point clouds were direct measurements, while the plant height map derived from UAS images was an indirect measurement. Thus, LiDAR&#x02019;s overall superior results were expected. The better performance of LiDAR compared to UAS also showed the advantage of proximal sensing over remote sensing; however, in terms of system throughput, it was difficult for a ground system to match with UAS: our ground phenotyping system normally took less than 15 min to scan 100 plots, whereas UAS could cover the same area within 6 min.</p><p>To improve LiDAR&#x02019;s plant height estimation performance, in the context of our static measurement style, denser point clouds&#x02014;i.e., collecting more data packets&#x02014;might provide more consistent results. In this study, due to the insufficient number of data collection campaigns, our data did not cover all the important growth stages, so we were, thus, unable to categorize data collection campaigns by growth stage. For future work optimal percentiles at each growth stage of wheat can be further investigated and established, which should provide more precise and accurate plant height estimations.</p></sec></sec><sec id="sec5-sensors-18-03731"><title>5. Conclusions</title><p>In this study, our proposed LiDAR point cloud pre-processing was demonstrated to be effective at reducing the influence of an uneven ground surface, and a LiDAR point cloud generated from a section of a plot was proven to be sufficient for providing precise and accurate plant height estimates. This methodology can be a reference for future studies that wish to adopt a static measurement style. With the reasonable results from UAS obtained in this study, considering the high-throughput for data collection, UAS can be a promising height estimation tool for a wide range of plants. The ultrasonic sensor, when used for plant height estimation in a static measurement style, is not suggested for plants with tall sward structures, such as mature wheat plants. In conclusion, LiDAR and UAS are both recommended as reliable alternative methods for wheat height evaluation.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors would like to thank Geng Bai and Scott Minchow for their help in developing the ground phenotyping system. The authors would also like to thank Yanni Yang, Arena Ezzati See, Arun Narenthiran Veeranampalayam Sivakumar and Jonathan Forbes for their assistance in data collection.</p></ack><notes><title>Author Contributions</title><p>Conceptualization, Y.G.; Data Curation, W.Y. and J.L.; Formal Analysis, W.Y., J.L. and M.B.; Funding Acquisition, Y.G., Y.S. and P.S.B.; Investigation, W.Y., J.L. and M.B.; Project Administration, W.Y. and Y.G.; Resources, Y.G., Y.S. and P.S.B.; Software, W.Y.; Supervision, Y.G., Y.S. and P.S.B.; Visualization, W.Y. and J.L.; Writing-Original Draft Preparation, W.Y., J.L. and M.B.; Writing-Review &#x00026; Editing, W.Y., J.L., Y.G., Y.S., and M.B.</p></notes><notes><title>Funding</title><p>The research was funded by University of Nebraska Foundation on wheat and small grain innovation.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><app-group><app id="app1-sensors-18-03731"><title>Appendix A</title><p>Steps for LiDAR point cloud pre-processing are explained here in detail.</p><sec id="secAdot1-sensors-18-03731"><title>Appendix A.1. Read Files</title><p>Three csv files containing XYZ coordinates of three-point clouds generated at the same measurement were read and combined back as one-point cloud (<xref ref-type="fig" rid="sensors-18-03731-f005">Figure 5</xref>).</p></sec><sec id="secAdot2-sensors-18-03731"><title>Appendix A.2. Y-Z Plane and X-Y Plane Rotation Correction</title><p>One reasonable assumption is that the points of a point cloud without the slanting issue should be evenly distributed along the Y dimension considering plants with the same genotype should have similar heights. A linear least-squares curve was fitted to the Y-Z plane (<xref ref-type="fig" rid="sensors-18-03731-f0A1">Figure A1</xref>b). The slope of the fitted curve was then converted to an angle &#x003b8; in radiance through the relationship:<disp-formula>&#x003b8; = arctan (slope).<label>(A1)</label></disp-formula></p><p>The point cloud was finally rotated clockwise by the angle &#x003b8; (<xref ref-type="fig" rid="sensors-18-03731-f0A1">Figure A1</xref>c). The rotation center could be set at any point, as later the point cloud will be repositioned in the Z dimension.</p><fig id="sensors-18-03731-f0A1" orientation="portrait" position="anchor"><label>Figure A1</label><caption><p>An example of Y-Z plane rotation correction: (<bold>a</bold>) Point cloud before rotation; (<bold>b</bold>) Fit a linear curve to points on Y-Z plane; (<bold>c</bold>) Rotate points on Y-Z plane by the angle &#x003b8;; (<bold>d</bold>) Point cloud after rotation.</p></caption><graphic xlink:href="sensors-18-03731-g0A1"/></fig><p>A similar procedure was also undertaken for the X-Y plane, which could be skipped as the slanting issue of point clouds on X-Y plane was minimum.</p></sec><sec id="secAdot3-sensors-18-03731"><title>Appendix A.3. X-Z Plane Rotation Correction</title><p>As the point distribution along the X-dimension could not be assumed to be even because points were representing plants with different genotypes, linear curve fitting couldn&#x02019;t be directly applied to the X-Z plane. The method proposed here was to find the rotation angle by finding the average Z value difference between the ground points of two alleyways.</p><p>The points were first sorted by their X values so that the line graph of the points on the X-Z plane would have a horizontal curve (<xref ref-type="fig" rid="sensors-18-03731-f0A2">Figure A2</xref>c). Then, a moving average filter with a 0.05-m span was applied to smooth the curve (<xref ref-type="fig" rid="sensors-18-03731-f0A2">Figure A2</xref>d). Since the FOV of LiDAR could cover two alleyways, the trend of the curve typically had four abrupt changes in the Z dimension as lasers would scan from the canopy to the ground and back to the canopy twice. After finding the position of the four most significant changes (<xref ref-type="fig" rid="sensors-18-03731-f0A2">Figure A2</xref>e), points with X values smaller than C<sub>1</sub>, larger than C<sub>4</sub>, or between C<sub>2</sub> and C<sub>3</sub> were deleted so that the portion of point cloud that contained two alleyways in the X dimension was extracted (<xref ref-type="fig" rid="sensors-18-03731-f0A2">Figure A2</xref>f).</p><fig id="sensors-18-03731-f0A2" orientation="portrait" position="anchor"><label>Figure A2</label><caption><p>An example of extracting coarse alleyway point clouds: (<bold>a</bold>) point cloud before rotation; (<bold>b</bold>) line graph before sorting; (<bold>c</bold>) line graph after sorting; (<bold>d</bold>) smoothed line; (<bold>e</bold>) positions of the four most significant changes; (<bold>f</bold>) deletion of points beyond the desired range.</p></caption><graphic xlink:href="sensors-18-03731-g0A2"/></fig><p>The point cloud containing two alleyways (<xref ref-type="fig" rid="sensors-18-03731-f0A2">Figure A2</xref>f) was separated into left and right alleyway point clouds using the border of X = 0. The non-ground points of two alleyway point clouds were further removed using the procedure explained below.</p><p>The kernel density in terms of Z values of the alleyway point cloud was first estimated (<xref ref-type="fig" rid="sensors-18-03731-f0A3">Figure A3</xref>b). As the points of ground were typically clustered at the bottom of the Z axis, a dominant peak P<sub>1</sub> could be observed from the kernel density graph, which was also the first peak in the Z axis direction. The first derivative of the kernel density curve was calculated (<xref ref-type="fig" rid="sensors-18-03731-f0A3">Figure A3</xref>c). Assuming ground points follow a normal distribution in the Z dimension, the first peak P<sub>2</sub> of the first derivate curve in the Z axis direction would be the inflection point of the normal distribution, and the distance between P<sub>1</sub> and P<sub>2</sub> would be one standard deviation of the distribution. For a normal distribution, the range &#x003bc; &#x000b1; 2&#x003c3; includes about 95.45% of the values. Here a threshold of &#x003bc; + 2&#x003c3; on the Z axis was used to separate non-ground points from ground points, where &#x003bc; is P<sub>1</sub> and &#x003c3; is P<sub>1</sub> &#x02212; P<sub>2</sub>, and points with Z values larger than the threshold were deleted (<xref ref-type="fig" rid="sensors-18-03731-f0A3">Figure A3</xref>d).</p><fig id="sensors-18-03731-f0A3" orientation="portrait" position="anchor"><label>Figure A3</label><caption><p>An example of extracting a refined alleyway point cloud: (<bold>a</bold>) point cloud of ground before cleaning; (<bold>b</bold>) point cloud kernel density in the Z dimension; (<bold>c</bold>) first derivative of the kernel density; (<bold>d</bold>) point cloud of ground after cleaning.</p></caption><graphic xlink:href="sensors-18-03731-g0A3"/></fig><p>After combining refined left and right alleyway point clouds (<xref ref-type="fig" rid="sensors-18-03731-f0A4">Figure A4</xref>a), a linear least-squares curve was fitted to the combined alleyway point cloud on the X-Z plane (<xref ref-type="fig" rid="sensors-18-03731-f0A4">Figure A4</xref>b), and the point cloud with the Y-Z and X-Y plane rotation correction performed (<xref ref-type="fig" rid="sensors-18-03731-f0A2">Figure A2</xref>a) was rotated by the angle &#x003c6;, which was derived from the slope of the fitted curve (<xref ref-type="fig" rid="sensors-18-03731-f0A4">Figure A4</xref>c).</p><fig id="sensors-18-03731-f0A4" orientation="portrait" position="anchor"><label>Figure A4</label><caption><p>An example of X-Z plane rotation correction: (<bold>a</bold>) point cloud of ground before rotation; (<bold>b</bold>) linear curve fitted to ground points on the X-Z plane; (<bold>c</bold>) rotation of points on the X-Z plane by the angle &#x003c6;; (<bold>d</bold>) point cloud after rotation.</p></caption><graphic xlink:href="sensors-18-03731-g0A4"/></fig></sec><sec id="secAdot4-sensors-18-03731"><title>Appendix A.4. Ground Baseline Correction</title><p>The logic of the X-Z plane rotation correction was again executed on the point cloud with the X-Z plane rotation correction already performed (<xref ref-type="fig" rid="sensors-18-03731-f0A4">Figure A4</xref>d) to extract the rotated and refined alleyway point clouds (<xref ref-type="fig" rid="sensors-18-03731-f0A5">Figure A5</xref>a). The average Z value of the alleyway point cloud was calculated (<xref ref-type="fig" rid="sensors-18-03731-f0A5">Figure A5</xref>b), and the Z values of the whole point cloud (<xref ref-type="fig" rid="sensors-18-03731-f0A4">Figure A4</xref>d) were adjusted so that the average Z value of the alleyway point cloud would be located at 0.</p><fig id="sensors-18-03731-f0A5" orientation="portrait" position="anchor"><label>Figure A5</label><caption><p>An example of ground baseline correction: (<bold>a</bold>) point cloud of ground before shifting; (<bold>b</bold>) the mean in the Z dimension; (<bold>c</bold>) points on the X-Z plane shifted by the offset; (<bold>d</bold>) point cloud after shifting.</p></caption><graphic xlink:href="sensors-18-03731-g0A5"/></fig></sec><sec id="secAdot5-sensors-18-03731"><title>Appendix A.5. Split Point Cloud</title><p>The mean X values S<sub>1</sub> and S<sub>2</sub> of two alleyway point clouds were calculated (<xref ref-type="fig" rid="sensors-18-03731-f0A6">Figure A6</xref>b) and used as the border between different plots to split the point cloud (<xref ref-type="fig" rid="sensors-18-03731-f0A6">Figure A6</xref>c).</p><fig id="sensors-18-03731-f0A6" orientation="portrait" position="anchor"><label>Figure A6</label><caption><p>An example of splitting a point cloud: (<bold>a</bold>) point cloud of ground after rotation and shifting; (<bold>b</bold>) the mean in the X dimension for each side; (<bold>c</bold>) point cloud of each plot after splitting.</p></caption><graphic xlink:href="sensors-18-03731-g0A6"/></fig></sec></app></app-group><ref-list><title>References</title><ref id="B1-sensors-18-03731"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhatta</surname><given-names>M.</given-names></name><name><surname>Eskridge</surname><given-names>K.M.</given-names></name><name><surname>Rose</surname><given-names>D.J.</given-names></name><name><surname>Santra</surname><given-names>D.K.</given-names></name><name><surname>Baenziger</surname><given-names>P.S.</given-names></name><name><surname>Regassa</surname><given-names>T.</given-names></name></person-group><article-title>Seeding rate, genotype, and topdressed nitrogen effects on yield and agronomic characteristics of winter wheat</article-title><source>Crop Sci.</source><year>2017</year><volume>57</volume><fpage>951</fpage><lpage>963</lpage><pub-id pub-id-type="doi">10.2135/cropsci2016.02.0103</pub-id></element-citation></ref><ref id="B2-sensors-18-03731"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Navabi</surname><given-names>A.</given-names></name><name><surname>Iqbal</surname><given-names>M.</given-names></name><name><surname>Strenzke</surname><given-names>K.</given-names></name><name><surname>Spaner</surname><given-names>D.</given-names></name></person-group><article-title>The relationship between lodging and plant height in a diverse wheat population</article-title><source>Can. J. Plant Sci.</source><year>2006</year><volume>86</volume><fpage>723</fpage><lpage>726</lpage><pub-id pub-id-type="doi">10.4141/P05-144</pub-id></element-citation></ref><ref id="B3-sensors-18-03731"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schirrmann</surname><given-names>M.</given-names></name><name><surname>Hamdorf</surname><given-names>A.</given-names></name><name><surname>Garz</surname><given-names>A.</given-names></name><name><surname>Ustyuzhanin</surname><given-names>A.</given-names></name><name><surname>Dammer</surname><given-names>K.H.</given-names></name></person-group><article-title>Estimating wheat biomass by combining image clustering with crop height</article-title><source>Comput. Electron. Agric.</source><year>2016</year><volume>121</volume><fpage>374</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2016.01.007</pub-id></element-citation></ref><ref id="B4-sensors-18-03731"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mao</surname><given-names>S.-L.</given-names></name><name><surname>Wei</surname><given-names>Y.-M.</given-names></name><name><surname>Cao</surname><given-names>W.</given-names></name><name><surname>Lan</surname><given-names>X.-J.</given-names></name><name><surname>Yu</surname><given-names>M.</given-names></name><name><surname>Chen</surname><given-names>Z.-M.</given-names></name><name><surname>Chen</surname><given-names>G.-Y.</given-names></name><name><surname>Zheng</surname><given-names>Y.-L.</given-names></name></person-group><article-title>Confirmation of the relationship between plant height and Fusarium head blight resistance in wheat (<italic>Triticum aestivum</italic> L.) by QTL meta-analysis</article-title><source>Euphytica</source><year>2010</year><volume>174</volume><fpage>343</fpage><lpage>356</lpage><pub-id pub-id-type="doi">10.1007/s10681-010-0128-9</pub-id></element-citation></ref><ref id="B5-sensors-18-03731"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shakoor</surname><given-names>N.</given-names></name><name><surname>Lee</surname><given-names>S.</given-names></name><name><surname>Mockler</surname><given-names>T.C.</given-names></name></person-group><article-title>High throughput phenotyping to accelerate crop breeding and monitoring of diseases in the field</article-title><source>Curr. Opin. Plant Biol.</source><year>2017</year><volume>38</volume><fpage>184</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1016/j.pbi.2017.05.006</pub-id><?supplied-pmid 28738313?><pub-id pub-id-type="pmid">28738313</pub-id></element-citation></ref><ref id="B6-sensors-18-03731"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Araus</surname><given-names>J.L.</given-names></name><name><surname>Cairns</surname><given-names>J.E.</given-names></name></person-group><article-title>Field high-throughput phenotyping: The new crop breeding frontier</article-title><source>Trends Plant Sci.</source><year>2014</year><volume>19</volume><fpage>52</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1016/j.tplants.2013.09.008</pub-id><?supplied-pmid 24139902?><pub-id pub-id-type="pmid">24139902</pub-id></element-citation></ref><ref id="B7-sensors-18-03731"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>J.W.</given-names></name><name><surname>Andrade-Sanchez</surname><given-names>P.</given-names></name><name><surname>Gore</surname><given-names>M.A.</given-names></name><name><surname>Bronson</surname><given-names>K.F.</given-names></name><name><surname>Coffelt</surname><given-names>T.A.</given-names></name><name><surname>Conley</surname><given-names>M.M.</given-names></name><name><surname>Feldmann</surname><given-names>K.A.</given-names></name><name><surname>French</surname><given-names>A.N.</given-names></name><name><surname>Heun</surname><given-names>J.T.</given-names></name><name><surname>Hunsaker</surname><given-names>D.J.</given-names></name><etal/></person-group><article-title>Field-based phenomics for plant genetics research</article-title><source>Field Crops Res.</source><year>2012</year><volume>133</volume><fpage>101</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1016/j.fcr.2012.04.003</pub-id></element-citation></ref><ref id="B8-sensors-18-03731"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virlet</surname><given-names>N.</given-names></name><name><surname>Sabermanesh</surname><given-names>K.</given-names></name><name><surname>Sadeghi-Tehran</surname><given-names>P.</given-names></name><name><surname>Hawkesford</surname><given-names>M.J.</given-names></name></person-group><article-title>Field Scanalyzer: An automated robotic field phenotyping platform for detailed crop monitoring</article-title><source>Funct. Plant Biol.</source><year>2017</year><volume>44</volume><fpage>143</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1071/FP16163</pub-id></element-citation></ref><ref id="B9-sensors-18-03731"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Underwood</surname><given-names>J.</given-names></name><name><surname>Wendel</surname><given-names>A.</given-names></name><name><surname>Schofield</surname><given-names>B.</given-names></name><name><surname>McMurray</surname><given-names>L.</given-names></name><name><surname>Kimber</surname><given-names>R.</given-names></name></person-group><article-title>Efficient in-field plant phenomics for row-crops with an autonomous ground vehicle</article-title><source>J. Field Robot.</source><year>2017</year><volume>34</volume><fpage>1061</fpage><lpage>1083</lpage><pub-id pub-id-type="doi">10.1002/rob.21728</pub-id></element-citation></ref><ref id="B10-sensors-18-03731"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deery</surname><given-names>D.</given-names></name><name><surname>Jimenez-Berni</surname><given-names>J.</given-names></name><name><surname>Jones</surname><given-names>H.</given-names></name><name><surname>Sirault</surname><given-names>X.</given-names></name><name><surname>Furbank</surname><given-names>R.</given-names></name></person-group><article-title>Proximal Remote Sensing Buggies and Potential Applications for Field-Based Phenotyping</article-title><source>Agronomy</source><year>2014</year><volume>4</volume><fpage>349</fpage><lpage>379</lpage><pub-id pub-id-type="doi">10.3390/agronomy4030349</pub-id></element-citation></ref><ref id="B11-sensors-18-03731"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jimenez-Berni</surname><given-names>J.A.</given-names></name><name><surname>Deery</surname><given-names>D.M.</given-names></name><name><surname>Rozas-Larraondo</surname><given-names>P.</given-names></name><name><surname>Condon</surname><given-names>A.G.</given-names></name><name><surname>Rebetzke</surname><given-names>G.J.</given-names></name><name><surname>James</surname><given-names>R.A.</given-names></name><name><surname>Bovill</surname><given-names>W.D.</given-names></name><name><surname>Furbank</surname><given-names>R.T.</given-names></name><name><surname>Sirault</surname><given-names>X.R.R.</given-names></name></person-group><article-title>High Throughput Determination of Plant Height, Ground Cover, and Above-Ground Biomass in Wheat with LiDAR</article-title><source>Front. Plant Sci.</source><year>2018</year><volume>9</volume><fpage>1</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.3389/fpls.2018.00237</pub-id><?supplied-pmid 29535749?><pub-id pub-id-type="pmid">29410674</pub-id></element-citation></ref><ref id="B12-sensors-18-03731"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>H.</given-names></name><name><surname>Tang</surname><given-names>L.</given-names></name><name><surname>Whitham</surname><given-names>S.A.</given-names></name><name><surname>Mei</surname><given-names>Y.</given-names></name></person-group><article-title>A robotic platform for corn seedling morphological traits characterization</article-title><source>Sensors</source><year>2017</year><volume>17</volume><elocation-id>2082</elocation-id><pub-id pub-id-type="doi">10.3390/s17092082</pub-id><?supplied-pmid 28895892?><pub-id pub-id-type="pmid">28895892</pub-id></element-citation></ref><ref id="B13-sensors-18-03731"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klose</surname><given-names>R.</given-names></name><name><surname>Penlington</surname><given-names>J.</given-names></name><name><surname>Ruckelshausen</surname><given-names>A.</given-names></name></person-group><article-title>Usability of 3D time-of-flight cameras for automatic plant phenotyping</article-title><source>Bornimer Agrartech. Berichte</source><year>2011</year><volume>69</volume><fpage>93</fpage><lpage>105</lpage></element-citation></ref><ref id="B14-sensors-18-03731"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Der Heijden</surname><given-names>G.</given-names></name><name><surname>Song</surname><given-names>Y.</given-names></name><name><surname>Horgan</surname><given-names>G.</given-names></name><name><surname>Polder</surname><given-names>G.</given-names></name><name><surname>Dieleman</surname><given-names>A.</given-names></name><name><surname>Bink</surname><given-names>M.</given-names></name><name><surname>Palloix</surname><given-names>A.</given-names></name><name><surname>Van Eeuwijk</surname><given-names>F.</given-names></name><name><surname>Glasbey</surname><given-names>C.</given-names></name></person-group><article-title>SPICY: Towards automated phenotyping of large pepper plants in the greenhouse</article-title><source>Funct. Plant Biol.</source><year>2012</year><volume>39</volume><fpage>870</fpage><lpage>877</lpage><pub-id pub-id-type="doi">10.1071/FP12019</pub-id></element-citation></ref><ref id="B15-sensors-18-03731"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>J.</given-names></name><name><surname>Kumar</surname><given-names>P.</given-names></name><name><surname>Chopin</surname><given-names>J.</given-names></name><name><surname>Miklavcic</surname><given-names>S.J.</given-names></name></person-group><article-title>Land-based crop phenotyping by image analysis: Accurate estimation of canopy height distributions using stereo images</article-title><source>PLoS ONE</source><year>2018</year><volume>13</volume><elocation-id>e0196671</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0196671</pub-id><?supplied-pmid 29795568?><pub-id pub-id-type="pmid">29795568</pub-id></element-citation></ref><ref id="B16-sensors-18-03731"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fricke</surname><given-names>T.</given-names></name><name><surname>Richter</surname><given-names>F.</given-names></name><name><surname>Wachendorf</surname><given-names>M.</given-names></name></person-group><article-title>Assessment of forage mass from grassland swards by height measurement using an ultrasonic sensor</article-title><source>Comput. Electron. Agric.</source><year>2011</year><volume>79</volume><fpage>142</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2011.09.005</pub-id></element-citation></ref><ref id="B17-sensors-18-03731"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>S.</given-names></name><name><surname>Li</surname><given-names>C.</given-names></name><name><surname>Paterson</surname><given-names>A.H.</given-names></name></person-group><article-title>In-field high-throughput phenotyping of cotton plant height using LiDAR</article-title><source>Remote Sens.</source><year>2017</year><volume>9</volume><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.3390/rs9040377</pub-id></element-citation></ref><ref id="B18-sensors-18-03731"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barker</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>N.</given-names></name><name><surname>Sharon</surname><given-names>J.</given-names></name><name><surname>Steeves</surname><given-names>R.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Wei</surname><given-names>Y.</given-names></name><name><surname>Poland</surname><given-names>J.</given-names></name></person-group><article-title>Development of a field-based high-throughput mobile phenotyping platform</article-title><source>Comput. Electron. Agric.</source><year>2016</year><volume>122</volume><fpage>74</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2016.01.017</pub-id></element-citation></ref><ref id="B19-sensors-18-03731"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadeghi-Tehran</surname><given-names>P.</given-names></name><name><surname>Virlet</surname><given-names>N.</given-names></name><name><surname>Sabermanesh</surname><given-names>K.</given-names></name><name><surname>Hawkesford</surname><given-names>M.J.</given-names></name></person-group><article-title>Multi-feature machine learning model for automatic segmentation of green fractional vegetation cover for high-throughput field phenotyping</article-title><source>Plant Methods</source><year>2017</year><volume>13</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1186/s13007-017-0253-8</pub-id><?supplied-pmid 29201134?><pub-id pub-id-type="pmid">28053646</pub-id></element-citation></ref><ref id="B20-sensors-18-03731"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yue</surname><given-names>J.</given-names></name><name><surname>Yang</surname><given-names>G.</given-names></name><name><surname>Li</surname><given-names>C.</given-names></name><name><surname>Li</surname><given-names>Z.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Feng</surname><given-names>H.</given-names></name><name><surname>Xu</surname><given-names>B.</given-names></name></person-group><article-title>Estimation of winter wheat above-ground biomass using unmanned aerial vehicle-based snapshot hyperspectral sensor and crop height improved models</article-title><source>Remote Sens.</source><year>2017</year><volume>9</volume><pub-id pub-id-type="doi">10.3390/rs9070708</pub-id></element-citation></ref><ref id="B21-sensors-18-03731"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tilly</surname><given-names>N.</given-names></name><name><surname>Aasen</surname><given-names>H.</given-names></name><name><surname>Bareth</surname><given-names>G.</given-names></name></person-group><article-title>Fusion of plant height and vegetation indices for the estimation of barley biomass</article-title><source>Remote Sens.</source><year>2015</year><volume>7</volume><fpage>11449</fpage><lpage>11480</lpage><pub-id pub-id-type="doi">10.3390/rs70911449</pub-id></element-citation></ref><ref id="B22-sensors-18-03731"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>N.</given-names></name><name><surname>Taylor</surname><given-names>R.K.</given-names></name><name><surname>Raun</surname><given-names>W.R.</given-names></name></person-group><article-title>Improvement of a ground-LiDAR-based corn plant population and spacing measurement system</article-title><source>Comput. Electron. Agric.</source><year>2015</year><volume>112</volume><fpage>92</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2014.11.026</pub-id></element-citation></ref><ref id="B23-sensors-18-03731"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>K.K.</given-names></name><name><surname>Chen</surname><given-names>G.</given-names></name><name><surname>Vogler</surname><given-names>J.B.</given-names></name><name><surname>Meentemeyer</surname><given-names>R.K.</given-names></name></person-group><article-title>When Big Data are Too Much: Effects of LiDAR Returns and Point Density on Estimation of Forest Biomass</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2016</year><volume>9</volume><fpage>3210</fpage><lpage>3218</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2016.2522960</pub-id></element-citation></ref><ref id="B24-sensors-18-03731"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>G.</given-names></name><name><surname>Liu</surname><given-names>J.</given-names></name><name><surname>Zhao</surname><given-names>C.</given-names></name><name><surname>Li</surname><given-names>Z.</given-names></name><name><surname>Huang</surname><given-names>Y.</given-names></name><name><surname>Yu</surname><given-names>H.</given-names></name><name><surname>Xu</surname><given-names>B.</given-names></name><name><surname>Yang</surname><given-names>X.</given-names></name><name><surname>Zhu</surname><given-names>D.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><etal/></person-group><article-title>Unmanned Aerial Vehicle Remote Sensing for Field-Based Crop Phenotyping: Current Status and Perspectives</article-title><source>Front. Plant Sci.</source><year>2017</year><volume>8</volume><pub-id pub-id-type="doi">10.3389/fpls.2017.01111</pub-id><?supplied-pmid 28713402?><pub-id pub-id-type="pmid">28713402</pub-id></element-citation></ref><ref id="B25-sensors-18-03731"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>K.K.</given-names></name><name><surname>Frazier</surname><given-names>A.E.</given-names></name></person-group><article-title>A meta-analysis and review of unmanned aircraft system (UAS) imagery for terrestrial applications</article-title><source>Int. J. Remote Sens.</source><year>2018</year><volume>39</volume><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1080/01431161.2017.1420941</pub-id></element-citation></ref><ref id="B26-sensors-18-03731"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>C.</given-names></name><name><surname>Kovacs</surname><given-names>J.M.</given-names></name></person-group><article-title>The application of small unmanned aerial systems for precision agriculture: A review</article-title><source>Precis. Agric.</source><year>2012</year><volume>13</volume><fpage>693</fpage><lpage>712</lpage><pub-id pub-id-type="doi">10.1007/s11119-012-9274-5</pub-id></element-citation></ref><ref id="B27-sensors-18-03731"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharma</surname><given-names>B.</given-names></name><name><surname>Ritchie</surname><given-names>G.L.</given-names></name></person-group><article-title>High-throughput phenotyping of cotton in multiple irrigation environments</article-title><source>Crop Sci.</source><year>2015</year><volume>55</volume><fpage>958</fpage><lpage>969</lpage><pub-id pub-id-type="doi">10.2135/cropsci2014.04.0310</pub-id></element-citation></ref><ref id="B28-sensors-18-03731"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andrade-Sanchez</surname><given-names>P.</given-names></name><name><surname>Gore</surname><given-names>M.A.</given-names></name><name><surname>Heun</surname><given-names>J.T.</given-names></name><name><surname>Thorp</surname><given-names>K.R.</given-names></name><name><surname>Carmo-Silva</surname><given-names>A.E.</given-names></name><name><surname>French</surname><given-names>A.</given-names></name><name><surname>Salvucci</surname><given-names>M.E.</given-names></name><name><surname>White</surname><given-names>J.W.</given-names></name></person-group><article-title>Development and evaluation of a field-based, high-thoughput phenotyping platform</article-title><source>Funct. Plant Biol.</source><year>2014</year><volume>41</volume><fpage>68</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1071/FP13126</pub-id></element-citation></ref><ref id="B29-sensors-18-03731"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pittman</surname><given-names>J.J.</given-names></name><name><surname>Arnall</surname><given-names>D.B.</given-names></name><name><surname>Interrante</surname><given-names>S.M.</given-names></name><name><surname>Moffet</surname><given-names>C.A.</given-names></name><name><surname>Butler</surname><given-names>T.J.</given-names></name></person-group><article-title>Estimation of biomass and canopy height in bermudagrass, alfalfa, and wheat using ultrasonic, laser, and spectral sensors</article-title><source>Sensors</source><year>2015</year><volume>15</volume><fpage>2920</fpage><lpage>2943</lpage><pub-id pub-id-type="doi">10.3390/s150202920</pub-id><?supplied-pmid 25635415?><pub-id pub-id-type="pmid">25635415</pub-id></element-citation></ref><ref id="B30-sensors-18-03731"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farooque</surname><given-names>A.A.</given-names></name><name><surname>Chang</surname><given-names>Y.K.</given-names></name><name><surname>Zaman</surname><given-names>Q.U.</given-names></name><name><surname>Groulx</surname><given-names>D.</given-names></name><name><surname>Schumann</surname><given-names>A.W.</given-names></name><name><surname>Esau</surname><given-names>T.J.</given-names></name></person-group><article-title>Performance evaluation of multiple ground based sensors mounted on a commercial wild blueberry harvester to sense plant height, fruit yield and topographic features in real-time</article-title><source>Comput. Electron. Agric.</source><year>2013</year><volume>91</volume><fpage>135</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2012.12.006</pub-id></element-citation></ref><ref id="B31-sensors-18-03731"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>Y.K.</given-names></name><name><surname>Zaman</surname><given-names>Q.U.</given-names></name><name><surname>Rehman</surname><given-names>T.U.</given-names></name><name><surname>Farooque</surname><given-names>A.A.</given-names></name><name><surname>Esau</surname><given-names>T.</given-names></name><name><surname>Jameel</surname><given-names>M.W.</given-names></name></person-group><article-title>A real-time ultrasonic system to measure wild blueberry plant height during harvesting</article-title><source>Biosyst. Eng.</source><year>2017</year><volume>157</volume><fpage>35</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1016/j.biosystemseng.2017.02.004</pub-id></element-citation></ref><ref id="B32-sensors-18-03731"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fricke</surname><given-names>T.</given-names></name><name><surname>Wachendorf</surname><given-names>M.</given-names></name></person-group><article-title>Combining ultrasonic sward height and spectral signatures to assess the biomass of legume-grass swards</article-title><source>Comput. Electron. Agric.</source><year>2013</year><volume>99</volume><fpage>236</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2013.10.004</pub-id></element-citation></ref><ref id="B33-sensors-18-03731"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barmeier</surname><given-names>G.</given-names></name><name><surname>Mistele</surname><given-names>B.</given-names></name><name><surname>Schmidhalter</surname><given-names>U.</given-names></name></person-group><article-title>Referencing laser and ultrasonic height measurements of barleycultivars by using a herbometre as standard</article-title><source>Crop Pasture Sci.</source><year>2016</year><volume>67</volume><fpage>1215</fpage><lpage>1222</lpage><pub-id pub-id-type="doi">10.1071/CP16238</pub-id></element-citation></ref><ref id="B34-sensors-18-03731"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scotford</surname><given-names>I.M.</given-names></name><name><surname>Miller</surname><given-names>P.C.H.</given-names></name></person-group><article-title>Combination of Spectral Reflectance and Ultrasonic Sensing to monitor the Growth of Winter Wheat</article-title><source>Biosyst. Eng.</source><year>2004</year><volume>87</volume><fpage>27</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1016/j.biosystemseng.2003.09.009</pub-id></element-citation></ref><ref id="B35-sensors-18-03731"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>And&#x000fa;jar</surname><given-names>D.</given-names></name><name><surname>Weis</surname><given-names>M.</given-names></name><name><surname>Gerhards</surname><given-names>R.</given-names></name></person-group><article-title>An ultrasonic system for weed detection in cereal crops</article-title><source>Sensors</source><year>2012</year><volume>12</volume><fpage>17343</fpage><lpage>17357</lpage><pub-id pub-id-type="doi">10.3390/s121217343</pub-id><?supplied-pmid 23443401?><pub-id pub-id-type="pmid">23443401</pub-id></element-citation></ref><ref id="B36-sensors-18-03731"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geipel</surname><given-names>J.</given-names></name><name><surname>Link</surname><given-names>J.</given-names></name><name><surname>Claupein</surname><given-names>W.</given-names></name></person-group><article-title>Combined spectral and spatial modeling of corn yield based on aerial images and crop surface models acquired with an unmanned aircraft system</article-title><source>Remote Sens.</source><year>2014</year><volume>6</volume><fpage>10335</fpage><lpage>10355</lpage><pub-id pub-id-type="doi">10.3390/rs61110335</pub-id></element-citation></ref><ref id="B37-sensors-18-03731"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malambo</surname><given-names>L.</given-names></name><name><surname>Popescu</surname><given-names>S.C.</given-names></name><name><surname>Murray</surname><given-names>S.C.</given-names></name><name><surname>Putman</surname><given-names>E.</given-names></name><name><surname>Pugh</surname><given-names>N.A.</given-names></name><name><surname>Horne</surname><given-names>D.W.</given-names></name><name><surname>Richardson</surname><given-names>G.</given-names></name><name><surname>Sheridan</surname><given-names>R.</given-names></name><name><surname>Rooney</surname><given-names>W.L.</given-names></name><name><surname>Avant</surname><given-names>R.</given-names></name><etal/></person-group><article-title>Multitemporal field-based plant height estimation using 3D point clouds generated from small unmanned aerial systems high-resolution imagery</article-title><source>Int. J. Appl. Earth Obs. Geoinf.</source><year>2018</year><volume>64</volume><fpage>31</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/j.jag.2017.08.014</pub-id></element-citation></ref><ref id="B38-sensors-18-03731"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varela</surname><given-names>S.</given-names></name><name><surname>Assefa</surname><given-names>Y.</given-names></name><name><surname>Vara Prasad</surname><given-names>P.V.</given-names></name><name><surname>Peralta</surname><given-names>N.R.</given-names></name><name><surname>Griffin</surname><given-names>T.W.</given-names></name><name><surname>Sharda</surname><given-names>A.</given-names></name><name><surname>Ferguson</surname><given-names>A.</given-names></name><name><surname>Ciampitti</surname><given-names>I.A.</given-names></name></person-group><article-title>Spatio-temporal evaluation of plant height in corn via unmanned aerial systems</article-title><source>J. Appl. Remote Sens.</source><year>2017</year><volume>11</volume><fpage>1</fpage><pub-id pub-id-type="doi">10.1117/1.JRS.11.036013</pub-id></element-citation></ref><ref id="B39-sensors-18-03731"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>Y.</given-names></name><name><surname>Thomasson</surname><given-names>J.A.</given-names></name><name><surname>Murray</surname><given-names>S.C.</given-names></name><name><surname>Pugh</surname><given-names>N.A.</given-names></name><name><surname>Rooney</surname><given-names>W.L.</given-names></name><name><surname>Shafian</surname><given-names>S.</given-names></name><name><surname>Rajan</surname><given-names>N.</given-names></name><name><surname>Rouze</surname><given-names>G.</given-names></name><name><surname>Morgan</surname><given-names>C.L.S.</given-names></name><name><surname>Neely</surname><given-names>H.L.</given-names></name><etal/></person-group><article-title>Unmanned Aerial Vehicles for High-Throughput Phenotyping and Agronomic Research</article-title><source>PLoS ONE</source><year>2016</year><volume>11</volume><elocation-id>e0159781</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0159781</pub-id><?supplied-pmid 27472222?><pub-id pub-id-type="pmid">27472222</pub-id></element-citation></ref><ref id="B40-sensors-18-03731"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watanabe</surname><given-names>K.</given-names></name><name><surname>Guo</surname><given-names>W.</given-names></name><name><surname>Arai</surname><given-names>K.</given-names></name><name><surname>Takanashi</surname><given-names>H.</given-names></name><name><surname>Kajiya-Kanegae</surname><given-names>H.</given-names></name><name><surname>Kobayashi</surname><given-names>M.</given-names></name><name><surname>Yano</surname><given-names>K.</given-names></name><name><surname>Tokunaga</surname><given-names>T.</given-names></name><name><surname>Fujiwara</surname><given-names>T.</given-names></name><name><surname>Tsutsumi</surname><given-names>N.</given-names></name><etal/></person-group><article-title>High-Throughput Phenotyping of Sorghum Plant Height Using an Unmanned Aerial Vehicle and Its Application to Genomic Prediction Modeling</article-title><source>Front. Plant Sci.</source><year>2017</year><volume>8</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.3389/fpls.2017.00421</pub-id><?supplied-pmid 28400784?><pub-id pub-id-type="pmid">28220127</pub-id></element-citation></ref><ref id="B41-sensors-18-03731"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bendig</surname><given-names>J.</given-names></name><name><surname>Bolten</surname><given-names>A.</given-names></name><name><surname>Bennertz</surname><given-names>S.</given-names></name><name><surname>Broscheit</surname><given-names>J.</given-names></name><name><surname>Eichfuss</surname><given-names>S.</given-names></name><name><surname>Bareth</surname><given-names>G.</given-names></name></person-group><article-title>Estimating biomass of barley using crop surface models (CSMs) derived from UAV-based RGB imaging</article-title><source>Remote Sens.</source><year>2014</year><volume>6</volume><fpage>10395</fpage><lpage>10412</lpage><pub-id pub-id-type="doi">10.3390/rs61110395</pub-id></element-citation></ref><ref id="B42-sensors-18-03731"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haghighattalab</surname><given-names>A.</given-names></name><name><surname>Crain</surname><given-names>J.</given-names></name><name><surname>Mondal</surname><given-names>S.</given-names></name><name><surname>Rutkoski</surname><given-names>J.</given-names></name><name><surname>Singh</surname><given-names>R.P.</given-names></name><name><surname>Poland</surname><given-names>J.</given-names></name></person-group><article-title>Application of geographically weighted regression to improve grain yield prediction from unmanned aerial system imagery</article-title><source>Crop Sci.</source><year>2017</year><volume>57</volume><fpage>2478</fpage><lpage>2489</lpage><pub-id pub-id-type="doi">10.2135/cropsci2016.12.1016</pub-id></element-citation></ref><ref id="B43-sensors-18-03731"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holman</surname><given-names>F.H.</given-names></name><name><surname>Riche</surname><given-names>A.B.</given-names></name><name><surname>Michalski</surname><given-names>A.</given-names></name><name><surname>Castle</surname><given-names>M.</given-names></name><name><surname>Wooster</surname><given-names>M.J.</given-names></name><name><surname>Hawkesford</surname><given-names>M.J.</given-names></name></person-group><article-title>High throughput field phenotyping of wheat plant height and growth rate in field plot trials using UAV based remote sensing</article-title><source>Remote Sens.</source><year>2016</year><volume>8</volume><pub-id pub-id-type="doi">10.3390/rs8121031</pub-id></element-citation></ref><ref id="B44-sensors-18-03731"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schirrmann</surname><given-names>M.</given-names></name><name><surname>Giebel</surname><given-names>A.</given-names></name><name><surname>Gleiniger</surname><given-names>F.</given-names></name><name><surname>Pflanz</surname><given-names>M.</given-names></name><name><surname>Lentschke</surname><given-names>J.</given-names></name><name><surname>Dammer</surname><given-names>K.H.</given-names></name></person-group><article-title>Monitoring agronomic parameters of winter wheat crops with low-cost UAV imagery</article-title><source>Remote Sens.</source><year>2016</year><volume>8</volume><pub-id pub-id-type="doi">10.3390/rs8090706</pub-id></element-citation></ref><ref id="B45-sensors-18-03731"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madec</surname><given-names>S.</given-names></name><name><surname>Baret</surname><given-names>F.</given-names></name><name><surname>de Solan</surname><given-names>B.</given-names></name><name><surname>Thomas</surname><given-names>S.</given-names></name><name><surname>Dutartre</surname><given-names>D.</given-names></name><name><surname>Jezequel</surname><given-names>S.</given-names></name><name><surname>Hemmerl&#x000e9;</surname><given-names>M.</given-names></name><name><surname>Colombeau</surname><given-names>G.</given-names></name><name><surname>Comar</surname><given-names>A.</given-names></name></person-group><article-title>High-Throughput Phenotyping of Plant Height: Comparing Unmanned Aerial Vehicles and Ground LiDAR Estimates</article-title><source>Front. Plant Sci.</source><year>2017</year><volume>8</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.3389/fpls.2017.02002</pub-id><?supplied-pmid 29230229?><pub-id pub-id-type="pmid">28220127</pub-id></element-citation></ref><ref id="B46-sensors-18-03731"><label>46.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>S.</given-names></name><name><surname>Li</surname><given-names>C.</given-names></name></person-group><article-title>Height estimation for blueberry bushes using LiDAR based on a field robotic platform</article-title><source>Proceedings of the 2016 ASABE Annual International Meeting</source><conf-loc>Orlando, FL, USA</conf-loc><conf-date>17&#x02013;20 July 2016</conf-date><fpage>2</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.13031/aim.20162461174</pub-id></element-citation></ref><ref id="B47-sensors-18-03731"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedli</surname><given-names>M.</given-names></name><name><surname>Kirchgessner</surname><given-names>N.</given-names></name><name><surname>Grieder</surname><given-names>C.</given-names></name><name><surname>Liebisch</surname><given-names>F.</given-names></name><name><surname>Mannale</surname><given-names>M.</given-names></name><name><surname>Walter</surname><given-names>A.</given-names></name></person-group><article-title>Terrestrial 3D laser scanning to track the increase in canopy height of both monocot and dicot crop species under field conditions</article-title><source>Plant Methods</source><year>2016</year><volume>12</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1186/s13007-016-0109-7</pub-id><?supplied-pmid 26834822?><pub-id pub-id-type="pmid">26788117</pub-id></element-citation></ref><ref id="B48-sensors-18-03731"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bai</surname><given-names>G.</given-names></name><name><surname>Ge</surname><given-names>Y.</given-names></name><name><surname>Hussain</surname><given-names>W.</given-names></name><name><surname>Baenziger</surname><given-names>P.S.</given-names></name><name><surname>Graef</surname><given-names>G.</given-names></name></person-group><article-title>A multi-sensor system for high throughput field phenotyping in soybean and wheat breeding</article-title><source>Comput. Electron. Agric.</source><year>2016</year><volume>128</volume><fpage>181</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2016.08.021</pub-id></element-citation></ref><ref id="B49-sensors-18-03731"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Demir</surname><given-names>N.</given-names></name><name><surname>S&#x000f6;nmez</surname><given-names>N.K.</given-names></name><name><surname>Akar</surname><given-names>T.</given-names></name><name><surname>&#x000dc;nal</surname><given-names>S.</given-names></name></person-group><article-title>Automated Measurement of Plant Height of Wheat Genotypes Using a DSM Derived From UAV Imagery</article-title><source>Proceedings</source><year>2018</year><volume>2</volume><elocation-id>350</elocation-id><pub-id pub-id-type="doi">10.3390/ecrs-2-05163</pub-id></element-citation></ref><ref id="B50-sensors-18-03731"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rusu</surname><given-names>R.B.</given-names></name><name><surname>Marton</surname><given-names>Z.C.</given-names></name><name><surname>Blodow</surname><given-names>N.</given-names></name><name><surname>Dolha</surname><given-names>M.</given-names></name><name><surname>Beetz</surname><given-names>M.</given-names></name></person-group><article-title>Towards 3D Point cloud based object maps for household environments</article-title><source>Robot. Auton. Syst.</source><year>2008</year><volume>56</volume><fpage>927</fpage><lpage>941</lpage><pub-id pub-id-type="doi">10.1016/j.robot.2008.08.005</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="sensors-18-03731-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Light detection and ranging (LiDAR) and ultrasonic sensor of the ground phenotyping system.</p></caption><graphic xlink:href="sensors-18-03731-g001"/></fig><fig id="sensors-18-03731-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>Schematic diagram showing the scanning areas of LiDAR and ultrasonic sensors at each measurement.</p></caption><graphic xlink:href="sensors-18-03731-g002"/></fig><fig id="sensors-18-03731-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>Customized LabVIEW program: (<bold>a</bold>) front panel; (<bold>b</bold>) flowchart of block diagram.</p></caption><graphic xlink:href="sensors-18-03731-g003"/></fig><fig id="sensors-18-03731-f004" orientation="portrait" position="float"><label>Figure 4</label><caption><p>The Cartesian coordinate system for LiDAR point cloud at each measurement.</p></caption><graphic xlink:href="sensors-18-03731-g004"/></fig><fig id="sensors-18-03731-f005" orientation="portrait" position="float"><label>Figure 5</label><caption><p>An example of raw LiDAR point cloud at each measurement.</p></caption><graphic xlink:href="sensors-18-03731-g005"/></fig><fig id="sensors-18-03731-f006" orientation="portrait" position="float"><label>Figure 6</label><caption><p>The slanting issue of the phenocart.</p></caption><graphic xlink:href="sensors-18-03731-g006"/></fig><fig id="sensors-18-03731-f007" orientation="portrait" position="float"><label>Figure 7</label><caption><p>Digital surface model (DSM) map of the investigated 100 plots with plot delineation.</p></caption><graphic xlink:href="sensors-18-03731-g007"/></fig><fig id="sensors-18-03731-f008" orientation="portrait" position="float"><label>Figure 8</label><caption><p>Statistical results of heights extracted at different percentiles from processed LiDAR point clouds over five data collection campaigns: (<bold>a</bold>) RMSE; (<bold>b</bold>) bias; (<bold>c</bold>) R<sup>2</sup>.</p></caption><graphic xlink:href="sensors-18-03731-g008"/></fig><fig id="sensors-18-03731-f009" orientation="portrait" position="float"><label>Figure 9</label><caption><p>Manually measured canopy heights versus instrument estimated canopy heights: (<bold>a</bold>) ultrasonic sensors; (<bold>b</bold>) UAS; (<bold>c</bold>) LiDAR.</p></caption><graphic xlink:href="sensors-18-03731-g009"/></fig><fig id="sensors-18-03731-f010" orientation="portrait" position="float"><label>Figure 10</label><caption><p>Two scenarios where ultrasonic sensor estimations disagree with manual measurements.</p></caption><graphic xlink:href="sensors-18-03731-g010"/></fig><table-wrap id="sensors-18-03731-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03731-t001_Table 1</object-id><label>Table 1</label><caption><p>Data collection campaign dates of manual measurement, the ground system and the unmanned aircraft system (UAS) for wheat height evaluation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Data Collection Campaign</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Growth Stage</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Manual</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Ground System</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UAS</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Date</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Date</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Date</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1st</td><td align="center" valign="middle" rowspan="1" colspan="1">Jointing stage: Feekes 6</td><td align="center" valign="middle" rowspan="1" colspan="1">7 May</td><td align="center" valign="middle" rowspan="1" colspan="1">A</td><td align="center" valign="middle" rowspan="1" colspan="1">7 May</td><td align="center" valign="middle" rowspan="1" colspan="1">7 May</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2nd</td><td align="center" valign="middle" rowspan="1" colspan="1">Flag leaf stage: Feekes 8</td><td align="center" valign="middle" rowspan="1" colspan="1">15 May</td><td align="center" valign="middle" rowspan="1" colspan="1">A</td><td align="center" valign="middle" rowspan="1" colspan="1">15 May</td><td align="center" valign="middle" rowspan="1" colspan="1">15 May</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3rd</td><td align="center" valign="middle" rowspan="1" colspan="1">Boot stage: Feekes 9</td><td align="center" valign="middle" rowspan="1" colspan="1">23 May</td><td align="center" valign="middle" rowspan="1" colspan="1">B</td><td align="center" valign="middle" rowspan="1" colspan="1">23 May</td><td align="center" valign="middle" rowspan="1" colspan="1">21 May</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4th</td><td align="center" valign="middle" rowspan="1" colspan="1">Grain filling period: Feekes 10.5.3</td><td align="center" valign="middle" rowspan="1" colspan="1">31 May</td><td align="center" valign="middle" rowspan="1" colspan="1">B</td><td align="center" valign="middle" rowspan="1" colspan="1">31 May</td><td align="center" valign="middle" rowspan="1" colspan="1">1 June</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5th</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Physiological maturity: Feekes 11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16 June</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">B</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15 June</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18 June</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-03731-t002" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03731-t002_Table 2</object-id><label>Table 2</label><caption><p>Optimal root-mean-square error (RMSE) and percentile of raw and processed point clouds at each data collection campaign.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Data Collection Campaign</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">1st</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">2nd</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">3rd</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">4th</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">5th</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Raw Point Clouds</td><td align="center" valign="middle" rowspan="1" colspan="1">Minimum RMSE (m)</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0462</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0389</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0643</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0467</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0521</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Optimal Percentile</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">67.5th</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85th</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.5th</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99th</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.5th</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Processed Point Clouds</td><td align="center" valign="middle" rowspan="1" colspan="1">Minimum RMSE (m)</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0290</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0300</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0354</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0407</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0420</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Optimal Percentile</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60th</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91st</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99th</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99th</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.5th</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-03731-t003" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03731-t003_Table 3</object-id><label>Table 3</label><caption><p>Effects of manual method and plot position on minimum RMSE of processed LiDAR point clouds.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Category</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Method A</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Method B</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">All</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Number of Plots</td><td colspan="2" align="center" valign="middle" rowspan="1">200</td><td colspan="2" align="center" valign="middle" rowspan="1">300</td><td colspan="2" align="center" valign="middle" rowspan="1">500</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Minimum RMSE (m)</td><td colspan="2" align="center" valign="middle" rowspan="1">0.0478</td><td colspan="2" align="center" valign="middle" rowspan="1">0.0398</td><td colspan="2" align="center" valign="middle" rowspan="1">0.0657</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Optimal Percentile</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">82nd</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">99th</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">98th</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Sub-Category</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Side</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Middle</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Side</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Middle</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Side</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Middle</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Number of Plots</td><td align="center" valign="middle" rowspan="1" colspan="1">140</td><td align="center" valign="middle" rowspan="1" colspan="1">60</td><td align="center" valign="middle" rowspan="1" colspan="1">200</td><td align="center" valign="middle" rowspan="1" colspan="1">100</td><td align="center" valign="middle" rowspan="1" colspan="1">340</td><td align="center" valign="middle" rowspan="1" colspan="1">160</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Minimum RMSE (m)</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0436</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0491</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0395</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0327</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0649</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0624</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Optimal Percentile</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77th</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89th</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99th</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.5th</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97th</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99th</td></tr></tbody></table></table-wrap></floats-group></article>