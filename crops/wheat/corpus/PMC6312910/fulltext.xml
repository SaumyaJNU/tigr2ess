<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="review-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Gigascience</journal-id><journal-id journal-id-type="iso-abbrev">Gigascience</journal-id><journal-id journal-id-type="publisher-id">gigascience</journal-id><journal-title-group><journal-title>GigaScience</journal-title></journal-title-group><issn pub-type="epub">2047-217X</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6312910</article-id><article-id pub-id-type="doi">10.1093/gigascience/giy153</article-id><article-id pub-id-type="publisher-id">giy153</article-id><article-categories><subj-group subj-group-type="heading"><subject>Review</subject></subj-group></article-categories><title-group><article-title>Computer vision-based phenotyping for improvement of plant productivity: a machine learning perspective</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-1299-0024</contrib-id><name><surname>Mochida</surname><given-names>Keiichi</given-names></name><!--<email>keiichi.mochida@riken.jp</email>--><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="corresp" rid="cor1"/></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-9187-9625</contrib-id><name><surname>Koda</surname><given-names>Satoru</given-names></name><xref ref-type="aff" rid="aff6">6</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-7645-7862</contrib-id><name><surname>Inoue</surname><given-names>Komaki</given-names></name><xref ref-type="aff" rid="aff1">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-3868-2380</contrib-id><name><surname>Hirayama</surname><given-names>Takashi</given-names></name><xref ref-type="aff" rid="aff3">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-0846-1912</contrib-id><name><surname>Tanaka</surname><given-names>Shojiro</given-names></name><xref ref-type="aff" rid="aff7">7</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-8109-6638</contrib-id><name><surname>Nishii</surname><given-names>Ryuei</given-names></name><xref ref-type="aff" rid="aff8">8</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-9745-3732</contrib-id><name><surname>Melgani</surname><given-names>Farid</given-names></name><xref ref-type="aff" rid="aff9">9</xref></contrib></contrib-group><aff id="aff1"><label>1</label>Bioproductivity Informatics Research Team, RIKEN Center for Sustainable Resource Science, 1-7-22 Suehiro-cho, Tsurumi-ku, Yokohama, Kanagawa 230-0045, Japan</aff><aff id="aff2"><label>2</label>Microalgae Production Control Technology Laboratory, RIKEN Baton Zone Program, RIKEN Cluster for Science, Technology and Innovation Hub, 1-7-22 Suehiro-cho, Tsurumi-ku, Yokohama, Kanagawa 230-0045, Japan</aff><aff id="aff3"><label>3</label>Institute of Plant Science and Resources, Okayama University, 2-20-1 Chuo, Kurashiki, Okayama 710-0046, Japan</aff><aff id="aff4"><label>4</label>Kihara Institute for Biological Research, Yokohama City University, 641-12 Maioka-cho, Totsuka-ku, Yokohama, Kanagawa 244&#x02013;0813, Japan</aff><aff id="aff5"><label>5</label>Graduate School of Nanobioscience, Yokohama City University, 22-2 Seto, Kanazawa-ku, Yokohama, Kanagawa 236-0027, Japan</aff><aff id="aff6"><label>6</label>Graduate School of Mathematics, Kyushu University, 744 Motooka, Nishi-ku, Fukuoka 819-0395, Japan</aff><aff id="aff7"><label>7</label>Hiroshima University of Economics, 5-37-1, Gion, Asaminami, Hiroshima-shi Hiroshima 731-0138, Japan</aff><aff id="aff8"><label>8</label>Institute of Mathematics for Industry, Kyushu University, 744 Motooka, Nishi-ku, Fukuoka 819-0395, Japan</aff><aff id="aff9"><label>9</label>Department of Information Engineering and Computer Science, University of Trento, Via Sommarive 9, 38123 Trento, Italy</aff><author-notes><corresp id="cor1"><bold>Correspondence address</bold>. Keiichi Mochida, Bioproductivity Informatics Research Team, RIKEN Center for Sustainable Resource Science, 1-7-22 Suehiro-cho, Tsurumi-ku, Yokohama, Kanagawa 230-0045, Japan. Tel: +81-45-503-9111; E-mail: <email>keiichi.mochida@riken.jp</email></corresp></author-notes><pub-date pub-type="collection"><month>1</month><year>2019</year></pub-date><pub-date pub-type="epub" iso-8601-date="2018-12-06"><day>06</day><month>12</month><year>2018</year></pub-date><pub-date pub-type="pmc-release"><day>06</day><month>12</month><year>2018</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. --><volume>8</volume><issue>1</issue><elocation-id>giy153</elocation-id><history><date date-type="received"><day>12</day><month>6</month><year>2018</year></date><date date-type="rev-recd"><day>06</day><month>9</month><year>2018</year></date><date date-type="accepted"><day>24</day><month>11</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2018. Published by Oxford University Press.</copyright-statement><copyright-year>2019</copyright-year><license license-type="cc-by" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="giy153.pdf"/><abstract><title>Abstract</title><p>Employing computer vision to extract useful information from images and videos is becoming a key technique for identifying phenotypic changes in plants. Here, we review the emerging aspects of computer vision for automated plant phenotyping. Recent advances in image analysis empowered by machine learning-based techniques, including convolutional neural network-based modeling, have expanded their application to assist high-throughput plant phenotyping. Combinatorial use of multiple sensors to acquire various spectra has allowed us to noninvasively obtain a series of datasets, including those related to the development and physiological responses of plants throughout their life. Automated phenotyping platforms accelerate the elucidation of gene functions associated with traits in model plants under controlled conditions. Remote sensing techniques with image collection platforms, such as unmanned vehicles and tractors, are also emerging for large-scale field phenotyping for crop breeding and precision agriculture. Computer vision-based phenotyping will play significant roles in both the nowcasting and forecasting of plant traits through modeling of genotype/phenotype relationships.</p></abstract><kwd-group kwd-group-type="keywords"><kwd>machine learning</kwd><kwd>deep neural network</kwd><kwd>unmanned aerial vehicles</kwd><kwd>noninvasive plant phenotyping</kwd><kwd>hyperspectral camera</kwd></kwd-group><funding-group><award-group award-type="grant"><funding-source><named-content content-type="funder-name">Core Research for Evolutionary Sciecne and Technology</named-content></funding-source></award-group><award-group award-type="grant"><funding-source><named-content content-type="funder-name">Japan Science and Technology Agency</named-content><named-content content-type="funder-identifier">10.13039/501100002241</named-content></funding-source></award-group></funding-group><counts><page-count count="12"/></counts></article-meta></front><body><sec id="sec1"><title>Background</title><p>Computer vision that extracts useful information from plant images and videos is rapidly becoming an essential technique in plant phenomics [<xref rid="bib1" ref-type="bibr">1</xref>]. Phenomics approaches to plant science aim to identify the relationships between genetic diversities and phenotypic traits in plant species using noninvasive and high-throughput measurements of quantitative parameters that reflect traits and physiological states throughout a plant's life [<xref rid="bib2" ref-type="bibr">2</xref>]. Recent advances in DNA sequencing technologies have enabled us to rapidly acquire a map of genomic variations at the population scale [<xref rid="bib3" ref-type="bibr">3</xref>, <xref rid="bib4" ref-type="bibr">4</xref>]. Combining high-throughput analytical platforms for DNA sequencing and plant phenotyping has provided opportunities for exploring genetic factors for complex quantitative traits in plants, such as growth, environmental stress tolerance, disease resistance [<xref rid="bib5" ref-type="bibr">5</xref>], and yield, by mapping genotypes to phenotypes using statistical genetics methods, including quantitative trait locus (QTL) analysis and genome-wide association studies (GWASs) [<xref rid="bib6" ref-type="bibr">6</xref>]. Moreover, a model of the relationship between the genotype/phenotype map of individuals in a breeding population can be used to compute genome-estimated breeding values to select the best parents for new crosses in genomic selection in crop breeding [<xref rid="bib7" ref-type="bibr">7</xref>, <xref rid="bib8" ref-type="bibr">8</xref>]. Thus, high-throughput phenotyping aided by computer vision with various sensors and algorithms for image analysis will play a crucial role for crop yield improvement in scenarios related to population demography and climate change [<xref rid="bib9" ref-type="bibr">9</xref>].</p><p>Machine learning (ML), an area of computer science, offers us data-driven prediction in various applications, including image analysis, which can aid typical steps of image analysis (i.e., preprocessing, segmentation, feature extraction, and classification) [<xref rid="bib10" ref-type="bibr">10</xref>]. ML accelerates and automates image analysis, which improves throughput when handling labor-intensive sensor data. Algorithms based on deep learning, an emerging subfield of ML, often show more accurate performance compared with traditional approaches to computer vision-based tasks, including plant identification, such as PlantCLEF [<xref rid="bib11" ref-type="bibr">11</xref>]. Moreover, ML-based algorithms often provide discriminative features associated with outputs extracted through their training process, which may enable us to dissect complex traits and determine visual signatures related to traits in plants. These outcomes of ML offer us opportunities for revitalizing methodologies in plant phenomics to improve throughput, accuracy, and resolution (Fig.&#x000a0;<xref ref-type="fig" rid="fig1">1</xref>).</p><fig id="fig1" orientation="portrait" position="float"><label>Figure 1:</label><caption><p>Schematic representation of a typical example scenario in computer vision-based plant phenotyping. Various sensors are used for collection of plant images. Large-scale collections of labeled image data are useful to design pretrained network models. A typical step of computer vision-based image analysis consists of the following steps: preprocessing, segmentation, feature extraction, and classification. Various ML-based algorithms, including convolutional neural network, are applied to the steps, such as segmentation, feature extraction, and classification. Pretrained networks are often adapted to reduce computational costs through fine-tuning. The classification step represents case-control phenotypes in plants; disease-resistance, sensitive-adaptive, morphological phenotypes; growth stages; and taxonomic classification. Exploration of associations among the classification results and genetic polymorphisms, agronomic traits, and meteorological observations will expand applications to areas such as ecology/paleobotany, agriculture, and genetics and breeding.</p></caption><graphic xlink:href="giy153fig1"/></fig><p>In this review, we provide an overview of recent advances in computer vision-based plant phenotyping, which can contribute to our understanding of genotype/phenotype relationships in plants. Specifically, we summarize sensors and platforms recently developed for high-throughput plant image collection. Then, we also address recent challenges in computer vision-based plant image analysis and the typical image analysis process (e.g., segmentation, feature extraction, and classification), as well as its applications to large-scale phenotyping in genetic studies in plants, by highlighting ML-based approaches. Moreover, we showcase datasets and software tools that are useful to plant image analysis. Then, we discuss perspectives and opportunities for computer vision in plant phenomics.</p></sec><sec id="h1content1545205981730"><title>Review</title><sec id="sec1-1"><title>High-throughput image collection for large-scale plant phenotyping: sensors and platforms</title><sec id="sec1-1-1"><title>Sensors for plant phenotyping</title><p>Various types of sensors can be encountered to acquire morphological and physiological information from plants [<xref rid="bib10" ref-type="bibr">10</xref>] (Fig.&#x000a0;<xref ref-type="fig" rid="fig1">1</xref>). The basic sensors are digital cameras that are typically adopted for quick color and/or texture-based phenotyping operations. In a previous study [<xref rid="bib12" ref-type="bibr">12</xref>], the authors presented a plant phenotyping system for stereoscopic red-green-blue (RGB) imaging to evaluate the growth rate of tree seedlings during post-seed germination through calculation of the increase in seeding height and the rate of greenness. Multispectral and hyperspectral sensors enable us to capture richer spectral information about plants of interest, thus allowing more in-depth phenotyping. Moreover, in another study [<xref rid="bib13" ref-type="bibr">13</xref>], a methodology to monitor the responses of plants to stress by inspecting the hyperspectral features of diseased plants was established, describing&#x000a0;a hyperspectral image "wordification" concept, in which images are treated as text documents by means of probabilistic topic models, which enabled automatic tracking of the growth of three foliar diseases in barley. An interesting analysis of vegetation-specific crop indices acquired by a multispectral camera mounted on an unmanned aerial vehicle (UAV) surveyed over a pilot trial of 30 plots was conducted in another prior analysis [<xref rid="bib14" ref-type="bibr">14</xref>]. In this study, the authors exploited multiple indices to estimate canopy cover and leaf area index; they reported that the significant correlations among the normalized difference vegetation index, enhanced vegetation index, and normalized difference red edge index, which estimates leaf chlorophyll content, were useful for characterizing leaf area senescence features of contrasting genotypes to assess the senescence patterns of sorghum genotypes. Moreover, thermal infrared sensors offer additional complementary and useful information, particularly for determining the previsual and early response of the canopy to abiotic [<xref rid="bib15" ref-type="bibr">15</xref>] and biotic stress [<xref rid="bib16" ref-type="bibr">16</xref>] conditions. LiDAR (light detection and ranging)is another form of sensor characterized as a traditional remote sensing technique that is capable of yielding accurate three-dimensional (3D) data; this approach has been recently applied to plant phenotyping coupled with other sensors [<xref rid="bib17" ref-type="bibr">17</xref>]. With these recent advancements, 3D reconstruction of plants enables us to identify phenotypic differences, including entire-plant and organ-level morphological changes, and combinatorial use of multiple sensors offers us opportunities to identify spectral markers associated with previsual signs of plant physiological responses.</p></sec><sec id="sec1-1-2"><title>Platforms</title><p>Plant phenotyping frameworks incorporate sensors with mobility systems, such as tray conveyors [<xref rid="bib18" ref-type="bibr">18</xref>], aerial and ground vehicles [<xref rid="bib19" ref-type="bibr">19</xref>], UAVs [<xref rid="bib20" ref-type="bibr">20</xref>], and motorized gantries [<xref rid="bib21" ref-type="bibr">21</xref>, <xref rid="bib22" ref-type="bibr">22</xref>], to continuously capture growth and physiology data from plants. An automated plant phenotyping system, called the phenome high-throughput investigator (PHI), allowed noninvasive tracking of plant growth under controlled conditions using an imaging station with various camera-based imaging units coupled with two growth rooms for growth of different types of plants (&#x0223c;200 crop plants and &#x0223c;3,500 <italic>Arabidopsis</italic>, respectively) [<xref rid="bib23" ref-type="bibr">23</xref>]. A computational pipeline for single leaf-based analysis with PHI was used to monitor leaf senescence and its progression in <italic>Arabidopsis</italic>. A high-throughput hyperspectral imaging system was designed for indoor phenotyping of rice plants [<xref rid="bib24" ref-type="bibr">24</xref>] and was applied to quantifying agronomic traits based on hyperspectral signatures in a global rice collection of 529 accessions [<xref rid="bib24" ref-type="bibr">24</xref>]. More recently, the RIKEN Integrated Plant Phenotyping System has been used owing to its accurate quantification of <italic>Arabidopsis</italic> growth responses and water use efficiency in the context of various water conditions [<xref rid="bib25" ref-type="bibr">25</xref>]. PhenoTrac 4, a mobile platform for phenotyping under field conditions that is equipped with multiple passive and active sensors, was used to perform canopy-scale phenotyping of barley and wheat [<xref rid="bib26" ref-type="bibr">26</xref>]. Another mobile platform, the Phenomobile system equipped with multiple sensors [<xref rid="bib27" ref-type="bibr">27</xref>], has been investigated for its potential in field-phenotyping applications to examine agronomically important traits, such as stay-green [<xref rid="bib28" ref-type="bibr">28</xref>]. These platforms for high-throughput plant phenotyping monitor plant growth noninvasively and continuously and evaluate phenotypic differences quantitatively throughout the life cycle at the population scale; this facilitates the identification of genetic factors associated with traits related to growth and development.</p></sec></sec><sec id="sec1-2"><title>Computer vision-based plant phenotyping</title><p>In this section, we discuss recent advances in image analysis methodologies for plant phenotyping; these methodologies consist of four major steps, i.e., preprocessing, segmentation, feature extraction, and classification. In each of the following subsections, we highlight ML-based approaches discussed in recently published literature.</p><sec id="sec1-2-1"><title>Preprocessing</title><p>Preprocessing is a preliminary step of image analysis that aims to organize data properties to facilitate subsequent steps and even derive reasonable final outcomes. Particularly when we target images acquired under field conditions, unlike in controlled environments, image preprocessing contributes to enhancement of image processing quality. A simple preprocessing step is image cropping, which extracts rectangles containing target objects out of an image. Data transformation techniques, such as gray scale conversion, normalization, standardization, and contrast enhancement, are also adopted during preprocessing. Data augmentation is another example of preprocessing whose underlying goal is to increase variations in images in datasets, resulting in making pattern analysis more robust and generalized. Various techniques, such as image scaling, rotation, flipping, and noise addition, are often used for data augmentation.</p></sec><sec id="sec1-2-2"><title>Segmentation</title><p>Segmentation represents a first important step to extract information of targets from preprocessed image data by separating a set of pixels including objects of interest in images (Fig.&#x000a0;<xref ref-type="fig" rid="fig1">1</xref>), enabling the identification and quantification of areas corresponding to particular organs in plants automatically. To develop a pipeline to automatically count maize tassels, a deep convolutional neural network (CNN) model, resulting from learning of the Maize Tassels Counting dataset [<xref rid="bib29" ref-type="bibr">29</xref>], was applied, and plausible results were obtained with an absolute error of 6.6 and a mean squared error of 9.6 [<xref rid="bib29" ref-type="bibr">29</xref>]. To automatically count tomato fruits, a deep CNN based on the Inception-ResNet was applied through training on synthetic data and tested on real data; 91% counting accuracy was obtained [<xref rid="bib30" ref-type="bibr">30</xref>]. In addition to these model-driven approaches, various image-driven approaches have been applied for autosegmentation of plant organs. For example, in a previous study [<xref rid="bib31" ref-type="bibr">31</xref>] in which images were acquired by X-ray micro-computed tomography, a method for accurate extraction and measurement of spike and grain morphometric parameters of wheat plants was established based on combinatorial use of adaptive threshold and morphology algorithm and applied to examine spike and grain growth of wheat exposed to high temperatures under two different water treatments. Another study [<xref rid="bib32" ref-type="bibr">32</xref>] proposed a method for resegmentation and assimilated details that were missed in the <italic>a priori</italic> segmentation, which was useful to improve the accuracy of determination of sharp features, such as leaf tips, twists, and axils of plants. Moreover, hybrid approaches integrating model-based and image-based approaches have been applied for segmentation of plant shape and organs. For example, in a previous study [<xref rid="bib33" ref-type="bibr">33</xref>], a decision tree-based ML method with multiple color space and a method combining mean shift and threshold based on the hue, saturation, and value color space, were applied for segmentation of top- and side-view images in maize, yielding&#x000a0;an accuracy of 86% in estimation of ear position in 60 maize hybrids. In wheat, researchers used an improved color index method for plant segmentation, followed by a neural network-based method with Laws texture energy; this method enabled them to detect spikes with an accuracy of more than 80% [<xref rid="bib34" ref-type="bibr">34</xref>]. Rzanny et al. [<xref rid="bib35" ref-type="bibr">35</xref>] reported systematic guidelines for workloads of image acquisition (perspective, illumination, and background) and preprocessing (nonprocessed, cropped, and segmented) and assessed the impact of segmentation and other preprocessing techniques on recognition performances. These recent attempts to improve the accuracy of segmentation enabled us to automatically identify and quantify plant organs and evaluate the biomass and yields of fruits and grains. We were also able to improve reproducibility in phenotyping by replacing conventional human-based phenotyping, which is often time consuming and labor intensive.</p></sec><sec id="sec1-2-3"><title>Feature extraction</title><p>Feature extraction is a step to create a set of significant and nonredundant information that can sufficiently represent images. Because pattern recognition performance in computer vision heavily depends on the quality of the extracted features, a number of approaches have been attempted in various areas, including plant phenotyping.</p><p>Typically, features are hand-chosen based on characteristics of objects in images, such as pixel intensities, gradient, texture, and shape. For example, in a previous study [<xref rid="bib36" ref-type="bibr">36</xref>, <xref rid="bib37" ref-type="bibr">37</xref>], the authors extracted features such as shape, color, and texture (contrast, correlation, homogeny, entropy) from wheat grains to classify their accessions. Moreover, in another study [<xref rid="bib38" ref-type="bibr">38</xref>, <xref rid="bib39" ref-type="bibr">39</xref>], the authors used an elliptic Fourier descriptor and the texture feature set called Haralick's texture descriptors to characterize seeds of plants for taxonomic classification. With a representative feature extraction tool, Scale Invariant Features Transforms (SIFT), which acts as an invariant feature descriptor not only to scale but also rotation, illumination, and viewpoint, Wilf et al. [<xref rid="bib40" ref-type="bibr">40</xref>] generated codebooks for dictionary learning, and their results demonstrated the effectiveness of their approach on taxonomic classification through leaves. The bag-of-keypoints/bag-of-visual-words method, an analogy to the bag-of-words method for text categorization using keywords [<xref rid="bib41" ref-type="bibr">41</xref>&#x02013;<xref rid="bib44" ref-type="bibr">44</xref>], has also been used as a feature representation tool in image analysis in which the SIFT algorithm is used for keypoint detection and local feature description [<xref rid="bib45" ref-type="bibr">45</xref>, <xref rid="bib46" ref-type="bibr">46</xref>]. The bag-of-keypoints method and the SIFT algorithm were applied to RGB color images of wheat under field conditions for growth stage identification [<xref rid="bib47" ref-type="bibr">47</xref>].</p><p>Recently, CNN-based approaches have shown remarkable advancement, and their applications have been expanded to myriad areas, including computer vision [<xref rid="bib48" ref-type="bibr">48</xref>&#x02013;<xref rid="bib50" ref-type="bibr">50</xref>], which can automatically extract features from images and classify them. Therefore, unlike hand-chosen feature-based algorithms, CNNs create and train classifiers without explicit feature extraction steps. Moreover, pretrained CNNs can be used as a simple feature extractor [<xref rid="bib51" ref-type="bibr">51</xref>]. Based on these advantages of CNNs, many CNN-based strategies have been developed and are now widely used for pattern-recognition and image-classification tasks, even for plant phenotyping. Notably, the authors of previous studies [<xref rid="bib52" ref-type="bibr">52</xref>, <xref rid="bib53" ref-type="bibr">53</xref>] illustrated feature extraction processes based on CNNs, which learn hierarchical features through network training for taxonomic classification tasks on leaf image datasets. Recent outcomes of CNN-based classification in plant phenotyping are discussed in the following sections.</p></sec><sec id="sec1-2-4"><title>Classification</title><p>In classification steps, outcomes from the previous three steps are obtained. Here, we address classification techniques, including ML-based techniques, recently applied in plant phenotyping, highlighting two major applications: taxonomic classification and classification of plant physiological states.</p><sec id="sec1-2-4-1"><title>Taxonomic classification</title><p>Computer vision-based taxonomic classification plays an essential role in plant phenotyping to automatically distinguish target species for phenotyping from other plants, which is particularly important for images from real fields. W&#x000e4;ldchen and M&#x000e4;der have thoroughly summarized the literature on computer vision-based species identification published by 2016 [<xref rid="bib54" ref-type="bibr">54</xref>]. In recent years, because techniques for computer vision-based species identification have shown dramatically improved accuracy and expanded applications for various plant groups through handcrafted feature-based and CNN-based approaches, we highlight studies describing plant taxonomic classification by means of these two distinctive approaches (Table&#x000a0;<xref rid="tbl1" ref-type="table">1</xref>).</p><table-wrap id="tbl1" orientation="portrait" position="float"><label>Table 1:</label><caption><p>Examples of taxonomic classification approaches</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Approach</th><th align="left" rowspan="1" colspan="1">Object</th><th align="left" rowspan="1" colspan="1">Features/feature extractor</th><th align="left" rowspan="1" colspan="1">Classifier</th><th align="left" rowspan="1" colspan="1">Reference</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">Custom feature-based approach</td><td rowspan="1" colspan="1">Seed</td><td rowspan="1" colspan="1">Elliptic Fourier descriptor, Haralick's texture descriptor, morpho-colorimetric feature</td><td rowspan="1" colspan="1">LDA</td><td rowspan="1" colspan="1">[<xref rid="bib38" ref-type="bibr">38</xref>, <xref rid="bib39" ref-type="bibr">39</xref>]</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Grain</td><td rowspan="1" colspan="1">Shape, color, texture features</td><td rowspan="1" colspan="1">MLP</td><td rowspan="1" colspan="1">[<xref rid="bib36" ref-type="bibr">36</xref>]</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">ANFIS</td><td rowspan="1" colspan="1">[<xref rid="bib37" ref-type="bibr">37</xref>]</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">SIFT, sparse coding</td><td rowspan="1" colspan="1">SVM</td><td rowspan="1" colspan="1">[<xref rid="bib40" ref-type="bibr">40</xref>]</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Fourier descriptor, leaf shapes, vein structure</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">[<xref rid="bib55" ref-type="bibr">55</xref>]</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Leaf</td><td rowspan="1" colspan="1">Pretrained CNN</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">[<xref rid="bib35" ref-type="bibr">35</xref>]</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Ffirst</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">[<xref rid="bib56" ref-type="bibr">56</xref>]</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Texture features</td><td rowspan="1" colspan="1">LWSRC</td><td rowspan="1" colspan="1">[<xref rid="bib57" ref-type="bibr">57</xref>]</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Bark</td><td rowspan="1" colspan="1">Ffirst</td><td rowspan="1" colspan="1">SVM</td><td rowspan="1" colspan="1">[<xref rid="bib56" ref-type="bibr">56</xref>]</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Tree</td><td rowspan="1" colspan="1">Reflectance, minimum noise fraction transformation, narrowband vegetation indices, airborne imaging spectroscopy features</td><td rowspan="1" colspan="1">SVM, RF</td><td rowspan="1" colspan="1">[<xref rid="bib58" ref-type="bibr">58</xref>]</td></tr><tr><td rowspan="1" colspan="1">CNN-based approach</td><td rowspan="1" colspan="1">Grain</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">[<xref rid="bib59" ref-type="bibr">59</xref>]</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Ear, spike, spikelet</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">[<xref rid="bib60" ref-type="bibr">60</xref>]</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Leaf</td><td rowspan="1" colspan="1">CNN</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">[<xref rid="bib52" ref-type="bibr">52</xref>, <xref rid="bib53" ref-type="bibr">53</xref>, <xref rid="bib61" ref-type="bibr">61</xref>, <xref rid="bib62" ref-type="bibr">62</xref>]</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Root</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">[<xref rid="bib62" ref-type="bibr">62</xref>]</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Various organs</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">[<xref rid="bib63" ref-type="bibr">63</xref>, <xref rid="bib61" ref-type="bibr">61</xref>, <xref rid="bib56" ref-type="bibr">56</xref>]</td></tr></tbody></table><table-wrap-foot><fn id="req-15450459558908190"><p>Ffirst: Fast Features Invariant to Rotation and Scale of Texture.</p></fn></table-wrap-foot></table-wrap><p>In a custom feature-based approach, Wilf et al. [<xref rid="bib40" ref-type="bibr">40</xref>] attempted to classify leaf images into labels of major groups (such as families and orders) in the taxonomic category. They used SIFT and a sparse coding approach to extract the discriminative features of leaf shapes and venation patterns, followed by a multiclass support vector machine (SVM) classifier for grouping. A sparse representation was also used by Zhang et al. [<xref rid="bib57" ref-type="bibr">57</xref>] as a part of their processes for classifying plant species from RGB color leaf images; they demonstrated the superiority of their approach in identification on leaf image datasets. As a case study, a Turkish research group investigated the capability of computer vision algorithms to classify wheat grains into bread wheat and durum wheat based on grain images captured by high-resolution cameras [<xref rid="bib36" ref-type="bibr">36</xref>, <xref rid="bib37" ref-type="bibr">37</xref>]. They used two types of neural networks: a multilayer perceptron (MLP) with a single hidden layer and an adaptive neuro-fuzzy inference system (ANFIS). They selected seven discriminative grain features, incorporating aspects of shape, color, and texture, and achieved greater than 99% accuracy on the grain classification task. Another group examined two taxonomic classification tasks: the <italic>Malva</italic> alliance taxa and genus <italic>Cistus</italic> taxa [<xref rid="bib38" ref-type="bibr">38</xref>, <xref rid="bib39" ref-type="bibr">39</xref>]. They acquired digital images of seeds using a flatbed scanner; extracted morphometric, colorimetric, and textural seed features; and then performed taxonomic classification with stepwise linear discriminant analysis (LDA). Species identification from herbarium specimens with computer vision approaches was first presented in 2016, in which Unger et al. classified German trees into tens of classes with images of herbarium specimens photographed at a high resolution [<xref rid="bib55" ref-type="bibr">55</xref>]. Their analytical processes were composed of preprocessing, normalization, and feature extraction with Fourier descriptors, leaf shape parameters, and vein texture, followed by SVM classification. In this study, they demonstrated the potential of computer visions for taxonomic identification, even when using discolored leaf images of herbarium specimens. Using rather different data for species classification, Piiroinen et al. [<xref rid="bib58" ref-type="bibr">58</xref>] attempted tree species identifications with airborne laser scanning and hyperspectral imaging in a diverse agroforestry area in Africa, where a few exotic tree species are dominant and most native species occur less frequently. Despite this challenge, they demonstrated that ML-based analytical approaches using SVMs and random forests (RFs) could achieve reasonable tree species identification based on airborne-sensor images.</p><p>In the last few years, many CNN-based approaches have been developed for the taxonomic classification of plants [<xref rid="bib52" ref-type="bibr">52</xref>, <xref rid="bib53" ref-type="bibr">53</xref>]. Using a dataset of accurately annotated images of wheat lines, the authors in a previous study [<xref rid="bib60" ref-type="bibr">60</xref>] applied a CNN-based model to perform feature location regression to identify spikes and spikelets and carried out image-level classification of wheat awns, suggesting the feasibility of employing CNN-based models in multiple tasks by coordinating their network architecture. In this study, the authors also suggested that the images of wheat in the training dataset, which were acquired using a consumer-grade 12 MP camera, could be favorable for training the CNN-based model. A comparative assessment between CNN-based and custom feature-based approaches was performed in a rice kernel classification task [<xref rid="bib59" ref-type="bibr">59</xref>]. In this assessment, the authors compared a deep CNN with <italic>k</italic>-nearest neighbor (kNN) algorithms and SVMs, along with custom features, such as a pyramid histogram of oriented gradients and GIST, and showed that the CNN surpassed the kNN and SVM algorithms in classification accuracy.</p><p>Although CNNs usually require large amounts of data and extensive computational load and time, transfer learning (i.e., the reuse and fine-tuning of pretrained networks for other tasks) is a promising technique for mitigating these costs [<xref rid="bib56" ref-type="bibr">56</xref>, <xref rid="bib63" ref-type="bibr">63</xref>, <xref rid="bib61" ref-type="bibr">61</xref>]. Ghazi et al. [<xref rid="bib63" ref-type="bibr">63</xref>] fine-tuned the three deep neural networks that performed well in the ImageNet Large-Scale Visual Recognition Challenge, i.e., AlexNet [<xref rid="bib64" ref-type="bibr">64</xref>], GoogLeNet [<xref rid="bib65" ref-type="bibr">65</xref>], and VGGNet [<xref rid="bib66" ref-type="bibr">66</xref>], for a large classification dataset of 1,000 species from PlantCLEF2015, aiming to construct a neural network model for taxonomic classification. In this study, the authors compared approaches based on fine-tuning and training from scratch and demonstrated that the fine-tuning approach had a slight edge in species identification. Carranza-Rojas et al. [<xref rid="bib61" ref-type="bibr">61</xref>] applied a pretrained CNN to herbarium species classification. Sulc and Matas [<xref rid="bib56" ref-type="bibr">56</xref>] utilized a pretrained 152-layer residual network model [<xref rid="bib67" ref-type="bibr">67</xref>] and the Inception-ResNet-v2 model [<xref rid="bib68" ref-type="bibr">68</xref>] for plant recognition in nature, in which views of plants or their organs differ significantly and in which the background is often cluttered. Moreover, the authors proposed the use of a textual feature, called Fast Features Invariant to Rotation and Scale of Texture (Ffirst), to computationally recognize bark and leaves from segmented images. They demonstrated improved recognition rates with this feature for a small computational cost. Pound et al. [<xref rid="bib62" ref-type="bibr">62</xref>] applied CNNs to two types of identification tasks, classification and localization, with megapixel images taken by multiple cameras. In this classification task, the authors succeeded in identifying root tips and leaf-ear tips with accuracies of 98.4% and 97.3%, respectively, with deep CNNs and extended trained classifiers for localizing plant root and shoot features. Rzanny et al. [<xref rid="bib35" ref-type="bibr">35</xref>] summarized workloads of image acquisition and the impact of preprocessing on accuracy in image classification and concluded that images taken from the top sides of leaves were most effective for processing of nondestructive leaf images. Interestingly, in this study, the authors recorded leaf images using a smartphone (an iPhone 6) in diverse situations, including natural background conditions, followed by feature extraction with the pretrained ResNet-50 CNN and classification with an SVM.</p></sec><sec id="sec1-2-4-2"><title>Classification of plant physiological states</title><p>The applications of computer vision-based image classification have been expanding to include description of developmental stages, physiological states, and qualities of plants (Table&#x000a0;<xref rid="tbl2" ref-type="table">2</xref>). Autonomous phenotyping systems equipped with multiple sensors for data acquisition have enabled us to collect information associated with internal and surface changes in plants [<xref rid="bib69" ref-type="bibr">69</xref>&#x02013;<xref rid="bib71" ref-type="bibr">71</xref>]. Through exploration of the relationships between multidimensional spectral signatures and the physiological properties of plants, we may be able to identify novel spectral markers that can reflect various plant physiological states [<xref rid="bib69" ref-type="bibr">69</xref>, <xref rid="bib72" ref-type="bibr">72</xref>&#x02013;<xref rid="bib74" ref-type="bibr">74</xref>]. Moreover, noninvasive data acquisition enables us to continuously monitor phenotypic changes over time in plant life courses [<xref rid="bib75" ref-type="bibr">75</xref>]. Therefore, computer vision-based plant phenotyping provides opportunities for early identification and detection of fine changes in plant growth, assisting crop diagnostics in precision agriculture.</p><table-wrap id="tbl2" orientation="portrait" position="float"><label>Table 2:</label><caption><p>Examples of approaches for classification of physiological states</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Approach</th><th align="left" rowspan="1" colspan="1">Object</th><th align="left" rowspan="1" colspan="1">Features/feature extractor</th><th align="left" rowspan="1" colspan="1">Classifier</th><th align="left" rowspan="1" colspan="1">Reference</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">Custom feature-based approach</td><td rowspan="1" colspan="1">Ear (growth stages)</td><td rowspan="1" colspan="1">SIFT + bag of keypoints</td><td rowspan="1" colspan="1">SVM</td><td rowspan="1" colspan="1">[<xref rid="bib47" ref-type="bibr">47</xref>]</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Grain (quality assessment)</td><td rowspan="1" colspan="1">Weibull distribution model parameter features</td><td rowspan="1" colspan="1">SVM</td><td rowspan="1" colspan="1">[<xref rid="bib76" ref-type="bibr">76</xref>]</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Leaf</td><td rowspan="1" colspan="1">Spectral vegetation indices</td><td rowspan="1" colspan="1">Spectral Angle Mapper</td><td rowspan="1" colspan="1">[<xref rid="bib77" ref-type="bibr">77</xref>]</td></tr><tr><td rowspan="1" colspan="1">CNN-based approach</td><td rowspan="1" colspan="1">Leaf</td><td rowspan="1" colspan="1">CNN</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">[<xref rid="bib78" ref-type="bibr">78</xref>, <xref rid="bib79" ref-type="bibr">79</xref>]</td></tr></tbody></table></table-wrap><p>ML-based and statistical algorithms have been used to extract structural features from plant images for tasks such as tissue segmentation, growth stage classification, and quality evaluation in plants [<xref rid="bib80" ref-type="bibr">80</xref>]. Multiple ML-based algorithms, such as kNN, naive Bayes classifier, and SVM algorithms, have been examined in segmentation processes for detecting aerial parts of plants, and the findings suggested that different algorithms would be preferable for segmenting images of the visible and near infrared spectra [<xref rid="bib81" ref-type="bibr">81</xref>]. The bag-of-keypoints method was recently applied to RGB color images of wheat under field conditions and demonstrated its ability to identify growth stages from heading to flowering [<xref rid="bib47" ref-type="bibr">47</xref>]. Quality inspection of harvested crop grains can also be assisted by computer vision-based approaches to describe the relationships between the visual appearance and qualities of grains. A method based on omnidirectional Gaussian derivative filtering was proposed to extract visual features from images of granulated products (e.g., cereal grains) and applied to automated rice quality classification [<xref rid="bib76" ref-type="bibr">76</xref>].</p><p>Computer vision-based image classification techniques have also been widely used to identify symptoms of disease in plants. Hyperspectral imaging was applied to detect and quantify downy mildew symptoms caused by <italic>Plasmopara viticola</italic> in grapevine plants [<xref rid="bib77" ref-type="bibr">77</xref>]. Recent deep learning-based techniques have led to improvements in throughput and accuracy for detecting disease symptoms in plants. Mohanty et al. [<xref rid="bib78" ref-type="bibr">78</xref>] demonstrated the feasibility of using a deep CNN to detect 26 diseases in 14 crop species by fine-tuning popular pretrained deep CNN architectures, such as AlexNet [<xref rid="bib64" ref-type="bibr">64</xref>] and GoogLeNet [<xref rid="bib65" ref-type="bibr">65</xref>], with a publicly available 54,306-image dataset of diseased and healthy plants from PlantVillage. Transfer learning was also used to train CNN models for detecting of disease symptoms in crops, such as olives [<xref rid="bib79" ref-type="bibr">79</xref>].</p></sec></sec><sec id="sec1-2-5"><title>Deep neural network-based image analysis with end-to-end learning</title><p>Beyond applications in each of the typical steps in computer vision-based image analysis, deep CNNs have automated approaches to directly identify biological instances from image data through end-to-end training. Faster region-based CNN (R-CNN) is a CNN-based region proposal network that enables representation of high-quality region proposals through end-to-end training [<xref rid="bib82" ref-type="bibr">82</xref>]. Jin et al. demonstrated the performance of a faster R-CNN-based model for segmentation of maize plants from terrestrial LiDAR data [<xref rid="bib83" ref-type="bibr">83</xref>]. In addition to faster R-CNN, Fuentes et al. examined two other CNN-based end-to-end frameworks for object detection: region-based fully convolutional network (FCN) and single shot multibox detector to detect diseases and pests in tomatoes [<xref rid="bib84" ref-type="bibr">84</xref>]. Shelhamer et al. proposed an FCN&#x000a0;that enables end-to-end training for sematic image segmentation by pixel-wise object labeling [<xref rid="bib85" ref-type="bibr">85</xref>], which has been applied to generate weed distribution maps from UAV images [<xref rid="bib86" ref-type="bibr">86</xref>, <xref rid="bib87" ref-type="bibr">87</xref>]. Moreover, FCN has also been applied to segment a particular region of an image into each instance (pixel-wise instance segmentation) for computer vision-based image and scene understanding, which should facilitate various instance segmentation tasks in plant phenotyping, such as the Leaf Segmentation and Counting Challenges [<xref rid="bib88" ref-type="bibr">88</xref>].</p></sec></sec><sec id="sec1-3"><title>Application of computer vision-assisted plant phenotyping for gene discovery</title><p>Modern techniques in computer vision can aid digital quantification of various morphological and physiological parameters in plants and are expected to improve the throughput and accuracy of plant phenotyping for population-scale analyses [<xref rid="bib89" ref-type="bibr">89</xref>, <xref rid="bib90" ref-type="bibr">90</xref>]. Combined with recent advances in high-throughput DNA sequencing, the automated acquisition of plant phenotypic data followed by computer vision-based extraction of phenotypic features provides opportunities for genome-scale exploration of useful genes and modeling of the molecular networks underlying complex traits related to plant productivity, such as growth, stress tolerance, disease resistance, and yield [<xref rid="bib9" ref-type="bibr">9</xref>, <xref rid="bib75" ref-type="bibr">75</xref>, <xref rid="bib91" ref-type="bibr">91</xref>&#x02013;<xref rid="bib93" ref-type="bibr">93</xref>].</p><sec id="sec1-3-1"><title>Autoscreening of mutants</title><p>Large-scale mutant resources have played crucial roles in reverse genetics approaches in plants, and computer vision-assisted phenotype analyses can provide new insights into gene functions and molecular networks related to traits in plants. A computer vision-based tracking approach to organ development revealed temperature-compensated cell production rates and elongation zone lengths in roots through comparative image analysis of wild-type <italic>Arabidopsis</italic> and a phytochrome-interacting factor 4- and 5<italic>-</italic>double mutant of <italic>Arabidopsis</italic> [<xref rid="bib94" ref-type="bibr">94</xref>]. A new clustering technique, nonparametric modeling, was applied to a high-throughput photosynthetic phenotype dataset and showed efficiency for discriminating <italic>Arabidopsis</italic> chloroplast mutant lines [<xref rid="bib95" ref-type="bibr">95</xref>]. In rice, a large-scale T-DNA insertional mutant resource was developed and applied to phenotyping 68 traits belonging to 11 categories and 3 quantitative traits, screened by well-trained breeders under field conditions [<xref rid="bib96" ref-type="bibr">96</xref>]. These findings led us to question whether using computer vision-based phenotyping to digitize growth patterns may bridge physiological features detected by machines and agronomically important traits observed by breeders.</p></sec><sec id="sec1-3-2"><title>Phenotyping for genetic mapping and prediction of agronomic traits</title><p>Phenotyping a set of accessions provides a dataset beneficial for exploring novel interactions between genetic factors that influence productivity [<xref rid="bib97" ref-type="bibr">97</xref>]. In several instances, automated plant phenotyping systems have been applied for characterizing the growth patterns of diverse crop accessions grown under controlled conditions. An automated plant phenotyping system, the rice automatic plant phenotyping platform, also assisted in quantifying 106 traits in a maize population composed of 167 recombinant inbred lines across 16 developmental stages and identified 998 QTLs for all investigated traits [<xref rid="bib98" ref-type="bibr">98</xref>]. In another study using a high-throughput phenotyping system, PhenoArch [<xref rid="bib99" ref-type="bibr">99</xref>] represented differences in daily growth among 254 maize hybrids in different soil and water conditions and revealed genetic loci affecting stomatal conductance through a genome-wide association study using the phenomic dataset [<xref rid="bib100" ref-type="bibr">100</xref>]. A study using multiple sensors, such as hyperspectral, fluorescence, and thermal infrared sensors, demonstrated a time course heritability of traits found in a set of 32 maize inbred lines in greenhouse conditions [<xref rid="bib101" ref-type="bibr">101</xref>]. These examples indicate that noninvasive phenotyping, unlike destructive measurement, enables us to characterize growth trajectories to identify phenotypic differences in development and phenological responses over time that may influence eventual traits, such as biomass and yield [<xref rid="bib102" ref-type="bibr">102</xref>].</p><p>For phenotyping crops under field conditions, the combined use of multiple sensors and techniques for image analysis has proven to be efficient for comprehensively identifying genetic and environmental factors related to phenotypic traits. With a dataset of 14 photosynthetic parameters and 4 morphological traits in a diverse rice population grown in different environments, a stepwise feature-selection approach based on linear regression models assisted in identifying physiological parameters related to the variance of biomass accumulation in rice [<xref rid="bib103" ref-type="bibr">103</xref>]. In a study of poplar trees, UAV-based thermal imaging of a full-sib F<sub>2</sub> population across water conditions showed the potential of UAV-based imaging for field phenotyping in tree genetic improvements [<xref rid="bib104" ref-type="bibr">104</xref>]. In a genetic study of iron deficiency chlorosis using an association panel of soybeans, supervised machine learning-based image classification allowed identification of genetic loci harboring a gene involved in iron acquisition, suggesting that computer vision-based plant phenotyping provides a promising framework for genomic prediction in crops [<xref rid="bib105" ref-type="bibr">105</xref>]. In sorghum, UAV-based remote sensing was used to measure plant height for genomic prediction modeling, demonstrating that UAV-based phenotyping with multiple sensors is efficient for generating datasets for genomic prediction modeling [<xref rid="bib106" ref-type="bibr">106</xref>].</p></sec></sec><sec id="sec1-4"><title>Datasets and software tools for plant phenotyping</title><sec id="sec1-4-1"><title>Datasets</title><p>Public datasets from various platforms for plant phenotyping will provide data for developing analytical methods in computer vision-based plant phenotyping. In a recent Kaggle competition, an image dataset of approximately 960 unique plants belonging to 12 species was used to create a classifier for plant taxonomic classification from a photograph of a plant seedling [<xref rid="bib107" ref-type="bibr">107</xref>]. In a previous study [<xref rid="bib108" ref-type="bibr">108</xref>], the authors introduced the first dataset for computer vision-based plant phenotyping, which was made available in a separate report [<xref rid="bib109" ref-type="bibr">109</xref>].</p><p>A comprehensive phenotype dataset is available in <italic>Arabidopsis</italic> and will be useful as a reference image-set for the growth and development of model plant species when assessing methods in computer vision-based plant phenotyping [<xref rid="bib110" ref-type="bibr">110</xref>]. In maize, the datasets used in two previous studies [<xref rid="bib33" ref-type="bibr">33</xref>, <xref rid="bib111" ref-type="bibr">111</xref>] are available in other reports [<xref rid="bib112" ref-type="bibr">112</xref>]. Moreover, the PlantCV website has provided image datasets acquired in grass species, such as rice, <italic>Setaria</italic>, and sorghum [<xref rid="bib114" ref-type="bibr">113</xref>]. Additionally, the importance of integrating traits, phenotypes, and gene functions based on ontologies has increased dramatically; plant ontology, plant trait ontology, plant experimental conditions ontology, and gene ontology can facilitate semantic integration of data and corpuses rapidly generated from plant genomics and phenomics [<xref rid="bib115" ref-type="bibr">114</xref>].</p></sec><sec id="sec1-4-2"><title>Software tools</title><p>Various types of software tools have been established to aid steps of image analysis in plant phenotyping. The Plant Image Analysis website [<xref rid="bib116" ref-type="bibr">115</xref>] showcases 172 software tools and 28 datasets (as of 9 August 2018) for analysis of plant image datasets, aiming to provide a user-friendly interface to find solutions and promote communication between users and developers [<xref rid="bib117" ref-type="bibr">116</xref>, <xref rid="bib118" ref-type="bibr">117</xref>]. Figure&#x000a0;<xref ref-type="fig" rid="fig2">2</xref> shows the ecosystem of software tools for plant phenotyping based on the plant image analysis database, in which software tools are connected to plant organs of an analytical target, indicating that the ecosystem is growing, particularly for images from leaves, shoots, and roots. Table&#x000a0;<xref rid="tbl3" ref-type="table">3</xref> shows examples of software tools recently developed for plant phenotyping by image processing, which take advantage of ML-based algorithms. Leaf Necrosis Classifier supports detection of leaf areas that show necrotic symptoms with combinatorial use of MLP and self-organizing maps [<xref rid="bib119" ref-type="bibr">118</xref>]. EasyPCC evaluates the ground coverage ratio accurately through image data acquired under field conditions and uses a pixel-based segmentation method that applies a decision-tree-based segmentation model [<xref rid="bib120" ref-type="bibr">119</xref>]. Leaf-GP is a software tool that is used for quantification of various growth phenotypes from large image series, applying Python-based machine learning libraries, which were used to analyze the growth of <italic>Arabidopsis</italic> and wheat [<xref rid="bib121" ref-type="bibr">120</xref>]. A deep CNN-based approach was applied to develop StomataCounter for detection of stomatal pores in microscopic images [<xref rid="bib122" ref-type="bibr">121</xref>]. Moreover, the mobile app Plantix enables diagnosis and customized options for detection of plant diseases, pests, and nutrient deficiencies to users who send a picture of a plant [<xref rid="bib123" ref-type="bibr">122</xref>], in which it synergistically uses a deep learning, crowd-sourced database to identify plant diseases on various crops worldwide.</p><fig id="fig2" orientation="portrait" position="float"><label>Figure 2:</label><caption><p>An ecosystem map of software tools for plant image analysis. The network-formed map consists of 169 software tools whose targets are particular plant organs based on the plant image analysis database [<xref rid="bib118" ref-type="bibr">117</xref>]. The nodes represent the software tools and their target plant organs represented using Cytoscape 3.0 [<xref rid="bib124" ref-type="bibr">123</xref>].</p></caption><graphic xlink:href="giy153fig2"/></fig><table-wrap id="tbl3" orientation="portrait" position="float"><label>Table 3:</label><caption><p>Software tools recently developed for plant image analysis that use machine learning-based algorithms</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Name</th><th align="left" rowspan="1" colspan="1">Algorithms</th><th align="left" rowspan="1" colspan="1">Functionalities</th><th align="left" rowspan="1" colspan="1">Reference, URL</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">Leaf Necrosis Classifier</td><td rowspan="1" colspan="1">Multilayer perceptron and self-organizing maps</td><td rowspan="1" colspan="1">Detection of leaf areas showing necrotic symptoms</td><td rowspan="1" colspan="1">[<xref rid="bib119" ref-type="bibr">118</xref>]</td></tr><tr><td rowspan="1" colspan="1">EasyPCC</td><td rowspan="1" colspan="1">Decision-tree-based segmentation model</td><td rowspan="1" colspan="1">Quantification of ground coverage ratio from image data acquired under field conditions</td><td rowspan="1" colspan="1">[<xref rid="bib120" ref-type="bibr">119</xref>, <xref rid="bib125" ref-type="bibr">124</xref>]</td></tr><tr><td rowspan="1" colspan="1">Leaf-GP</td><td rowspan="1" colspan="1">Python-based machine learning libraries</td><td rowspan="1" colspan="1">Quantification of multiple growth phenotypes from large image series</td><td rowspan="1" colspan="1">[<xref rid="bib121" ref-type="bibr">120</xref>, <xref rid="bib126" ref-type="bibr">125</xref>]</td></tr><tr><td rowspan="1" colspan="1">StomataCounter</td><td rowspan="1" colspan="1">Deep CNN</td><td rowspan="1" colspan="1">Counting stomate pores</td><td rowspan="1" colspan="1">[<xref rid="bib122" ref-type="bibr">121</xref>]</td></tr><tr><td rowspan="1" colspan="1">Plantix</td><td rowspan="1" colspan="1">Deep learning</td><td rowspan="1" colspan="1">Diagnosing plant diseases, pest damage, and nutrient deficiencies</td><td rowspan="1" colspan="1">[<xref rid="bib123" ref-type="bibr">122</xref>]</td></tr></tbody></table></table-wrap></sec></sec></sec><sec id="sec2"><title>Conclusions and Perspectives</title><p>In recent years, computer vision-based plant phenotyping has rapidly grown as a multidisciplinary area that integrates knowledge from plant science, ML, spectral sensing, and mechanical engineering. With large-scale plant image datasets and successful CNN-based algorithms, the tools available for computer vision-based plant phenotyping have shown remarkable advancements in plant recognition and taxonomic classification. Repositories for pretrained models for plant identification play significant roles in rapidly implementing models for new phenotyping frameworks through fine-tuning. Moreover, these models aid in the further improvement of recognition accuracy in more challenging tasks, such as multilabel segmentation of multiple organs and species under natural environments. These efforts to improve accuracy, throughput, and computational costs for automated plant identification will provide an analytical basis for computer vision-based plant phenotyping beyond the capacity of human vision-based observation.</p><p>Computer vision-based plant phenotyping has already played important roles in monitoring the physiological states of plants for agricultural applications, such as disease symptoms and grain quality. Meta-analysis of the spectral signatures of crops associated with growth stage, physiological states, and environmental conditions will provide useful clues for preventive interventions in farming. Moreover, spectral signatures observed during earlier growth stages of crops, which are associated with eventual agronomic traits such as yield and quality, will be beneficial phenotypes for dissecting the interactions between genetic and environmental factors and for increasing genetic gain in crop breeding.</p><p>Assorted sensors have assisted plant phenotyping under both controlled and field conditions and will aid our discovery of genes involved in agronomic traits and our understanding of their functions through statistical explorations of genome-phenome relationships, such as GWASs and phenome-wide association studies [<xref rid="bib127" ref-type="bibr">126</xref>, <xref rid="bib128" ref-type="bibr">127</xref>] in plants. High-throughput automated phenotyping will allow common garden experiments to be performed with diverse genetic resources in order to elucidate the genetic bases of adaptive traits in plants [<xref rid="bib129" ref-type="bibr">128</xref>]. Noninvasive and population-scale plant phenotyping will provide us opportunities to investigate interactions between internal and external factors related to plant growth and development, dissecting the effects of earlier life-course exposures onto later agronomic outcomes. Moreover, with the recent success of ML-based approaches in predicting individual traits in genomic prediction [<xref rid="bib130" ref-type="bibr">129</xref>] and cohort studies [<xref rid="bib131" ref-type="bibr">130</xref>, <xref rid="bib132" ref-type="bibr">131</xref>], computer vision-based phenotyping will play significant roles in both nowcasting and forecasting of plant traits through modeling genotype/phenotype relationships.</p></sec><sec id="sec3"><title/><sec disp-level="2" id="h1content1545046095166"><title>Abbreviations</title><p>3D: three-dimensional; ANFIS: adaptive neuro-fuzzy inference system; CNN: convolutional neural network; FCN: fully convolutional network; Ffirst: Fast Features Invariant to Rotation and Scale of Texture; GWAS: genome-wide association study ; kNN: k-nearest neighbor; LDA: linear discriminant analysis; ML: machine learning; MLP: multilayer perceptron; PHI: phenome high-throughput investigator; QTL: quantitative trait locus; R-CNN: region-based convolutional neural network; RF: random forest; RGB: red-green-blue; SIFT: Scale Invariant Features Transforms; SVM: support vector machine; UAV: unmanned aerial vehicle.</p></sec><sec disp-level="2" id="h1content1545046026359"><title>Competing interests</title><p>The authors declare that they have no competing interests.</p></sec><sec disp-level="2" id="sec4" sec-type="funding"><title>Funding</title><p>The work was supported by Core Research for Evolutionary Sciecne and Technology (CREST) of the Japan Science and Technology Agency (JST).</p></sec><sec disp-level="2" id="sec5"><title>Author contributions</title><p>Conceptualization: K.M. and F.M. Supervision: T.H. and R.N. Funding acquisition: K.M. and T.H. Writing&#x02014;original draft preparation: K.M., S.K., K.I., T.H., S.T., R.N., and F.M. Writing&#x02014;review and editing: K.M., S.K., K.I., and R.N. Visualization: K.M., S.K., and K.I.</p></sec></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="sup1"><label>GIGA-D-18-00215_Original_Submission.pdf</label><media xlink:href="giy153_giga-d-18-00215_original_submission.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="sup2"><label>GIGA-D-18-00215_Revision_1.pdf</label><media xlink:href="giy153_giga-d-18-00215_revision_1.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="sup3"><label>GIGA-D-18-00215_Revision_2.pdf</label><media xlink:href="giy153_giga-d-18-00215_revision_2.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="sup4"><label>Response_to_Reviewer_Comments_Original_Submission.pdf</label><media xlink:href="giy153_response_to_reviewer_comments_original_submission.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="sup5"><label>Response_to_Reviewer_Comments_Revision_1.pdf</label><media xlink:href="giy153_response_to_reviewer_comments_revision_1.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="sup6"><label>Reviewer_1_Report_1_(Original_Submission) -- Andrew French</label><caption><p>7/3/2018 Reviewed</p></caption><media xlink:href="giy153_reviewer_1_report_1_(original_submission).pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="sup7"><label>Reviewer_1_Report_1_Revision_1 -- Andrew French</label><caption><p>9/25/2018 Reviewed</p></caption><media xlink:href="giy153_reviewer_1_report_1_revision_1.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack><title>ACKNOWLEDGEMENTS</title><p>The authors gratefully thank Nobuko Kimura and Kyoko Ikebe for their assistance with the preparation of this manuscript.</p></ack><ref-list><title>References</title><ref id="bib1"><label>1.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Tardieu</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Cabrera-Bosquet</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Pridmore</surname><given-names>T</given-names></name><etal/></person-group>
<article-title>Plant phenomics, from sensors to knowledge</article-title>. <source>Curr Biol</source>. <year>2017</year>;<volume>27</volume>(<issue>15</issue>):<fpage>R770</fpage>&#x02013;<lpage>R83</lpage>.<pub-id pub-id-type="pmid">28787611</pub-id></mixed-citation></ref><ref id="bib2"><label>2.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Crisp</surname><given-names>PA</given-names></name>, <name name-style="western"><surname>Ganguly</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Eichten</surname><given-names>SR</given-names></name>, <etal/></person-group>
<article-title>Reconsidering plant memory: intersections between stress recovery, RNA turnover, and epigenetics</article-title>. <source>Sci Adv</source>. <year>2016</year>;<volume>2</volume>(<issue>2</issue>):<fpage>e1501340</fpage>.<pub-id pub-id-type="pmid">26989783</pub-id></mixed-citation></ref><ref id="bib3"><label>3.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Onda</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Mochida</surname><given-names>K</given-names></name></person-group>
<article-title>Exploring genetic diversity in plants using high-throughput sequencing techniques</article-title>. <source>Curr Genomics</source>. <year>2016</year>;<volume>17</volume>(<issue>4</issue>):<fpage>358</fpage>&#x02013;<lpage>67</lpage>.<pub-id pub-id-type="pmid">27499684</pub-id></mixed-citation></ref><ref id="bib4"><label>4.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Sharma</surname><given-names>TR</given-names></name>, <name name-style="western"><surname>Devanna</surname><given-names>BN</given-names></name>, <name name-style="western"><surname>Kiran</surname><given-names>K</given-names></name><etal/></person-group>
<article-title>Status and prospects of next generation sequencing technologies in crop plants</article-title>. <source>Curr Issues Mol Biol</source>. <year>2018</year>;<volume>27</volume>:<fpage>1</fpage>&#x02013;<lpage>36</lpage>.<pub-id pub-id-type="pmid">28885172</pub-id></mixed-citation></ref><ref id="bib5"><label>5.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Simko</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Jimenez-Berni</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Sirault</surname><given-names>XR</given-names></name></person-group>
<article-title>Phenomic approaches and tools for phytopathologists</article-title>. <source>Phytopathology</source>. <year>2017</year>;<volume>107</volume>(<issue>1</issue>):<fpage>6</fpage>&#x02013;<lpage>17</lpage>.<pub-id pub-id-type="pmid">27618193</pub-id></mixed-citation></ref><ref id="bib6"><label>6.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Bazakos</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Hanemian</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Trontin</surname><given-names>C</given-names></name>, <etal/></person-group>
<article-title>New strategies and tools in quantitative genetics: how to go from the phenotype to the genotype</article-title>. <source>Annu Rev Plant Biol</source>. <year>2017</year>;<volume>68</volume>:<fpage>435</fpage>&#x02013;<lpage>55</lpage>.<pub-id pub-id-type="pmid">28226236</pub-id></mixed-citation></ref><ref id="bib7"><label>7.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Crossa</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Perez-Rodriguez</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Cuevas</surname><given-names>J</given-names></name>, <etal/></person-group>
<article-title>Genomic selection in plant breeding: methods, models, and perspectives</article-title>. <source>Trends Plant Sci</source>. <year>2017</year>;<volume>22</volume>(<issue>11</issue>):<fpage>961</fpage>&#x02013;<lpage>75</lpage>.<pub-id pub-id-type="pmid">28965742</pub-id></mixed-citation></ref><ref id="bib8"><label>8.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Cabrera-Bosquet</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Crossa</surname><given-names>J</given-names></name>, <name name-style="western"><surname>von&#x000a0;Zitzewitz</surname><given-names>J</given-names></name>, <etal/></person-group>
<article-title>High-throughput phenotyping and genomic selection: the frontiers of crop breeding converge</article-title>. <source>J Integr Plant Biol</source>. <year>2012</year>;<volume>54</volume>(<issue>5</issue>):<fpage>312</fpage>&#x02013;<lpage>20</lpage>.<pub-id pub-id-type="pmid">22420640</pub-id></mixed-citation></ref><ref id="bib9"><label>9.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Araus</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Kefauver</surname><given-names>SC</given-names></name>, <name name-style="western"><surname>Zaman-Allah</surname><given-names>M</given-names></name>, <etal/></person-group>
<article-title>Translating high-throughput phenotyping into genetic gain</article-title>. <source>Trends Plant Sci</source>. <year>2018</year>;<volume>23</volume>(<issue>5</issue>):<fpage>451</fpage>&#x02013;<lpage>66</lpage>.<pub-id pub-id-type="pmid">29555431</pub-id></mixed-citation></ref><ref id="bib10"><label>10.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Perez-Sanz</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Navarro</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Egea-Cortines</surname><given-names>M</given-names></name></person-group>
<article-title>Plant phenomics: an overview of image acquisition technologies and image data analysis algorithms</article-title>. <source>GigaScience</source>. <year>2017</year>;<volume>6</volume>(<issue>11</issue>):<fpage>1</fpage>&#x02013;<lpage>18</lpage>.</mixed-citation></ref><ref id="bib11"><label>11.</label><mixed-citation publication-type="book">
<source>Department of Information Studies UoS: ImageCLEF.</source>
<comment><ext-link ext-link-type="uri" xlink:href="http://www.imageclef.org/lifeclef/2017/plant">http://www.imageclef.org/lifeclef/2017/plant</ext-link></comment>(<year>2003</year>). <comment>Accessed 11 June 2018</comment>.</mixed-citation></ref><ref id="bib12"><label>12.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Montagnoli</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Terzaghi</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Fulgaro</surname><given-names>N</given-names></name>. <etal/></person-group>
<article-title>Non-destructive phenotypic analysis of early stage tree seedling growth using an automated stereovision imaging method</article-title>. <source>Front Plant Sci</source>. <year>2016</year>;<volume>7</volume>:<fpage>1644</fpage>.<pub-id pub-id-type="pmid">27840632</pub-id></mixed-citation></ref><ref id="bib13"><label>13.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Wahabzada</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Mahlein</surname><given-names>AK</given-names></name>, <name name-style="western"><surname>Bauckhage</surname><given-names>C.</given-names></name><etal/></person-group>
<article-title>Plant phenotyping using probabilistic topic models: uncovering the hyperspectral language of plants</article-title>. <source>Sci Rep</source>. <year>2016</year>;<volume>6</volume>:<fpage>22482</fpage>.<pub-id pub-id-type="pmid">26957018</pub-id></mixed-citation></ref><ref id="bib14"><label>14.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Potgieter</surname><given-names>AB</given-names></name>, <name name-style="western"><surname>George-Jaeggli</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Chapman</surname><given-names>SC</given-names></name><etal/></person-group>
<article-title>Multi-spectral imaging from an unmanned aerial vehicle enables the assessment of seasonal leaf area dynamics of sorghum breeding lines</article-title>. <source>Front Plant Sci</source>. <year>2017</year>;<volume>8</volume>:<fpage>1532</fpage>.<pub-id pub-id-type="pmid">28951735</pub-id></mixed-citation></ref><ref id="bib15"><label>15.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Poblete</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Ortega-Farias</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Ryu</surname><given-names>D</given-names></name></person-group>
<article-title>Automatic coregistration algorithm to remove canopy shaded pixels in uav-borne thermal images to improve the estimation of crop water stress index of a drip-irrigated cabernet sauvignon vineyard</article-title>. <source>Sensors (Basel)</source>. <year>2018</year>;<volume>18</volume>(<issue>2</issue>):<fpage>pii: E397</fpage>.</mixed-citation></ref><ref id="bib16"><label>16.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Zarco-Tejada</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Camino</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Beck</surname><given-names>PSA</given-names></name><etal/></person-group>
<article-title>Previsual symptoms of <italic>Xylella fastidiosa</italic> infection revealed in spectral plant-trait alterations</article-title>. <source>Nat Plants</source>. <year>2018</year>;<volume>4</volume>(<issue>7</issue>):<fpage>432</fpage>&#x02013;<lpage>9</lpage>.<pub-id pub-id-type="pmid">29942047</pub-id></mixed-citation></ref><ref id="bib17"><label>17.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>Q</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Pang</surname><given-names>S</given-names></name><etal/></person-group>
<article-title>Crop 3D-a LiDAR based platform for 3D high-throughput crop phenotyping</article-title>. <source>Sci China Life Sci</source>. <year>2018</year>;<volume>61</volume>(<issue>3</issue>):<fpage>328</fpage>&#x02013;<lpage>39</lpage>.<pub-id pub-id-type="pmid">28616808</pub-id></mixed-citation></ref><ref id="bib18"><label>18.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Frolov</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Fripp</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Nguyen</surname><given-names>CV</given-names></name><etal/></person-group>
<article-title>Automated plant and leaf separation: application in 3D meshes of wheat plants</article-title>. In: <source><italic toggle="yes">Digital Image Computing: Techniques and Applications</italic>,</source>. <publisher-loc>Gold Coast, QLD, Australia</publisher-loc>, <publisher-name>IEEE</publisher-name>, <year>2016</year>.</mixed-citation></ref><ref id="bib19"><label>19.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Underwood</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Wendel</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Schofield</surname><given-names>B</given-names></name>, <etal/></person-group>
<article-title>Efficient in-field plant phenomics for row-crops with an autonomous ground vehicle</article-title>. <source>J Field Robot</source>. <year>2017</year>;<volume>34</volume>(<issue>6</issue>):<fpage>1061</fpage>&#x02013;<lpage>83</lpage>.</mixed-citation></ref><ref id="bib20"><label>20.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Zhao</surname><given-names>C</given-names></name><etal/></person-group>
<article-title>Unmanned aerial vehicle remote sensing for field-based crop phenotyping: current status and perspectives</article-title>. <source>Front Plant Sci</source>. <year>2017</year>;<volume>8</volume>:<fpage>1111</fpage>.<pub-id pub-id-type="pmid">28713402</pub-id></mixed-citation></ref><ref id="bib21"><label>21.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Virlet</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Sabermanesh</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Sadeghi-Tehran</surname><given-names>P</given-names></name><etal/></person-group>
<article-title>Field scanalyzer: an automated robotic field phenotyping platform for detailed crop monitoring</article-title>. <source>Funct Plant Biol</source>. <year>2017</year>;<volume>44</volume>(<issue>1</issue>):<fpage>143</fpage>&#x02013;<lpage>53</lpage>.</mixed-citation></ref><ref id="bib22"><label>22.</label><mixed-citation publication-type="book">
<source>Reference Phenotyping System Team: TERRA-REF: ADVANVED FIELD CROP ANALYTICS</source>. <comment><ext-link ext-link-type="uri" xlink:href="http://terraref.org">http://terraref.org</ext-link></comment>
<comment>Accessed 11 June 2018</comment>.</mixed-citation></ref><ref id="bib23"><label>23.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Lyu</surname><given-names>JI</given-names></name>, <name name-style="western"><surname>Baek</surname><given-names>SH</given-names></name>, <name name-style="western"><surname>Jung</surname><given-names>S</given-names></name>, <etal/></person-group>
<article-title>High-throughput and computational study of leaf senescence through a phenomic approach</article-title>. <source>Front Plant Sci</source>. <year>2017</year>;<volume>8</volume>:<fpage>1</fpage>&#x02013;<lpage>8</lpage>.<pub-id pub-id-type="pmid">28220127</pub-id></mixed-citation></ref><ref id="bib24"><label>24.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Feng</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Guo</surname><given-names>ZL</given-names></name>, <name name-style="western"><surname>Yang</surname><given-names>WN</given-names></name><etal/></person-group>
<article-title>An integrated hyperspectral imaging and genome-wide association analysis platform provides spectral and genetic insights into the natural variation in rice</article-title>. <source>Sci Rep</source>. <year>2017</year>;<volume>7</volume>:<fpage>4401</fpage><pub-id pub-id-type="pmid">28667309</pub-id></mixed-citation></ref><ref id="bib25"><label>25.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Fujita</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Tanabata</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Urano</surname><given-names>K</given-names></name><etal/></person-group>
<article-title>RIPPS: a plant phenotyping system for quantitative evaluation of growth under controlled environmental stress conditions</article-title>. <source>Plant Cell Physiol</source>. <year>2018</year>;<volume>59</volume>(<issue>10</issue>):<fpage>2030</fpage>&#x02013;<lpage>8</lpage>.<pub-id pub-id-type="pmid">30010970</pub-id></mixed-citation></ref><ref id="bib26"><label>26.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Barmeier</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Schmidhalter</surname><given-names>U</given-names></name></person-group>
<article-title>High-throughput field phenotyping of leaves, leaf sheaths, culms and ears of spring barley cultivars at anthesis and dough ripeness</article-title>. <source>Front Plant Sci</source>. <year>2017</year>;<volume>8</volume>:<fpage>1920</fpage>.<pub-id pub-id-type="pmid">29163629</pub-id></mixed-citation></ref><ref id="bib27"><label>27.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Deery</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Jimenez-Berni</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Jones</surname><given-names>H</given-names></name>, <etal/></person-group>
<article-title>Proximal remote sensing buggies and potential applications for field-based phenotyping</article-title>. <source>Agronomy</source>. <year>2014</year>;<volume>4</volume>(<issue>4</issue>):<fpage>349</fpage>&#x02013;<lpage>79</lpage>.</mixed-citation></ref><ref id="bib28"><label>28.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Rebetzke</surname><given-names>GJ</given-names></name>, <name name-style="western"><surname>Jimenez-Berni</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Bovill</surname><given-names>WD</given-names></name>, <etal/></person-group>
<article-title>High-throughput phenotyping technologies allow accurate selection of stay-green</article-title>. <source>J Exp Bot</source>. <year>2016</year>;<volume>67</volume>(<issue>17</issue>):<fpage>4919</fpage>&#x02013;<lpage>24</lpage>.<pub-id pub-id-type="pmid">27604804</pub-id></mixed-citation></ref><ref id="bib29"><label>29.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Cao</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Xiao</surname><given-names>Y</given-names></name><etal/></person-group>
<article-title>TasselNet: counting maize tassels in the wild via local counts regression network</article-title>. <source>Plant Methods</source>. <year>2017</year>;<volume>13</volume>:<fpage>79</fpage>.<pub-id pub-id-type="pmid">29118821</pub-id></mixed-citation></ref><ref id="bib30"><label>30.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Rahnemoonfar</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Sheppard</surname><given-names>C</given-names></name></person-group>
<article-title>Deep count: fruit counting based on deep simulated learning</article-title>. <source>Sensors (Basel)</source>. <year>2017</year>;<volume>17</volume>(<issue>4</issue>):<fpage>905</fpage>.</mixed-citation></ref><ref id="bib31"><label>31.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Hughes</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Askew</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Scotson</surname><given-names>CP</given-names></name><etal/></person-group>
<article-title>Non-destructive, high-content analysis of wheat grain traits using X-ray micro computed tomography</article-title>. <source>Plant Methods</source>. <year>2017</year>;<volume>13</volume>:<fpage>76</fpage>.<pub-id pub-id-type="pmid">29118820</pub-id></mixed-citation></ref><ref id="bib32"><label>32.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Chopin</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Laga</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Miklavcic</surname><given-names>SJ</given-names></name></person-group>
<article-title>A hybrid approach for improving image segmentation: application to phenotyping of wheat leaves</article-title>. <source>PLoS One</source>. <year>2016</year>;<volume>11</volume>(<issue>12</issue>):<fpage>e0168496</fpage>.<pub-id pub-id-type="pmid">27992594</pub-id></mixed-citation></ref><ref id="bib33"><label>33.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Brichet</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Fournier</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Turc</surname><given-names>O</given-names></name><etal/></person-group>
<article-title>A robot-assisted imaging pipeline for tracking the growths of maize ear and silks in a high-throughput phenotyping platform</article-title>. <source>Plant Methods</source>. <year>2017</year>;<volume>13</volume>(<issue>1</issue>):<fpage>96</fpage>.<pub-id pub-id-type="pmid">29176999</pub-id></mixed-citation></ref><ref id="bib34"><label>34.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>QY</given-names></name>, <name name-style="western"><surname>Cai</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>Berger</surname><given-names>B</given-names></name>, <etal/></person-group>
<article-title>Detecting spikes of wheat plants using neural networks with Laws texture energy</article-title>. <source>Plant Methods</source>. <year>2017</year>;<volume>13</volume>:<fpage>83</fpage>.<pub-id pub-id-type="pmid">29046709</pub-id></mixed-citation></ref><ref id="bib35"><label>35.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Rzanny</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Seeland</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Waldchen</surname><given-names>J</given-names></name>, <etal/></person-group>
<article-title>Acquiring and preprocessing leaf images for automated plant identification: understanding the tradeoff between effort and information gain</article-title>. <source>Plant Methods</source>. <year>2017</year>;<volume>13</volume>:<fpage>97</fpage>.<pub-id pub-id-type="pmid">29151843</pub-id></mixed-citation></ref><ref id="bib36"><label>36.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Sabanci</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Kayabasi</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Toktas</surname><given-names>A</given-names></name></person-group>
<article-title>Computer vision-based method for classification of wheat grains using artificial neural network</article-title>. <source>J Sci Food Agr</source>. <year>2017</year>;<volume>97</volume>(<issue>8</issue>):<fpage>2588</fpage>&#x02013;<lpage>93</lpage>.<pub-id pub-id-type="pmid">27718230</pub-id></mixed-citation></ref><ref id="bib37"><label>37.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Sabanci</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Toktas</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Kayabasi</surname><given-names>A</given-names></name></person-group>
<article-title>Grain classifier with computer vision using adaptive neuro-fuzzy inference system</article-title>. <source>J Sci Food Agr</source>. <year>2017</year>;<volume>97</volume>(<issue>12</issue>):<fpage>3994</fpage>&#x02013;<lpage>4000</lpage>.<pub-id pub-id-type="pmid">28194800</pub-id></mixed-citation></ref><ref id="bib38"><label>38.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Lo&#x000a0;Bianco</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Grillo</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Escobar&#x000a0;Garcia</surname><given-names>P</given-names></name>, <etal/></person-group>
<article-title>Morpho-colorimetric characterisation of Malva alliance taxa by seed image analysis</article-title>. <source>Plant Biol (Stuttg)</source>. <year>2017</year>;<volume>19</volume>(<issue>1</issue>):<fpage>90</fpage>&#x02013;<lpage>8</lpage>.<pub-id pub-id-type="pmid">27385321</pub-id></mixed-citation></ref><ref id="bib39"><label>39.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Lo&#x000a0;Bianco</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Grillo</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Canadas</surname><given-names>E</given-names></name><etal/></person-group>
<article-title>Inter- and intraspecific diversity in Cistus L. (Cistaceae) seeds, analysed with computer vision techniques</article-title>. <source>Plant Biology</source>. <year>2017</year>;<volume>19</volume>(<issue>2</issue>):<fpage>183</fpage>&#x02013;<lpage>90</lpage>.<pub-id pub-id-type="pmid">27917577</pub-id></mixed-citation></ref><ref id="bib40"><label>40.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Wilf</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>SP</given-names></name>, <name name-style="western"><surname>Chikkerur</surname><given-names>S</given-names></name><etal/></person-group>
<article-title>Computer vision cracks the leaf code</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2016</year>;<volume>113</volume>(<issue>12</issue>):<fpage>3305</fpage>&#x02013;<lpage>10</lpage>.<pub-id pub-id-type="pmid">26951664</pub-id></mixed-citation></ref><ref id="bib41"><label>41.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>QQ</given-names></name>, <name name-style="western"><surname>Zhong</surname><given-names>YF</given-names></name>, <name name-style="western"><surname>Zhao</surname><given-names>B</given-names></name><etal/></person-group>
<article-title>Bag-of-visual-words scene classifier with local and global features for high spatial resolution remote sensing imagery</article-title>. <source>Ieee Geosci Remote S</source>. <year>2016</year>;<volume>13</volume>(<issue>6</issue>):<fpage>747</fpage>&#x02013;<lpage>51</lpage>.</mixed-citation></ref><ref id="bib42"><label>42.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Sonoyama</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Hirakawa</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Tamaki</surname><given-names>T</given-names></name>, <etal/></person-group>
<article-title>Transfer learning for bag-of-visual words approach to NBI endoscopic image classification</article-title>. <source>Conf Proc IEEE Eng Med Biol Soc</source>. <year>2015</year>;<volume>2015</volume>:<fpage>785</fpage>&#x02013;<lpage>8</lpage>.<pub-id pub-id-type="pmid">26736379</pub-id></mixed-citation></ref><ref id="bib43"><label>43.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Lu</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Yu</surname><given-names>M</given-names></name>, <etal/></person-group>
<article-title>Content-based retrieval of focal liver lesions using bag-of-visual-words representations of single- and multiphase contrast-enhanced CT images</article-title>. <source>J Digit Imaging</source>. <year>2012</year>;<volume>25</volume>(<issue>6</issue>):<fpage>708</fpage>&#x02013;<lpage>19</lpage>.<pub-id pub-id-type="pmid">22692772</pub-id></mixed-citation></ref><ref id="bib44"><label>44.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Lin</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Hu</surname><given-names>H</given-names></name><etal/></person-group>
<article-title>Texture-specific bag of visual words model and spatial cone matching-based method for the retrieval of focal liver lesions using multiphase contrast-enhanced CT images</article-title>. <source>Int J Comput Assist Radiol Surg</source>. <year>2018</year>;<volume>13</volume>(<issue>1</issue>):<fpage>151</fpage>&#x02013;<lpage>64</lpage>.<pub-id pub-id-type="pmid">29105019</pub-id></mixed-citation></ref><ref id="bib45"><label>45.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>JY</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>YP</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>Y</given-names></name>, <etal/></person-group>
<article-title>Bag-of-features based medical image retrieval via multiple assignment and visual words weighting</article-title>. <source>Ieee T Med Imaging</source>. <year>2011</year>;<volume>30</volume>(<issue>11</issue>):<fpage>1996</fpage>&#x02013;<lpage>2011</lpage>.</mixed-citation></ref><ref id="bib46"><label>46.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Inoue</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Shinoda</surname><given-names>K</given-names></name></person-group>
<article-title>Fast coding of feature vectors using neighbor-to-neighbor search</article-title>. <source>Ieee T Pattern Anal</source>. <year>2016</year>;<volume>38</volume>(<issue>6</issue>):<fpage>1170</fpage>&#x02013;<lpage>84</lpage>.</mixed-citation></ref><ref id="bib47"><label>47.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Sadeghi-Tehran</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Sabermanesh</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Virlet</surname><given-names>N</given-names></name>, <etal/></person-group>
<article-title>Automated method to determine two critical growth stages of wheat: heading and flowering</article-title>. <source>Front Plant Sci</source>. <year>2017</year>;<volume>8</volume>:<fpage>252</fpage>.<pub-id pub-id-type="pmid">28289423</pub-id></mixed-citation></ref><ref id="bib48"><label>48.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Bengio</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Hinton</surname><given-names>G</given-names></name></person-group>
<article-title>Deep learning</article-title>. <source>Nature</source>. <year>2015</year>;<volume>521</volume>(<issue>7553</issue>):<fpage>436</fpage>&#x02013;<lpage>44</lpage>.<pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation></ref><ref id="bib49"><label>49.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group>
<article-title>Deep neural networks: a new framework for modeling biological vision and brain information processing</article-title>. <source>Annu Rev Vis Sc</source>. <year>2015</year>;<volume>1</volume>:<fpage>417</fpage>&#x02013;<lpage>46</lpage>.<pub-id pub-id-type="pmid">28532370</pub-id></mixed-citation></ref><ref id="bib50"><label>50.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Sharma</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Singh</surname><given-names>A</given-names></name></person-group>
<article-title>Era of deep neural networks: A review</article-title>. In: <source>International Conference on Computing</source>, <publisher-name>Communication and Networking Technologies (ICCCNT)</publisher-name>, <publisher-loc>Delhi, India</publisher-loc>; <year>2017</year>.</mixed-citation></ref><ref id="bib51"><label>51.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Shin</surname><given-names>HC</given-names></name>, <name name-style="western"><surname>Roth</surname><given-names>HR</given-names></name>, <name name-style="western"><surname>Gao</surname><given-names>M</given-names></name>, <etal/></person-group>
<article-title>Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning</article-title>. <source>IEEE Trans Med Imaging</source>. <year>2016</year>;<volume>35</volume>(<issue>5</issue>):<fpage>1285</fpage>&#x02013;<lpage>98</lpage>.<pub-id pub-id-type="pmid">26886976</pub-id></mixed-citation></ref><ref id="bib52"><label>52.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>SH</given-names></name>, <name name-style="western"><surname>Chan</surname><given-names>CS</given-names></name>, <name name-style="western"><surname>Mayo</surname><given-names>SJ</given-names></name><etal/></person-group>
<article-title>How deep learning extracts and learns leaf features for plant classification</article-title>. <source>Pattern Recogn</source>. <year>2017</year>;<volume>71</volume>:<fpage>1</fpage>&#x02013;<lpage>13</lpage>.</mixed-citation></ref><ref id="bib53"><label>53.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Barre</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Stover</surname><given-names>BC</given-names></name>, <name name-style="western"><surname>Muller</surname><given-names>KF</given-names></name><etal/></person-group>
<article-title>LeafNet: a computer vision system for automatic plant species identification</article-title>. <source>Ecol Inform</source>. <year>2017</year>;<volume>40</volume>:<fpage>50</fpage>&#x02013;<lpage>6</lpage>.</mixed-citation></ref><ref id="bib54"><label>54.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>W&#x000e4;ldchen</surname><given-names>J</given-names></name>, <name name-style="western"><surname>M&#x000e4;der</surname><given-names>P</given-names></name></person-group>
<article-title>Plant species identification using computer vision techniques: a systematic literature review</article-title>. <source>Arch Comput Meth Eng</source>. <year>2017</year>;<volume>25</volume>;<fpage>507</fpage>&#x02013;<lpage>543</lpage>.</mixed-citation></ref><ref id="bib55"><label>55.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Unger</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Merhof</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Renner</surname><given-names>S</given-names></name></person-group>
<article-title>Computer vision applied to herbarium specimens of German trees: testing the future utility of the millions of herbarium specimen images for automated identification</article-title>. <source>Bmc Evol Biol</source>. <year>2016</year>;<volume>16</volume>:<fpage>248</fpage>.<pub-id pub-id-type="pmid">27852219</pub-id></mixed-citation></ref><ref id="bib56"><label>56.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Sulc</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Matas</surname><given-names>J</given-names></name></person-group>
<article-title>Fine-grained recognition of plants from images</article-title>. <source>Plant Methods</source>. <year>2017</year>;<volume>13</volume>:<fpage>115</fpage>.<pub-id pub-id-type="pmid">29299049</pub-id></mixed-citation></ref><ref id="bib57"><label>57.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>SW</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Huang</surname><given-names>WZ</given-names></name></person-group>
<article-title>Two-stage plant species recognition by local mean clustering and weighted sparse representation classification</article-title>. <source>Cluster Comput</source>. <year>2017</year>;<volume>20</volume>(<issue>2</issue>):<fpage>1517</fpage>&#x02013;<lpage>25</lpage>.</mixed-citation></ref><ref id="bib58"><label>58.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Piiroinen</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Heiskanen</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Maeda</surname><given-names>E</given-names></name>, <etal/></person-group>
<article-title>Classification of tree species in a diverse African agroforestry landscape using imaging spectroscopy and laser scanning</article-title>. <source>Remote Sens-Basel</source>. <year>2017</year>;<volume>9</volume>(<issue>9</issue>):<fpage>875</fpage>.</mixed-citation></ref><ref id="bib59"><label>59.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>XL</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>YM</given-names></name><etal/></person-group>
<article-title>A deep convolutional neural network architecture for boosting image discrimination accuracy of rice species</article-title>. <source>Food Bioprocess Tech</source>. <year>2018</year>;<volume>11</volume>(<issue>4</issue>):<fpage>765</fpage>&#x02013;<lpage>73</lpage>.</mixed-citation></ref><ref id="bib60"><label>60.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Pound</surname><given-names>MP</given-names></name>, <name name-style="western"><surname>Atkinson</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Wells</surname><given-names>DM</given-names></name>, <etal/></person-group>
<article-title>Deep learning for multi-task plant phenotyping</article-title>. In: <source>International Conference on Computer Vision (ICCV)</source>, <publisher-loc>Venice, Italy</publisher-loc>, <publisher-name>IEEE</publisher-name><year>2017</year>.</mixed-citation></ref><ref id="bib61"><label>61.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Carranza-Rojas</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Goeau</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Bonnet</surname><given-names>P</given-names></name><etal/></person-group>
<article-title>Going deeper in the automated identification of herbarium specimens</article-title>. <source>Bmc Evol Biol</source>. <year>2017</year>;<volume>17</volume>:<fpage>1</fpage>&#x02013;<lpage>14</lpage>.<pub-id pub-id-type="pmid">28049419</pub-id></mixed-citation></ref><ref id="bib62"><label>62.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Pound</surname><given-names>MP</given-names></name>, <name name-style="western"><surname>Atkinson</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Townsend</surname><given-names>AJ</given-names></name>, <etal/></person-group>
<article-title>Deep machine learning provides state-of-the-art performance in image-based plant phenotyping</article-title>. <source>GigaScience</source>. <year>2017</year>;<volume>6</volume>(<issue>10</issue>):<fpage>1</fpage>&#x02013;<lpage>10</lpage>.</mixed-citation></ref><ref id="bib63"><label>63.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Ghazi</surname><given-names>MM</given-names></name>, <name name-style="western"><surname>Yanikoglu</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Aptoula</surname><given-names>E</given-names></name></person-group>
<article-title>Plant identification using deep neural networks via optimization of transfer learning parameters</article-title>. <source>Neurocomputing</source>. <year>2017</year>;<volume>235</volume>:<fpage>228</fpage>&#x02013;<lpage>35</lpage>.</mixed-citation></ref><ref id="bib64"><label>64.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Krizhevsky</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Sutskever</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name></person-group>
<article-title>ImageNet classification with deep convolutional neural networks</article-title>. <source>Commun Acm</source>. <year>2017</year>;<volume>60</volume>(<issue>6</issue>):<fpage>84</fpage>&#x02013;<lpage>90</lpage>.</mixed-citation></ref><ref id="bib65"><label>65.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Szegedy</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Jia</surname><given-names>Y</given-names></name><etal/></person-group>
<article-title>Going deeper with convolutions</article-title>. In: <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>, Boston, MA, USA, <publisher-name>IEEE</publisher-name>, <year>2015</year>.</mixed-citation></ref><ref id="bib66"><label>66.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Simonyan</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Zisserman</surname><given-names>A</given-names></name></person-group>
<article-title>Very deep convolutional networks for large-scale image recognition</article-title>, <comment>arXiv preprint</comment><year>2015 Apr</year>;</mixed-citation></ref><ref id="bib67"><label>67.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Ren</surname><given-names>S</given-names></name>, <etal/></person-group>
<article-title>Deep residual learning for image recognition</article-title>. In: <source>Computer Vision and Pattern Recognition (CVPR)</source>, <publisher-loc>Las Vegas, NV, USA</publisher-loc>, <publisher-name>IEEE</publisher-name>; <year>2016</year>.</mixed-citation></ref><ref id="bib68"><label>68.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Szegedy</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Ioffe</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Vanhoucke</surname><given-names>V</given-names></name>, <etal/></person-group>
<article-title>Inception-v4, inception-ResNet and the impact of residual connections on learning</article-title>, <comment>arXiv preprint</comment> In; <year>2016</year> Aug.</mixed-citation></ref><ref id="bib69"><label>69.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Singh</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Ganapathysubramanian</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Singh</surname><given-names>AK</given-names></name><etal/></person-group>
<article-title>Machine learning for high-throughput stress phenotyping in plants</article-title>. <source>Trends Plant Sci</source>. <year>2016</year>;<volume>21</volume>(<issue>2</issue>):<fpage>110</fpage>&#x02013;<lpage>24</lpage>.<pub-id pub-id-type="pmid">26651918</pub-id></mixed-citation></ref><ref id="bib70"><label>70.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Mahlein</surname><given-names>AK</given-names></name></person-group>
<article-title>Plant disease detection by imaging sensors - parallels and specific demands for precision agriculture and plant phenotyping</article-title>. <source>Plant Dis</source>. <year>2016</year>;<volume>100</volume>(<issue>2</issue>):<fpage>241</fpage>&#x02013;<lpage>51</lpage>.</mixed-citation></ref><ref id="bib71"><label>71.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Liew</surname><given-names>OW</given-names></name>, <name name-style="western"><surname>Chong</surname><given-names>PC</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>B</given-names></name><etal/></person-group>
<article-title>Signature optical cues: emerging technologies for monitoring plant health</article-title>. <source>Sensors (Basel)</source>. <year>2008</year>;<volume>8</volume>(<issue>5</issue>):<fpage>3205</fpage>&#x02013;<lpage>39</lpage>.<pub-id pub-id-type="pmid">27879874</pub-id></mixed-citation></ref><ref id="bib72"><label>72.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Maimaitiyiming</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ghulam</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Bozzolo</surname><given-names>A</given-names></name>, <etal/></person-group>
<article-title>Early detection of plant physiological responses to different levels of water stress using reflectance spectroscopy</article-title>. <source>Remote Sens-Basel</source>. <year>2017</year>;<volume>9</volume>(<issue>7</issue>):<fpage>745</fpage>.</mixed-citation></ref><ref id="bib73"><label>73.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Altangerel</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Ariunbold</surname><given-names>GO</given-names></name>, <name name-style="western"><surname>Gorman</surname><given-names>C</given-names></name>, <etal/></person-group>
<article-title>Reply to Dong and Zhao: plant stress via raman spectroscopy</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2017</year>;<volume>114</volume>(<issue>28</issue>):<fpage>E5488</fpage>&#x02013;<lpage>E90</lpage>.<pub-id pub-id-type="pmid">28655836</pub-id></mixed-citation></ref><ref id="bib74"><label>74.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Pandey</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Ge</surname><given-names>YF</given-names></name>, <name name-style="western"><surname>Stoerger</surname><given-names>V</given-names></name><etal/></person-group>
<article-title>High throughput in vivo analysis of plant leaf chemical properties using hyperspectral imaging</article-title>. <source>Front Plant Sci</source>. <year>2017</year>;<volume>8</volume>:<fpage>1348</fpage>.<pub-id pub-id-type="pmid">28824683</pub-id></mixed-citation></ref><ref id="bib75"><label>75.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Shakoor</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Mockler</surname><given-names>TC</given-names></name></person-group>
<article-title>High throughput phenotyping to accelerate crop breeding and monitoring of diseases in the field</article-title>. <source>Curr Opin Plant Biol</source>. <year>2017</year>;<volume>38</volume>:<fpage>184</fpage>&#x02013;<lpage>92</lpage>.<pub-id pub-id-type="pmid">28738313</pub-id></mixed-citation></ref><ref id="bib76"><label>76.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Tang</surname><given-names>ZH</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>J</given-names></name>, <etal/></person-group>
<article-title>Visual perception-based statistical modeling of complex grain image for product quality monitoring and supervision on assembly production line</article-title>. <source>PLoS One</source>. <year>2016</year>;<volume>11</volume>(<issue>3</issue>):<fpage>e0146484</fpage>.<pub-id pub-id-type="pmid">26986726</pub-id></mixed-citation></ref><ref id="bib77"><label>77.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Oerke</surname><given-names>EC</given-names></name>, <name name-style="western"><surname>Herzog</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Toepfer</surname><given-names>R</given-names></name></person-group>
<article-title>Hyperspectral phenotyping of the reaction of grapevine genotypes to <italic>Plasmopara viticola</italic></article-title>. <source>J Exp Bot</source>. <year>2016</year>;<volume>67</volume>(<issue>18</issue>):<fpage>5529</fpage>&#x02013;<lpage>43</lpage>.<pub-id pub-id-type="pmid">27567365</pub-id></mixed-citation></ref><ref id="bib78"><label>78.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Mohanty</surname><given-names>SP</given-names></name>, <name name-style="western"><surname>Hughes</surname><given-names>DP</given-names></name>, <name name-style="western"><surname>Salathe</surname><given-names>M</given-names></name></person-group>
<article-title>Using deep learning for image-based plant disease detection</article-title>. <source>Front Plant Sci</source>. <year>2016</year>;<volume>7</volume>:<fpage>1419</fpage>.<pub-id pub-id-type="pmid">27713752</pub-id></mixed-citation></ref><ref id="bib79"><label>79.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Cruz</surname><given-names>AC</given-names></name>, <name name-style="western"><surname>Luvisi</surname><given-names>A</given-names></name>, <name name-style="western"><surname>De&#x000a0;Bellis</surname><given-names>L</given-names></name><etal/></person-group>
<article-title>X-FIDO: An effective application for detecting olive quick decline syndrome with deep learning and data fusion</article-title>. <source>Front Plant Sci</source>. <year>2017</year>;<volume>8</volume>:<fpage>1741</fpage>.<pub-id pub-id-type="pmid">29067037</pub-id></mixed-citation></ref><ref id="bib80"><label>80.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Blasco</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Munera</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Aleixos</surname><given-names>N</given-names></name><etal/></person-group>
<article-title>Machine vision-based measurement systems for fruit and vegetable quality control in postharvest</article-title>. <source>Adv Biochem Eng Biotechnol</source>. <year>2017</year>;<volume>161</volume>:<fpage>71</fpage>&#x02013;<lpage>91</lpage>.<pub-id pub-id-type="pmid">28289768</pub-id></mixed-citation></ref><ref id="bib81"><label>81.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Navarro</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Perez</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Weiss</surname><given-names>J</given-names></name><etal/></person-group>
<article-title>Machine learning and computer vision system for phenotype data acquisition and analysis in plants</article-title>. <source>Sensors (Basel)</source>. <year>2016</year>;<volume>16</volume>(<issue>5</issue>):<fpage>pii: E641</fpage>.</mixed-citation></ref><ref id="bib82"><label>82.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>S</given-names></name>, <name name-style="western"><surname>He</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Girshick</surname><given-names>R</given-names></name><etal/></person-group>
<article-title>Faster R-CNN: towards real-time object detection with region proposal networks</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2017</year>;<volume>39</volume>(<issue>6</issue>):<fpage>1137</fpage>&#x02013;<lpage>49</lpage>.<pub-id pub-id-type="pmid">27295650</pub-id></mixed-citation></ref><ref id="bib83"><label>83.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Jin</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Su</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Gao</surname><given-names>S</given-names></name><etal/></person-group>
<article-title>Deep learning: individual maize segmentation from terrestrial lidar data using faster R-CNN and regional growth algorithms</article-title>. <source>Front Plant Sci</source>. <year>2018</year>;<volume>9</volume>:<fpage>866</fpage>.<pub-id pub-id-type="pmid">29988466</pub-id></mixed-citation></ref><ref id="bib84"><label>84.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Fuentes</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Yoon</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>SC</given-names></name>, <etal/></person-group>
<article-title>A robust deep-learning-based detector for real-time tomato plant diseases and pests recognition</article-title>. <source>Sensors (Basel)</source>. <year>2017</year>;<volume>17</volume>(<issue>9</issue>):<fpage>pii: E2022</fpage>.</mixed-citation></ref><ref id="bib85"><label>85.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Shelhamer</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Long</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Darrell</surname><given-names>T</given-names></name></person-group>
<article-title>Fully convolutional networks for semantic segmentation</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2017</year>;<volume>39</volume>(<issue>4</issue>):<fpage>640</fpage>&#x02013;<lpage>51</lpage>.<pub-id pub-id-type="pmid">27244717</pub-id></mixed-citation></ref><ref id="bib86"><label>86.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Lan</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Deng</surname><given-names>J</given-names></name>, <etal/></person-group>
<article-title>A semantic labeling approach for accurate weed mapping of high resolution UAV imagery</article-title>. <source>Sensors (Basel)</source>. <year>2018</year>;<volume>18</volume>(<issue>7</issue>):<fpage>2113</fpage>.</mixed-citation></ref><ref id="bib87"><label>87.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Deng</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Lan</surname><given-names>Y</given-names></name>, <etal/></person-group>
<article-title>Accurate weed mapping and prescription map generation based on fully convolutional networks using UAV imagery</article-title>. <source>Sensors (Basel)</source>. <year>2018</year>;<volume>18</volume>(<issue>10</issue>):<fpage>pii: E3299</fpage>.</mixed-citation></ref><ref id="bib88"><label>88.</label><mixed-citation publication-type="book">
<source>Leaf segmentation and counting challenges</source>. <comment><ext-link ext-link-type="uri" xlink:href="https://www.plant-phenotyping.org/CVPPP2017-challenge">https://www.plant-phenotyping.org/CVPPP2017-challenge</ext-link></comment>
<comment>Accessed 14 November 2018</comment>.</mixed-citation></ref><ref id="bib89"><label>89.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Ghanem</surname><given-names>ME</given-names></name>, <name name-style="western"><surname>Marrou</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Sinclair</surname><given-names>TR</given-names></name></person-group>
<article-title>Physiological phenotyping of plants for crop improvement</article-title>. <source>Trends Plant Sci</source>. <year>2015</year>;<volume>20</volume>(<issue>3</issue>):<fpage>139</fpage>&#x02013;<lpage>44</lpage>.<pub-id pub-id-type="pmid">25524213</pub-id></mixed-citation></ref><ref id="bib90"><label>90.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Araus</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Cairns</surname><given-names>JE</given-names></name></person-group>
<article-title>Field high-throughput phenotyping: the new crop breeding frontier</article-title>. <source>Trends Plant Sci</source>. <year>2014</year>;<volume>19</volume>(<issue>1</issue>):<fpage>52</fpage>&#x02013;<lpage>61</lpage>.<pub-id pub-id-type="pmid">24139902</pub-id></mixed-citation></ref><ref id="bib91"><label>91.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Fernandez</surname><given-names>MGS</given-names></name>, <name name-style="western"><surname>Bao</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Tang</surname><given-names>L</given-names></name><etal/></person-group>
<article-title>A high-throughput, field-based phenotyping technology for tall biomass crops</article-title>. <source>Plant Physiol</source>. <year>2017</year>;<volume>174</volume>(<issue>4</issue>):<fpage>2008</fpage>&#x02013;<lpage>22</lpage>.<pub-id pub-id-type="pmid">28620124</pub-id></mixed-citation></ref><ref id="bib92"><label>92.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Valliyodan</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Ye</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Song</surname><given-names>L</given-names></name>, <etal/></person-group>
<article-title>Genetic diversity and genomic strategies for improving drought and waterlogging tolerance in soybeans</article-title>. <source>J Exp Bot</source>. <year>2017</year>;<volume>68</volume>(<issue>8</issue>):<fpage>1835</fpage>&#x02013;<lpage>49</lpage>.<pub-id pub-id-type="pmid">27927997</pub-id></mixed-citation></ref><ref id="bib93"><label>93.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Shi</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Pape</surname><given-names>JM</given-names></name>, <etal/></person-group>
<article-title>Predicting plant biomass accumulation from image-derived parameters</article-title>. <source>GigaScience</source>. <year>2018</year>;<volume>7</volume>(<issue>2</issue>):<fpage>1</fpage>&#x02013;<lpage>13</lpage>.</mixed-citation></ref><ref id="bib94"><label>94.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Dong</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Palaniappan</surname><given-names>K</given-names></name><etal/></person-group>
<article-title>Temperature-compensated cell production rate and elongation zone length in the root of <italic>Arabidopsis thaliana</italic></article-title>. <source>Plant Cell Environ</source>. <year>2017</year>;<volume>40</volume>(<issue>2</issue>):<fpage>264</fpage>&#x02013;<lpage>76</lpage>.<pub-id pub-id-type="pmid">27813107</pub-id></mixed-citation></ref><ref id="bib95"><label>95.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Gao</surname><given-names>Q</given-names></name>, <name name-style="western"><surname>Ostendorf</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Cruz</surname><given-names>JA</given-names></name>, <etal/></person-group>
<article-title>Inter-functional analysis of high-throughput phenotype data by non-parametric clustering and its application to photosynthesis</article-title>. <source>Bioinformatics</source>. <year>2016</year>;<volume>32</volume>(<issue>1</issue>):<fpage>67</fpage>&#x02013;<lpage>76</lpage>.<pub-id pub-id-type="pmid">26342101</pub-id></mixed-citation></ref><ref id="bib96"><label>96.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>HP</given-names></name>, <name name-style="western"><surname>Wei</surname><given-names>FJ</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>CC</given-names></name><etal/></person-group>
<article-title>Large-scale phenomics analysis of a T-DNA tagged mutant population</article-title>. <source>GigaScience</source>. <year>2017</year>;<volume>6</volume>(<issue>8</issue>):<fpage>1</fpage>&#x02013;<lpage>7</lpage>.</mixed-citation></ref><ref id="bib97"><label>97.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Al-Tamimi</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Brien</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Oakey</surname><given-names>H</given-names></name>, <etal/></person-group>
<article-title>Salinity tolerance loci revealed in rice using high-throughput non-invasive phenotyping</article-title>. <source>Nat Commun</source>. <year>2016</year>;<volume>7</volume>:<fpage>13342</fpage>.<pub-id pub-id-type="pmid">27853175</pub-id></mixed-citation></ref><ref id="bib98"><label>98.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Huang</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>D</given-names></name><etal/></person-group>
<article-title>High-throughput phenotyping and QTL mapping reveals the genetic architecture of maize plant growth</article-title>. <source>Plant Physiol</source>. <year>2017</year>;<volume>173</volume>(<issue>3</issue>):<fpage>1554</fpage>&#x02013;<lpage>64</lpage>.<pub-id pub-id-type="pmid">28153923</pub-id></mixed-citation></ref><ref id="bib99"><label>99.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Cabrera-Bosquet</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Fournier</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Brichet</surname><given-names>N</given-names></name><etal/></person-group>
<article-title>High-throughput estimation of incident light, light interception and radiation-use efficiency of thousands of plants in a phenotyping platform</article-title>. <source>New Phytol</source>. <year>2016</year>;<volume>212</volume>(<issue>1</issue>):<fpage>269</fpage>&#x02013;<lpage>81</lpage>.<pub-id pub-id-type="pmid">27258481</pub-id></mixed-citation></ref><ref id="bib100"><label>100.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Prado</surname><given-names>SA</given-names></name>, <name name-style="western"><surname>Cabrera-Bosquet</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Grau</surname><given-names>A</given-names></name><etal/></person-group>
<article-title>Phenomics allows identification of genomic regions affecting maize stomatal conductance with conditional effects of water deficit and evaporative demand</article-title>. <source>Plant Cell Environ</source>. <year>2018</year>;<volume>41</volume>(<issue>2</issue>):<fpage>314</fpage>&#x02013;<lpage>26</lpage>.<pub-id pub-id-type="pmid">29044609</pub-id></mixed-citation></ref><ref id="bib101"><label>101.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Liang</surname><given-names>ZK</given-names></name>, <name name-style="western"><surname>Pandey</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Stoerger</surname><given-names>V</given-names></name>, <etal/></person-group>
<article-title>Conventional and hyperspectral time-series imaging of maize lines widely used in field trials</article-title>. <source>GigaScience</source>. <year>2018</year>;<volume>7</volume>(<issue>2</issue>):<fpage>1</fpage>&#x02013;<lpage>11</lpage>.</mixed-citation></ref><ref id="bib102"><label>102.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Mochida</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Saisho</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Hirayama</surname><given-names>T</given-names></name></person-group>
<article-title>Crop improvement using life cycle datasets acquired under field conditions</article-title>. <source>Front Plant Sci</source>. <year>2015</year>;<volume>6</volume>:<fpage>740</fpage>.<pub-id pub-id-type="pmid">26442053</pub-id></mixed-citation></ref><ref id="bib103"><label>103.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Qu</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Zheng</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Hamdani</surname><given-names>S</given-names></name><etal/></person-group>
<article-title>Leaf photosynthetic parameters related to biomass accumulation in a global rice diversity survey</article-title>. <source>Plant Physiol</source>. <year>2017</year>;<volume>175</volume>(<issue>1</issue>):<fpage>248</fpage>&#x02013;<lpage>58</lpage>.<pub-id pub-id-type="pmid">28739819</pub-id></mixed-citation></ref><ref id="bib104"><label>104.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Ludovisi</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Tauro</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Salvati</surname><given-names>R</given-names></name>, <etal/></person-group>
<article-title>UAV-based thermal imaging for high-throughput field phenotyping of black poplar response to drought</article-title>. <source>Front Plant Sci</source>. <year>2017</year>;<volume>8</volume>:<fpage>1681</fpage>.<pub-id pub-id-type="pmid">29021803</pub-id></mixed-citation></ref><ref id="bib105"><label>105.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Naik</surname><given-names>HS</given-names></name>, <name name-style="western"><surname>Assefa</surname><given-names>T</given-names></name>, <etal/></person-group>
<article-title>Computer vision and machine learning for robust phenotyping in genome-wide studies</article-title>. <source>Sci Rep</source>. <year>2017</year>;<volume>7</volume>:<fpage>44048</fpage>.<pub-id pub-id-type="pmid">28272456</pub-id></mixed-citation></ref><ref id="bib106"><label>106.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Watanabe</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Guo</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Arai</surname><given-names>K</given-names></name><etal/></person-group>
<article-title>High-throughput phenotyping of sorghum plant height using an unmanned aerial vehicle and its application to genomic prediction modeling</article-title>. <source>Front Plant Sci</source>. <year>2017</year>;<volume>8</volume>:<fpage>421</fpage>.<pub-id pub-id-type="pmid">28400784</pub-id></mixed-citation></ref><ref id="bib107"><label>107.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Giselsson</surname><given-names>TM</given-names></name>, <name name-style="western"><surname>J&#x000f8;rgensen</surname><given-names>RN</given-names></name>, <name name-style="western"><surname>Jensen</surname><given-names>PK</given-names></name><etal/></person-group>
<source>A Public Image Database for Benchmark of Plant Seedling Classification Algorithms</source>. <year>2017</year>
<comment><ext-link ext-link-type="uri" xlink:href="https://vision.eng.au.dk/plant-seedlings-dataset/">https://vision.eng.au.dk/plant-seedlings-dataset/</ext-link></comment>, <comment>Accessed 8 August 2018</comment>.</mixed-citation></ref><ref id="bib108"><label>108.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Minervini</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Fischbach</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Scharr</surname><given-names>H</given-names></name>, <etal/></person-group>
<article-title>Finely-grained annotated datasets for image-based plant phenotyping</article-title>. <source>Pattern Recogn Lett</source>. <year>2016</year>;<volume>81</volume>:<fpage>80</fpage>&#x02013;<lpage>9</lpage>.</mixed-citation></ref><ref id="bib109"><label>109.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Minervini</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Fischbach</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Scharr</surname><given-names>H</given-names></name>, <etal/></person-group>
<source>Plant Phenotyping Datasets</source>. <comment><ext-link ext-link-type="uri" xlink:href="http://www.plant-phenotyping.org/datasets">http://www.plant-phenotyping.org/datasets</ext-link></comment>
<year>2015</year>
<comment>Accessed 14 August 2018</comment>.</mixed-citation></ref><ref id="bib110"><label>110.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Arend</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Lange</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Pape</surname><given-names>JM</given-names></name><etal/></person-group>
<article-title>Quantitative monitoring of <italic>Arabidopsis thaliana</italic> growth and development using high-throughput plant phenotyping</article-title>. <source>Sci Data</source>. <year>2016</year>;<volume>3</volume>:<fpage>160055</fpage>.<pub-id pub-id-type="pmid">27529152</pub-id></mixed-citation></ref><ref id="bib111"><label>111.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Choudhury</surname><given-names>SD</given-names></name>, <name name-style="western"><surname>Bashyam</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Qiu</surname><given-names>Y</given-names></name>, <etal/></person-group>
<article-title>Holistic and component plant phenotyping using temporal image sequence</article-title>. <source>Plant Methods</source>. <year>2018</year>;<volume>14</volume>:<fpage>35</fpage>.<pub-id pub-id-type="pmid">29760766</pub-id></mixed-citation></ref><ref id="bib112"><label>112.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Brichet</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Cabrera-Bosquet</surname><given-names>L</given-names></name></person-group>
<source>Maize whole plant image dataset</source>. Zenodo. <year>2017</year>
<pub-id pub-id-type="doi">10.5281/zenodo.1002675</pub-id>.</mixed-citation></ref><ref id="bib114"><label>113.</label><mixed-citation publication-type="book">
<source>Center DDPS: Public Image Datasets</source>. <comment><ext-link ext-link-type="uri" xlink:href="https://plantcv.danforthcenter.org/pages/data.html">https://plantcv.danforthcenter.org/pages/data.html</ext-link></comment>
<year>2014</year>
<comment>Accessed 5 August 2018</comment>.</mixed-citation></ref><ref id="bib115"><label>114.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Cooper</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Meier</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Laporte</surname><given-names>MA</given-names></name>, <etal/></person-group>
<article-title>The planteome database: an integrated resource for reference ontologies, plant genomics and phenomics</article-title>. <source>Nucleic Acids Res</source>. <year>2018</year>;<volume>46</volume>(<issue>D1</issue>):<fpage>D1168</fpage>&#x02013;<lpage>D80</lpage>.<pub-id pub-id-type="pmid">29186578</pub-id></mixed-citation></ref><ref id="bib116"><label>115.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Lobet</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Draye</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Perilleux</surname><given-names>C</given-names></name></person-group>
<article-title>An online database for plant image analysis software tools</article-title>. <source>Plant Methods</source>. <year>2013</year>;<volume>9</volume>(<issue>1</issue>):<fpage>38</fpage>.<pub-id pub-id-type="pmid">24107223</pub-id></mixed-citation></ref><ref id="bib117"><label>116.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Lobet</surname><given-names>G</given-names></name></person-group>
<article-title>Image analysis in plant sciences: publish then perish</article-title>. <source>Trends Plant Sci</source>. <year>2017</year>;<volume>22</volume>(<issue>7</issue>):<fpage>559</fpage>&#x02013;<lpage>66</lpage>.<pub-id pub-id-type="pmid">28571940</pub-id></mixed-citation></ref><ref id="bib118"><label>117.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Lobet</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Draye</surname><given-names>X</given-names></name>, <name name-style="western"><surname>P&#x000e9;rilleux</surname><given-names>C</given-names></name></person-group>
<source>Plants database</source>. <comment><ext-link ext-link-type="uri" xlink:href="http://www.plant-image-analysis.org/dataset/plant-database">http://www.plant-image-analysis.org/dataset/plant-database</ext-link></comment>
<comment>Accessed 9 August 2018</comment>.</mixed-citation></ref><ref id="bib119"><label>118.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Obo&#x00159;il</surname><given-names>M</given-names></name></person-group>
<source>Quantification of leaf necrosis by biologically inspired algorithms</source>. <comment><ext-link ext-link-type="uri" xlink:href="https://lnc.proteomics.ceitec.cz/non_source_files/LNC_presentation_short.pdf">https://lnc.proteomics.ceitec.cz/non_source_files/LNC_presentation_short.pdf.</ext-link></comment><year>2017</year>
<comment>Accessed 15 August 2018</comment>.</mixed-citation></ref><ref id="bib120"><label>119.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Zheng</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Duan</surname><given-names>T</given-names></name>, <etal/></person-group>
<article-title>EasyPCC: benchmark datasets and tools for high-throughput measurement of the plant canopy coverage ratio under field conditions</article-title>. <source>Sensors (Basel)</source>. <year>2017</year>;<volume>17</volume>(<issue>4</issue>):<fpage>798</fpage>.</mixed-citation></ref><ref id="bib121"><label>120.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Applegate</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Alonso</surname><given-names>AD</given-names></name><etal/></person-group>
<article-title>Leaf-GP: an open and automated software application for measuring growth phenotypes for arabidopsis and wheat</article-title>. <source>Plant Methods</source>. <year>2017</year>;<volume>13</volume>:<fpage>117</fpage>.<pub-id pub-id-type="pmid">29299051</pub-id></mixed-citation></ref><ref id="bib122"><label>121.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Fetter</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Eberhardt</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Barclay</surname><given-names>RS</given-names></name><etal/></person-group>
<article-title>StomataCounter: a deep learning method applied to automatic stomatal identification and counting</article-title>. <source>bioRxiv</source>. <year>2018</year>; doi:<pub-id pub-id-type="doi">10.1101/327494</pub-id>.</mixed-citation></ref><ref id="bib123"><label>122.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><collab>PEAT: plantix</collab></person-group>
<comment><ext-link ext-link-type="uri" xlink:href="https://plantix.net">https://plantix.net</ext-link></comment>
<year>2017</year>
<comment>Accessed 15 August 2018</comment>.</mixed-citation></ref><ref id="bib124"><label>123.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Shannon</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Markiel</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Ozier</surname><given-names>O</given-names></name>, <etal/></person-group>
<article-title>Cytoscape: a software environment for integrated models of biomolecular interaction networks</article-title>. <source>Genome Res</source>. <year>2003</year>;<volume>13</volume>(<issue>11</issue>):<fpage>2498</fpage>&#x02013;<lpage>504</lpage>.<pub-id pub-id-type="pmid">14597658</pub-id></mixed-citation></ref><ref id="bib125"><label>124.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Rage</surname><given-names>UK</given-names></name>, <name name-style="western"><surname>Ninomiya</surname><given-names>S</given-names></name></person-group>
<article-title>Illumination invariant segmentation of vegetation for time series wheat images based on decision tree model</article-title>. <source>Comput Electron Agric</source>. <year>2013</year>;<volume>96</volume>:<fpage>58</fpage>&#x02013;<lpage>66</lpage>.</mixed-citation></ref><ref id="bib126"><label>125.</label><mixed-citation publication-type="book">
<source>Crop-Phenomics-Group: Leaf-GP</source>. <comment><ext-link ext-link-type="uri" xlink:href="http://www.plant-image-analysis.org/software/leaf-gp">http://www.plant-image-analysis.org/software/leaf-gp</ext-link></comment>
<year>2017</year>
<comment>Accessed 11 June 2018</comment>.</mixed-citation></ref><ref id="bib127"><label>126.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Pendergrass</surname><given-names>SA</given-names></name>, <name name-style="western"><surname>Brown-Gentry</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Dudek</surname><given-names>S</given-names></name><etal/></person-group>
<article-title>Phenome-wide association study (PheWAS) for detection of pleiotropy within the population architecture using genomics and epidemiology (PAGE) network</article-title>. <source>Plos Genet</source>. <year>2013</year>;<volume>9</volume>(<issue>1</issue>):<fpage>e1003087</fpage>.<pub-id pub-id-type="pmid">23382687</pub-id></mixed-citation></ref><ref id="bib128"><label>127.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Verma</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Ritchie</surname><given-names>MD</given-names></name></person-group>
<article-title>Current scope and challenges in phenome-wide association studies</article-title>. <source>Curr Epidemiol Rep</source>. <year>2017</year>;<volume>4</volume>(<issue>4</issue>):<fpage>321</fpage>&#x02013;<lpage>9</lpage>.<pub-id pub-id-type="pmid">29545989</pub-id></mixed-citation></ref><ref id="bib129"><label>128.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>de&#x000a0;Villemereuil</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Gaggiotti</surname><given-names>OE</given-names></name>, <name name-style="western"><surname>Mouterde</surname><given-names>M</given-names></name>, <etal/></person-group>
<article-title>Common garden experiments in the genomic era: new perspectives and opportunities</article-title>. <source>Heredity</source>. <year>2016</year>;<volume>116</volume>(<issue>3</issue>):<fpage>249</fpage>&#x02013;<lpage>54</lpage>.<pub-id pub-id-type="pmid">26486610</pub-id></mixed-citation></ref><ref id="bib130"><label>129.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>D</given-names></name></person-group>
<article-title>Application of deep learning in genomic selection</article-title>. In: <source>Bioinformatics and Biomedicine (BIBM)</source>, <publisher-loc>Kansas City, MO, USA</publisher-loc>; <year>2017</year>; doi:<pub-id pub-id-type="doi">10.1109/BIBM.2017.8218025</pub-id>.</mixed-citation></ref><ref id="bib131"><label>130.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>BJ</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>SH</given-names></name></person-group>
<article-title>Prediction of inherited genomic susceptibility to 20 common cancer types by a supervised machine-learning method</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2018</year>;<volume>115</volume>(<issue>6</issue>):<fpage>1322</fpage>&#x02013;<lpage>7</lpage>.<pub-id pub-id-type="pmid">29358382</pub-id></mixed-citation></ref><ref id="bib132"><label>131.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Lippert</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Sabatini</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Maher</surname><given-names>MC</given-names></name>, <etal/></person-group>
<article-title>Identification of individuals by trait prediction using whole-genome sequencing data</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2017</year>;<volume>114</volume>(<issue>38</issue>):<fpage>10166</fpage>&#x02013;<lpage>71</lpage>.<pub-id pub-id-type="pmid">28874526</pub-id></mixed-citation></ref></ref-list></back></article>