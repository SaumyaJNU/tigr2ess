<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName journalpublishing.dtd?><?SourceDTD.Version 2.3?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Plant Sci</journal-id><journal-id journal-id-type="iso-abbrev">Front Plant Sci</journal-id><journal-id journal-id-type="publisher-id">Front. Plant Sci.</journal-id><journal-title-group><journal-title>Frontiers in Plant Science</journal-title></journal-title-group><issn pub-type="epub">1664-462X</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6234915</article-id><article-id pub-id-type="doi">10.3389/fpls.2018.01519</article-id><article-categories><subj-group subj-group-type="heading"><subject>Plant Science</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Automated Alignment of Multi-Modal Plant Images Using Integrative Phase Correlation Approach</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Henke</surname><given-names>Michael</given-names></name><xref ref-type="corresp" rid="c001"><sup>*</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/329659/overview"/></contrib><contrib contrib-type="author"><name><surname>Junker</surname><given-names>Astrid</given-names></name><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/52705/overview"/></contrib><contrib contrib-type="author"><name><surname>Neumann</surname><given-names>Kerstin</given-names></name><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/461113/overview"/></contrib><contrib contrib-type="author"><name><surname>Altmann</surname><given-names>Thomas</given-names></name><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/122873/overview"/></contrib><contrib contrib-type="author"><name><surname>Gladilin</surname><given-names>Evgeny</given-names></name><xref ref-type="corresp" rid="c002"><sup>*</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/174290/overview"/></contrib></contrib-group><aff><institution>Molecular Genetics, Leibniz Institute of Plant Genetics and Crop Plant Research (IPK)</institution>, <addr-line>Gatersleben</addr-line>, <country>Germany</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Stefano Santabarbara, Consiglio Nazionale Delle Ricerche (CNR), Italy</p></fn><fn fn-type="edited-by"><p>Reviewed by: Shigeichi Kumazaki, Kyoto University, Japan; Jin Chen, University of Kentucky, United States; Victor Sanchez, University of Warwick, United Kingdom</p></fn><corresp id="c001">*Correspondence: Michael Henke <email>mhenke@ipk-gatersleben.de</email></corresp><corresp id="c002">Evgeny Gladilin <email>gladilin@ipk-gatersleben.de</email></corresp><fn fn-type="other" id="fn001"><p>This article was submitted to Plant Physiology, a section of the journal Frontiers in Plant Science</p></fn></author-notes><pub-date pub-type="epub"><day>16</day><month>10</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>9</volume><elocation-id>1519</elocation-id><history><date date-type="received"><day>23</day><month>5</month><year>2018</year></date><date date-type="accepted"><day>27</day><month>9</month><year>2018</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2018 Henke, Junker, Neumann, Altmann and Gladilin.</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Henke, Junker, Neumann, Altmann and Gladilin</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>Modern facilities for high-throughput phenotyping provide plant scientists with a large amount of multi-modal image data. Combination of different image modalities is advantageous for image segmentation, quantitative trait derivation, and assessment of a more accurate and extended plant phenotype. However, visible light (VIS), fluorescence (FLU), and near-infrared (NIR) images taken with different cameras from different view points in different spatial resolutions exhibit not only relative geometrical transformations but also considerable structural differences that hamper a straightforward alignment and combined analysis of multi-modal image data. Conventional techniques of image registration are predominantly tailored to detection of relative geometrical transformations between two otherwise identical images, and become less accurate when applied to partially similar optical scenes. Here, we focus on a relatively new technical problem of FLU/VIS plant image registration. We present a framework for automated alignment of FLU/VIS plant images which is based on extension of the phase correlation (PC) approach &#x02212; a frequency domain technique for image alignment, which relies on detection of a phase shift between two Fourier-space transforms. Primarily tailored to detection of affine image transformations between two structurally identical images, PC is known to be sensitive to structural image distortions. We investigate effects of image preprocessing and scaling on accuracy of image registration and suggest an integrative algorithmic scheme which allows to overcome shortcomings of conventional single-step PC by application to non-identical multi-modal images. Our experimental tests with FLU/VIS images of different plant species taken on different phenotyping facilities at different developmental stages, including difficult cases such as small plant shoots of non-specific shape and non-uniformly moving leaves, demonstrate improved performance of our extended PC approach within the scope of high-throughput plant phenotyping.</p></abstract><kwd-group><kwd>high-throughput plant phenotyping</kwd><kwd>automated image analysis</kwd><kwd>multi-modal image registration</kwd><kwd>affine transformations</kwd><kwd>non-uniform motion</kwd><kwd>Fourier-Mellin phase correlation</kwd></kwd-group><funding-group><award-group><funding-source id="cn001">German Federal Ministry of Education and Research</funding-source><award-id rid="cn001">031A053</award-id></award-group></funding-group><counts><fig-count count="6"/><table-count count="1"/><equation-count count="9"/><ref-count count="19"/><page-count count="10"/><word-count count="4545"/></counts></article-meta></front><body><sec id="s1"><title>1. Introduction</title><p>In recent years, plant phenotyping became an indispensable analytical tool in quantitative plant sciences. Modern multi-camera systems such as LemnaTec-Scanalyzer3D (LemnaTec GmbH, Aachen, Germany) enable acquisition of large amount of multi-modal image data, including visible light (VIS), fluorescence (FLU), and near-infrared (NIR) images. To derive reliable quantitative traits of plant morphology, development and functions from large amount of multi-modal image data, efficient algorithmic solutions for detection and quantification of plant structures are required (Minervini et al., <xref rid="B11" ref-type="bibr">2015</xref>).</p><p>Quantitative analysis of plant images begins with image segmentation which aims to identify image regions corresponding to whole plant or particular plant organs. Reliability of phenotypic plant traits essentially depends on accuracy and robustness of image segmentation algorithms. Straightforward segmentation of VIS plant images by means of global thresholding is often hampered by a number of natural and technical reasons including variable plant coloring, inhomogeneous illumination, shadows and reflections in plant and background regions. Differently from VIS, intensity of FLU images strongly correlates with chlorophyll content of plant structures which provides a natural contrast to chlorophyll-free background regions. Higher contrast between intensity of plant and background regions makes fluorescent images to a natural reference for detection and segmentation of plant structures. Once appropriately aligned, the binary mask of segmented FLU images can be applied for segmentation of VIS images. Such a segmentation-via-registration scheme has a considerable advantage of being generic and avoids diverse difficulties by the segmentation of structurally more complex and variable VIS images, see Figure <xref ref-type="fig" rid="F1">1</xref>.</p><fig id="F1" position="float"><label>Figure 1</label><caption><p>Principle scheme of VIS image segmentation by means of VIS/FLU image alignment. Higher plant-background contrast enables a straightforward segmentation of FLU images <bold>(A)</bold> resulting in a binary mask of the plant region <bold>(B)</bold>. Inhomogeneous illumination and visibility of diverse background structures challenge an accurate segmentation of VIS images <bold>(C)</bold>. By applying the binary mask of the registered FLU image, automated segmentation of the VIS image <bold>(D)</bold> is performed.</p></caption><graphic xlink:href="fpls-09-01519-g0001"/></fig><p>Two images of different modalities may, in general, differ by a relative affine transformation (i.e., translation, rotation and scaling), but also structurally. For example, contours of walls, carriers and other light reflecting/absorbing objects in VIS images are typically not present in FLU images, see Figure <xref ref-type="fig" rid="F2">2</xref>. Consequently, alignment of multi-modal images is associated with the problem of finding correspondences between two structurally non-identical images that exhibit only partial similarities.</p><fig id="F2" position="float"><label>Figure 2</label><caption><p>Examples of VIS/FLU images of different plant species acquired from high-throughput plant phenotyping experiments. Due to differences in camera resolution and position, multi-modal images exhibit differences in the relative size, position, and spatial orientation. Blends of images pairs in the lower row show differences of the VIS/FLU resolution.</p></caption><graphic xlink:href="fpls-09-01519-g0002"/></fig><p>A broad spectrum of methods for image registration has been previously developed in context of biomedical and geographic imaging (Zitova and Flusser, <xref rid="B19" ref-type="bibr">2003</xref>; Xiong and Zhang, <xref rid="B18" ref-type="bibr">2010</xref>; Lahat et al., <xref rid="B10" ref-type="bibr">2015</xref>; Brock et al., <xref rid="B3" ref-type="bibr">2017</xref>; Goshtasby, <xref rid="B7" ref-type="bibr">2017</xref>). To establish correspondences between two images, manually or automatically generated landmarks (spatial feature-points), intensity information or frequency-domain features were used. The frequency-space based techniques such as Fourier-Mellin phase correlation (PC) rely on the Fourier-shift theorem, which enables detection of a spatial shift in Cartesian or polar systems of coordinates from the phase-shift of their Fourier transforms (Kuglin and Hines, <xref rid="B9" ref-type="bibr">1975</xref>; Reddy and Chatterji, <xref rid="B13" ref-type="bibr">1996</xref>; Wolberg and Zokai, <xref rid="B17" ref-type="bibr">2000</xref>). From previous works (Stone et al., <xref rid="B14" ref-type="bibr">2001</xref>; Foroosh et al., <xref rid="B5" ref-type="bibr">2002</xref>; Argyriou and Vlachos, <xref rid="B2" ref-type="bibr">2006</xref>), it is known that PC is surprisingly robust with respect to noise, but becomes less accurate in presence of multiple structurally similar patterns or considerable structural distortions such as non-rigid image transformations (e.g., deformation, non-uniform motion, etc.). Requirements of additional pre-processing steps by applying PC for registration of non-identical and multi-modal images were reported in Wisetphanichkij and Dejhan (<xref rid="B16" ref-type="bibr">2005</xref>), Wang et al. (<xref rid="B15" ref-type="bibr">2013</xref>), Gladilin and Eils (<xref rid="B6" ref-type="bibr">2015</xref>), and Almonacid-Caballer et al. (<xref rid="B1" ref-type="bibr">2017</xref>).</p><p>Applications of image registration techniques in context of plant image analysis are still relatively scarce (De Vylder et al., <xref rid="B4" ref-type="bibr">2012</xref>; Raza et al., <xref rid="B12" ref-type="bibr">2015</xref>). Structural differences between multi-modal plant images and presence of non-uniform image motion due to uncorrelated movements of leaves make alignment of multi-modal plant images a challenging task. Here, we are concerned with investigation of diverse facets of multi-modal plant image alignment and suggest extensions to the conventional single-step PC approach for improved robustness and accuracy of FLU/VIS image registration.</p></sec><sec id="s2"><title>2. Methods</title><sec><title>2.1. Image acquisition</title><p>Time-series of VIS and FLU top-/side-view images of developing maize, wheat and arabidopsis shoots were acquired from high-throughput experiments performed over more than 2 weeks using LemnaTec-Scanalyzer3D high-throughput phenotypic platforms (LemnaTec GmbH, Aachen, Germany). In the highest expansion stage, the LemnaTec Scanalyzer3D consists of three measuring boxes, each equipped with one (or more) different sensor system. Following a measuring plan, plants are moved automatically from the greenhouse to the measuring facility where they are successively transported from one measuring box (e.g., VIS) to the next one (e.g., FLU). Corresponding VIS and FLU images are therefore taken within few seconds one after another, which are required to move the plants from the VIS to the FLU measuring box, respectively. Table <xref rid="T1" ref-type="table">1</xref> summarizes image data modalities and formats used in this study.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p>An overview of image data used in this study including three different experiments of three different species, each taken in visible light and fluorescence, obtained by three different LemnaTec high-throughput phenotyping facilities for large, intermediate size, and small plants at the IPK Gatersleben.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1"><bold> Species, views</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold># Plants</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold># Days</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold># Angles</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold># VIS/FLU pairs</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>VIS size</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>FLU size</bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Arabidopsis, top</td><td valign="top" align="center" rowspan="1" colspan="1">4</td><td valign="top" align="center" rowspan="1" colspan="1">20</td><td valign="top" align="center" rowspan="1" colspan="1">1</td><td valign="top" align="center" rowspan="1" colspan="1">80</td><td valign="top" align="center" rowspan="1" colspan="1">2,056 &#x000d7; 2,454</td><td valign="top" align="center" rowspan="1" colspan="1">1,234 &#x000d7; 1,624</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Wheat, side</td><td valign="top" align="center" rowspan="1" colspan="1">4</td><td valign="top" align="center" rowspan="1" colspan="1">47</td><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">564</td><td valign="top" align="center" rowspan="1" colspan="1">1,234 &#x000d7; 1,624</td><td valign="top" align="center" rowspan="1" colspan="1">1,234 &#x000d7; 1,624</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Maize, side</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="center" rowspan="1" colspan="1">22</td><td valign="top" align="center" rowspan="1" colspan="1">4</td><td valign="top" align="center" rowspan="1" colspan="1">526</td><td valign="top" align="center" rowspan="1" colspan="1">2,056 &#x000d7; 2,454</td><td valign="top" align="center" rowspan="1" colspan="1">1,038 &#x000d7; 1,390</td></tr></tbody></table></table-wrap></sec><sec><title>2.2. Image preprocessing</title><p>To increase the robustness of PC calculation, FLU images are uniformly pre-scaled to the height of VIS images prior to affine PC registration. In order to assess effects of structural differences between VIS and FLU images on accuracy and robustness of PC registration, evaluation tests were carried out with original as well as manually segmented images. Manual segmentation was performed using variable cut-off thresholds for different background regions followed by a subsequent manual removal of remaining structural artifacts. Since PC is known to rely on edge information, edge images were generated using color-edge algorithm (Henriques, <xref rid="B8" ref-type="bibr">2010</xref>) and used in addition to grayscale images for finding global affine transformations. Furthermore, image scaling and cropping was introduced to investigate effects of absolute and relative image size on accuracy and robustness of PC registration. In cropped images, the crop-mask was defined by the dimension of the bounding box of all manually segmented plant structures for a particular day of experiment, i.e., the developmental stage of the plant. No further preprocessing steps were applied with exception of Arabidopsis images, where blue-dominant pixels were removed to eliminate the blue mat used for improvement of contrast in top view images of small plants.</p></sec><sec><title>2.3. Affine image alignment using fourier-mellin phase correlation</title><p>Phase correlation between each two images is computed as Fourier inverse of the normalized cross-power spectrum (<italic>CPS</italic>):</p><disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="-tex-caligraphic">F</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><p>where</p><disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mtable style="text-align:axis;" equalrows="false" columnlines="" equalcolumns="false" class="array"><mml:mtr><mml:mtd><mml:mi>&#x003b1;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="-tex-caligraphic">F</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>&#x003b2;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="-tex-caligraphic">F</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>are the complex Fourier transforms of the images <italic>A</italic> and <italic>B</italic> and</p><disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>C</mml:mi><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:msup><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:msup><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>is the so-called cross-power spectrum (CPS). According to the Fourier shift theorem, relative displacement (&#x00394;<italic>x</italic>, &#x00394;<italic>y</italic>) in the Cartesian system or coordinates (or, alternatively, scaling and rotation in the polar system of coordinates) between two otherwise structurally identical images, i.e.,</p><disp-formula id="E4"><label>(4)</label><mml:math id="M4"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mo>&#x00394;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mo>&#x00394;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>leads to phase-shift in the frequency domain</p><disp-formula id="E5"><label>(5)</label><mml:math id="M5"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold"><mml:mtext>e</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mi>i</mml:mi><mml:mi>&#x003c6;</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>where <inline-formula><mml:math id="M6"><mml:mi>&#x003c6;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>u</mml:mi><mml:mo>&#x00394;</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#x00394;</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and is <italic>N</italic>&#x000d7;<italic>M</italic> are the image dimensions. As a consequence, the cross power spectrum between two identical images with a relative shift in the Cartesian system of coordinates (or scaled/rotated in the polar system of coordinates) describes the phase-shifts of the Fourier transform in the frequency domain:</p><disp-formula id="E6"><label>(6)</label><mml:math id="M7"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>C</mml:mi><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mstyle mathvariant="bold"><mml:mtext>e</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mi>i</mml:mi><mml:mi>&#x003c6;</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mstyle mathvariant="bold"><mml:mtext>e</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mi>i</mml:mi><mml:mi>&#x003c6;</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold"><mml:mtext>e</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mi>i</mml:mi><mml:mi>&#x003c6;</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>For two identical images with the relative spatial displacement (&#x00394;<italic>x</italic>, &#x00394;<italic>y</italic>), the inverse Fourier integral of (6) represents a <italic>N</italic>&#x000d7;<italic>M</italic> map exhibiting a single singularity at the point (<italic>x</italic> = &#x00394;<italic>x, y</italic> = &#x00394;<italic>y</italic>)</p><disp-formula id="E7"><label>(7)</label><mml:math id="M8"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003b4;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mo>&#x00394;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mo>&#x00394;</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>This means that the maximum peak of phase correlation between two identical images yields the relative image translation in the Cartesian system of coordinates, or their relative scaling and rotation in polar coordinates, see examples in Figure <xref ref-type="fig" rid="F3">3</xref>.</p><fig id="F3" position="float"><label>Figure 3</label><caption><p>Examples of phase correlation between FLU and VIS images of young and small <bold>(A)</bold> vs. older/larger <bold>(B)</bold> maize shoots. For calculation of relative geometrical transformations, alternative PC registrations were performed using 2D grayscale and edge images that were derived from original FLU and VIS images, respectively. <bold>(A)</bold> Phase correlation between VIS/FLU images of small shoots exhibit a high level of noise with multiple maxima of nearly same height. <bold>(B)</bold> Correlation between large and unique patters in images of older plants lead to a single maximum peak corresponding to the relative image transformation.</p></caption><graphic xlink:href="fpls-09-01519-g0003"/></fig><p>Calculation of affine image transformations from Fourier-Mellin phase correlation was performed using a modified version of the MATLAB <italic>imregcorr</italic> routine which in addition to the affine transformation matrix returns the height of the maximum PC peak. For assessment of reliability of image transformation, a fixed threshold of <italic>H</italic> &#x0003e; 0.03 was used as suggested in Reddy and Chatterji (<xref rid="B13" ref-type="bibr">1996</xref>). Transformations obtained with <italic>H</italic> &#x0003c; 0.03 typically indicate a failure of PC registration, for example, due to low and missing structural similarities between two images.</p></sec><sec><title>2.4. Evaluation of image registration</title><p>To evaluate the results of image registration two criterions for characterization of algorithmic robustness and accuracy are introduced.</p><sec><title>2.4.1. Success rate of image registration</title><p>The success rate (SR) of image registration is calculated as the ratio between the number of successfully performed image registrations (<italic>n</italic><sub><italic>s</italic></sub>) divided by the total number of registered image pairs (<italic>n</italic>):</p><disp-formula id="E8"><label>(8)</label><mml:math id="M9"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>S</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>Thereby, the criterion of successful image alignment was defined by the minimum admissible height of the maximum PC peak (<italic>H</italic> &#x0003e; 0.03) as suggested by Reddy and Chatterji (<xref rid="B13" ref-type="bibr">1996</xref>) as well as reasonable bounds of image translation, rotation and scaling. Geometrical transformations that do not match these criterions were treated as failure of PC registration.</p></sec><sec><title>2.4.2. Overlap ration of registered image regions</title><p>The second criterion is constructed to quantify the overlap ratio (OR) between the area of plant regions in VIS images that are covered by the registered FLU image (<italic>a</italic><sub><italic>r</italic></sub>) and the total area of manually segmented plant regions (<italic>a</italic>):</p><disp-formula id="E9"><label>(9)</label><mml:math id="M10"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>O</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>While SR serves as a criterion indicating that PC routine succeed in producing some reasonable transformation, OR describes the accuracy of successful transformations.</p></sec></sec></sec><sec id="s3"><title>3. Results</title><sec><title>3.1. Single-step PC registration of full-size images</title><p>First, PC registration of original, full-size FLU and VIS images of maize, wheat and arabidopsis shoots was performed using the conventional single-step PC approach. Thereby, eight different preprocessing variants including</p><list list-type="bullet"><list-item><p>Gray-scale version of unprocessed full-size VIS/FLU images</p></list-item><list-item><p>Color-edges version of unprocessed full-size VIS/FLU images</p></list-item><list-item><p>Gray-scale version of unprocessed full-size and adaptively cropped VIS/FLU images</p></list-item><list-item><p>Color-edges version of unprocessed full-size and adaptively cropped VIS/FLU images</p></list-item><list-item><p>Gray-scale version of manually segmented full-size VIS/FLU images</p></list-item><list-item><p>Color-edges version of manually segmented full-size VIS/FLU images</p></list-item><list-item><p>Gray-scale version of manually segmented full-size and adaptively cropped VIS/FLU images</p></list-item><list-item><p>Color-edges version of manually segmented full-size and adaptively cropped VIS/FLU images</p></list-item></list><p>were compared. To assess the performance of PC registration for different preprocessing conditions, cumulative statistics of successful image alignment was calculated for all days of each experiment. As one can see from Figure <xref ref-type="fig" rid="F4">4A</xref>, manual segmentation significantly improves the success rate of PC registration. Surprisingly, cropping of plant regions does not always improve and sometimes even worsens the PC performance. This rather unexpected result could be traced back to higher probability of misalignment of partially similar plant structures with the larger relative size in relationship to the size of (cropped) image. This was, in particular, observed in juvenile arabidopsis plants with only a few similar leaves. The relationship between the size of plant structures and the image size has, in turn, an impact on their spectral representation, i.e., different weights of lower and higher frequencies, which, in the case of partially similar, blurry and/or repetitive pattern can lead to maximization of PC peak related to locally optimal alignment. An example of such a case is shown in Figures <xref ref-type="fig" rid="F4">4B&#x02013;F</xref>. We found that image downscaling can help to avoid such misalignments and to enhance the PC peak corresponding to globally optimal image registration, cf. Figure <xref ref-type="fig" rid="F4">4E</xref> vs. Figure <xref ref-type="fig" rid="F4">4F</xref>.</p><fig id="F4" position="float"><label>Figure 4</label><caption><p>Statistics of single-step PC registration of original and preprocessed plant images. <bold>(A)</bold> Pie charts show differences in the relative success rate of PC registration (Equation 8) between different preprocessing conditions including full-size vs. cropped as well as original vs. manually segmented FLU and VIS images. Enhancement of image similarity by means of manual background elimination leads to substantial improvement of PC registration rate. However, cropping of target regions turned out to be not advantageous. Example of a pair of VIS/FLU images <bold>(B,C)</bold> demonstrates that PC of a young Arabidopsis shoot with very similar leaf exhibits multiple peaks <bold>(D)</bold>. As a consequence of noisy PC the maximum peak may not correspond to the optimal image alignment <bold>(E)</bold>. Downscaling effectively performs image smoothing which improves phase correlation. For the scale factor 0.32, an optimal image alignment was found <bold>(F)</bold>. <bold>(G)</bold> Plots of success rates and overlap ratios (Equation 9) of PC registration for original (ORIG), hand segmented (HAND full) and cropped (HAND cropped) grayscale (GS) and color-edge (CE) images as a function of scaling ratio [0.1, 1.0]. As one can see, downscaling has strong impact on accuracy of PC registration, however, the optimal scaling factor should not be too low and the optimum lays in the range between [0.3, 0.6]. The overlap ratio lower than 1, especially, for wheat and maize shoots means that single-step PC registration results in an alignment which does not produce a complete coverage of manually segmented plant region in the VIS image by the registered FLU mask.</p></caption><graphic xlink:href="fpls-09-01519-g0004"/></fig></sec><sec><title>3.2. Effects of downscaling on robustness of PC image alignment</title><p>In order to systematically analyzed the effects of image downscaling on robustness of PC registration, tests with downscaled images in the range of scaling factors between [0.1, 1.0] and the step-size 0.02 were performed. Plots in Figure <xref ref-type="fig" rid="F4">4G</xref> show the success rate (Equation 8) and the overlap ratio (Equation 9) as a function of scale factor. As one can see, downscaling improves both accuracy and robustness of PC registration. However, the robust algorithmic performance is achieved in the range of intermediate scaling factors [0.3, 0.6] that probably correspond to the optimal degree of image smoothing. Detailed analysis of geometrical transformations calculated for differently scaled images reveals that they correspond to optimal registration of some but not all leaves. Consequently, all components of the affine transformation matrix that stand for the relative image scaling, translation, and rotation undergo variations, see Figure <xref ref-type="fig" rid="F5">5A</xref>. This sort of locally-optimal alignment is particularly evident for plants exhibiting a non-uniform motion, for example, due to uncorrelated leaf movements that occur, for example, shortly after abrupt stop of carriers, e.g., after movements or rotations.</p><fig id="F5" position="float"><label>Figure 5</label><caption><p>Ambiguity of affine image alignment due to non-uniform leaf motion. In addition to global geometrical transformations, such as image scaling, translations and rotations, plants exhibit uncorrelated motion of leaves which can not be compensated by a single affine alignment. <bold>(A)</bold> Diagonal (<italic>T</italic><sub>11</sub>), off-diagonal (<italic>T</italic><sub>12</sub>), and translational components (<italic>T</italic><sub>31</sub>, <italic>T</italic><sub>32</sub>) of the affine transformation matrix determined for different scale factors undergo considerable variation which reflects ambiguity of affine alignment of structurally non-identical images. <bold>(B)</bold> Single-step PC registrations of differently scaled images (S<sub><italic>i</italic></sub>) may lead to partial overlaps of different parts of the plant. In order to improve accuracy of PC registration of non-uniformly moving plant structures, the results of multiple registrations are integrated into a single integrated mask (IM) which provides significantly better coverage of the entire plant shoot in FLU and VIS images.</p></caption><graphic xlink:href="fpls-09-01519-g0005"/></fig></sec><sec><title>3.3. Integration of multiple PC registrations into a single mask</title><p>Since downscaling of images with different scaling factors results in slightly different geometrical transformations that tend to be locally- but not globally-optimal, integration of a series of PC registrations into a single registration mask was introduced. Figure <xref ref-type="fig" rid="F5">5B</xref> shows examples of single-step locally-optimal image alignments followed by their integration into a single integrated mask (see the right raw). Using the iterative PC strategy, the overlap ratio of 100% between integrated FLU mask and VIS regions was achieved for all images of three different experiments with arabidopsis, wheat, and maize shoots.</p></sec><sec><title>3.4. Dependency of PC performance on plant growth</title><p>As the accuracy of PC registration is essentially dependent on unique spectral characteristics of target plant structures, a reduced PC performance was observed for young plant shoots exhibiting redundant shapes (e.g., thin vertical lines, blobs, etc.). Similar to the problem of multiple similar leaves, non-specific shape of plant shoots causes ambiguity and inaccuracy of the PC image alignment. Figure <xref ref-type="fig" rid="F6">6A</xref> summarizes success rates of the PC registration calculated for three age/growth phases of arabidopsis, wheat and maize phenotyping experiments including young, intermediate stage and adult plant shoots. As one can see, success rate of the PC registration of wheat and maize shoots gradually improves with the plant age (i.e., phase of experiment). Figure <xref ref-type="fig" rid="F6">6B</xref> gives examples of successful and failed image registration of young and adult maize shoots. From certain views (here, for example, the rotation degree 45&#x000b0;), young maize shoots exhibit a non-specific shape (&#x0201c;thin vertical line&#x0201d;) similar to some non-plant structures (e.g., boundaries of carriers, background markers, etc.). Obviously, it is the combination of several factors (i.e., optical plant appearance (shape/size) at certain developmental stages from certain views, and the presence of non-plant background structures) which causes dependency of the PC performance on plant age/growth in our setup.</p><fig id="F6" position="float"><label>Figure 6</label><caption><p>Dependency of the PC registration on plant growth. <bold>(A)</bold> Median success rates of single-step PC registration of FLU/VIS images for three plant age/growth phases of Arabidopsis (top-view), wheat and maize (side-view) phenotyping experiments including young (1. phase), intermediate stage (2. phase) and adult shoots (3. phase). Reduced success rate of PC registration is caused by a more frequent occurrence of non-specific shape of young wheat and maize shoots in side-view from certain rotation angles. <bold>(B)</bold> Examples of FLU/VIS images of young and adult maize shoots taken from different rotation angles. The redundant shape (&#x0201c;thin vertical line&#x0201d;) of the young maize shoot in the FLU/VIS image pair taken from the rotation degree 45&#x000b0; does not exhibit unique spectral characteristics causing a failure of the PC registration.</p></caption><graphic xlink:href="fpls-09-01519-g0006"/></fig></sec></sec><sec id="s4"><title>4. Conclusion</title><p>Here, we approached the problem of multi-modal plant image registration using the Fourier-Mellin phase correlation technique. We began this explorative study with assumption that FLU/VIS image registration can be performed using a global affine image transformation. Our investigations showed, however, that structural differences and non-uniform image motion between FLU/VIS plant images require substantial extensions of the conventional single-step PC approach. Our experimental tests with large amount of different plant images confirm previous observations that PC registration of multi-modal non-identical images is sensitive to structural noise and ambiguous image content which can be caused by repetitive self-similar plant structures, combination of young shoots with non-specific shape and background structures, image blurring or non-uniform motion due to frequently observed inertial leaf movements. Some of these problems can be avoided by optimization of the optical scene and the measurement protocol. For example, homogenization and elimination of complexity of background regions as well as longer relaxation times after relocation of plants from VIS to FLU chambers will certainly be helpful. We demonstrate that the accuracy of PC registration can be improved when PC is applied to appropriately preprocessed and downscaled images that exhibit higher degree of structural similarity such as color-edge and background-filtered images. In contrast, cropping of target regions may be counterproductive as it enhances spectral differences between non-identical images and makes phase correlation rather noisy. Strictly speaking, non-uniform image motion represents a non-rigid image transformation which goes beyond the scope of applicability of the affine PC-based registration. To overcome this limitation, we introduced an extension to the conventional single-step PC which is based on integration of a series of locally-optimal PC registrations resulting from alignment of differently scaled images. Suggested iterative scheme for calculation of an integrated registration mask turned out to provide a significantly better overlap between registered FLU and VIS images in the case of non-uniform leaf motion. The disadvantage of the present algorithmic implementation consists in computationally inefficient search for different locally-optimal image transformations in the scale space. Alternative algorithmic approaches are required for a more efficient detection of the relevant peaks of a noisy phase correlation. In summary, our extended PC scheme represents a promising approach to fully automated alignment and segmentation of optically complex and heterogeneous multi-modal plant images suitable for application within the scope of high-throughput plant image analysis and phenotyping.</p></sec><sec id="s5"><title>Author contributions</title><p>MH and EG conceived, designed and performed the computational experiments, analyzed the data, wrote the paper, prepared figures and tables, and reviewed drafts of the paper. AJ and KN executed the laboratory experiments, acquired image data, co-wrote the paper, and reviewed drafts of the paper. TA co-conceptualized the project, and reviewed drafts of the paper.</p><sec><title>Conflict of interest statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></sec></body><back><fn-group><fn fn-type="financial-disclosure"><p><bold>Funding.</bold> This work was performed within the German Plant-Phenotyping Network (DPPN) which is funded by the German Federal Ministry of Education and Research (BMBF) (project identification number: 031A053).</p></fn></fn-group><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Almonacid-Caballer</surname><given-names>J.</given-names></name><name><surname>Pardo-Pascual</surname><given-names>J. E.</given-names></name><name><surname>Riuz</surname><given-names>L. A.</given-names></name></person-group> (<year>2017</year>). <article-title>Evaluating fourier cross-correlation sub-pixel registration in landsat images</article-title>. <source>Remote Sens.</source>
<volume>9</volume>:<fpage>1051</fpage>
<pub-id pub-id-type="doi">10.3390/rs9101051</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Argyriou</surname><given-names>V.</given-names></name><name><surname>Vlachos</surname><given-names>T.</given-names></name></person-group> (<year>2006</year>). <article-title>A study of sub-pixel motion estimation using phase correlation</article-title>, in <source>Proc. of British Machine Vision Conference</source> (<publisher-loc>Edinburgh, UK</publisher-loc>), <fpage>387</fpage>&#x02013;<lpage>396</lpage>.</mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brock</surname><given-names>K. K.</given-names></name><name><surname>Mutic</surname><given-names>S.</given-names></name><name><surname>McNutt</surname><given-names>T. R.</given-names></name><name><surname>Li</surname><given-names>H.</given-names></name><name><surname>Kessler</surname><given-names>M. L.</given-names></name></person-group> (<year>2017</year>). <article-title>Use of image registration and fusion algorithms and techniques in radiotherapy: report of the AAPM Radiation Therapy Committee Task Group No. 132</article-title>. <source>Med. Phys.</source>
<volume>44</volume>, <fpage>e43</fpage>&#x02013;<lpage>e76</lpage>. <pub-id pub-id-type="doi">10.1002/mp.12256</pub-id><?supplied-pmid 28376237?><pub-id pub-id-type="pmid">28376237</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>De Vylder</surname><given-names>J.</given-names></name><name><surname>Douterloigne</surname><given-names>K.</given-names></name><name><surname>Prince</surname><given-names>G.</given-names></name><name><surname>Van Der Straeten</surname><given-names>D.</given-names></name><name><surname>Philips</surname><given-names>W.</given-names></name></person-group> (<year>2012</year>). <article-title>A non-rigid registration method for multispectral imaging of plants</article-title>, in <source>Proc. of SPIE Sensing for Agriculture and Food Quality and Safety IV</source>, <volume>Vol. 8369</volume> (<publisher-loc>Baltimore, MD</publisher-loc>), <fpage>6</fpage>.</mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foroosh</surname><given-names>H.</given-names></name><name><surname>Zerubia</surname><given-names>J. B.</given-names></name><name><surname>Berthod</surname><given-names>M.</given-names></name></person-group> (<year>2002</year>). <article-title>Extension of phase correlation to subpixel registration</article-title>. <source>IEEE Trans. Image Process.</source>
<volume>11</volume>, <fpage>188</fpage>&#x02013;<lpage>200</lpage>. <pub-id pub-id-type="doi">10.1109/83.988953</pub-id><?supplied-pmid 18244623?><pub-id pub-id-type="pmid">18244623</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gladilin</surname><given-names>E.</given-names></name><name><surname>Eils</surname><given-names>R.</given-names></name></person-group> (<year>2015</year>). <article-title>On the role of spatial phase and phase correlation in vision, illusion, and cognition</article-title>. <source>Front. Comput. Neurosci.</source>
<volume>9</volume>:<fpage>45</fpage>. <pub-id pub-id-type="doi">10.3389/fncom.2015.00045</pub-id><?supplied-pmid 25954190?><pub-id pub-id-type="pmid">25954190</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goshtasby</surname><given-names>A. A.</given-names></name></person-group> (<year>2017</year>). <source>Theory and Applications of Image Registration</source>. <publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>John Wiley &#x00026; Sons</publisher-name>.</mixed-citation></ref><ref id="B8"><mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Henriques</surname><given-names>J. F.</given-names></name></person-group> (<year>2010</year>). <source>COLOREDGES: Edges of a Color Image by the Max Gradient Method</source>. Available online at: <ext-link ext-link-type="uri" xlink:href="https://de.mathworks.com/matlabcentral/fileexchange/28114">https://de.mathworks.com/matlabcentral/fileexchange/28114</ext-link></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kuglin</surname><given-names>C. D.</given-names></name><name><surname>Hines</surname><given-names>D. C.</given-names></name></person-group> (<year>1975</year>). <article-title>The phase correlation image alignment method</article-title>, in <source>Proc. of Intl. Conf. on Cybernetics and Society</source>, <volume>Vol. 1</volume> (<publisher-loc>New York, NY</publisher-loc>), <fpage>163</fpage>&#x02013;<lpage>165</lpage>.</mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lahat</surname><given-names>D.</given-names></name><name><surname>Adali</surname><given-names>T.</given-names></name><name><surname>Jutten</surname><given-names>C.</given-names></name></person-group> (<year>2015</year>). <article-title>Multimodal data fusion: an overview of methods</article-title>. <source>Proc. IEEE</source>
<volume>103</volume>, <fpage>1449</fpage>&#x02013;<lpage>1477</lpage>. <pub-id pub-id-type="doi">10.1109/JPROC.2015.2460697</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Minervini</surname><given-names>M.</given-names></name><name><surname>Scharr</surname><given-names>H.</given-names></name><name><surname>Tsaftaris</surname><given-names>S. A.</given-names></name></person-group> (<year>2015</year>). <article-title>Image analysis: the new bottleneck in plant phenotyping</article-title>. <source>IEEE Signal Process. Mag.</source>
<volume>32</volume>, <fpage>126</fpage>&#x02013;<lpage>131</lpage>. <pub-id pub-id-type="doi">10.1109/MSP.2015.2405111</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raza</surname><given-names>S.</given-names></name><name><surname>Sanchez</surname><given-names>V.</given-names></name><name><surname>Prince</surname><given-names>G.</given-names></name><name><surname>Clarkson</surname><given-names>J. P.</given-names></name><name><surname>Rajpoot</surname><given-names>N. M.</given-names></name></person-group> (<year>2015</year>). <article-title>Registration of thermal and visible light images of diseased plants using silhouette extraction in the wavelet domain</article-title>. <source>Pat. Recogn.</source>
<volume>48</volume>, <fpage>2119</fpage>&#x02013;<lpage>2128</lpage>. <pub-id pub-id-type="doi">10.1016/j.patcog.2015.01.027</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reddy</surname><given-names>B. S.</given-names></name><name><surname>Chatterji</surname><given-names>B. N.</given-names></name></person-group> (<year>1996</year>). <article-title>An FFT-based technique for translation, rotation, and scale-invariant image registration</article-title>. <source>IEEE Trans. Image Process.</source>
<volume>5</volume>, <fpage>1266</fpage>&#x02013;<lpage>1271</lpage>. <?supplied-pmid 18285214?><pub-id pub-id-type="pmid">18285214</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stone</surname><given-names>H. S.</given-names></name><name><surname>Orchard</surname><given-names>M. T.</given-names></name><name><surname>Chang</surname><given-names>E. C.</given-names></name><name><surname>Martucci</surname><given-names>S. A.</given-names></name></person-group> (<year>2001</year>). <article-title>A fast direct Fourier-based algorithm for subpixel registration of images</article-title>. <source>IEEE Trans. Geosci. Remote Sens.</source>, <volume>39</volume>, <fpage>2235</fpage>&#x02013;<lpage>2243</lpage>. <pub-id pub-id-type="doi">10.1109/36.957286</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Xu</surname><given-names>Z.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name></person-group> (<year>2013</year>). <article-title>Image registration with hyperspectral data based on Fourier-Mellin transform</article-title>. <source>Int. J. Signal. Process. Syst.</source>
<volume>1</volume>, <fpage>107</fpage>&#x02013;<lpage>110</lpage>. <pub-id pub-id-type="doi">10.12720/ijsps.1.1.107-110</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wisetphanichkij</surname><given-names>S.</given-names></name><name><surname>Dejhan</surname><given-names>K.</given-names></name></person-group> (<year>2005</year>). <article-title>Fast Fourier transform technique and affine transform estimation-based high precision image registration method</article-title>. <source>GESTS Intl. Trans. Comp. Sci. Eng.</source>
<volume>20</volume>:<fpage>179</fpage>.</mixed-citation></ref><ref id="B17"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wolberg</surname><given-names>G.</given-names></name><name><surname>Zokai</surname><given-names>S.</given-names></name></person-group> (<year>2000</year>). <article-title>Robust image registration using Log-Polar transform</article-title>, in <source>Proc. of IEEE Intl. Conf. on Image Process.</source>, <volume>Vol. 1</volume> (<publisher-loc>Vancouver, BC</publisher-loc>), <fpage>493</fpage>&#x02013;<lpage>496</lpage>.</mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiong</surname><given-names>Z.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name></person-group> (<year>2010</year>). <article-title>A critical review of image registration methods</article-title>. <source>Intl. J. Image Data Fusion</source>
<volume>1</volume>, <fpage>137</fpage>&#x02013;<lpage>158</lpage>. <pub-id pub-id-type="doi">10.1080/19479831003802790</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zitova</surname><given-names>B.</given-names></name><name><surname>Flusser</surname><given-names>J.</given-names></name></person-group> (<year>2003</year>). <article-title>Image registration methods: a survey</article-title>. <source>Image Vis. Comput.</source>
<volume>21</volume>, <fpage>977</fpage>&#x02013;<lpage>1000</lpage>. <pub-id pub-id-type="doi">10.1016/S0262-8856(03)00137-9</pub-id></mixed-citation></ref></ref-list></back></article>