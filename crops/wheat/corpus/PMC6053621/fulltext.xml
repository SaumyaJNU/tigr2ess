<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName journalpublishing.dtd?><?SourceDTD.Version 2.3?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Plant Sci</journal-id><journal-id journal-id-type="iso-abbrev">Front Plant Sci</journal-id><journal-id journal-id-type="publisher-id">Front. Plant Sci.</journal-id><journal-title-group><journal-title>Frontiers in Plant Science</journal-title></journal-title-group><issn pub-type="epub">1664-462X</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6053621</article-id><article-id pub-id-type="doi">10.3389/fpls.2018.01024</article-id><article-categories><subj-group subj-group-type="heading"><subject>Plant Science</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Wheat Ears Counting in Field Conditions Based on Multi-Feature Optimization and TWSVM</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhou</surname><given-names>Chengquan</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="author-notes" rid="fn002"><sup>&#x02020;</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/486717/overview"/></contrib><contrib contrib-type="author"><name><surname>Liang</surname><given-names>Dong</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="author-notes" rid="fn002"><sup>&#x02020;</sup></xref></contrib><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Xiaodong</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Hao</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/553301/overview"/></contrib><contrib contrib-type="author"><name><surname>Yue</surname><given-names>Jibo</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/536842/overview"/></contrib><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Guijun</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="corresp" rid="c001"><sup>*</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/460218/overview"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>School of Electronics and Information Engineering, Anhui University</institution>, <addr-line>Hefei</addr-line>, <country>China</country></aff><aff id="aff2"><sup>2</sup><institution>Key Laboratory of Quantitative Remote Sensing in Agriculture of Ministry of Agriculture P. R. China, Beijing Research Center for Information Technology in Agriculture</institution>, <addr-line>Beijing</addr-line>, <country>China</country></aff><aff id="aff3"><sup>3</sup><institution>National Engineering Research Center for Information Technology in Agriculture</institution>, <addr-line>Beijing</addr-line>, <country>China</country></aff><aff id="aff4"><sup>4</sup><institution>International Institute for Earth System Science, Nanjing University</institution>, <addr-line>Nanjing</addr-line>, <country>China</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Roger Deal, Emory University, United States</p></fn><fn fn-type="edited-by"><p>Reviewed by: Bo Li, NIAB EMR, United Kingdom; Marco Seeland, Technische Universit&#x000e4;t Ilmenau, Germany</p></fn><corresp id="c001">*Correspondence: Guijun Yang <email>guijun.yang@163.com</email></corresp><fn fn-type="other" id="fn001"><p>This article was submitted to Technical Advances in Plant Science, a section of the journal Frontiers in Plant Science</p></fn><fn fn-type="other" id="fn002"><p>&#x02020;These authors have contributed equally to this work and are co-first authors.</p></fn></author-notes><pub-date pub-type="epub"><day>13</day><month>7</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>9</volume><elocation-id>1024</elocation-id><history><date date-type="received"><day>10</day><month>4</month><year>2018</year></date><date date-type="accepted"><day>25</day><month>6</month><year>2018</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2018 Zhou, Liang, Yang, Yang, Yue and Yang.</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Zhou, Liang, Yang, Yang, Yue and Yang</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>The number of wheat ears in the field is very important data for predicting crop growth and estimating crop yield and as such is receiving ever-increasing research attention. To obtain such data, we propose a novel algorithm that uses computer vision to accurately recognize wheat ears in a digital image. First, red-green-blue images acquired by a manned ground vehicle are selected based on light intensity to ensure that this method is robust with respect to light intensity. Next, the selected images are cut to ensure that the target can be identified in the remaining parts. The simple linear iterative clustering method, which is based on superpixel theory, is then used to generate a patch from the selected images. After manually labeling each patch, they are divided into two categories: wheat ears and background. The color feature &#x0201c;Color Coherence Vectors,&#x0201d; the texture feature &#x0201c;Gray Level Co-Occurrence Matrix,&#x0201d; and a special image feature &#x0201c;Edge Histogram Descriptor&#x0201d; are then exacted from these patches to generate a high-dimensional matrix called the &#x0201c;feature matrix.&#x0201d; Because each feature plays a different role in the classification process, a feature-weighting fusion based on kernel principal component analysis is used to redistribute the feature weights. Finally, a twin-support-vector-machine segmentation (TWSVM-Seg) model is trained to understand the differences between the two types of patches through the features, and the TWSVM-Seg model finally achieves the correct classification of each pixel from the testing sample and outputs the results in the form of binary image. This process thus segments the image. Next, we use a statistical function in Matlab to get the exact a precise number of ears. To verify these statistical numerical results, we compare them with field measurements of the wheat plots. The result of applying the proposed algorithm to ground-shooting image data sets correlates strongly (with a precision of 0.79&#x02013;0.82) with the data obtained by manual counting. An average running time of 0.1 s is required to successfully extract the correct number of ears from the background, which shows that the proposed algorithm is computationally efficient. These results indicate that the proposed method provides accurate phenotypic data on wheat seedlings.</p></abstract><kwd-group><kwd>superpixel theory</kwd><kwd>multi-feature optimization</kwd><kwd>support-vector-machine segmentation</kwd><kwd>wheat ear counting</kwd><kwd>yield estimation</kwd></kwd-group><counts><fig-count count="13"/><table-count count="4"/><equation-count count="11"/><ref-count count="35"/><page-count count="16"/><word-count count="8480"/></counts></article-meta></front><body><sec sec-type="intro" id="s1"><title>Introduction</title><p>Wheat is an important primary food for a large proportion of the world's population, so methods to estimate its yield have received significant research attention (Bogn&#x000e1;r et al., <xref rid="B4" ref-type="bibr">2017</xref>). The number of ears per unit area, the number of grains per ear, and 1000 grain weight are known as the three elements of wheat yield (Plovdiv, <xref rid="B24" ref-type="bibr">2013</xref>). Of these, the number of ears per unit area is mainly obtained in the field. The wheat ear is an important agronomic component (Jin et al., <xref rid="B12" ref-type="bibr">2017</xref>) not only is closely associated with yield but also plays an important role in disease detection, nutrition examination, and growth-period determination. Thus, an accurate determination of the number of ears is vital for estimating wheat yield and is a key step in field phenotyping (Zhang et al., <xref rid="B34" ref-type="bibr">2007</xref>). At present, two main statistical methods exist to obtain the number of ears per unit area: manual field investigation and image-based crop recognition (Nerson, <xref rid="B21" ref-type="bibr">1980</xref>). Manual field investigation, which is the traditional method, is inefficient and costly, resulting in more and more interest in image-based crop recognition. However, because of the complexity of the field environment (e.g., illumination intensity, soil reflectance, and weeds, which alters the colors, textures, and shapes in wheat-ear images), accurate wheat ear segmentation and recognition remains a significant challenge (Mussavi and M. Sc. of Agronomy Ramin Agricultural and Natural Resources, <xref rid="B19" ref-type="bibr">2011</xref>). In the field of image segmentation, a number of meaningful research results have emerged in recent years. These methods mostly focus on two approaches, the first of which is based solely on color information (Naemura et al., <xref rid="B20" ref-type="bibr">2000</xref>). For example, Chen et al. proposed a threshold-selection algorithm for image segmentation based on the Otsu rule (Chen et al., <xref rid="B5" ref-type="bibr">2012</xref>). Subsequently, Khokher et al. introduced an efficient method for color-image segmentation that uses adaptive mean shift and normalized cuts (Khokher et al., <xref rid="B14" ref-type="bibr">2013</xref>). Moreover, Liao et al. used an edge-region active contour model for simultaneous magnetic resonance image segmentation and denoising (Liao et al., <xref rid="B15" ref-type="bibr">2017</xref>). Additionally, the color information for wheat changes over the reproductive stage. Thus, different methods usually apply to different stages of reproduction. Therefore, in addition to the disadvantages described above, an excessive dependence on color information will lead to incomplete extraction.</p><p>The second approach involves machine learning. For example, Kandaswamy et al. used the meta-segmentation evaluation technique to deal with the problem of image segmentation (Kandaswamy et al., <xref rid="B13" ref-type="bibr">2013</xref>). Linares et al. introduced an image-segmentation algorithm based on the machine learning of features (Linares et al., <xref rid="B16" ref-type="bibr">2017</xref>). Soh et al. proposed a method based on a linear classifier that reveals a new method of segmentation (Soh and Tsatsoulis, <xref rid="B27" ref-type="bibr">1999</xref>). In addition, Lizarazo et al. used a support vector machine (SVM) classifier to segment remote-sensing data (Lizarazo, <xref rid="B17" ref-type="bibr">2008</xref>). Because of its high accuracy and robustness, target segmentation based on classifiers was widely used for target recognition in the field of complex environments (Lizarazo and Elsner, <xref rid="B18" ref-type="bibr">2009</xref>). This method mainly includes two key steps: (i) extraction and combination of image features and (ii) selection of classifiers to be trained.</p><p>The first step above forms the basis of image recognition (Song et al., <xref rid="B28" ref-type="bibr">2016</xref>). Choosing the appropriate features directly impacts the final segmentation and recognition accuracy (Ding et al., <xref rid="B8" ref-type="bibr">2017</xref>). Hu et al. proposed an image-feature-extraction method based on shape characteristics (Hu et al., <xref rid="B9" ref-type="bibr">2016</xref>), and Yang et al. introduced multi-structure feature fusion for face recognition based on multi-resolution exaction (Yang et al., <xref rid="B33" ref-type="bibr">2011</xref>). Datta et al. applied kernel principal component analysis (KPCA) to classify object-based vegetation species to fuse color and texture features, which has good results (Datta et al., <xref rid="B7" ref-type="bibr">2017</xref>). To summarize, compared with the single-feature method, using a variety of features to express the red-green-blue (RGB) images can be more comprehensive and effective for improving the descriptive ability.</p><p>Next, another key step of the classifier-based segmentation method is to use a general classifier to classify the features. The representative image classifier to be trained mainly includes a rough set, a Bayesian, and a SVM. Banerjee et al. used rough set theory to solve the problem of multispectral image classification (Banerjee and Maji, <xref rid="B3" ref-type="bibr">2015</xref>). Zhang et al. proposed a method for multiple categories based on Bayesian decisions (Zhang et al., <xref rid="B35" ref-type="bibr">2014</xref>). Finally, Park et al. introduced an automatic image-segmentation method that uses principal pixel analysis and SVM (Park et al., <xref rid="B22" ref-type="bibr">2016</xref>). Upon comparing with the other two classifiers, the SVM proves simpler in structure and offers global optimality and good generalization, so it has been widely used in the fields of image recognition and classification. However, the speed with which SVM learns a model is a major challenge for multi-class classification problems.</p><p>To overcome these problems, the present study proposes a segmentation algorithm based on multi-feature optimization and twin-support-vector-machine (TWSVM) (Jayadeva et al., <xref rid="B11" ref-type="bibr">2007</xref>). First, the algorithm extracts the color feature, texture feature, and edge histogram descriptor of wheat-ear images. Second, we use the KPCA to obtain the corresponding weights for each feature to rationally construct the feature space. The feature space is composed of multiple features to more comprehensively describe the target images, through which the advantages of each feature for classifying the different classes are manifested. Finally, the training of the TWSVM model is completed and better performance is obtained.</p><p>The remainder of this paper is organized as follows: The next section describes in detail both the study area and image preprocessing. Section Methods describes the methodology. Section Results describes the experimental results and demonstrates the robustness of the method. Finally, we finish the paper with concluding remarks and possible directions for future work.</p></sec><sec id="s2"><title>Study area and data preprocessing</title><sec><title>Study area</title><p>The field planted with wheat was located in the Xinxiang comprehensive experimental base of the Chinese Academy of Agricultural Sciences. (Xinxiang, China, 35&#x000b0;9&#x02032;32&#x02033; latitude North, 113&#x000b0;48&#x02032;28&#x02033; longitude East). The sowing date was October 16, 2015. The experiment was conducted from 10 a.m. to 2 p.m. on June 9, 2016. For this paper, we collected data in overcast weather conditions, which resulted in totally scattered skylight with no direct illumination. These conditions eliminate shadows. While obtaining image data in the field, we made manual ground measurements of the corresponding plot to obtain manual-recognition data at the same time. The manual investigation area is 4 m<sup>2</sup> in each plot and the total area covered is 1200 m<sup>2</sup> with 300 plots. (Figure <xref ref-type="fig" rid="F1">1</xref>).</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>(A)</bold> Location of basement in China. <bold>(B)</bold> Location of basement in Xinxiang City. <bold>(C)</bold> Satellite map of experimental area. <bold>(D)</bold> Working state in the field.</p></caption><graphic xlink:href="fpls-09-01024-g0001"/></fig></sec><sec><title>Data acquisition and preprocessing</title><sec><title>Image acquisition</title><p>For each observation of an individual wheat ear, we systematically varied the illumination factors. Figure <xref ref-type="fig" rid="F2">2</xref> shows an example of an image collected during a single observation. We imaged wheat ears from the side at 45&#x000b0; above the horizontal because color and texture are typically substantial from this perspective. The camera aperture was f/3.9 with an exposure time of 1/90 s. The focal length of the camera was 50 mm.</p><fig id="F2" position="float"><label>Figure 2</label><caption><p>Preprocessing strategy for original image.</p></caption><graphic xlink:href="fpls-09-01024-g0002"/></fig></sec><sec><title>Image preprocessing</title><p>(A) Selection of dataset samples</p><p>In conditions of varying illumination intensity from morning to afternoon, 1000 images were obtained with the same shooting mode (2 m imaging distance, 1.5 m imaging height, imaging angle at 45&#x000b0; above horizontal) and the same camera parameters as mentioned above. As a result of the limitation of the number of sample images, 700 images were used as the training sample and the rest 300 images were used as the testing sample. This procedure gave us images under differing light intensities. We next divided these images into the following three categories by visual analysis: (a) high light intensity, (b) medium light intensity, and (c) low light intensity. All the images were selected from each category as the source of training set. This processing guarantees the robustness of the light intensity of the training results.</p><p>(B) Image cutting</p><p>The images used in this work were all obtained from oblique photography. As a result of the perspective, the wheat ears far from the shooting position are not well rendered in the images. The limitations imposed by camera resolution and the position of the camera focus make this part of the image low quality, so we cut the image to remove these parts and ensure uniform data quality. After this cutting process, the image size was reduced to 3500 &#x000d7; 1800 (Figure <xref ref-type="fig" rid="F2">2</xref>).</p><p>(c) Counting results validation</p><p>The performance of the image processing system to automatically counting the ears appearing in an image was tested in the images. In order to validate the algorithm, the <italic>machine counting result</italic> was compared with the manual image-based ear counting on the same image. <italic>Machine counting result</italic> depicts the binary image where the connected pixels in white color are considered as a wheat ear automatically detected by the image processing system; each of these regions are added and the final result is referred to as the algorithm counting. Besides, the number of ears in a subset of images has been counted manually and is referred to as the <italic>manual counting result</italic>. To ensure the precision of the manual statistics, two people repeated the counting operation according to the field range of the cut images. Moreover, in order to judge the accuracy of the segmentation, the wheat ear area is manual labeled as red small block. Then the labeled images were used as the mask images to compare with the machine segmentation and recognition results in order to ensure the accuracy of the method.</p></sec></sec></sec><sec sec-type="methods" id="s3"><title>Methods</title><p>After image acquisition, the main flow diagram of the proposed method includes off-line training of on-line segmentation, as shown in Figure <xref ref-type="fig" rid="F3">3</xref>. This research framework consists of five consecutive steps: (i) generating patches, (ii) establishing training and test sample sets, (iii) optical combination of multi-features space, (iv) training a classifier, and (v) noise reduction. Below, we discuss each step in detail and refer in particular to the variables, image types, and preprocessing strategies that we studied in our experiments.</p><fig id="F3" position="float"><label>Figure 3</label><caption><p>Schematic representation of method.</p></caption><graphic xlink:href="fpls-09-01024-g0003"/></fig><p>The specific algorithm (workflow) is as follows:</p><list list-type="simple"><list-item><p>Step 1: Select <italic>N</italic> images as training samples and extract patches of a certain size (20 &#x000d7; 20) from these samples;</p></list-item><list-item><p>Step 2: Extract the color feature, texture feature, and edge histogram descriptor feature from the samples;</p></list-item><list-item><p>Step 3: Use KPCA to extract the principal component features and calculate the weight for each feature in each class of samples;</p></list-item><list-item><p>Step 4: Train the TWSVM classification model with the weighted features updated in Step 3;</p></list-item><list-item><p>Step 5: Perform a weighting to the feature in the test sample with feature weights in each class, use the TWSVM-Seg model obtained in Step 4 to classify, and determine the image segmentation (Figure <xref ref-type="fig" rid="F3">3</xref>).</p></list-item></list><sec><title>Generation of patches based on simple linear iterative clustering method and training set and validation set building</title><p>Pixel-level segmentation approaches have achieved a moderate degree of success. At the same time, ignoring the neighborhood information seriously impacts the edge-preservation of the segmentation algorithm. Thus, processing the image patches with similar characteristics instead of single pixels contributed to overcoming the influence of noise, accelerating the processing speed, and improving edge-preservation. Moreover, the TWSVM requires uniform-sized images as input. To achieve this goal, simple linear iterative clustering (SLIC) was applied to extract superpixel image patches (Achanta et al., <xref rid="B1" ref-type="bibr">2012</xref>). The computing speed is faster than the other superpixel generation algorithm, and the algorithm offers superior edge preservation. SLIC is an adaptation of k-means for superpixel generation, with two important distinctions:</p><list list-type="order"><list-item><p>The number of distance calculations in the optimization is dramatically reduced by limiting the search space to a region proportional to the superpixel size. This reduces the complexity to be linear in the number <italic>N</italic> of pixels&#x02014;and independent of the number <italic>k</italic> of superpixels.</p></list-item><list-item><p>A weighted distance measure combines color and spatial proximity while simultaneously controlling the size and compactness of the superpixels.</p></list-item></list><p>However, these SLIC superpixel regions are irregularly shaped, so they cannot be used directly as TWSVM input. Therefore, a small window called a patch (20 &#x000d7; 20 pixels) and centered on the weighted center of the current SLIC superpixel region is given to the TWSVM. Note that the code to implement the SLIC operation is based on open source code provided available at <ext-link ext-link-type="uri" xlink:href="https://ivrl.epfl.ch/research/superpixels">https://ivrl.epfl.ch/research/superpixels</ext-link>. After the SLIC generates the irregular superpixel regions as discussed above, the center of the patch is determined by the weighted center of the region. Then a regular patch is built according to the position of the center point. The percentages in each patch represent the ratio between the wheat ear area to the corresponding areas of the SLIC superpixel region. The sample patch is labeled category zero (background) if the percentage of the current patch is zero; otherwise, it is labeled category one (foreground). The fundamental part of any classification operation involves specifying the output or action, as determined based on a given set of inputs or training data. The classification system is formulated as a two-class model: positive patches and negative patches. The positive class contains image patches manually labeled from wheat ears under different illumination intensities. The negative class contains background images manually segmented from soil, rocks, etc. The dataset for the positive training class contains 8647 foreground patches, and that for the negative training class contains 7412 background patches. Meanwhile, the testing set was also generated by the SLIC through the same steps above.</p></sec><sec><title>Multi-feature exaction and combination</title><p>(1) <bold>Multi-feature exaction</bold></p><p>Visual features are fundamental for processing digital images to represent image content. A good set of features should contain sufficient discrimination power to discriminate image contents. The feature-extraction section uses color coherence vectors (CCV) as the color feature (Roy and Mukherjee, <xref rid="B25" ref-type="bibr">2013</xref>), the gray level co-occurrence matrix (GLCM) as the texture feature (Varish and Pal, <xref rid="B31" ref-type="bibr">2018</xref>), and introduces the edge histogram descriptor (EHD) feature (Agarwal et al., <xref rid="B2" ref-type="bibr">2013</xref>). Among them, CCV is sufficiently robust to handle background complications and invariants in size, orientation, and partial occlusion of the canopy image. The GLCM feature has good performance in extracting information from local and frequency domains and it can provide good direction selection and scale selection characteristics. The EHD feature can effectively distinguish the images with very high similarity for colors and has good robustness for the color and brightness changes which are the features with very strong stability. Each feature describes the image content from different angles, performing a reasonable optimization and integration to achieve a more comprehensive description of the image content. The final feature matrix contains of three elements: <inline-formula><mml:math id="M1"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>C</mml:mtext></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="M2"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mtext>G</mml:mtext></mml:mrow></mml:msubsup></mml:math></inline-formula>, and <inline-formula><mml:math id="M3"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow></mml:msubsup></mml:math></inline-formula>.</p><p><bold>(2) Multi-Feature Combination based on Kernel Principal Component Analysis</bold></p><p>Based on the above description, we obtain a matrix composed of multi-dimensional features. Differences clearly exist for the importance of each feature in the classification process, then reasonably constructing the feature space, so it is important to assign weights to the features according to the importance of features. To achieve this goal, we use KPCA to extract the principal component of features, combining different features to determine the feature weights for the importance of different image classes (Twining and Taylor, <xref rid="B29" ref-type="bibr">2003</xref>). The following details the specific method of classifying feature weights.</p><p>We first normalized the fused feature to unify the range of values. The importance of features is inversely proportional to the dispersion of the feature distribution; features with a higher dispersion have a lower importance, which means that a smaller standard deviation leads to a higher importance for features. We thus use <italic>I</italic><sub><italic>n</italic></sub> to indicate the importance of features:</p><disp-formula id="E1"><label>(1)</label><mml:math id="M4"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>Where <italic>k</italic><sub><italic>n</italic></sub> represents the standard deviation in class <italic>j</italic> of the sample set. When the distribution of one-dimensional features is more concentrated, the standard deviation is smaller, the corresponding <italic>k</italic><sub><italic>n</italic></sub> is smaller, and the importance of the feature is greater. The formula for calculating the weight of features in each dimension is</p><disp-formula id="E2"><label>(2)</label><mml:math id="M5"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>Through the above operation, we can merge the multiple feature vectors into a new feature matrix so that it can be used for machine learning with our own model.</p></sec><sec><title>Image segmentation method based on twin-support-vector-machine segmentation model and noise reduction</title><p>We introduce a twin-support-vector-machine segmentation (TWSVM-Seg) model, which is based on the traditional SVM model is better for segmentation of wheat-ear images (Peng et al., <xref rid="B23" ref-type="bibr">2016</xref>). It is similar in form to a traditional SWM with all its advantages. Moreover, it deals better with large-scale data.</p><p>In the data <italic>X</italic> &#x02208; <italic>R</italic><sup>(<italic>m</italic>*<italic>n</italic>)</sup> to be classified, we take positive samples <italic>m</italic><sub>1</sub> with the &#x0201c;1&#x0201d; class from the training set to obtain matrix <italic>A</italic><sub><italic>m</italic><sub>1</sub>&#x000b7;<italic>n</italic></sub> We then take negative samples <italic>m</italic><sub>2</sub> with the &#x0201c;0&#x0201d; class from the train set to obtain matrix<italic>B</italic><sub><italic>m</italic><sub>2</sub>&#x000b7;<italic>n</italic></sub>. We obtain a classification plane for each of the two classes. The data that belong to each class are, to the extent possible, near the corresponding classification plane.</p><p>The required hyperplane parameters can be obtained by solving the following optimization problem:</p><disp-formula id="E3"><label>(3)</label><mml:math id="M6"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:mtable style="text-align:axis;" equalrows="false" equalcolumns="false" class="array"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:munder class="msub"><mml:mrow><mml:mo class="qopname">min</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:munder></mml:mstyle><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>K</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup><mml:mi>q</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>.</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>q</mml:mi><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="E4"><label>(4)</label><mml:math id="M7"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:mtable style="text-align:axis;" equalrows="false" equalcolumns="false" class="array"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:munder class="msub"><mml:mrow><mml:mo class="qopname">min</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:munder></mml:mstyle><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>K</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup><mml:mi>q</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>.</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mtext class="textrm" mathvariant="normal">1</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>q</mml:mi><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>where <italic>K</italic> is the kernel function, <italic>A</italic> refers to <italic>m</italic><sub>1</sub> positive (wheat ear) samples and <italic>B</italic> refers to <italic>m</italic><sub>2</sub> negative (background) samples., e<sub>1</sub> and e<sub>2</sub> indicate the unit vector of the corresponding dimension, c<sub>1</sub> and c<sub>2</sub> are penalty coefficients, <italic>w</italic> is the normal vector of the optimal hyperplane, and <italic>b</italic> is the offset of the optimal hyperplane. <italic>q</italic> represents the discriminant coefficient. Here, the kernel function <italic>K</italic> is used to populate the TWSVM. Analysis shows that different kernel functions have very large impact on performance of TWSVM, and kernel function is also one of the adjustable parameters in TWSVM. Kernel functions, nuclear parameters and high-dimensional mapping space have a one-to-one relationship, so only select the proper kernel functions, nuclear parameters and high-dimensional mapping space when solving classification problem, we can get the separator with excellent learning and generalization ability. In this paper, we use the radial basis function (RBF) kernel <italic>K</italic> because of its excellent learning ability given large samples and low dimensions. We optimize the parameters of the kernel function after selecting. The error penalty factor <italic>c</italic> and gramma in the RBF are critical factors that impact the performance of the TWSVM, so these parameters strongly influence the classification accuracy and generalization ability of TWSVM. Here, we use the grid-search method to optimize and select parameters to obtain the global optimum results. Thus, the linear non-separable problem can be solved. Each sample in the training set belongs only to one of the two classes.</p><p>By solving Equations (3) and (4), we get the following two hyperplanes:</p><disp-formula id="E5"><label>(5)</label><mml:math id="M8"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:mi>K</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>C</mml:mi></mml:mstyle><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>b</mml:mi></mml:mstyle><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="E6"><label>(6)</label><mml:math id="M9"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:mi>K</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>C</mml:mi></mml:mstyle><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>b</mml:mi></mml:mstyle><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>The two hyperplanes correspond to two different classes. For a sample to be classified, the distance to these two hyperplanes must be calculated. For each sample, the distance to each hyperplane is compared and the sample is classified into the nearest class.</p><p>Through the above operation, the pixels in the test samples could be divided into two classes (wheat ear = 1, background = 0), which generates a binary image to achieve image segment.</p><p>After all these operations, we then use the median filter <italic>w</italic> to minimize the noise and remove the result of burrs and noise over the binary image (Igoe et al., <xref rid="B10" ref-type="bibr">2018</xref>). For this, we slide a window size of three pixels over the entire image, pixel by pixel, and numerically sort the pixel values in the window and replace them with a median value of neighboring pixels.</p><p>This process provides several separate and disconnected bright areas, each of which represents an unidentified wheat ear. Here, we use the regionprops function in Matlab R2017b (Mathworks Inc., Massachusetts, USA) to count the independent regions in the image, which corresponds to counting the number of wheat ears. In addition, we apply the ground truth function to each image, and manually label the wheat ears in the image so as to compare with the result of computer recognition.</p></sec><sec><title>Criteria to evaluation algorithm</title><p>To evaluate the quality of the segmentation, we use the six indicators <italic>Qseg, Sr</italic>, structural similarity index (SSIM), Precision, Recall, and the F-measure. The following is a detailed description of the meaning and range of each index (Xiong et al., <xref rid="B32" ref-type="bibr">2017</xref>).</p><p><italic>Qseg</italic>, which is based on both plants and background regions, ranges from 0 to 1. The closer <italic>Qseg</italic> is to unity, the more accurate is the segmentation. Thus, <italic>Qseg</italic> reflects the consistency of all the image pixels, including foreground ear part and the background part. <italic>Sr</italic> represents the consistency of only the ear part of the image. From the perspective of an image, it reflects the completeness of the segmentation results. The SSIM describes the degree of similarity between the segmentation images and the ground truth images. The SSIM ranges from 0 to 1, with higher values indicating more similarity between images. Precision and Recall are the most basic indicators for revealing the final segmentation results. Precision illustrates the accuracy of the segmentation algorithm, and Recall represents the completeness of the segmented images. In practice, Precision and Recall interact with each other. When Precision is high, Recall is low. The F-measure is proposed to balance these two indicators. The higher the value of the F-measure, the better the segmentation results. Table <xref rid="T1" ref-type="table">1</xref> shows how to calculate these indicators.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p>Formulas to calculate criteria for evaluating segmentation precision.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1"><bold>Evaluation criteria</bold></th><th valign="top" align="left" rowspan="1" colspan="1"><bold>Calculation formula</bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1"><italic>Q<sub><italic>seg</italic></sub></italic></td><td valign="top" align="left" rowspan="1" colspan="1"><disp-formula id="E7"><label>(7)</label><mml:math id="M10"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:munderover><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02229;</mml:mo><mml:mi>N</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:munderover><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0222a;</mml:mo><mml:mi>N</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:math></disp-formula></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"><italic>S<sub><italic>r</italic></sub></italic></td><td valign="top" align="left" rowspan="1" colspan="1"><disp-formula id="E8"><label>(8)</label><mml:math id="M11"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:munderover><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02229;</mml:mo><mml:mi>N</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:munderover><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:munderover><mml:mi>N</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Precision</td><td valign="top" align="left" rowspan="1" colspan="1"><disp-formula id="E9"><label>(9)</label><mml:math id="M12"><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Recall</td><td valign="top" align="left" rowspan="1" colspan="1"><disp-formula id="E10"><label>(10)</label><mml:math id="M13"><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">F-measure</td><td valign="top" align="left" rowspan="1" colspan="1"><disp-formula id="E11"><label>(11)</label><mml:math id="M14"><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac><mml:mi>%</mml:mi></mml:math></disp-formula></td></tr></tbody></table></table-wrap><p>In Equations (7) and (8), <italic>M</italic> represent the ear pixels (with &#x003c9; = 255) or background pixels (with &#x003c9; = 0). <italic>N</italic> in Equations (1) and (2) represents a reference set of manually segmented ear pixels (with &#x003c9; = 255) or background pixels (with &#x003c9; = 0). <italic>a</italic> and <italic>b</italic> give the row and column of the image and <italic>i, j</italic> give the pixel coordinate of the image. In Equations (9&#x02013;11), <italic>TP, TN, FP</italic>, and <italic>FN</italic> are the number of true positives, true negatives, false positives, and false negatives, respectively. True positives (<italic>TP</italic>) means when the predicted results and the corresponding ground truth are both wheat ear pixels. True negatives (<italic>TN</italic>) are when the predicted results and the corresponding ground truth are both background pixels. False positives (<italic>FP</italic>) are the pixels that are classified as wheat ear pixels, but the ground truth of those pixels is background. False negatives (<italic>FN</italic>) are the pixels that belong to the ground truth but are not correctly discriminated.</p></sec></sec><sec sec-type="results" id="s4"><title>Results</title><p>The performance of the proposed machine learning method is evaluated based on comparing its results against manual measurements. The algorithms were developed in Matlab R2017b. Segmenting a 3500 &#x000d7; 1800 image takes only 0.1 s on average on a Windows 10 PC with 4-core Intel Core i5 processor (2.71 GHz) with 12 GB RAM. For this paper, we separated the image dataset of 300 plots into three categories of equal size with different illumination conditions and show their segmentation results and corresponding ground truths.</p><sec><title>Results of several image-segmentation methods</title><p>We apply three traditional segmentation methods to compare their results with those of the proposed method. The unsupervised methods are the Otsu method, mean shift and normalized cuts (MSNC), and the edge-region active contour model.</p><p>The Otsu method is a global thresholding method. The Otsu threshold is found by searching across the entire range of pixel values of an image until the intra-class variances are minimized. MSNC first applies the mean shift algorithm to obtain subgraphs and then applies the normalized cut. Currency denomination and detection is an application of image segmentation. The edge-region active contour model consists of two main energy terms: an edge-region term and a regularization term. This model not only has the desirable property of processing inhomogeneous regions but also provides satisfactory convergence speed (Cheng et al., <xref rid="B6" ref-type="bibr">2001</xref>) (Figure <xref ref-type="fig" rid="F4">4</xref>).</p><fig id="F4" position="float"><label>Figure 4</label><caption><p>Examples of test images after segmentation and their corresponding ground truths. The test images are randomly selected from the image dataset under different illumination conditions: <bold>(A)</bold> high illumination intensity, <bold>(B)</bold> medium illumination intensity, <bold>(C)</bold> low illumination intensity.</p></caption><graphic xlink:href="fpls-09-01024-g0004"/></fig><p>Figure <xref ref-type="fig" rid="F4">4</xref> shows the input and output images that there are mainly three cases where these method has not worked properly: (i) pixels labeled as ear actually corresponded to leaves; (ii) contrast between the ear and soil was not great enough and (iii) whereas the algorithm labeled the area as an ear, those pixels are noise.</p><p>Furthermore, the linear regression between the manual counting and the algorithm counting was calculated for 300 plots with different illumination (Figure <xref ref-type="fig" rid="F5">5</xref>, Table <xref rid="T2" ref-type="table">2</xref>).</p><fig id="F5" position="float"><label>Figure 5</label><caption><p>Plots of Mannual counting with different segmentation method in different illumination. <bold>(A)</bold> Proposed method, <bold>(B)</bold> Otsu method, <bold>(C)</bold> MSNC method and <bold>(D)</bold> Edge-region method.</p></caption><graphic xlink:href="fpls-09-01024-g0005"/></fig><table-wrap id="T2" position="float"><label>Table 2</label><caption><p>Results of counting wheat ears by using different segmentation methods in different illumination conditions.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th valign="top" align="center" rowspan="1" colspan="1"><bold>Proposed method</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Otsu method</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>MSNC method</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Edge-region method</bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">High</td><td valign="top" align="center" rowspan="1" colspan="1">R<sup>2</sup> = 0.99</td><td valign="top" align="center" rowspan="1" colspan="1">R<sup>2</sup> = 0.92</td><td valign="top" align="center" rowspan="1" colspan="1">R<sup>2</sup> = 0.90</td><td valign="top" align="center" rowspan="1" colspan="1">R<sup>2</sup> = 0.92</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">illumination</td><td valign="top" align="center" rowspan="1" colspan="1">SD = 4.07</td><td valign="top" align="center" rowspan="1" colspan="1">SD = 15.82</td><td valign="top" align="center" rowspan="1" colspan="1">SD = 15.72</td><td valign="top" align="center" rowspan="1" colspan="1">SD = 15.82</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Medium</td><td valign="top" align="center" rowspan="1" colspan="1">R<sup>2</sup> = 0.99</td><td valign="top" align="center" rowspan="1" colspan="1">R<sup>2</sup> = 0.94</td><td valign="top" align="center" rowspan="1" colspan="1">R<sup>2</sup> = 0.90</td><td valign="top" align="center" rowspan="1" colspan="1">R<sup>2</sup> = 0.85</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">illumination</td><td valign="top" align="center" rowspan="1" colspan="1">SD = 4.07</td><td valign="top" align="center" rowspan="1" colspan="1">SD = 13.33</td><td valign="top" align="center" rowspan="1" colspan="1">SD = 18.10</td><td valign="top" align="center" rowspan="1" colspan="1">SD = 22.05</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Low</td><td valign="top" align="center" rowspan="1" colspan="1">R<sup>2</sup> = 0.99</td><td valign="top" align="center" rowspan="1" colspan="1">R<sup>2</sup> = 0.94</td><td valign="top" align="center" rowspan="1" colspan="1">R<sup>2</sup> = 0.90</td><td valign="top" align="center" rowspan="1" colspan="1">R<sup>2</sup> = 0.88</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">illumination</td><td valign="top" align="center" rowspan="1" colspan="1">SD = 4.08</td><td valign="top" align="center" rowspan="1" colspan="1">SD = 14.22</td><td valign="top" align="center" rowspan="1" colspan="1">SD = 17.29</td><td valign="top" align="center" rowspan="1" colspan="1">SD = 18.35</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Total result</td><td valign="top" align="center" rowspan="1" colspan="1">R<sup>2</sup> = 0.99</td><td valign="top" align="center" rowspan="1" colspan="1">R<sup>2</sup> = 0.94</td><td valign="top" align="center" rowspan="1" colspan="1">R<sup>2</sup> = 0.90</td><td valign="top" align="center" rowspan="1" colspan="1">R<sup>2</sup> = 0.86</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">SD = 4.05</td><td valign="top" align="center" rowspan="1" colspan="1">SD = 14.44</td><td valign="top" align="center" rowspan="1" colspan="1">SD = 17.21</td><td valign="top" align="center" rowspan="1" colspan="1">SD = 20.57</td></tr></tbody></table></table-wrap><p>We see from Figure <xref ref-type="fig" rid="F5">5</xref> and Table <xref rid="T2" ref-type="table">2</xref> that the results of the proposed method correlate strongly (<bold><italic>R</italic></bold><sup>2</sup> = 0.99) with the manual measurements for all selected images. Moreover, the standard deviation (<bold><italic>SD</italic></bold>) between the test set is smallest which means that the proposed method is the most stable. But the simple use of the correlation index cannot accurately evaluate the recognition accuracy, so we introduce below more evaluation criteria to verify the performance of these methods.</p><p>We can draw a conclusion from the Figure <xref ref-type="fig" rid="F4">4</xref> that there are mainly three kinds of regions in the image indicating examples where the algorithm has not worked properly: (a) Region 1 shows the case where two ears overlap together and are considered as one; (b) In Region 2, false negatives resulted in wheat ears that were not detected by the algorithm because the contrast between the wheat ear and soil was not great enough and the segmentation algorithm discarded that region; (c) In Region 3, whereas the algorithm labeled the area as a wheat ear, those targets are noise being a result of background brightness caused by a foreign object.</p><p>Comparing the manual counting results with the statistical results obtained by different segmentation methods gives satisfactory results. To evaluate the segmentation results more comprehensively, six indices were introduced to judge the effect of the segmentation (Figure <xref ref-type="fig" rid="F6">6</xref>).</p><fig id="F6" position="float"><label>Figure 6</label><caption><p>Results of evaluating segmentation with different methods. The color columns represent the means value and the black lines represent the standard deviations for the test images. In addition, the color differences between columns refers to the categories of segmentation methods. Blue is for the proposed method, green is for the Otsu method, yellow is for the MSNC, and purple is for the edge-region active contour model.</p></caption><graphic xlink:href="fpls-09-01024-g0006"/></fig><p>Figure <xref ref-type="fig" rid="F6">6</xref> shows that, compared with other three common methods mentioned in this paper, the proposed method gives the maximum mean value of the six indicators. The mean values of <italic>Qseg, Sr</italic>, SSIM, Precision, Recall and F-measure (%) are 0.62, 0.72, 0.82, 0.82, 0.73, and 0.73, respectively. Moreover, Figure <xref ref-type="fig" rid="F6">6</xref> shows that the proposed method gives the minimum standard deviation for each evaluation index, which means that it gives the most stable performance with images under different illumination conditions.</p></sec><sec><title>Results of segmentation accuracy with different classifiers</title><p>Differences in selecting the classifier can lead to quite different segmentation precision. To verify the proposed algorithm (TWSVM), we compare it against three well-established algorithms: rough set, Bayesian, SVM (Figures <xref ref-type="fig" rid="F7">7</xref>, <xref ref-type="fig" rid="F8">8</xref>).</p><fig id="F7" position="float"><label>Figure 7</label><caption><p>Segmentation results for different classifiers under different illumination conditions: <bold>(A)</bold> high illumination intensity, <bold>(B)</bold> medium illumination intensity, <bold>(C)</bold> low illumination intensity.</p></caption><graphic xlink:href="fpls-09-01024-g0007"/></fig><fig id="F8" position="float"><label>Figure 8</label><caption><p>Comparison of segmentation with different classifiers. The color columns represent the mean values and the black lines represent the standard deviations for the testing images. Blue represent TWSWM, green is for rough set, yellow is for Bayesian, and purple is for SVM.</p></caption><graphic xlink:href="fpls-09-01024-g0008"/></fig><p>Figure <xref ref-type="fig" rid="F8">8</xref> shows that the TWSVM provides better segmentation, and the wheat-ear integrity is well maintained. Except for the TWSVM, the SD of the other three algorithms is relatively large, which reflects their weak adaptability to different field testing images. In addition, the average of <italic>Q</italic><sub><italic>seg</italic></sub> for the proposed algorithm is about 0.626, which is significantly greater than for the other three algorithms. Thus, the proposed algorithm is more consistent for both the panicle foreground part and the background part. In addition, the mean value of the SSIM for the proposed algorithm is greater than that of the other three contrast algorithms. Moreover, the F-measure is a comprehensive indicator and accounts for Precision and Recall; it is as high as 0.738 using our proposed algorithm compared with 0.398, 0.452, and 0.578 for the other algorithms, respectively. These results show that the proposed algorithm accurately segments the wheat ears and guarantees the integrity of segmentation.</p></sec><sec><title>Results of recognition accuracy with different image features</title><p>The color feature, texture feature, and EHD feature are optimized to perform the segmentation of images, in Figure <xref ref-type="fig" rid="F8">8</xref>, the segmentation testing results by using different number of features are given (Figure <xref ref-type="fig" rid="F9">9</xref>).</p><fig id="F9" position="float"><label>Figure 9</label><caption><p>Comparison of segmentation accuracy with different feature numbers. The color columns represent the means value. The color differences between columns means the categories of evaluating indicator. [Precision (Blue), Recall (Green), and F-measure (Yellow)].</p></caption><graphic xlink:href="fpls-09-01024-g0009"/></fig><p>Results are found by Figure <xref ref-type="fig" rid="F9">9</xref> that for each class of wheat ear image, the segmentation accuracy of the proposed algorithm is obviously better than that when using a single feature. And it can be seen that for the selected color feature, texture feature, and EHD feature, each feature has very different segmentation results, which also shows that there is a complementary relationship between each feature. After optimizing each feature, the proposed algorithm gives the weight to each feature, each sample constructs a reasonable feature space, and the average precision of the whole image is more than 82%, which is 12.4, 7.5, and 9.2% higher than the recognition accuracy of color feature, texture feature, and EHD feature, respectively.</p><p>We use <italic>Q</italic><sub><italic>seg</italic></sub>, and <italic>S</italic><sub><italic>r</italic></sub> to judge the segmentation accuracy; the results are given in Table <xref rid="T3" ref-type="table">3</xref>.</p><table-wrap id="T3" position="float"><label>Table 3</label><caption><p>Comparison of mean accuracy rate between multi-feature and single feature.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th valign="top" align="center" rowspan="1" colspan="1"><bold>Multi-feature</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Color</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Texture</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>EHD</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Color+Texture</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Color+EHD</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Texture+EHD</bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1"><italic>Q<sub><italic>seg</italic></sub></italic></td><td valign="top" align="center" rowspan="1" colspan="1">0.62</td><td valign="top" align="center" rowspan="1" colspan="1">0.41</td><td valign="top" align="center" rowspan="1" colspan="1">0.52</td><td valign="top" align="center" rowspan="1" colspan="1">0.47</td><td valign="top" align="center" rowspan="1" colspan="1">0.54</td><td valign="top" align="center" rowspan="1" colspan="1">0.55</td><td valign="top" align="center" rowspan="1" colspan="1">0.58</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"><italic>S<sub><italic>r</italic></sub></italic></td><td valign="top" align="center" rowspan="1" colspan="1">0.72</td><td valign="top" align="center" rowspan="1" colspan="1">0.49</td><td valign="top" align="center" rowspan="1" colspan="1">0.55</td><td valign="top" align="center" rowspan="1" colspan="1">0.67</td><td valign="top" align="center" rowspan="1" colspan="1">0.71</td><td valign="top" align="center" rowspan="1" colspan="1">0.70</td><td valign="top" align="center" rowspan="1" colspan="1">0.68</td></tr></tbody></table></table-wrap><p>As shown in Table <xref rid="T3" ref-type="table">3</xref>, the use of multiple features is more robust against background noise and variations in illumination. As a result, we select the multi-feature method as the optimum technique and compare it with the state-of-the-art vegetation segmentation described herein.</p><p>Moreover, robust hue histograms (<bold>RHH</bold>) (color) and Scale-invariant feature transform (<bold>SIFT</bold>) (texture) were used as two typical features to participate in comparative experiments to verify the reliability of feature selection (van de Sande et al., <xref rid="B30" ref-type="bibr">2010</xref>; Seeland et al., <xref rid="B26" ref-type="bibr">2017</xref>). Here, CCV and GLCM were replaced with RHH and SIFT in order to test the variation of precision after different combination of features (Table <xref rid="T4" ref-type="table">4</xref>).</p><table-wrap id="T4" position="float"><label>Table 4</label><caption><p>Comparison of mean accuracy rate between different feature combination strategy.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th valign="top" align="center" rowspan="1" colspan="1"><bold>Proposed</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>RHH</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>SIFT</bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1"><italic>Q<sub><italic>seg</italic></sub></italic></td><td valign="top" align="center" rowspan="1" colspan="1">0.62</td><td valign="top" align="center" rowspan="1" colspan="1">0.60</td><td valign="top" align="center" rowspan="1" colspan="1">0.63</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"><italic>S<sub><italic>r</italic></sub></italic></td><td valign="top" align="center" rowspan="1" colspan="1">0.72</td><td valign="top" align="center" rowspan="1" colspan="1">0.73</td><td valign="top" align="center" rowspan="1" colspan="1">0.71</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SSIM</td><td valign="top" align="center" rowspan="1" colspan="1">0.82</td><td valign="top" align="center" rowspan="1" colspan="1">0.72</td><td valign="top" align="center" rowspan="1" colspan="1">0.69</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Precision</td><td valign="top" align="center" rowspan="1" colspan="1">0.82</td><td valign="top" align="center" rowspan="1" colspan="1">0.78</td><td valign="top" align="center" rowspan="1" colspan="1">0.71</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Recall</td><td valign="top" align="center" rowspan="1" colspan="1">0.73</td><td valign="top" align="center" rowspan="1" colspan="1">0.65</td><td valign="top" align="center" rowspan="1" colspan="1">0.61</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">F-measure</td><td valign="top" align="center" rowspan="1" colspan="1">0.73</td><td valign="top" align="center" rowspan="1" colspan="1">0.68</td><td valign="top" align="center" rowspan="1" colspan="1">0.67</td></tr></tbody></table></table-wrap><p>Table <xref rid="T4" ref-type="table">4</xref> could provide a conclusion that the combination of features given in this paper could get better segmentation effect. Although on some indices, for example, <italic>Q</italic><sub><italic>seg</italic></sub> and <italic>S</italic><sub><italic>r</italic></sub>, the proposed feature combination strategy was slightly lower than the match group (&#x0003c; 5%). In general, it usually gives more accurate results especially in Precision and F-measure.</p></sec></sec><sec sec-type="discussion" id="s5"><title>Discussion</title><p>To be relevant for high-throughput phenotyping in field conditions, the segmentation algorithms must be sufficiently robust to handle dynamic illumination conditions and complex canopy architecture throughout the entire observation period. We find that the recognition accuracy of the classifiers differs substantially depending on the number of features and the illumination intensity. Here, we analyze how these factors affect the accuracy of the segmentation results, respectively.</p><sec><title>Effect of illumination intensity and shadow on recognition accuracy</title><p>Analyzing images acquired outdoors is a challenging task because the ambient illumination varies throughout the growing season. Unlike single plants grown in pots in greenhouse facilities, segmenting the vegetation from a field-grown plot is complex because of overlapping leaves and because portions of the canopy are shadowed or have high specular reflectance, each of which contribute to underestimating vegetation pixels in an image. To study the robustness of the method under different illumination conditions, we use the image brightness adjustment function of Photoshop CS6 (Adobe Systems Incorporated, California, USA) to adjust the luminance components. The original image brightness is called the &#x0201c;central value of brightness adjustment,&#x0201d; and the image results of five different luminance conditions are simulated by varying from dark to bright. The results are then associated with the artificial recognition results by using the proposed method to determine how the different illumination conditions affect this recognition method (Figure <xref ref-type="fig" rid="F10">10</xref>).</p><fig id="F10" position="float"><label>Figure 10</label><caption><p>Comparison of segmentation accuracy under different illumination intensities.</p></caption><graphic xlink:href="fpls-09-01024-g0010"/></fig><p>Figure <xref ref-type="fig" rid="F10">10</xref> shows that the segmentation accuracy reaches the highest value under conditions of lower brightness, which corresponds to overcast sky without overexposure or underexposure. We thus conclude that the illumination condition affects the recognition accuracy.</p><p>Unlike the use of artificial light and enclosures, our flexible and fast image acquisition technique presents some major challenges related to image processing. Sharp shadows and bright surfaces may appear in the images as a product of the light conditions. As such, in order to provide robust results, the image processing algorithm pipeline must consider effects related with shadows. When the training set is set up, the images under different shading conditions are included. The recognition results in Figure <xref ref-type="fig" rid="F5">5</xref> and Table <xref rid="T2" ref-type="table">2</xref> show that there is not much difference in the recognition results under different shading conditions.</p></sec><sec><title>Analysis of effect of noise on recognition accuracy</title><p>Noise may be generated through the entire process of the image processing and may be divided into two categories: system noise and environmental noise. System noise is usually caused by the imaging system itself and includes electronic noise and photoelectron noise. Environmental noise is caused by a poor image-acquisition environment and unreasonable image-acquisition methods. The proposed method relies on counting disconnected regions and fitting the obtained number to the manually counted amount of wheat ears via linear regression. So the excessive noise points will increase the error in statistical results. Here, Gauss noise, Rayleigh noise, exponential noise, and salt-and-pepper noise were introduced to test the noise robustness of the proposed method (Figure <xref ref-type="fig" rid="F11">11</xref>).</p><fig id="F11" position="float"><label>Figure 11</label><caption><p>Analysis of the noise resistance of the proposed method.</p></caption><graphic xlink:href="fpls-09-01024-g0011"/></fig><p>Figure <xref ref-type="fig" rid="F11">11</xref> shows that noise affects the accuracy of segmentation. Specifically, Rayleigh noise and salt-and-pepper noise reduce the accuracy by over 40% whereas the two other types of noise have little effect on the result (&#x0003c; 20%). The first two types of noise are denser and larger and are easily mistaken for wheat ears. However, a <italic>Median filter</italic> or a <italic>Laplacian filter</italic> can effectively filter out these two types of noise and may be considered for denoising in actual production.</p></sec><sec><title>Effect of different camera angles and field-of-view on recognition accuracy</title><p>The performance of the algorithm was further tested through the different camera angles and fields of view. First, the images are taken at six different angles: 90, 75, 60, 45, 30, 15, and 0&#x000b0; under same light intensity and camera parameters. Then, the center of each image is taken as the center of shooting, and the image is cut at 1/2, 1/4, and 1/8 long sides, respectively, then the imaging results of different fields of view are obtained. We use the same algorithm pipeline proposed for different camera angles and field-of-view images. As before, manual image-based counting is used as the validation data (Figure <xref ref-type="fig" rid="F12">12</xref>).</p><fig id="F12" position="float"><label>Figure 12</label><caption><p>Analysis of different camera angles and field-of-view on recognition accuracy. <bold>(A)</bold> The variation of accuracy under different observation angles; <bold>(B)</bold> The variation of accuracy under different field of view.</p></caption><graphic xlink:href="fpls-09-01024-g0012"/></fig><p>The different camera angles show, with respect to the original images taken at 45&#x000b0;, a decrease of over 20% in success rate while the shooting angle is close to 0&#x000b0;. The interference of leaves and stems and the mutual occlusion between ears make it impossible to get an accurate number near the horizontal position. Meanwhile, the accuracy of image recognition from the vertical angle is also reduced by about 15%. The significant difference in wheat morphology between vertical and oblique observations may at the origin of this result.</p><p>The different field-of-view results show an increase of 8% in success rate when the images are reduced to 75% of their original size. Success rates increased by a maximum of 13 and 15% for image size divided by 50 and 25% values, respectively. Near the edge of the image, distortion of the wheat ear shape reduces the recognition accuracy. At the same time, other interference factors affect the edge parts, such as noise, which will also affect the final result. In future work, the proper range of field of view should be studied.</p></sec><sec><title>Analysis of algorithm efficiency</title><p>We use the average running time of each segmentation method as a metric for the efficiency of the algorithm (Figure <xref ref-type="fig" rid="F13">13</xref>).</p><fig id="F13" position="float"><label>Figure 13</label><caption><p>Algorithm efficiency analysis: <bold>(A)</bold> Operating efficiency of different segmentation methods. <bold>(B)</bold> The running time of each step of the proposed method. &#x0201c;A&#x0201d; represents the images obtained under low intensity illumination, &#x0201c;B&#x0201d; represent images obtained under normal illumination conditions, and &#x0201c;C&#x0201d; represent images obtained under high-intensity illumination.</p></caption><graphic xlink:href="fpls-09-01024-g0013"/></fig><p>We conclude from Figure <xref ref-type="fig" rid="F13">13</xref> that the average running time of the proposed algorithm is 0.1 s for calculating the number of wheat ears in a single scene, which means that the proposed algorithm is an efficient method. Moreover, the running time increases as the number of wheat ears increases (compare Table <xref rid="T3" ref-type="table">3</xref> and Figure <xref ref-type="fig" rid="F13">13A</xref>). It seems that the increase in the number of target objects may lead to an increase in the time complexity of the algorithm. Thus, the proposed method may be used as a high-throughput post processing method to measure seeding statistics for large-scale breeding programs. We can also draw a conclusion from the Figure <xref ref-type="fig" rid="F13">13B</xref> that the most time-consuming step is patch classification by TWSVM. An intuitive improvement to further improve algorithm efficiency would be to parallelize all the procedures. However, such an improvement is likely to be hardware limited (due to the input-output speed of the memory and hard drive).</p><p>To summarize, the proposed machine learning approach offers the advantage of versatility and can extract the number of green vegetation, such as wheat, maize, etc. Given an adequate training dataset, it could even detect disease or pest symptoms. As already mentioned, the performance of any supervised learning model strongly depends on the training datasets. Therefore, to have a good model, a substantial set of training data is important. Acquiring a training data is time consuming and can be subjective. Our aim is to expand this study by integrating a semi-adaptive approach to semi-automatically generate larger and more reliable training datasets. In addition, we must test the model on more varieties and different crops.</p></sec></sec><sec sec-type="conclusions" id="s6"><title>Conclusion</title><p>Accurately estimating wheat yield requires accurate statistics of the number of wheat ears per unit area. This is achieved in this study by using a method for automatic segmentation of target plant material in RGB images of wheat ears and by splitting these images into individual targets. The initial step in this proposed method requires minimal manual intervention to generate patches from the original images. This technique is partially verified by comparing its results with those of manual and automated measures of image segmentation. The good correlation between manual and automated measurements confirms the value of the proposed segmentation method. The segmentation performance is evaluated in this way because manual image segmentation is labor intensive and subject to observer bias. Manual inspection of segmented images indicates good quality segmentation in all images. Compared with other approaches, the proposed algorithm provides better segmentation and recognition accuracy. Moreover, this method can be expanded for use in different field environments and with different light intensities and soil reflectance.</p></sec><sec id="s7"><title>Author contributions</title><p>CZ and GY anlayzed the data and drafted the article. DL directed images processing. XY and HY designed the experiments, and JY conducted the field measurements. All authors gave final approval for publication.</p><sec><title>Conflict of interest statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></sec></body><back><fn-group><fn fn-type="financial-disclosure"><p><bold>Funding.</bold> This work was supported by the National Key Research and Development Program of China (2016YFD0700303), the Beijing Natural Science Foundation (6182011), the Natural Science Foundation of China (61661136003, 41471351), and the Special Funds for Technology innovation capacity building sponsored by the Beijing Academy of Agriculture and Forestry Sciences (KJCX20170423, PT2018-23).</p></fn></fn-group><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Achanta</surname><given-names>R.</given-names></name><name><surname>Shaji</surname><given-names>A.</given-names></name><name><surname>Smith</surname><given-names>K.</given-names></name><name><surname>Lucchi</surname><given-names>A.</given-names></name><name><surname>Fua</surname><given-names>P.</given-names></name><name><surname>S&#x000fc;sstrunk</surname><given-names>S.</given-names></name></person-group> (<year>2012</year>). <article-title>Slic superpixels compared to state-of-the-art superpixel methods</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
<volume>34</volume>, <fpage>2274</fpage>&#x02013;<lpage>2282</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2012.120</pub-id><?supplied-pmid 22641706?><pub-id pub-id-type="pmid">22641706</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Agarwal</surname><given-names>S.</given-names></name><name><surname>Verma</surname><given-names>A. K.</given-names></name><name><surname>Singh</surname><given-names>P.</given-names></name></person-group> (<year>2013</year>). <article-title>Content based image retrieval using discrete wavelet transform and edge histogram descriptor</article-title>, in <source>International Conference on Information Systems and Computer Networks</source> (<publisher-loc>Mathura</publisher-loc>), <fpage>19</fpage>&#x02013;<lpage>23</lpage>.</mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banerjee</surname><given-names>A.</given-names></name><name><surname>Maji</surname><given-names>P.</given-names></name></person-group> (<year>2015</year>). <article-title>Rough sets and stomped normal distribution for simultaneous segmentation and bias field correction in brain MR images</article-title>. <source>IEEE Trans. Image Process.</source>
<volume>24</volume>, <fpage>5764</fpage>&#x02013;<lpage>5776</lpage>. <pub-id pub-id-type="doi">10.1109/TIP.2015.2488900</pub-id><?supplied-pmid 26462197?><pub-id pub-id-type="pmid">26462197</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogn&#x000e1;r</surname><given-names>P.</given-names></name><name><surname>Kern</surname><given-names>A.</given-names></name><name><surname>P&#x000e1;sztor</surname><given-names>S.</given-names></name><name><surname>Lichtenberger</surname><given-names>J.</given-names></name><name><surname>Koronczay</surname><given-names>D.</given-names></name><name><surname>Ferencz</surname><given-names>C.</given-names></name></person-group> (<year>2017</year>). <article-title>Yield estimation and forecasting for winter wheat in hungary using time series of modis data</article-title>. <source>Int. J. Rem. Sens.</source>
<volume>9653</volume>:<fpage>96530</fpage>
<pub-id pub-id-type="doi">10.1117/12.2196293</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Q.</given-names></name><name><surname>Zhao</surname><given-names>L.</given-names></name><name><surname>Lu</surname><given-names>J.</given-names></name><name><surname>Kuang</surname><given-names>G.</given-names></name><name><surname>Wang</surname><given-names>N.</given-names></name><name><surname>Jiang</surname><given-names>Y.</given-names></name></person-group> (<year>2012</year>). <article-title>Modified two-dimensional otsu image segmentation algorithm and fast realisation</article-title>. <source>Iet Image Process.</source>
<volume>6</volume>, <fpage>426</fpage>&#x02013;<lpage>433</lpage>. <pub-id pub-id-type="doi">10.1049/iet-ipr.2010.0078</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>H. D.</given-names></name><name><surname>Jiang</surname><given-names>X. H.</given-names></name><name><surname>Sun</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name></person-group> (<year>2001</year>). <article-title>Color image segmentation: advances and prospects</article-title>. <source>Patt. Recognit.</source>
<volume>34</volume>, <fpage>2259</fpage>&#x02013;<lpage>2281</lpage>. <pub-id pub-id-type="doi">10.1016/S0031-3203(00)00149-7</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Datta</surname><given-names>A.</given-names></name><name><surname>Ghosh</surname><given-names>S.</given-names></name><name><surname>Ghosh</surname><given-names>A.</given-names></name></person-group> (<year>2017</year>). <article-title>Unsupervised band extraction for hyperspectral images using clustering and kernel principal component analysis</article-title>. <source>Int. J. Remote Sens.</source>
<volume>38</volume>, <fpage>850</fpage>&#x02013;<lpage>873</lpage>. <pub-id pub-id-type="doi">10.1080/01431161.2016.1271470</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>W.</given-names></name><name><surname>Gu</surname><given-names>J.</given-names></name><name><surname>Shang</surname><given-names>Z.</given-names></name><name><surname>Tang</surname><given-names>S.</given-names></name><name><surname>Wu</surname><given-names>Q.</given-names></name><name><surname>Duodu</surname><given-names>E. A.</given-names></name></person-group> (<year>2017</year>). <article-title>Semantic recognition of workpiece using computer vision for shape feature extraction and classification based on learning databases</article-title>. <source>Optik</source>
<volume>130</volume>, <fpage>1426</fpage>&#x02013;<lpage>1437</lpage>. <pub-id pub-id-type="doi">10.1016/j.ijleo.2016.11.155</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>Y.</given-names></name><name><surname>Liang</surname><given-names>Z.</given-names></name><name><surname>Song</surname><given-names>B.</given-names></name><name><surname>Han</surname><given-names>H.</given-names></name><name><surname>Pickhardt</surname><given-names>P.</given-names></name><name><surname>Zhu</surname><given-names>W.</given-names></name></person-group> (<year>2016</year>). <article-title>Texture feature extraction and analysis for polyp differentiation via computed tomography colonography</article-title>. <source>IEEE Trans. Med. Imaging</source>
<volume>36</volume>, <fpage>4131</fpage>&#x02013;<lpage>4143</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2016.2518958</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Igoe</surname><given-names>D. P.</given-names></name><name><surname>Parisi</surname><given-names>A. V.</given-names></name><name><surname>Amar</surname><given-names>A.</given-names></name><name><surname>Rummenie</surname><given-names>K. J.</given-names></name></person-group> (<year>2018</year>). <article-title>Median filters as a tool to determine dark noise thresholds in high resolution smartphone image sensors for scientific imaging</article-title>. <source>Rev. Sci. Instr.</source>
<volume>89</volume>:<fpage>015003</fpage>. <pub-id pub-id-type="doi">10.1063/1.5006000</pub-id><?supplied-pmid 29390698?><pub-id pub-id-type="pmid">29390698</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jayadeva</surname></name><name><surname>Khemchandani</surname><given-names>R.</given-names></name><name><surname>Chandra</surname><given-names>S.</given-names></name></person-group> (<year>2007</year>). <article-title>Twin support vector machines for pattern classification</article-title>. <source>IEEE Trans. Patt. Anal. Mach. Intell.</source>
<volume>29</volume>, <fpage>905</fpage>&#x02013;<lpage>910</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2007.1068</pub-id><?supplied-pmid 17469239?><pub-id pub-id-type="pmid">17469239</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>X.</given-names></name><name><surname>Liu</surname><given-names>S.</given-names></name><name><surname>Baret</surname><given-names>F.</given-names></name><name><surname>Hemerl,&#x000e9;</surname><given-names>M.</given-names></name><name><surname>Comar</surname><given-names>A.</given-names></name></person-group> (<year>2017</year>). <article-title>Estimates of plant density of wheat crops at emergence from very low altitude uav imagery</article-title>. <source>Remote Sens. Environ</source>. <volume>198</volume>, <fpage>105</fpage>&#x02013;<lpage>114</lpage>. <pub-id pub-id-type="doi">10.1016/j.rse.2017.06.007</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kandaswamy</surname><given-names>U.</given-names></name><name><surname>Rotman</surname><given-names>Z.</given-names></name><name><surname>Watt</surname><given-names>D.</given-names></name><name><surname>Schillebeeckx</surname><given-names>I.</given-names></name><name><surname>Cavalli</surname><given-names>V.</given-names></name><name><surname>Klyachko</surname><given-names>V. A.</given-names></name></person-group> (<year>2013</year>). <article-title>Automated condition-invariable neurite segmentation and synapse classification using textural analysis-based machine-learning algorithms</article-title>. <source>J. Neurosci. Methods</source>
<volume>213</volume>, <fpage>84</fpage>&#x02013;<lpage>98</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2012.12.011</pub-id><?supplied-pmid 23261652?><pub-id pub-id-type="pmid">23261652</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khokher</surname><given-names>M. R.</given-names></name><name><surname>Ghafoor</surname><given-names>A.</given-names></name><name><surname>Siddiqui</surname><given-names>A. M.</given-names></name></person-group> (<year>2013</year>). <article-title>Image segmentation using multilevel graph cuts and graph development using fuzzy rule-based system</article-title>. <source>Iet Image Process.</source>
<volume>7</volume>, <fpage>201</fpage>&#x02013;<lpage>211</lpage>. <pub-id pub-id-type="doi">10.1049/iet-ipr.2012.0082</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liao</surname><given-names>X.</given-names></name><name><surname>Yuan</surname><given-names>Z.</given-names></name><name><surname>Tong</surname><given-names>Q.</given-names></name><name><surname>Zhao</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>Q.</given-names></name></person-group> (<year>2017</year>). <article-title>Adaptive localised region and edge-based active contour model using shape constraint and sub-global information for uterine fibroid segmentation in ultrasound-guided hifu therapy</article-title>. <source>Iet Image Process.</source>
<volume>11</volume>, <fpage>1142</fpage>&#x02013;<lpage>1151</lpage>. <pub-id pub-id-type="doi">10.1049/iet-ipr.2016.0651</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linares</surname><given-names>O. A. C.</given-names></name><name><surname>Botelho</surname><given-names>G. M.</given-names></name><name><surname>Rodrigues</surname><given-names>F. A.</given-names></name><name><surname>Neto</surname><given-names>J. B.</given-names></name></person-group> (<year>2017</year>). <article-title>Segmentation of large images based on super-pixels and community detection in graphs</article-title>. <source>Iet Image Process.</source>
<volume>11</volume>, <fpage>1219</fpage>&#x02013;<lpage>1228</lpage>. <pub-id pub-id-type="doi">10.1049/iet-ipr.2016.0072</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lizarazo</surname><given-names>I.</given-names></name></person-group> (<year>2008</year>). <article-title>Svm-based segmentation and classification of remotely sensed data</article-title>. <source>Int J Remote Sens.</source>
<volume>29</volume>, <fpage>7277</fpage>&#x02013;<lpage>7283</lpage>. <pub-id pub-id-type="doi">10.1080/01431160802326081</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lizarazo</surname><given-names>I.</given-names></name><name><surname>Elsner</surname><given-names>P.</given-names></name></person-group> (<year>2009</year>). <article-title>Fuzzy segmentation for object-based image classification</article-title>. <source>Int. J. Remote Sens.</source>
<volume>30</volume>, <fpage>1643</fpage>&#x02013;<lpage>1649</lpage>. <pub-id pub-id-type="doi">10.1080/01431160802460062</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mussavi</surname><given-names>S. H.</given-names></name><collab>M. Sc. of Agronomy Ramin Agricultural Natural Resources</collab></person-group> (<year>2011</year>). <article-title>Effect of seed density and molinit rates on barnyardgrass (Echinochloa crus-galli) control in direct-seeded rice in ahwaz</article-title>. <source>Agron, J</source>. <volume>90</volume>, <fpage>83</fpage>&#x02013;<lpage>92</lpage>.</mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naemura</surname><given-names>M.</given-names></name><name><surname>Fukuda</surname><given-names>A.</given-names></name><name><surname>Mizutani</surname><given-names>Y.</given-names></name><name><surname>Izumi</surname><given-names>Y.</given-names></name></person-group> (<year>2000</year>). <article-title>Morphological segmentation of sport scenes using color information</article-title>. <source>IEEE Trans. Broadcast.</source>
<volume>46</volume>, <fpage>181</fpage>&#x02013;<lpage>188</lpage>. <pub-id pub-id-type="doi">10.1109/11.892154</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nerson</surname><given-names>H.</given-names></name></person-group> (<year>1980</year>). <article-title>Effects of population density and number of ears on wheat yield and its components</article-title>. <source>Field Crops Res</source>. <volume>3</volume>, <fpage>225</fpage>&#x02013;<lpage>234</lpage>. <pub-id pub-id-type="doi">10.1016/0378-4290(80)90031-3</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>S.</given-names></name><name><surname>Lee</surname><given-names>H. S.</given-names></name><name><surname>Kim</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). <article-title>Seed growing for interactive image segmentation using svm classification with geodesic distance</article-title>. <source>Electron Lett.</source>
<volume>53</volume>, <fpage>22</fpage>&#x02013;<lpage>24</lpage>. <pub-id pub-id-type="doi">10.1049/el.2016.3919</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>X.</given-names></name><name><surname>Xu</surname><given-names>D.</given-names></name><name><surname>Kong</surname><given-names>L.</given-names></name><name><surname>Chen</surname><given-names>D.</given-names></name></person-group> (<year>2016</year>). <article-title>L1-norm loss based twin support vector machine for data recognition</article-title>. <source>Inform. Sci. Int. J.</source>
<volume>340</volume>, <fpage>86</fpage>&#x02013;<lpage>103</lpage>. <pub-id pub-id-type="doi">10.1016/j.ins.2016.01.023</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Plovdiv</surname><given-names>D. G.</given-names></name></person-group> (<year>2013</year>). <article-title>Correlations between the main structural elements of yield in common wheat cultivars</article-title>. <source>Plant Sci</source>. <volume>36</volume>, <fpage>4131</fpage>&#x02013;<lpage>4143</lpage>. <pub-id pub-id-type="doi">10.1080/01431161.2015.1071897</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roy</surname><given-names>K.</given-names></name><name><surname>Mukherjee</surname><given-names>J.</given-names></name></person-group> (<year>2013</year>). <article-title>Image similarity measure using color histogram, color coherence vector, and sobel method</article-title>. <source>Int. J. Sci. Res</source>. <volume>2</volume>, <fpage>538</fpage>&#x02013;<lpage>543</lpage>.</mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seeland</surname><given-names>M.</given-names></name><name><surname>Rzanny</surname><given-names>M.</given-names></name><name><surname>Alaqraa</surname><given-names>N.</given-names></name><name><surname>W&#x000e4;ldchen</surname><given-names>J.</given-names></name><name><surname>M&#x000e4;der</surname><given-names>P.</given-names></name></person-group> (<year>2017</year>). <article-title>Plant species classification using flower images-a comparative study of local feature representations</article-title>. <source>PLoS ONE</source>
<volume>12</volume>:<fpage>e0170629</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0170629</pub-id><?supplied-pmid 28234999?><pub-id pub-id-type="pmid">28234999</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soh</surname><given-names>L. K.</given-names></name><name><surname>Tsatsoulis</surname><given-names>C.</given-names></name></person-group> (<year>1999</year>). <article-title>Unsupervised segmentation of ers and radarsat sea ice images using multiresolution peak detection and aggregated population equalization</article-title>. <source>Int. J. Remote Sens.</source>
<volume>20</volume>, <fpage>3087</fpage>&#x02013;<lpage>3109</lpage>. <pub-id pub-id-type="doi">10.1080/014311699211633</pub-id></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>X.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Han</surname><given-names>Y.</given-names></name><name><surname>Jiang</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). <article-title>Semi-supervised feature selection via hierarchical regression for web image classification</article-title>. <source>Multimed. Syst.</source>
<volume>22</volume>, <fpage>41</fpage>&#x02013;<lpage>49</lpage>. <pub-id pub-id-type="doi">10.1007/s00530-014-0390-0</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Twining</surname><given-names>C. J.</given-names></name><name><surname>Taylor</surname><given-names>C. J.</given-names></name></person-group> (<year>2003</year>). <article-title>The use of kernel principal component analysis to model data distributions</article-title>. <source>Patt. Recognit.</source>
<volume>22</volume>, <fpage>41</fpage>&#x02013;<lpage>49</lpage>. <pub-id pub-id-type="doi">10.1016/S0031-3203(02)00051-1</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van de Sande</surname><given-names>K. E.</given-names></name><name><surname>Gevers</surname><given-names>T.</given-names></name><name><surname>Snoek</surname><given-names>C.</given-names></name></person-group> (<year>2010</year>). <article-title>Evaluating color descriptors for object and scene recognition</article-title>. <source>IEEE Trans. Patt. Anal. Mach. Intell.</source>
<volume>32</volume>, <fpage>1582</fpage>&#x02013;<lpage>1596</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2009.154</pub-id><?supplied-pmid 20634554?><pub-id pub-id-type="pmid">20634554</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varish</surname><given-names>N.</given-names></name><name><surname>Pal</surname><given-names>A. K.</given-names></name></person-group> (<year>2018</year>). <article-title>A novel image retrieval scheme using gray level co-occurrence matrix descriptors of discrete cosine transform based residual image</article-title>. <source>Appl. Intell.</source>
<volume>12</volume>, <fpage>1</fpage>&#x02013;<lpage>24</lpage>. <pub-id pub-id-type="doi">10.1007/s10489-017-1125-7</pub-id></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiong</surname><given-names>X.</given-names></name><name><surname>Duan</surname><given-names>L.</given-names></name><name><surname>Liu</surname><given-names>L.</given-names></name><name><surname>Tu</surname><given-names>H.</given-names></name><name><surname>Yang</surname><given-names>P.</given-names></name><name><surname>Wu</surname><given-names>D.</given-names></name></person-group> (<year>2017</year>). <article-title>Panicle-seg: a robust image segmentation method for rice panicles in the field based on deep learning and superpixel optimization</article-title>. <source>Plant Methods</source>
<volume>13</volume>:<fpage>104</fpage>. <pub-id pub-id-type="doi">10.1186/s13007-017-0254-7</pub-id><?supplied-pmid 29209408?><pub-id pub-id-type="pmid">29209408</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>W.</given-names></name><name><surname>Sun</surname><given-names>C.</given-names></name><name><surname>Lei</surname><given-names>Z.</given-names></name></person-group> (<year>2011</year>). <article-title>A multi-manifold discriminant analysis method for image feature extraction</article-title>. <source>Patt. Recognit.</source>
<volume>44</volume>, <fpage>1649</fpage>&#x02013;<lpage>1657</lpage>. <pub-id pub-id-type="doi">10.1016/j.patcog.2011.01.019</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H.</given-names></name><name><surname>Turner</surname><given-names>N. C.</given-names></name><name><surname>Poole</surname><given-names>M. L.</given-names></name><name><surname>Asseng</surname><given-names>S.</given-names></name></person-group> (<year>2007</year>). <article-title>High ear number is key to achieving high wheat yields in the high-rainfall zone of south-western australia</article-title>. <source>Crop Pasture Sci.</source>
<volume>58</volume>, <fpage>21</fpage>&#x02013;<lpage>27</lpage>. <pub-id pub-id-type="doi">10.1071/AR05170</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H.</given-names></name><name><surname>Wu</surname><given-names>Q. M. J.</given-names></name><name><surname>Zheng</surname><given-names>Y.</given-names></name><name><surname>Nguyen</surname><given-names>T. M.</given-names></name></person-group> (<year>2014</year>). <article-title>Effective fuzzy clustering algorithm with bayesian model and mean template for image segmentation</article-title>. <source>Iet Image Process.</source>
<volume>8</volume>, <fpage>571</fpage>&#x02013;<lpage>581</lpage>. <pub-id pub-id-type="doi">10.1049/iet-ipr.2013.0178</pub-id></mixed-citation></ref></ref-list></back></article>