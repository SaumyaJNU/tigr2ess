<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName journalpublishing.dtd?><?SourceDTD.Version 2.3?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">G3 (Bethesda)</journal-id><journal-id journal-id-type="iso-abbrev">Genetics</journal-id><journal-id journal-id-type="hwp">G3: Genes, Genomes, Genetics</journal-id><journal-id journal-id-type="pmc">G3: Genes, Genomes, Genetics</journal-id><journal-id journal-id-type="publisher-id">G3: Genes, Genomes, Genetics</journal-id><journal-title-group><journal-title>G3: Genes|Genomes|Genetics</journal-title></journal-title-group><issn pub-type="epub">2160-1836</issn><publisher><publisher-name>Genetics Society of America</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6288841</article-id><article-id pub-id-type="publisher-id">GGG_200740</article-id><article-id pub-id-type="doi">10.1534/g3.118.200740</article-id><article-categories><subj-group subj-group-type="heading"><subject>Genomic Prediction</subject></subj-group></article-categories><title-group><article-title>Multi-environment Genomic Prediction of Plant Traits Using Deep Learners With Dense Architecture</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Montesinos-L&#x000f3;pez</surname><given-names>Abelardo</given-names></name><xref ref-type="aff" rid="aff1">*</xref></contrib><contrib contrib-type="author"><name><surname>Montesinos-L&#x000f3;pez</surname><given-names>Osval A.</given-names></name><xref ref-type="aff" rid="aff2"><sup>&#x02020;</sup></xref><xref ref-type="corresp" rid="cor1"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Gianola</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="aff3"><sup>&#x02021;</sup></xref></contrib><contrib contrib-type="author"><name><surname>Crossa</surname><given-names>Jos&#x000e9;</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9429-5855</contrib-id><xref ref-type="aff" rid="aff4"><sup>&#x000a7;</sup></xref><xref ref-type="corresp" rid="cor1"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Hern&#x000e1;ndez-Su&#x000e1;rez</surname><given-names>Carlos M.</given-names></name><xref ref-type="aff" rid="aff5">**</xref></contrib><aff id="aff1"><label>*</label>Departamento de Matem&#x000e1;ticas, Centro Universitario de Ciencias Exactas e Ingenier&#x000ed;as (CUCEI), Universidad de Guadalajara, 44430, Guadalajara, Jalisco, M&#x000e9;xico</aff><aff id="aff2"><label>&#x02020;</label>Facultad de Telem&#x000e1;tica, Universidad de Colima, 28040, Colima, M&#x000e9;xico</aff><aff id="aff3"><label>&#x02021;</label>Departments of Animal Sciences, Dairy Science, and Biostatistics and Medical Informatics, University of Wisconsin-Madison, 53706, Madison, Wisconsin</aff><aff id="aff4"><label>&#x000a7;</label>International Maize and Wheat Improvement Center (CIMMYT), Apdo. Postal 6-641, 06600, Ciudad de M&#x000e9;xico, M&#x000e9;xico</aff><aff id="aff5"><label>**</label>Facultad de Ciencias, Universidad de Colima, 28040, Colima, Colima, M&#x000e9;xico</aff></contrib-group><author-notes><corresp id="cor1"><label>1</label>Corresponding Authors: Facultad de Telem&#x000e1;tica, Universidad de Colima, 2804, Colima, M&#x000e9;xico. E-mail: <email>oamontes1@ucol.mx</email>; and Biometrics and Statistics Unit, International Maize and Wheat Improvement Center (CIMMYT), Apdo. Postal 6-641, Apdo. Postal 6-641, 06600 M&#x000e9;xico City, M&#x000e9;xico. E-mail: <email>j.crossa@cgiar.org</email>.</corresp></author-notes><pub-date pub-type="epub"><day>28</day><month>9</month><year>2018</year></pub-date><pub-date pub-type="collection"><month>12</month><year>2018</year></pub-date><volume>8</volume><issue>12</issue><fpage>3813</fpage><lpage>3828</lpage><history><date date-type="received"><day>23</day><month>7</month><year>2018</year></date><date date-type="accepted"><day>26</day><month>9</month><year>2018</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2018 Montesinos-Lopez <italic>et al.</italic></copyright-statement><copyright-year>2018</copyright-year><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:title="pdf" xlink:type="simple" xlink:href="3813.pdf"/><abstract><p>Genomic selection is revolutionizing plant breeding and therefore methods that improve prediction accuracy are useful. For this reason, active research is being conducted to build and test methods from other areas and adapt them to the context of genomic selection. In this paper we explore the novel deep learning (DL) methodology in the context of genomic selection. We compared DL methods with densely connected network architecture to one of the most often used genome-enabled prediction models: Genomic Best Linear Unbiased Prediction (GBLUP). We used nine published real genomic data sets to compare a fraction of all possible deep learning models to obtain a &#x0201c;meta picture&#x0201d; of the performance of DL methods with densely connected network architecture. In general, the best predictions were obtained with the GBLUP model when genotype&#x000d7;environment interaction (G&#x000d7;E) was taken into account (8 out of 9 data sets); when the interactions were ignored, the DL method was better than the GBLUP in terms of prediction accuracy in 6 out of the 9 data sets. For this reason, we believe that DL should be added to the data science toolkit of scientists working on animal and plant breeding. This study corroborates the view that there are no universally best prediction machines.</p></abstract><kwd-group><kwd>GBLUP</kwd><kwd>deep learning</kwd><kwd>neural network</kwd><kwd>genomic prediction</kwd><kwd>prediction accuracy</kwd><kwd>GenPred</kwd><kwd>Shared Data Resources</kwd></kwd-group><counts><fig-count count="10"/><table-count count="1"/><equation-count count="1"/><ref-count count="43"/><page-count count="16"/></counts></article-meta></front><body><p>It is important to use new technologies to increase food production, given that the world population will reach 10.4 billion by 2067, with 81% residing in Africa or Asia. Due to the increase in population, there will be a decrease of 0.15 ha per person in the arable land available for food production. Further, temperature is expected to increase in tropical and temperate zones, especially in the Northern Hemisphere, which will push growing seasons and farming areas away from arid areas into more northern latitudes (<xref rid="bib4" ref-type="bibr">Britt <italic>et al.</italic>, 2018</xref>). Under these scenarios, increasing world food production is a challenge. Genomic selection is a promising development in agriculture that aims to improve production by exploiting molecular genetic markers to design novel breeding programs and develop marker-based methods for genetic evaluation of plants and animals (<xref rid="bib18" ref-type="bibr">Jonas and de Koning 2015</xref>; <xref rid="bib17" ref-type="bibr">Hickey <italic>et al.</italic>, 2017</xref>).</p><p>Genomic selection (GS) is a type of marker-assisted selection that uses dense molecular markers from the entire genome simultaneously in a linear regression model (<xref rid="bib30" ref-type="bibr">Meuwissen <italic>et al.</italic>, 2001</xref>). A predictive model using individuals with known genotypic and phenotypic information is then constructed. With this model, genomic estimated breeding values (GEBVs) for the desired trait are calculated and used to rank individuals with unknown phenotypes for subsequent selection. The accuracy of the predictions is evaluated using some form of cross-validation. Originally proposed in animal breeding, this method has revolutionized and transformed breeding programs worldwide, and is being implemented in most developed nations. The fast growing popularity of GS can be attributed to a continuous reduction in the cost of obtaining large numbers of DNA markers of plant or animal genomes, and to the empirical evidence that this approach indeed improves genetic gains per unit of time, facilitating the rapid selection of superior genotypes and accelerating the breeding cycles (<xref rid="bib43" ref-type="bibr">Weller <italic>et al.</italic>, 2017</xref>).</p><p>For these reasons, genomic selection is being implemented by commercial companies and national breeding programs of maize and wheat (<xref rid="bib8" ref-type="bibr">Crossa <italic>et al.</italic>, 2017</xref>), cassava (<xref rid="bib44" ref-type="bibr">Wolfe <italic>et al.</italic>, 2017</xref>), oil palm (<xref rid="bib21" ref-type="bibr">Kwong <italic>et al.</italic>, 2017</xref>), and macadamia (<xref rid="bib34" ref-type="bibr">O&#x02019;Connor <italic>et al.</italic>, 2018</xref>), among others. The goal of most breeding programs is to predict the genetic merit of unphenotyped individuals and thus enable targeted combinations of desired alleles to improve the performance of the next generation(s). However, to effectively implement GS in crop breeding also requires prediction models that can improve prediction accuracy in large-scale data sets and are robust across trait-environment combinations. Prediction models often perform poorly for some trait-environment combinations, so the search for better genomic prediction models is an active area of research.</p><p>Machine learning (ML) is a field of computer science that uses statistical techniques to give computer systems the ability to &#x0201c;learn&#x0201d; (<italic>i.e.</italic>, progressively improve performance on a specific task) from data, without being explicitly programmed to do this (<xref rid="bib39" ref-type="bibr">Samuel 1959</xref>). ML is closely related to (and often overlaps with) computational statistics, which also focuses on making predictions through the use of computers. In general, ML explores algorithms that can learn from current data and make predictions on new data, by building a model from sample inputs (<xref rid="bib39" ref-type="bibr">Samuel 1959</xref>). The fields of statistics and ML have some goals in common and will continue to come closer together in the future. Although applications of ML in genomic selection (<xref rid="bib14" ref-type="bibr">Gonz&#x000e1;lez-Camacho <italic>et al.</italic>, 2012</xref>) exist, application of DL methods in genomic prediction is lacking.</p><p>This paper evaluates prediction accuracy in the context of genomic selection of Deep Learning (DL) methods with a densely connected network architecture, which is a type of ML algorithm that uses an artificial neural network with multiple layers linked nonlinearly. The &#x0201c;deep&#x0201d; in DL refers to the number of layers through which the data are transformed. The layers in these methods consist of multiple stages of nonlinear data transformations, where features of the data are represented by successively higher and more abstract layers. The goal of a DL method is either to predict or to classify a response variable using inputs. Traditional linear regression models are not considered deep because they do not apply multiple layers of non-linear transformations to the data. The prediction performance of DL methods has proved to be similar or better than that of traditional methods in many areas like health care, image processing, natural language processing, speech recognition, military target recognition, marketing, investment portfolio management, financial fraud detection, stock market forecasting, optical character recognition and traffic sign classification (<xref rid="bib12" ref-type="bibr">Deng and Yu 2013</xref>). Also, companies such as Microsoft, Google, IBM, Yahoo, Twitter, Baidu, Paypal and Facebook are exploiting DL methods to understand consumers (<xref rid="bib12" ref-type="bibr">Deng and Yu 2013</xref>).</p><p>There have been successful applications of DL in the biological sciences. For example, <xref rid="bib29" ref-type="bibr">Menden <italic>et al.</italic> (2013)</xref> applied a DL method to predict the viability of a cancer cell line exposed to a drug. <xref rid="bib1" ref-type="bibr">Alipanahi <italic>et al.</italic> (2015)</xref> used DL with a convolutional network architecture to predict specificities of DNA- and RNA-binding proteins. <xref rid="bib41" ref-type="bibr">Tavanaei <italic>et al.</italic> (2017)</xref> used a DL method for predicting tumor suppressor genes and oncogenes. DL methods have also made accurate predictions of single-cell DNA methylation states (<xref rid="bib2" ref-type="bibr">Angermueller <italic>et al.</italic>, 2017</xref>). In the area of genomic selection, we found two reports only: (a) <xref rid="bib28" ref-type="bibr">McDowell and Grant (2016)</xref> found that DL methods performed similarly to several Bayesian and linear regression techniques that are commonly employed for phenotype prediction and genomic selection in plant breeding; (b) <xref rid="bib27" ref-type="bibr">Ma <italic>et al.</italic> (2017)</xref> also used a DL method with a convolutional neural network architecture to predict phenotypes from genotypes in wheat and found that the DL method outperformed the GBLUP method.</p><p>In this study we examine a DL method with a densely connected network architecture in the context of GS in plants to have a better idea of its prediction performance. We compare the DL method with GBLUP, the most widely used method. Our study involved 9 multi-environment real data sets used in genomic selection of wheat and maize breeding programs. The data sets comprise a large number of wheat and maize lines with several traits that were measured in several environments.</p><sec sec-type="materials|methods" id="s1"><title>Materials and Methods</title><sec id="s2"><title>Model implementation</title><sec id="s3"><title>Multiple-environment Genomic best linear unbiased predictor (GBLUP) model:</title><p>Since genotype<inline-formula><mml:math id="me1"><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:math></inline-formula>environment interaction is of paramount importance in plant breeding, the following univariate linear mixed model is often used for each trait:<disp-formula id="eq1"><mml:math id="me2"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><label>(1)</label></disp-formula>where <inline-formula><mml:math id="me3"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the response of the <inline-formula><mml:math id="me4"><mml:mi>j</mml:mi></mml:math></inline-formula>th line in the <inline-formula><mml:math id="me5"><mml:mi>i</mml:mi></mml:math></inline-formula>th environment (<inline-formula><mml:math id="me6"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>...</mml:mo><mml:mo>,</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="me7"><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>...</mml:mo><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="me8"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:math></inline-formula>represents the fixed effect of the <inline-formula><mml:math id="me9"><mml:mi>i</mml:mi></mml:math></inline-formula>th environment, <inline-formula><mml:math id="me10"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:math></inline-formula> represents the random genomic effect of the <inline-formula><mml:math id="me11"><mml:mi>j</mml:mi></mml:math></inline-formula>th line, with <inline-formula><mml:math id="me12"><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>...</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>J</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>&#x0223c;</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mi mathvariant="bold-italic">&#x000a0;</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">G</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="me13"><mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> is a genomic variance and <inline-formula><mml:math id="me14"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">G</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is of order <inline-formula><mml:math id="me15"><mml:mrow><mml:mi>J</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>J</mml:mi></mml:mrow></mml:math></inline-formula>, represents the genomic relationship matrix (GRM) and is calculated (<xref rid="bib42" ref-type="bibr">VanRaden 2008</xref>) as <inline-formula><mml:math id="me16"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">G</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mi>p</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="me17"><mml:mi>p</mml:mi></mml:math></inline-formula> denotes the number of markers and <inline-formula><mml:math id="me18"><mml:mi mathvariant="bold-italic">W</mml:mi></mml:math></inline-formula> is the matrix of markers of order <inline-formula><mml:math id="me19"><mml:mrow><mml:mi>J</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula>. The <inline-formula><mml:math id="me20"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">G</mml:mi><mml:mi mathvariant="bold-italic">g</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> matrix is constructed using the observed similarity at the genomic level between lines, rather than the expected similarity based on pedigree. Further, <inline-formula><mml:math id="me21"><mml:mrow><mml:mi>g</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the random interaction term between the genomic effect of the <inline-formula><mml:math id="me22"><mml:mi>j</mml:mi></mml:math></inline-formula>th line and the <inline-formula><mml:math id="me23"><mml:mi>i</mml:mi></mml:math></inline-formula>th environment; let <inline-formula><mml:math id="me24"><mml:mrow><mml:mi mathvariant="bold-italic">gE</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>...</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>J</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>&#x0223c;</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mi mathvariant="bold-italic">&#x000a0;</mml:mi><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>I</mml:mi></mml:mstyle><mml:mi>I</mml:mi></mml:msub><mml:mo>&#x02297;</mml:mo><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="me25"><mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> is an interaction variance, and <inline-formula><mml:math id="me26"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a random residual associated with the <inline-formula><mml:math id="me27"><mml:mi>j</mml:mi></mml:math></inline-formula>th line in the <inline-formula><mml:math id="me28"><mml:mi>i</mml:mi></mml:math></inline-formula>th environment distributed as <inline-formula><mml:math id="me29"><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="me30"><mml:mrow><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is the residual variance.</p></sec><sec id="s4"><title>Deep learning model:</title><p>Popular neural network architectures are: (a) densely connected networks, (b) convolutional networks, and (c) recurrent networks. Details on each type of network, its assumptions and input characteristics can be found in <xref rid="bib16" ref-type="bibr">Gulli and Sujit (2017)</xref>, <xref rid="bib5" ref-type="bibr">Chollet and Allaire (2017)</xref> and <xref rid="bib3" ref-type="bibr">Angermueller <italic>et al.</italic> (2016)</xref>. In this study we implemented type (a), which is a typical feedforward neural network also known as multilayer perceptron, which does not assume a specific structure in the input features (<xref rid="bib15" ref-type="bibr">Goodfellow <italic>et al.</italic>, 2016</xref>). In general, the basic structure of a densely connected network consists of an input layer, an output layer and multiple hidden layers between the input and output layers. Neurons (units) are connected in the network; the strength of the connection between neurons is called weight. The weight values of the connections between the layers are how neural networks encode the learned information extracted from the raw training data. The input layer neurons correspond to the number of features (called independent variables by the statistics community) you wish to feed into the neural network. The hidden layer neurons are generally used to perform non-linear transformation of the original input attributes (<xref rid="bib22" ref-type="bibr">Lewis 2016</xref>). The number of output neurons corresponds to the number of response variables (traits in plant breeding) you wish to predict or classify and they receive as input the output of hidden neurons and produce as output the prediction values of interest (<xref rid="bib15" ref-type="bibr">Goodfellow <italic>et al.</italic>, 2016</xref>).</p><p>The layer is the core building block of a neural network; it is a data-processing step that we can think of as a filter for data, since the data that go in are transformed and come out in a more useful form. Specific layers extract representations out of the data, which are fed into representations that are more meaningful for the problem at hand. Most DL methods consist of joining together simple layers that will implement a form of progressive data distillation (<xref rid="bib5" ref-type="bibr">Chollet and Allaire 2017</xref>).</p><p>In training neural networks, one epoch means one pass (forward and backward) of the full training set through the neural network. Since one epoch is too big to feed into the computer at one time, we divide it into several smaller batches. A batch consists of a number of training samples in one forward/backward pass. The larger the batch size, the more memory is needed to run the model. For example, suppose you had a batch size of 500, with 1000 training samples. It will take only two iterations to complete one epoch. An iteration is the number of batches needed to complete one epoch. We used more than one epoch because too few epochs lead to underfitting of DL models. Therefore, as the number of epochs increases, the weights are changed in the neural network and the DL model goes from underfitting to optimal fitting or to overfitting. Unfortunately, the right number of epochs is data dependent.</p><p>Also, due to the sensitivity of DL models to overfitting, constraints are put on the complexity of a neural network by forcing its weights to take on only small values, which makes the distribution of weight values more regular. This is called weight regularization, and it is done by adding to the loss function of the network a cost (penalty) associated with having large weights. There are many types of regularization but in this paper we implemented dropout regularization, which consists of temporarily removing a random subset (%) of neurons with their connections during training. This means that their contribution to the activation of downstream neurons is temporarily removed on the forward pass and any weight updates are not applied to the neurons on the backward pass. In other words, dropout consists of randomly dropping out (setting to zero) a number of output features of the layer during training. Unfortunately, choosing the optimal values for each of these hyperparameters is challenging; the process of choosing these values is art and science.</p><sec id="s5"><title>Model selection in DL.</title><p>Hyperparameters govern many aspects of the behavior of DL models, such as their ability to learn features from data, the models&#x02019; exhibited degree of generalizability in performance when presented with new data, as well as the time and memory cost of training the model, since different hyperparameters often result in models with significantly different performance. This means that tuning hyperparameter values is a critical aspect of the model training process and a key element for the quality of the resulting prediction accuracies. However, in DL models, making a good choice of the number of layers, number of units (neurons), number of epochs, type of regularization penalty, type of activation function, among others is challenging.</p><p>Manual tuning of DL models is of course possible, but relies heavily on the user&#x02019;s expertise and understanding of the underlying problem. Additionally, due to factors such as time-consuming model evaluations, non-linear hyperparameter interactions in the case of large models, and tens or even hundreds of hyperparameters, manual tuning may not be feasible. For this reason, the four most common approaches for hyperparameter tuning reported in the literature are: (a) grid search, (b) random search, (c) Latin hypercube sampling, and (d) optimization (<xref rid="bib19" ref-type="bibr">Koch <italic>et al.</italic>, 2017</xref>). In the grid search method, each hyperparameter of interest is discretized into a desired set of values to be studied, and models are trained and assessed for all combinations of the values across all hyperparameters (that is, a &#x0201c;grid&#x0201d;). Although fairly simple and straightforward to carry out, a grid search is quite costly because the expense grows exponentially with the number of hyperparameters and the number of discrete levels of each.</p><p>A random search differs from a grid search in that we no longer provide a discrete set of values to explore for each hyperparameter; rather, we provide a statistical distribution for each hyperparameter from which values may be randomly sampled. This allows a much greater chance of finding effective values for each hyperparameter. While Latin hypercube sampling is similar to the previous method, it is a more structured approach because it uses a random Latin hypercube sample (LHS) (<xref rid="bib26" ref-type="bibr">McKay 1992</xref>), an experimental design in which samples are exactly uniform across each hyperparameter but random in combinations. These so-called low-discrepancy point sets attempt to ensure that points are approximately equidistant from one another in order to fill the space efficiently. This sampling allows for coverage across the entire range of each hyperparameter and is more likely to find good values of each hyperparameter.</p><p>The previous two methods for hyperparameter tuning perform individual experiments by building models with various hyperparameter values and recording the model performance for each. Because each experiment is performed in isolation, this process is parallelized, but is unable to use the information from one experiment to improve the next experiment. Optimization methods, on the other hand, consist of sequential model-based optimization that allows using the results of previous experiments to improve the sampling method of the next experiment. These methods are designed to make intelligent use of fewer evaluations and thus save on the overall computation time (<xref rid="bib19" ref-type="bibr">Koch <italic>et al.</italic>, 2017</xref>). Optimization algorithms that have been used in machine learning generally for hyperparameter tuning include Broyden-Fletcher-Goldfarb-Shanno (BFGS) (<xref rid="bib20" ref-type="bibr">Konen <italic>et al.</italic>, 2011</xref>), covariance matrix adaptation evolution strategy (CMA-ES) (<xref rid="bib20" ref-type="bibr">Konen <italic>et al.</italic>, 2011</xref>), particle swarm (PS) (<xref rid="bib37" ref-type="bibr">Renukadevi and Thangaraj 2014</xref>), tabu search (TS), genetic algorithms (GA) (<xref rid="bib24" ref-type="bibr">Lorena and de Carvalho 2008</xref>), and more recently, surrogate-based Bayesian optimization (<xref rid="bib11" ref-type="bibr">Dewancker <italic>et al.</italic>, 2016</xref>). Also, recently the use of the surface response methodology has been explored for tuning hyperparameters in random forest models (<xref rid="bib25" ref-type="bibr">Lujan-Moreno <italic>et al.</italic>, 2018</xref>). However, the implementation of these optimization methods is not straightforward because it requires expensive computation; also, software development is required for implementing these algorithms automatically. There have been advances in this direction for some machine learning algorithms in the statistical analysis system (SAS) software (<xref rid="bib19" ref-type="bibr">Koch <italic>et al.</italic>, 2017</xref>). An additional challenge is the unpredictable computation expense of training and validating predictive models using different hyperparameter values. Finally, although it is challenging, the tuning process often leads to hyperparameter settings that are better than the default values, since it provides a heuristic validation of these settings, giving greater assurance that a model configuration that has higher accuracy has not been overlooked.</p></sec><sec id="s6"><title>Real data sets.</title><p>Three maize and six wheat data sets were analyzed.</p></sec></sec><sec id="s7"><title>Maize data sets 1-3:</title><p>These three data sets are made up of a total of 309 maize lines which were used by Crossa <italic>et al.</italic> (2013) and <xref rid="bib32" ref-type="bibr">Montesinos-L&#x000f3;pez <italic>et al.</italic> (2016</xref>, <xref rid="bib31" ref-type="bibr">2017</xref>). Traits evaluated were grain yield (GY; data set 1), anthesis-silking interval (ASI; data set 2), and plant height (PH; data set 3); each of these traits was measured in three environments (Env1, Env2, and Env3). Phenotypes of each trait were pre-analyzed and adjusted for the experimental field design. The number of single nucleotide polymorphisms (SNP), after filtering for missing values and minor allele frequency, was 158,281.</p></sec><sec id="s8"><title>Wheat data sets 4-6:</title><p>These three data sets were used by <xref rid="bib23" ref-type="bibr">L&#x000f3;pez-Cruz <italic>et al.</italic> (2015)</xref> and <xref rid="bib9" ref-type="bibr">Cuevas <italic>et al.</italic> (2016)</xref>. The phenotypes in the three data sets are grain yield (GY, tons/hectare) adjusted for the experimental design. The data sets came from CIMMYT and were obtained from its wheat breeding station at Cd. Obregon, Sonora, Mexico. The environments were three irrigation regimes (moderate drought stress, optimal irrigation, and drought stress), two planting systems (bed and flat planting), and two different planting dates (normal and late). Wheat data set 4 had 693 wheat lines evaluated in four environments; wheat data set 5 included 670 wheat lines evaluated in four environments, and wheat data set 6 had 807 lines evaluated in five environments. Genotypes were derived using genotype by sequencing (GBS) technology; in all the analyses we used 15,744 GBS markers that resulted after quality control.</p></sec><sec id="s9"><title>Wheat data sets 7-8:</title><p>These two wheat data sets came from a total of 250 wheat lines that were extracted from a large set of 39 yield trials grown during the 2013-2014 crop season in Ciudad Obregon, Sonora, Mexico (<xref rid="bib38" ref-type="bibr">Rutkoski <italic>et al.</italic>, 2016</xref>). The traits measured were: (1) plant height (PH) recorded in centimeters (data set 7), and (2) days to heading (DTHD) recorded as the number of days from germination until 50% of spikes had emerged in each plot (data set 8), in the first replicate of each trial. Phenotypes were adjusted by experimental design as well. The genomic information was obtained by GBS and we used a total of 12,083 markers that remained after quality control.</p></sec><sec id="s10"><title>Wheat Iranian data set 9:</title><p>This data set was used in <xref rid="bib7" ref-type="bibr">Crossa <italic>et al.</italic> (2016)</xref>, where full details are presented. It consists of 2374 wheat lines evaluated in a drought environment (D) and a heat environment (H) at the CIMMYT experiment station near Ciudad Obreg&#x000f3;n, Sonora, Mexico (27 &#x000b0; 20 &#x02019;N, 109 &#x000b0; 54 &#x02019;W, 38 meters above sea level), during the 2010-2011 cycle. The measured trait was days to maturity (DTM). The number of markers used was 39,758 that remained after the quality control process from a total of 40,000 markers.</p><sec id="s11"><title>Method implementation.</title><p>The GBLUP method was implemented with the BGLR package (<xref rid="bib13" ref-type="bibr">de los Campos and P&#x000e9;rez-Rodr&#x000ed;guez 2014</xref>) in the R statistical software (<xref rid="bib35" ref-type="bibr">R Core Team 2018</xref>). DL methods were fitted with the Keras package (<xref rid="bib16" ref-type="bibr">Gulli and Sujit 2017</xref>; <xref rid="bib5" ref-type="bibr">Chollet and Allaire 2017</xref>) with a densely connected network architecture also in the R statistical software. In both GBLUP and DL, we used two different sets of covariates: the first set was composed of information on environments and genomes (that takes into account genomic information), while the second set of covariates included genotype<inline-formula><mml:math id="me31"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>environment interaction (G&#x000d7;E) information as well. It is important to point out that marker information was not included directly as covariates in both models (DL and GBLUP) since information on markers was included in the design matrix of genotypes and G&#x000d7;E through Cholesky decomposition of the genomic relationship matrix (GRM) that was calculated with the marker information as mentioned above with the <xref rid="bib42" ref-type="bibr">VanRaden (2008)</xref> method. The GBLUP and DL models were compared with and without the G&#x000d7;E term. Since the DL method requires values of some tuning parameters, we first ran several DL scenarios by choosing as tuning parameters some values recommended in the DL literature. Based on such runs, we implemented the grid search method with a full factorial design with the following three factors: (a) number of units (U), (b) number of epochs (E), and (c) number of layers (L). For U we used 50, 60, 70, 80, 90 and 100; for E we used 20, 40, 60, 80 and 100; and for L we used 1, 2 and 3. Thus 6<inline-formula><mml:math id="me32"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>5<inline-formula><mml:math id="me33"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>3 = 90 experiments were run for each data set with a densely connected DL method. It is important to point out that the 90 DL experiments used dropout regularization, which is one of the most effective and commonly used regularization techniques in neural networks, developed by <xref rid="bib40" ref-type="bibr">Srivastava <italic>et al.</italic> (2014)</xref> at the University of Toronto. In our case, the dropout rate was fixed at 0.3 (30%); this meant that the percentage of features that were set to zero was 30% in each layer; this value was selected following the suggestions of <xref rid="bib16" ref-type="bibr">Gulli and Sujit (2017)</xref>, <xref rid="bib5" ref-type="bibr">Chollet and Allaire (2017)</xref> and <xref rid="bib40" ref-type="bibr">Srivastava <italic>et al.</italic> (2014)</xref>. Also, concerning the activation function we implemented in the deep layers and output layer the Rectified linear unit (Relu).</p></sec></sec><sec id="s12"><title>Cross-validation:</title><p>Prediction accuracy of both DL and GBLUP was evaluated with random cross-validation (CV): the whole data set was divided into a training (TRN) and a testing (TST) set. This cross-validation is the same as the so-called replicated TRN-TST in the publication of <xref rid="bib10" ref-type="bibr">Daetwyler <italic>et al.</italic> (2012)</xref> since some individuals can never be part of the training set. The percentages of the whole data set assigned to the TRN and TST sets were 65% and 35%, respectively. Our random CV used sampling with replacement, which means that one observation can appear in more than one partition. The design we implemented mimics a prediction problem faced by breeders in incomplete field trials where lines are evaluated in some, but not all, target environments. More explicitly, TRN-TST partitions were obtained as follows: since the total number of records per trait available for the data set with multi-environments is <inline-formula><mml:math id="me34"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>J</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula>, to select lines in the TST data set, we fixed the percentage of data to be used for TST (PTesting = 35%). Then we chose 0.35<inline-formula><mml:math id="me35"><mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> (lines) at random, and subsequently, one environment per line was randomly picked from <inline-formula><mml:math id="me36"><mml:mi>I</mml:mi></mml:math></inline-formula> environments. The resulting cells (<inline-formula><mml:math id="me37"><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> were assigned to the TST data set, while cells not selected through this algorithm were allocated to the TRN data set. Lines were sampled without replacement if <inline-formula><mml:math id="me38"><mml:mrow><mml:mi>J</mml:mi><mml:mo>&#x02265;</mml:mo><mml:mn>0.35</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>, and with replacement otherwise (<xref rid="bib23" ref-type="bibr">Lopez-Cruz <italic>et al.</italic>, 2015</xref>).</p><p>The cross-validation we just described is called the outer CV and was applied for both models. However, in the DL model, an inner CV strategy was also applied for tuning the hyperparameters using the grid of hyperparameter values defined above (90 experiments). The inner CV strategy consisted of splitting each training set of the outer CV, where 20% of data were assigned to testing-inner and 80% to training-inner.The training-inner data set was used to train the DL model using the grid of hyperparameter values. This inner CV strategy was facilitated by using the internal capabilities of Keras and the validation_split argument on the fit() function. The predictive power is assessed in the second part of the data set (testing-inner). With this, a set of best-fitting hyperparameters (the best combination of units, epochs and layers) from the inner CV loop is obtained. Finally, this set of hyperparameters was used to predict the performance of the independent testing data set (testing-outer). For each data set, 10 random outer CV partitions were implemented, and with the observed and predicted values of each testing-outer data sets, we calculated the average Pearson&#x02019;s correlation as a measure of prediction accuracy. It is important to point out that the outer cross-validation we implemented did not allow forward prediction because our TRN and TST sets were not separated across generational lines (<xref rid="bib10" ref-type="bibr">Daetwyler <italic>et al.</italic>, 2012</xref>). The accuracy reported in terms of Pearson&#x02019;s correlation was divided by the square root of the heritability of each trait-environment combination since heretabilities change in each trait-environment combination.</p><sec id="s13"><title>Data availability and software.</title><p>The phenotypic and genotypic data used in this study can be found in several articles (see the description of the data above). The readers can download the DataSets_DK.rar used in this study from the following link <ext-link ext-link-type="uri" xlink:href="http://hdl.handle.net/11529/10548082">hdl:11529/10548082</ext-link>. Furthermore, R codes for fitting the DL methods used in this study are given in the Appendix.</p></sec></sec></sec></sec><sec sec-type="results" id="s14"><title>Results</title><p>The results are given in 10 sections, one for each real data set plus one where all data sets are compared. In the first 9 sections, we provide a figure with the predictions disaggregated by environment obtained with the DL model and those of the GBLUP model. For the DL model, the predictions reported correspond to the best combination (in terms of epochs, layers and units) obtained from the grid search. Finally, in <xref ref-type="fig" rid="fig10">Figure 10</xref> we provide a meta-picture of the prediction performance of the 9 data sets, where the prediction performance of the best DL model is compared to that of the GBLUP model across environments in each data set.</p><sec id="s15"><title>Maize data set 1-trait GY</title><p><xref ref-type="fig" rid="fig1">Figure 1</xref> shows that the average Pearson&#x02019;s correlation (APC) prediction accuracies under the GBLUP method desegregated by environment when the G<inline-formula><mml:math id="me39"><mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula> term interaction was taken into account were: 0.394 for environment 1, 0.411 for environment 2 and 0.319 for environment 3. The predictions (<xref ref-type="fig" rid="fig1">Figure 1</xref>) under the DL method were: 0.382 for environment 1, 0.365 for environment 2 and 0.230 for environment 3. When the covariates corresponding to the G<inline-formula><mml:math id="me40"><mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula> interaction term were ignored in both methods, the APCs were 0.274 for environment 1, 0.272 for environment 2 and 0.323 for environment 3 under the GBLUP method. On the other hand, the predictions (<xref ref-type="fig" rid="fig1">Figure 1</xref>) with the DL method under APC were 0.393, 0.388 and 0.306 for environments 1, 2 and 3, respectively. The corresponding standard errors (SE) for the APCs are given in Table B1 of Appendix B.</p><fig id="fig1" fig-type="figure" position="float"><label>Figure 1</label><caption><p><italic>Maize data set 1-Trait GY</italic>. Mean Pearson&#x02019;s correlation for each environment. The first vertical sub-panel corresponds to the model with genotype<inline-formula><mml:math id="me41"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>environment interaction (Maize data set 1 I), and the second vertical sub-panel corresponds to the same model but without genotype<inline-formula><mml:math id="me42"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>environment interaction (Maize data set 1 WI).</p></caption><graphic xlink:href="3813f1"/></fig></sec><sec id="s16"><title>Maize data set 2-trait ASI</title><p><xref ref-type="fig" rid="fig2">Figure 2</xref> shows that the APC for each environment for the GBLUP method including the interaction term was 0.542 for environment 1, 0.512 for environment 2 and 0.312 for environment 3. On the other hand, the predictions (<xref ref-type="fig" rid="fig2">Figure 2</xref>) with the DL method with interaction in terms of APC were 0.402 for environment 1, 0.451 for environment 2 and 0.194 for environment 3. On the other hand, when the G<inline-formula><mml:math id="me43"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>E interaction term was ignored, the predictions of the GBLUP method were 0.427 for environment 1, 0.414 for environment 2 and 0.339 for environment 3. Under the DL method, the predictions (<xref ref-type="fig" rid="fig2">Figure 2</xref>) in APC terms were 0.496 for environment 1, 0.509 for environment 2 and 0.319 for environment 3 (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><fig id="fig2" fig-type="figure" position="float"><label>Figure 2</label><caption><p><italic>Maize data set 2- Trait ASI</italic>. Mean Pearson&#x02019;s correlation for each environment. The first vertical sub-panel corresponds to the model with genotype<inline-formula><mml:math id="me44"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>environment interaction (Maize data set 2 I), and the second vertical sub-panel corresponds to the same model but without genotype<inline-formula><mml:math id="me45"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula> environment interaction (Maize data set 2 WI).</p></caption><graphic xlink:href="3813f2"/></fig></sec><sec id="s17"><title>Maize data set 3-trait PH</title><p><xref ref-type="fig" rid="fig3">Figure 3</xref> shows that the APCs for the GBLUP method with the G<inline-formula><mml:math id="me46"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>E interaction term were: 0.481 for environment 1, 0.489 for environment 2 and 0.529 for environment 3. On the other hand, the predictions (<xref ref-type="fig" rid="fig3">Figure 3</xref>) obtained with the DL method under the APC were: 0.506 for environment 1, 0.436 for environment 2 and 0.455 for environment 3. When the G<inline-formula><mml:math id="me47"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>E interaction term was not taken into account, the APCs of the GBLUP method were: 0.232 for environment 1, 0.296 for environment 2 and 0.471 for environment 3. Under the DL method, the predictions (<xref ref-type="fig" rid="fig3">Figure 3</xref>) resulting in APC terms were 0.499 for environment 1, 0.482 for environment 2 and 0.491 for environment 3 (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p><fig id="fig3" fig-type="figure" position="float"><label>Figure 3</label><caption><p><italic>Maize data set 3- Trait PH</italic>. Mean Pearson&#x02019;s correlation for each environment. The first vertical sub-panel corresponds to the model with genotype<inline-formula><mml:math id="me48"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>environment interaction (Maize data set 3 I), and the second vertical sub-panel corresponds to the same model but without genotype<inline-formula><mml:math id="me49"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula> environment interaction (Maize data set 3 WI).</p></caption><graphic xlink:href="3813f3"/></fig></sec><sec id="s18"><title>Wheat data set 4-trait GY</title><p>The predictions with the APC for each environment under the GBLUP method with the interaction term were 0.902 for environment 1, 0.841 for environment 2, 0.712 for environment 3 and 0.800 for environment 4 (<xref ref-type="fig" rid="fig4">Figure 4</xref>). On the other hand, the predictions (<xref ref-type="fig" rid="fig4">Figure 4</xref>) with the APC under the DL method were 0.594 for environment 1, 0.559 for environment 2, 0.534 for environment 3 and 0.348 for environment 4 (<xref ref-type="fig" rid="fig4">Figure 4</xref>). When the G<inline-formula><mml:math id="me50"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>E interaction was ignored, the predictions under the GBLUP method were 0.848 for environment 1, 0.779 for environment 2, 0.585 for environment 3 and 0.666 for environment 4, while under the DL method, the predictions (<xref ref-type="fig" rid="fig4">Figure 4</xref>) were 0.689 for environment 1, 0.620 for environment 2, 0.548 for environment 3 and 0.488 for environment 4 (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><fig id="fig4" fig-type="figure" position="float"><label>Figure 4</label><caption><p><italic>Wheat data set 4- Trait GY</italic>. Mean Pearson&#x02019;s correlation for each environment. The first vertical sub-panel corresponds to the model with genotype<inline-formula><mml:math id="me51"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>environment interaction (Wheat data set 4 I), and the second vertical sub-panel corresponds to the same model but without genotype<inline-formula><mml:math id="me52"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>environment interaction (Wheat data set 4 WI).</p></caption><graphic xlink:href="3813f4"/></fig></sec><sec id="s19"><title>Wheat data set 5-trait GY</title><p><xref ref-type="fig" rid="fig5">Figure 5</xref> shows that the APCs under the GBLUP method disaggregated by environment with the G<inline-formula><mml:math id="me53"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>E interaction term were 0.661 for environment 1, 0.787 for environment 2, 0.713 for environment 3 and 0.843 for environment 4 (<xref ref-type="fig" rid="fig5">Figure 5</xref>). On the other hand, the predictions (<xref ref-type="fig" rid="fig5">Figure 5</xref>) under the DL method were 0.584 for environment 1, 0.557 for environment 2, 0.548 for environment 3 and 0.571 for environment 4 (<xref ref-type="fig" rid="fig5">Figure 5</xref>). When the G<inline-formula><mml:math id="me54"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>E term was ignored, the predictions for the GBLUP method were 0.380 for environment 1, 0.707 for environment 2, 0.568 for environment 3 and 0.791 for environment 4, while the predictions (<xref ref-type="fig" rid="fig5">Figure 5</xref>) under the DL method were 0.597 for environment 1, 0.706 for environment 2, 0.599 for environment 3 and 0.647 for environment 4 (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p><fig id="fig5" fig-type="figure" position="float"><label>Figure 5</label><caption><p><italic>Wheat data set 5-Trait GY</italic>. Mean Pearson&#x02019;s correlation for each environment. The first vertical sub-panel corresponds to the model with genotype<inline-formula><mml:math id="me55"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>environment interaction (Wheat data set 5 I), and the second vertical sub-panel corresponds to the same model but without genotype<inline-formula><mml:math id="me56"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>environment interaction (Wheat data set 5 WI).</p></caption><graphic xlink:href="3813f5"/></fig></sec><sec id="s20"><title>Wheat data set 6-trait GY</title><p><xref ref-type="fig" rid="fig6">Figure 6</xref> shows that the APCs under the GBLUP method disaggregated by environment with interaction were 0.664 for environment 1, 0.552 for environment 2, 0.724 for environment 3, 0.498 for environment 4 and 0.511 for environment 5 (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Under the DL method with the G<inline-formula><mml:math id="me57"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>E interaction term, the predictions (<xref ref-type="fig" rid="fig6">Figure 6</xref>) in terms of APC were 0.682 for environment 1, 0.555 for environment 2, 0.731 for environment 3, 0.405 for environment 4 and 0.422 for environment 5 (<xref ref-type="fig" rid="fig6">Figure 6</xref>). When the G<inline-formula><mml:math id="me58"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>E term was ignored under the GBLUP, the predictions were 0.321 for environment 1, 0.209 for environment 2, 0.356 for environment 3, 0.337 for environment 4 and 0.324 for environment 5 (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Under the DL method, the predictions in terms of APC were 0.634 for environment 1, 0.497 for environment 2, 0.705 for environment 3, 0.359 for environment 4 and 0.363 for environment 5 (<xref ref-type="fig" rid="fig6">Figure 6</xref>).</p><fig id="fig6" fig-type="figure" position="float"><label>Figure 6</label><caption><p><italic>Wheat data set 6- Trait GY</italic>. Mean Pearson&#x02019;s correlation for each environment. The first vertical sub-panel corresponds to the model with genotype<inline-formula><mml:math id="me59"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>environment interaction (Wheat data set 6 I), and the second vertical sub-panel corresponds to the same model but without genotype<inline-formula><mml:math id="me60"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>environment interaction (Wheat data set 6 WI).</p></caption><graphic xlink:href="3813f6"/></fig></sec><sec id="s21"><title>Wheat data set 7-trait PH</title><p><xref ref-type="fig" rid="fig7">Figure 7</xref> shows that the APCs for the GBLUP method were 0.388 for environment 1, 0.684 for environment 2, and 0.724 for environment 3 (<xref ref-type="fig" rid="fig7">Figure 7</xref>). The predictions (<xref ref-type="fig" rid="fig7">Figure 7</xref>) under the DL method with interaction in terms of Pearson&#x02019;s correlation were 0.554 for environment 1, 0.563 for environment 2 and 0.733 for environment 3 (<xref ref-type="fig" rid="fig7">Figure 7</xref>). On the other hand when the interaction term was ignored, the APCs under the GBLUP method were 0.119 for environment 1, 0.719 for environment 2 and 0.672 for environment 3 (<xref ref-type="fig" rid="fig7">Figure 7</xref>). Under the DL method, the predictions in terms of APC were 0.430 for environment 1, 0.573 for environment 2 and 0.836 for environment 3 (<xref ref-type="fig" rid="fig7">Figure 7</xref>).</p><fig id="fig7" fig-type="figure" position="float"><label>Figure 7</label><caption><p><italic>Wheat data set 7- Trait PH</italic><underline>.</underline> Mean Pearson&#x02019;s correlation for each environment. The first vertical sub-panel corresponds to the model with genotype<inline-formula><mml:math id="me61"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>environment interaction (Wheat data set 7 I), and the second vertical sub-panel corresponds to the same model but without genotype<inline-formula><mml:math id="me62"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>environment interaction (Wheat data set 7 WI).</p></caption><graphic xlink:href="3813f7"/></fig></sec><sec id="s22"><title>Wheat data set 8-trait DTHD</title><p><xref ref-type="fig" rid="fig8">Figure 8</xref> shows that the APCs of the GBLUP method with G<inline-formula><mml:math id="me63"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>E interaction were 1.00 for environment 1, 1.00 for environment 2 and 1.00 for environment 3 (<xref ref-type="fig" rid="fig8">Figure 8</xref>). Under the DL method with G<inline-formula><mml:math id="me64"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>E interaction, the predictions (<xref ref-type="fig" rid="fig8">Figure 8</xref>) in terms of APC were 0.75 for environment 1, 1.00 for environment 2 and 0.978 for environment 3 (<xref ref-type="fig" rid="fig8">Figure 8</xref>). When the G<inline-formula><mml:math id="me65"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>E interaction term was ignored, the APCs for environments 1, 2 and 3 were 1.00, 1.00 and 1.00, respectively, under the GBLUP method, while the predictions (<xref ref-type="fig" rid="fig8">Figure 8</xref>) under the DL method were 0.967 for environment 1, 1.00 for environment 2 and 1.00 for environment 3 (<xref ref-type="fig" rid="fig8">Figure 8</xref>).</p><fig id="fig8" fig-type="figure" position="float"><label>Figure 8</label><caption><p><italic>Wheat data set 8- Trait DTHD</italic>. Mean Pearson&#x02019;s correlation for each environment. The first vertical sub-panel corresponds to the model with genotype<inline-formula><mml:math id="me66"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>environment interaction (Wheat data set 8 I), and the second vertical sub-panel corresponds to the same model but without genotype<inline-formula><mml:math id="me67"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>environment interaction (Wheat data set 8 WI).</p></caption><graphic xlink:href="3813f8"/></fig></sec><sec id="s23"><title>Wheat data set 9-trait DTM</title><p><xref ref-type="fig" rid="fig9">Figure 9</xref> shows that the APCs of the GBLUP method with interaction were 1.00 for environment 1 and 0.918 for environment 2 (<xref ref-type="fig" rid="fig9">Figure 9</xref>). Under the DL method, the predictions (<xref ref-type="fig" rid="fig9">Figure 9</xref>) in terms of APC were 1.00 for environment 1 and 0.792 for environment 2 (<xref ref-type="fig" rid="fig9">Figure 9</xref>). On the other hand, when the interaction term was ignored, the APCs for environments 1 and 2 were 1.00 and 0.633, respectively, under the GBLUP method without interaction (<xref ref-type="fig" rid="fig9">Figure 9</xref>). The predictions under the DL without G&#x000d7;E method for DTM were 0.633 for environment 2 and 0.552 for environment 1 (<xref ref-type="fig" rid="fig9">Figure 9</xref>).</p><fig id="fig9" fig-type="figure" position="float"><label>Figure 9</label><caption><p><italic>Wheat data set 9- Trait DTM</italic>. Mean Pearson&#x02019;s correlation for each environment. The first vertical sub-panel corresponds to the model with genotype<inline-formula><mml:math id="me68"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>environment interaction (Wheat data set 9 I), and the inferior horizontal sub-panels correspond to the same model but without genotype<inline-formula><mml:math id="me69"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>environment interaction (Wheat data set 9 WI).</p></caption><graphic xlink:href="3813f9"/></fig></sec><sec id="s24"><title>A meta-picture of the DL method <italic>vs.</italic> the GBLUP model</title><p><xref ref-type="fig" rid="fig10">Figure 10</xref> shows the mean Pearson&#x02019;s correlation across environments of the GBLUP model and DL model, with and without G&#x000d7;E interaction for each data set. Here it is evident that for data sets 1, 2, 3, 5, 6 and 7 when the G&#x000d7;E interaction term was not taken into account, the DL method was better than the GBLUP model. When the G&#x000d7;E interaction term was taken into account, the GBLUP model was the best in 8 out of 9 of data sets under study; only in data set 7, the DL method was better than the GBLUP model.</p><fig id="fig10" fig-type="figure" position="float"><label>Figure 10</label><caption><p>Mean Pearson&#x02019;s correlation across environments for the GBLUP and the DL model. The first vertical sub-panel corresponds to the model with genotype<inline-formula><mml:math id="me70"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>environment interaction (I), and the inferior horizontal sub-panels correspond to the same model but without genotype<inline-formula><mml:math id="me71"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>environment interaction (WI).</p></caption><graphic xlink:href="3813f10"/></fig></sec></sec><sec sec-type="discussion" id="s25"><title>Discussion</title><p>The rapid increase in the genomic data dimension and acquisition rate is challenging conventional genomic analysis strategies. The DL method that recently appeared in the biological arena promises to leverage very large data sets to find hidden structures within them, and make accurate predictions (<xref rid="bib3" ref-type="bibr">Angermueller <italic>et al.</italic>, 2016</xref>). In other words, DL algorithms dive into data in ways that humans cannot, detecting features that might otherwise be impossible to catch. In our study, we explored a fraction of all possible combinations of hyperparameters of DL methods. Based on our results, we found that the DL method with densely connected network architecture competes well with the GBLUP method, since in many scenarios under study we did not find great differences between these two approaches. The network structure implemented with the DL method is a feedforward multilayer neural network whose structure (topology) is composed of an input layer, one or many hidden layers, and a single output layer. Each layer can have a different number of neurons and each layer is fully connected to the adjacent layer. The connections between the neurons in the layers form an acyclic graph.</p><p>One possible explanation for the good performance of the GBLUP method compared with the DL method is that, as has been documented, when the data are scarce (no really large data sets in terms of observations), many times the most commonly used statistical (or machine) learning method outperforms the DL method. Given that with small data sets, one of the major challenges when training a DL method is dealing with the risk of overfitting (<italic>i.e.</italic>, when the training error is low but the testing error is high), the method fails to learn a proper generalization of the knowledge contained in the data. For this reason, in our application of DL with a densely connected network, we used dropout regularization, which consists of temporarily removing a random subset (30%) of neurons with their connections during training. However, even with regularization, DL results were not superior in general terms to GBLUP results when the interaction term was taken into account.</p><p>It is important to point out that the DL method was superior when the G&#x000d7;E interaction term was not included in the method under the grid of parameters implemented. This can be attributed to the fact that DL methods are capable of capturing complex relationships hidden in the data without requiring strong assumptions about the underlying mechanisms, which are frequently unknown or insufficiently defined (<xref rid="bib3" ref-type="bibr">Angermueller <italic>et al.</italic> 2016</xref>). Also, DL methods are a type of general-purpose approach for learning functional relationships from data that do not require prior information, as do the GBLUP and other genomic Bayesian methods. However, three main disadvantages of DL are: (a) it is really hard to train a DL method because we need to test different combinations of hyperparameters corresponding to the number of layers, the number of units, the number of epochs, the type of regularization (and the dropout percentage in the context of dropout regularization) and the type of activation function in each layer; (b) the computational time required to implement a DL method, since it increases as the number of layers and units increases; and (c) a DL method requires a level of experience in computer science and statistics that is not always available in organizations working with biological data.</p><p>Also, according to our results, the best combination of hyperparameters (<italic>i.e.</italic>, number of layers, number of units and number of epochs) is data dependent since the best prediction in each data set can be obtained with a different combination of hyperparameters, which corroborates that the process of hyperparameter tuning in DL is a challenging process that required further investigation.</p><p>Based on our results, the DL methods are a powerful complement of classic genomic-enabled prediction tools and other analysis strategies. For these reasons, DL methods have been applied successfully in many areas of science, from social science to engineering. However, the results obtained here only apply to DL methods with densely connected network architecture and for the studied hyperparameters; but there are still opportunities to evaluate the performance of other network architectures such as convolutional neural networks and recurrent neural networks.</p><p>Furthermore, in the companion article of <xref rid="bib33" ref-type="bibr">Montesinos-L&#x000f3;pez <italic>et al.</italic> (2018)</xref> the authors extended the multi-environment DL model of this research to the case of multi-trait multi-environment DL model (MTDL) and found challenging aspects for the selection of the hyperparameters. However, the authors have concluded that that MTDL is feasible, and practical in the GS framework with important savings on computing resources as compared to other multi-trait multi-environment models.</p><p>Finally, it should be noted that although the DL method performed well compared to the most popular Bayesian genomic selection method (GBLUP), its prediction accuracy was always lower than that of the GBLUP method. However, the boom of DL methods is very widespread and the media are selling DL as the panacea for predicting any type of phenomenon. However, as pointed out above, the DL method also has many limitations that need to be improved, since it is a methodology with a rational thought process that is entirely dependent on the problem we are trying to solve. A lot of time is needed to understand its essence and be able to take advantage of its virtues when trying to apply it to solve real-world problems. However, we must also point out that DL is an alternative approach that can help explore other pathways that underlie biological data.</p></sec><sec sec-type="conclusions" id="s26"><title>Conclusions</title><p>In this paper we compare a DL method with densely connected network architecture to the most popular genomic prediction method, the GBLUP. Our results show that the DL method with densely connected network architecture performed as well as the GBLUP method, but that in general terms, the GBLUP method was superior when the covariates corresponding to G&#x000d7;E interaction were taken into account. However, the DL method was superior (in terms of Pearson&#x02019;s correlation) to the GBLUP method when G&#x000d7;E interaction was ignored, since in 6 out of the 9 data sets under this scenario, the DL method was better than the GBLUP method in terms of prediction accuracy. Based on this empirical evidence, we can say that DL methods with densely connected network architecture were competitive with the most popular genomic prediction method (GBLUP). For this reason, DL methods should be added to the data science toolkit of statisticians, animal and plant breeding scientists so they can use them to evaluate other data sets and other types of network architectures of DL methods that have been applied successfully in other scientific domains.</p></sec></body><back><ack><title>Acknowledgments</title><p>We thank all CIMMYT scientists who were involved in conducting the extensive multi-environment maize and wheat trials and generated the phenotypic and genotypic data used in this study. We acknowledge the financial support provided by the CIMMYT CRP (maize and wheat), Bill and Melinda Gates Foundation as well the USAID projects (Cornell University and Kansas State University) for CIMMYT maize and wheat breeding that generated the data analyzed in this study. Funding for this work was received from the Foundation for Research Levy on Agricultural Products (FFL) and the Agricultural Agreement Research Fund (JA) in Norway through NFR grant 267806. The authors highly appreciate the meticulous, rigorous, and efficient work of the anonymous reviewers and G3: Genes|Genomes|Genetics associate editor Dr. Emma Huang.</p><p><italic>Note added in proof:</italic> See Montesinos-L&#x000f3;pez et al. 2018 (pp. 3829&#x02013;3840) in this issue, for a related work.</p></ack><fn-group><fn id="fn1"><p>Communicating editor: E. Huang</p></fn></fn-group><ref-list><title>Literature Cited</title><ref id="bib1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alipanahi</surname><given-names>B.</given-names></name><name><surname>Delong</surname><given-names>A.</given-names></name><name><surname>Weirauch</surname><given-names>M. T.</given-names></name><name><surname>Frey</surname><given-names>B. J.</given-names></name></person-group>, <year>2015</year>&#x02003;<article-title>Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning.</article-title>
<source>Nat. Biotechnol.</source>
<volume>33</volume>: <fpage>831</fpage>&#x02013;<lpage>838</lpage>. <pub-id pub-id-type="doi">10.1038/nbt.3300</pub-id><pub-id pub-id-type="pmid">26213851</pub-id></mixed-citation></ref><ref id="bib2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angermueller</surname><given-names>C.</given-names></name><name><surname>Lee</surname><given-names>H. J.</given-names></name><name><surname>Reik</surname><given-names>W.</given-names></name><name><surname>Stegle</surname><given-names>O.</given-names></name></person-group>, <year>2017</year>&#x02003;<article-title>DeepCpG: accurate prediction of single-cell DNA methylation states using deep learning.</article-title>
<source>Genome Biol.</source>
<volume>18</volume>: <fpage>1</fpage>&#x02013;<lpage>13</lpage>.<pub-id pub-id-type="pmid">28077169</pub-id></mixed-citation></ref><ref id="bib3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angermueller</surname><given-names>C.</given-names></name><name><surname>P&#x000e4;rnamaa</surname><given-names>T.</given-names></name><name><surname>Parts</surname><given-names>L.</given-names></name><name><surname>Stegle</surname><given-names>O.</given-names></name></person-group>, <year>2016</year>&#x02003;<article-title>Deep learning for computational biology.</article-title>
<source>Mol. Syst. Biol.</source>
<volume>12</volume>: <fpage>878</fpage>
<pub-id pub-id-type="doi">10.15252/msb.20156651</pub-id><pub-id pub-id-type="pmid">27474269</pub-id></mixed-citation></ref><ref id="bib4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britt</surname><given-names>J. H.</given-names></name><name><surname>Cushman</surname><given-names>R. A.</given-names></name><name><surname>Dechow</surname><given-names>C. D.</given-names></name><name><surname>Dobson</surname><given-names>H.</given-names></name><name><surname>Humblot</surname><given-names>P.</given-names></name><etal/></person-group>, <year>2018</year>&#x02003;<article-title>Invited review: Learning from the future&#x02014;A vision for dairy farms and cows in 2067.</article-title>
<source>J. Dairy Sci.</source>
<volume>101</volume>: <fpage>3722</fpage>&#x02013;<lpage>3741</lpage>. <pub-id pub-id-type="doi">10.3168/jds.2017-14025</pub-id><pub-id pub-id-type="pmid">29501340</pub-id></mixed-citation></ref><ref id="bib5"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chollet</surname><given-names>F.</given-names></name><name><surname>Allaire</surname><given-names>J. J.</given-names></name></person-group>, <year>2017</year>&#x02003;<source><italic>Deep Learning with R. Manning Publications</italic>, <italic>Manning Early Access Program</italic></source>, <edition>Ed. 1st</edition>
<publisher-name>MEA</publisher-name>, <publisher-loc>New Delhi, India</publisher-loc>.</mixed-citation></ref><ref id="bib6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crossa</surname><given-names>J.</given-names></name></person-group>, Y. <person-group person-group-type="author"><name><surname>Beyene</surname><given-names>S.</given-names></name></person-group>Kassa, P. P&#x000e9;rez-Rodr&#x000ed;guez, J. M. Hickey, <italic>et al</italic>, <year>2013</year>&#x02003;<comment>Genomic prediction in maize breeding populations with genotyping-by-sequencing. G3: Genes|Genomes|Genetics (Bethesda) 3, 1903&#x02013;1926</comment>
<pub-id pub-id-type="doi">10.1534/g3.113.008227</pub-id></mixed-citation></ref><ref id="bib7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crossa</surname><given-names>J.</given-names></name><name><surname>Jarqu&#x000ed;n</surname><given-names>D.</given-names></name><name><surname>Franco</surname><given-names>J.</given-names></name><name><surname>P&#x000e9;rez-Rodr&#x000ed;guez</surname><given-names>P.</given-names></name><name><surname>Burgue&#x000f1;o</surname><given-names>J.</given-names></name><etal/></person-group>, <year>2016</year>&#x02003;<comment>Genomic Prediction of Gene Bank Wheat Landraces. G3: Genes|Genomes|Genetics (Bethesda), 6: 1819&#x02013;1834</comment>
<pub-id pub-id-type="doi">10.1534/g3.116.029637</pub-id></mixed-citation></ref><ref id="bib8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crossa</surname><given-names>J.</given-names></name><name><surname>P&#x000e9;rez-Rodr&#x000ed;guez</surname><given-names>P.</given-names></name><name><surname>Cuevas</surname><given-names>J.</given-names></name><name><surname>Montesinos-L&#x000f3;pez</surname><given-names>O. A.</given-names></name><name><surname>Jarqu&#x000ed;n</surname><given-names>D.</given-names></name><etal/></person-group>, <year>2017</year>&#x02003;<article-title>Genomic Selection in Plant Breeding: Methods, Models, and Perspectives.</article-title>
<source>Trends Plant Sci.</source>
<volume>22</volume>: <fpage>961</fpage>&#x02013;<lpage>975</lpage>. <pub-id pub-id-type="doi">10.1016/j.tplants.2017.08.011</pub-id><pub-id pub-id-type="pmid">28965742</pub-id></mixed-citation></ref><ref id="bib9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cuevas</surname><given-names>J.</given-names></name><name><surname>Crossa</surname><given-names>J.</given-names></name><name><surname>Soberanis</surname><given-names>V.</given-names></name><name><surname>P&#x000e9;rez-Elizalde</surname><given-names>S.</given-names></name><name><surname>P&#x000e9;rez-Rodr&#x000ed;guez</surname><given-names>P.</given-names></name><etal/></person-group>, <year>2016</year>&#x02003;<article-title>Genomic Prediction of Genotype Environment Interaction Kernel Regression Models.</article-title>
<source>Plant Genome</source>
<volume>9</volume>: <fpage>1</fpage>&#x02013;<lpage>20</lpage>. <pub-id pub-id-type="doi">10.3835/plantgenome2016.03.0024</pub-id></mixed-citation></ref><ref id="bib10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daetwyler</surname><given-names>H. D.</given-names></name><name><surname>Calus</surname><given-names>M. P. L.</given-names></name><name><surname>Pong-Wong</surname><given-names>R.</given-names></name><name><surname>de los Campos</surname><given-names>G.</given-names></name><name><surname>Hickey</surname><given-names>J. M.</given-names></name></person-group>, <year>2012</year>&#x02003;<article-title>Genomic prediction in animals and plants: simulation of data, validation, reporting and benchmarking.</article-title>
<source>Genetics</source>
<volume>193</volume>: <fpage>347</fpage>&#x02013;<lpage>365</lpage>. <pub-id pub-id-type="doi">10.1534/genetics.112.147983</pub-id><pub-id pub-id-type="pmid">23222650</pub-id></mixed-citation></ref><ref id="bib11"><mixed-citation publication-type="webpage">Dewancker, I., M. McCourt, S. Clark, P. Hayes, A. Johnson <italic>et al.</italic>, 2016&#x02003;A Stratified Analysis of Bayesian Optimization Methods. arXiv:1603.09441v1.</mixed-citation></ref><ref id="bib12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>L.</given-names></name><name><surname>Yu</surname><given-names>D.</given-names></name></person-group>, <year>2014</year>&#x02003;<article-title>Deep Learning: Method and Applications.</article-title>
<source>Foundations and TrendsR in Signal Processing</source>
<volume>7</volume>: <fpage>197</fpage>&#x02013;<lpage>387</lpage>. <pub-id pub-id-type="doi">10.1561/2000000039</pub-id></mixed-citation></ref><ref id="bib13"><mixed-citation publication-type="webpage">de los Campos, G., and P. P&#x000e9;rez-Rodr&#x000ed;guez, 2014&#x02003;Bayesian Generalized Linear Regression. R package version 1.0.4. <ext-link ext-link-type="uri" xlink:href="http://CRAN.R-project.org/package=BGLR">http://CRAN.R-project.org/package=BGLR</ext-link>.</mixed-citation></ref><ref id="bib14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gonz&#x000e1;lez-Camacho</surname><given-names>J. M.</given-names></name><name><surname>de los Campos</surname><given-names>G.</given-names></name><name><surname>P&#x000e9;rez-Rodr&#x000ed;guez</surname><given-names>P.</given-names></name><name><surname>Gianola</surname><given-names>D.</given-names></name><name><surname>Cairns</surname><given-names>J. E.</given-names></name><etal/></person-group>, <year>2012</year>&#x02003;<article-title>Genome-enabled prediction of genetic values using radial basis function neural networks.</article-title>
<source>Theor. Appl. Genet.</source>
<volume>125</volume>: <fpage>759</fpage>&#x02013;<lpage>771</lpage>. <pub-id pub-id-type="doi">10.1007/s00122-012-1868-9</pub-id><pub-id pub-id-type="pmid">22566067</pub-id></mixed-citation></ref><ref id="bib15"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Courville</surname><given-names>A.</given-names></name></person-group>, <year>2016</year>&#x02003;<source>Deep Learning</source>, <publisher-name>MIT Press</publisher-name>, <publisher-loc>Cambridge, Massachusetts</publisher-loc>.</mixed-citation></ref><ref id="bib16"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gulli</surname><given-names>A.</given-names></name><name><surname>Sujit</surname><given-names>P.</given-names></name></person-group>, <year>2017</year>&#x02003;<source>Deep Learning with Keras: Implementing deep learning method and neural networks with the power of Python</source>, <publisher-name>Packt Publishing Ltd.</publisher-name>, <publisher-loc>Birmingham, UK</publisher-loc>.</mixed-citation></ref><ref id="bib17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickey</surname><given-names>J. M.</given-names></name><name><surname>Chiurugwi</surname><given-names>T.</given-names></name><name><surname>Mackay</surname><given-names>I.</given-names></name><name><surname>Powell</surname><given-names>W.</given-names></name><name><surname>Eggen</surname><given-names>A.</given-names></name><etal/></person-group>, <year>2017</year>&#x02003;<article-title>Genomic prediction unifies animal and plant breeding programs to form platforms for biological discovery.</article-title>
<source>Nat. Genet.</source>
<volume>49</volume>: <fpage>1297</fpage>&#x02013;<lpage>1303</lpage>. <pub-id pub-id-type="doi">10.1038/ng.3920</pub-id><pub-id pub-id-type="pmid">28854179</pub-id></mixed-citation></ref><ref id="bib18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jonas</surname><given-names>E.</given-names></name><name><surname>de Koning</surname><given-names>D. J.</given-names></name></person-group>, <year>2015</year>&#x02003;<article-title>Genomic selection needs to be carefully assessed to meet specific requirements in livestock breeding programs.</article-title>
<source>Front. Genet.</source>
<volume>6</volume>: <fpage>49</fpage>
<pub-id pub-id-type="doi">10.3389/fgene.2015.00049</pub-id><pub-id pub-id-type="pmid">25750652</pub-id></mixed-citation></ref><ref id="bib19"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Koch</surname><given-names>P.</given-names></name><name><surname>Wujek</surname><given-names>B.</given-names></name><name><surname>Golovidov</surname><given-names>O.</given-names></name><name><surname>Gardner</surname><given-names>S.</given-names></name></person-group>, <year>2017</year>&#x02003;<article-title>Automated Hyperparameter Tuning for Effective Machine Learning</article-title>, <source>Proceedings of the SAS Global Forum 2017 Conference</source>, <publisher-name>SAS Institute Inc.</publisher-name>, <publisher-loc>Cary, NC</publisher-loc>, Available at <ext-link ext-link-type="uri" xlink:href="http://support.sas.com/resources/papers/proceedings17/SAS514&#x02013;2017.pdf">http://support.sas.com/resources/papers/ proceedings17/SAS514&#x02013;2017.pdf</ext-link>.</mixed-citation></ref><ref id="bib20"><mixed-citation publication-type="confproc">Konen, W., P. Koch, O. Flasch, T. Bartz-Beielstein, M. Friese <italic>et al.</italic>, 2011 &#x02003;Tuned Data Mining: A Benchmark Study on Different Tuners. In Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation (GECCO-2011). New York: SIGEVO/ACM.</mixed-citation></ref><ref id="bib21"><mixed-citation publication-type="other">Kwong, Q.B., A. L. Ong, C. K. Teh, F. T. Chew, M. Tammi, <italic>et al</italic>, 2017 &#x02003;Genomic Selection in Commercial Perennial Crops: Applicability and Improvement in Oil Palm (<italic>Elaeis guineensis</italic> Jacq.). Scientific Reports, 7(2872):1:9.</mixed-citation></ref><ref id="bib22"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>N. D.</given-names></name></person-group>, <year>2016</year>&#x02003;<source>Deep learning made easy with R. A gentle introduction for data science</source>, <publisher-name>CreateSpace Independent Publishing Platform</publisher-name>, &#x0200e;<publisher-loc>Scotts Valley, CA</publisher-loc>.</mixed-citation></ref><ref id="bib23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopez-Cruz</surname><given-names>M.</given-names></name><name><surname>Crossa</surname><given-names>J.</given-names></name><name><surname>Bonnett</surname><given-names>D.</given-names></name><name><surname>Dreisigacker</surname><given-names>S.</given-names></name><name><surname>Poland</surname><given-names>J.</given-names></name><etal/></person-group>, <year>2015</year>&#x02003;<comment>Increased prediction accuracy in wheat breeding trials using a marker &#x000d7; environment interaction genomic selection method. G3: Genes|Genomes|Genetics (Bethesda). 5: 569&#x02013;82</comment>
<pub-id pub-id-type="doi">10.1534/g3.114.016097</pub-id></mixed-citation></ref><ref id="bib24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorena</surname><given-names>A. C.</given-names></name><name><surname>de Carvalho</surname><given-names>A. C. P. L. F.</given-names></name></person-group>, <year>2008</year>&#x02003;<article-title>Evolutionary Tuning of SVM Parameter Values in Multiclass Problems.</article-title>
<source>Neurocomputing</source>
<volume>71</volume>: <fpage>3326</fpage>&#x02013;<lpage>3334</lpage>. <pub-id pub-id-type="doi">10.1016/j.neucom.2008.01.031</pub-id></mixed-citation></ref><ref id="bib25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lujan-Moreno</surname><given-names>G. A.</given-names></name><name><surname>Howard</surname><given-names>P. R.</given-names></name><name><surname>Rojas</surname><given-names>O. G.</given-names></name><name><surname>Montgomery</surname><given-names>D. C.</given-names></name></person-group>, <year>2018</year>&#x02003;<article-title>Design of Experiments and Response Surface Methodology to Tune Machine Learning Hyperparameters, with a Random Forest Case-Study.</article-title>
<source>Expert Syst. Appl.</source>
<volume>109</volume>: <fpage>195</fpage>&#x02013;<lpage>205</lpage>. <pub-id pub-id-type="doi">10.1016/j.eswa.2018.05.024</pub-id></mixed-citation></ref><ref id="bib26"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McKay</surname><given-names>M. D.</given-names></name></person-group>, <year>1992</year>&#x02003;Latin Hypercube Sampling as a Tool in Uncertainty Analysis of Computer Models. In Proceedings of the 24th Conference on Winter Simulation (WSC 1992), edited by <person-group person-group-type="editor"><name><surname>J.J.</surname><given-names>Swain</given-names></name><name><surname>Goldsman</surname><given-names>D.</given-names></name><name><surname>Crain</surname><given-names>R.C.</given-names></name><name><surname>Wilson</surname><given-names>J.R.</given-names></name></person-group>; pp. <fpage>557</fpage>&#x02013;<lpage>564</lpage>. <publisher-loc>New York</publisher-loc>: <publisher-name>ACM</publisher-name>.</mixed-citation></ref><ref id="bib27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>W.</given-names></name><name><surname>Qiu</surname><given-names>Z.</given-names></name><name><surname>Song</surname><given-names>J.</given-names></name><name><surname>Cheng</surname><given-names>Q.</given-names></name><name><surname>Ma</surname><given-names>C.</given-names></name></person-group>, <year>2017</year>&#x02003;<article-title>DeepGS: Predicting phenotypes from genotypes using Deep Learning.</article-title>
<source>bioRxiv</source>. <pub-id pub-id-type="doi">10.1101/241414</pub-id></mixed-citation></ref><ref id="bib28"><mixed-citation publication-type="webpage">McDowell, R., and D. Grant, 2016&#x02003;Genomic Selection with Deep Neural Networks. Graduate Theses and Dissertations. 15973. <ext-link ext-link-type="uri" xlink:href="https://lib.dr.iastate.edu/etd/15973">https://lib.dr.iastate.edu/etd/15973</ext-link>.</mixed-citation></ref><ref id="bib29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menden</surname><given-names>M. P.</given-names></name><name><surname>Iorio</surname><given-names>F.</given-names></name><name><surname>Garnett</surname><given-names>M.</given-names></name><name><surname>McDermott</surname><given-names>U.</given-names></name><name><surname>Benes</surname><given-names>C. H.</given-names></name><etal/></person-group>, <year>2013</year>&#x02003;<article-title>Machine learning prediction of cancer cell sensitivity to drugs based on genomic and chemical properties.</article-title>
<source>PLoS One</source>
<volume>8</volume>: <fpage>e61318</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pone.0061318</pub-id><pub-id pub-id-type="pmid">23646105</pub-id></mixed-citation></ref><ref id="bib30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meuwissen</surname><given-names>T. H.</given-names></name><name><surname>Hayes</surname><given-names>B. J.</given-names></name><name><surname>Goddard</surname><given-names>M. E.</given-names></name></person-group>, <year>2001</year>&#x02003;<article-title>Prediction of total genetic value using genome-wide dense marker maps.</article-title>
<source>Genetics</source>
<volume>157</volume>: <fpage>1819</fpage>&#x02013;<lpage>1829</lpage>.<pub-id pub-id-type="pmid">11290733</pub-id></mixed-citation></ref><ref id="bib31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montesinos-L&#x000f3;pez</surname><given-names>O. A.</given-names></name><name><surname>Montesinos-L&#x000f3;pez</surname><given-names>A.</given-names></name><name><surname>Crossa</surname><given-names>J.</given-names></name><name><surname>Montesinos-L&#x000f3;pez</surname><given-names>J. C.</given-names></name><name><surname>Luna-V&#x000e1;zquez</surname><given-names>F. J.</given-names></name><etal/></person-group>, <year>2017</year>&#x02003;<article-title>A Variational Bayes Genomic-Enabled Prediction Method with Genotype &#x000d7; Environment Interaction. G3: Genes, Genomes</article-title>. <source>Genetics</source>
<volume>7</volume>: <fpage>1833</fpage>&#x02013;<lpage>1853</lpage>.</mixed-citation></ref><ref id="bib32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montesinos-L&#x000f3;pez</surname><given-names>O. A.</given-names></name><name><surname>Montesinos-L&#x000f3;pez</surname><given-names>A.</given-names></name><name><surname>Crossa</surname><given-names>J.</given-names></name><name><surname>Toledo</surname><given-names>F.</given-names></name><name><surname>P&#x000e9;rez-Hern&#x000e1;ndez</surname><given-names>O.</given-names></name><etal/></person-group>, <year>2016</year>&#x02003;<comment>A Genomic Bayesian Multi-trait and Multi-environment model. G3: Genes|Genomes|Genetics (Bethesda), 6:2725&#x02013;2744</comment>
<pub-id pub-id-type="doi">10.1534/g3.116.032359</pub-id></mixed-citation></ref><ref id="bib33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montesinos-L&#x000f3;pez</surname><given-names>O. A.</given-names></name><name><surname>Montesinos-L&#x000f3;pez</surname><given-names>A.</given-names></name><name><surname>Crossa</surname><given-names>J.</given-names></name><name><surname>Gianola</surname><given-names>D.</given-names></name><name><surname>Hern&#x000e1;ndez-Su&#x000e1;rez</surname><given-names>C. M.</given-names></name><etal/></person-group>, <year>2018</year>&#x02003;Multi-trait, multi-environment deep learning modeling for genomic-enabled prediction of plant traits. G3: Genes|Genomes|Genetics (<comment>in press</comment>).</mixed-citation></ref><ref id="bib34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O&#x02019;Connor</surname><given-names>K.</given-names></name><name><surname>Hayes</surname><given-names>B.</given-names></name><name><surname>Topp</surname><given-names>B.</given-names></name></person-group>, <year>2018</year>&#x02003;<article-title>Prospects for increasing yield in macadamia using component traits and genomics.</article-title>
<source>Tree Genet. Genomes</source>
<volume>14</volume>: <fpage>7</fpage>
<pub-id pub-id-type="doi">10.1007/s11295-017-1221-1</pub-id></mixed-citation></ref><ref id="bib35"><mixed-citation publication-type="webpage">R Core Team, 2018&#x02003;R: A language and environment for statistical computing. R Foundation for Statistical Computing. Vienna. Austria. ISBN 3&#x02013;900051&#x02013;07&#x02013;0. URL <ext-link ext-link-type="uri" xlink:href="http://www.R-project.org/">http://www.R-project.org/</ext-link></mixed-citation></ref><ref id="bib37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renukadevi</surname><given-names>N. T.</given-names></name><name><surname>Thangaraj</surname><given-names>P.</given-names></name></person-group>, <year>2014</year>&#x02003;<article-title>Performance Analysis of Optimization Techniques for Medical Image Retrieval.</article-title>
<source>Journal of Theoretical and Applied Information Technology</source>
<volume>59</volume>: <fpage>390</fpage>&#x02013;<lpage>399</lpage>.</mixed-citation></ref><ref id="bib38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rutkoski</surname><given-names>J.</given-names></name><name><surname>Poland</surname><given-names>J.</given-names></name><name><surname>Mondal</surname><given-names>S.</given-names></name><name><surname>Autrique</surname><given-names>E.</given-names></name><name><surname>Crossa</surname><given-names>J.</given-names></name><etal/></person-group>, <year>2016</year>&#x02003;Predictor traits from high-throughput phenotyping improve accuracy of pedigree and genomic selection for yield in wheat. G3:Genes|Genomes|Genetics (Bethesda) 6: 2799&#x02013;2808.</mixed-citation></ref><ref id="bib39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samuel</surname><given-names>A. L.</given-names></name></person-group>, <year>1959</year>&#x02003;<article-title>Some Studies in Machine Learning Using the Game of Checkers.</article-title>
<source>IBM J. Res. Develop.</source>
<volume>3</volume>: <fpage>210</fpage>&#x02013;<lpage>229</lpage>. <pub-id pub-id-type="doi">10.1147/rd.33.0210</pub-id></mixed-citation></ref><ref id="bib40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>N.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name><name><surname>Krizhevsky</surname><given-names>A.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Salakhutdinov</surname><given-names>R.</given-names></name></person-group>, <year>2014</year>&#x02003;<article-title>Dropout: A Simple Way to Prevent Neural Networks from Overfitting.</article-title>
<source>J. Mach. Learn. Res.</source>
<volume>15</volume>: <fpage>1929</fpage>&#x02013;<lpage>1958</lpage>.</mixed-citation></ref><ref id="bib41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tavanaei</surname><given-names>A.</given-names></name><name><surname>Anandanadarajah</surname><given-names>N.</given-names></name><name><surname>Maida</surname><given-names>A. S.</given-names></name><name><surname>Loganantharaj</surname><given-names>R.</given-names></name></person-group>, <year>2017</year>&#x02003;<article-title>A Deep Learning Method for Predicting Tumor Suppressor Genes and Oncogenes from PDB Structure.</article-title>
<source>bioRxiv</source>
<fpage>1</fpage>&#x02013;<lpage>10</lpage>. <pub-id pub-id-type="doi">10.1101/177378</pub-id></mixed-citation></ref><ref id="bib42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>VanRaden</surname><given-names>P. M.</given-names></name></person-group>, <year>2008</year>&#x02003;<article-title>Efficient method to compute genomic predictions.</article-title>
<source>J. Dairy Sci.</source>
<volume>91</volume>: <fpage>4414</fpage>&#x02013;<lpage>4423</lpage>. <pub-id pub-id-type="doi">10.3168/jds.2007-0980</pub-id><pub-id pub-id-type="pmid">18946147</pub-id></mixed-citation></ref><ref id="bib43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weller</surname><given-names>J. I.</given-names></name><name><surname>Ezra</surname><given-names>E.</given-names></name><name><surname>Ron</surname><given-names>M.</given-names></name></person-group>, <year>2017</year>&#x02003;<article-title>Invited review: A perspective on the future of genomic selection in dairy cattle.</article-title>
<source>J. Dairy Sci.</source>
<volume>100</volume>: <fpage>8633</fpage>&#x02013;<lpage>8644</lpage>. <pub-id pub-id-type="doi">10.3168/jds.2017-12879</pub-id><pub-id pub-id-type="pmid">28843692</pub-id></mixed-citation></ref><ref id="bib44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname><given-names>M. D.</given-names></name><name><surname>Del Carpio</surname><given-names>D. P.</given-names></name><name><surname>Alabi</surname><given-names>O.</given-names></name><name><surname>Ezenwaka</surname><given-names>L. C.</given-names></name><name><surname>Ikeogu</surname><given-names>U. N.</given-names></name><etal/></person-group>, <year>2017</year>&#x02003;<article-title>Prospects for Genomic Selection in Cassava Breeding.</article-title>
<source>Plant Genome</source>
<volume>10</volume>: <fpage>0</fpage>
<pub-id pub-id-type="doi">10.3835/plantgenome2017.03.0015</pub-id></mixed-citation></ref></ref-list><app-group><app><title>Appendix</title><sec id="s27"><title>Deep learning R codes for a densely connected network</title><p>setwd(&#x0201c;C:\\TELEMATICA 2017\\Deep Learning CONTINUOUS&#x0201d;)</p><p>rm(list = ls())</p><p>######Libraries required##################################</p><p>library(tensorflow)</p><p>library(keras)</p><p>#############Loading data###############################</p><p>load(&#x0201c;Data_Maize_1to3.RData&#x0201d;)</p><p>####Genomic relationship matrix (GRM)) and phenotipic data#####</p><p>G=G_maize_1to3</p><p>Pheno=Pheno_maize_1to3</p><p>head(Pheno)</p><p>###########Cholesky decomposition of the GRM##############</p><p>LG=t(chol(G))</p><p>########Creating the desing matrices ########################</p><p>Z1G=model.matrix(&#x0223c;0+as.factor(Pheno$Line))</p><p>ZE=model.matrix(&#x0223c;0+as.factor(Pheno$Env))</p><p>Z1G=Z1G%*%LG ####Incorporating marker information to lines</p><p>Z2GE=model.matrix(&#x0223c;0+as.factor(Pheno$Line):as.factor(Pheno$Env))</p><p>G2=kronecker(diag(3),data.matrix(G))</p><p>LG2=t(chol(G2))</p><p>Z2GE=Z2GE%*%LG2</p><p>###Defining the number of epoch and units#####################</p><p>units_M=50</p><p>epochs_M=20</p><p>##########Data for trait GY#################################</p><p>y =Pheno$Yield</p><p>X = cbind(ZE, Z1G, Z2GE)</p><p>#############Training and testing sets########################</p><p>n=dim(X)[1]</p><p>Post_trn=sample(1:n,round(n*0.65))</p><p>X_tr = X[Post_trn,]</p><p>X_ts = X[-Post_trn,]</p><p>y_tr = scale(y[Post_trn])</p><p>Mean_trn=mean(y[Post_trn])</p><p>SD_trn=sd(y[Post_trn])</p><p>y_ts = (y[-Post_trn]- Mean_trn)/SD_trn</p><p>#########Model fitting in Keras################################</p><p>model &#x0003c;- keras_model_sequential()</p><p>#########Layers specification ################################</p><p>model %&#x0003e;%</p><p>layer_dense(</p><p>units =units_M,</p><p>activation = &#x0201c;relu&#x0201d;,</p><p>input_shape = c(dim(X_tr)[2])) %&#x0003e;%</p><p>layer_dropout(rate = 0.3) %&#x0003e;% ###Input Layer</p><p>layer_dense(units = units_M, activation = &#x0201c;relu&#x0201d;) %&#x0003e;%</p><p>layer_dropout(rate = 0.3) %&#x0003e;% ###Hidden layer 1</p><p>layer_dense(units = units_M, activation = &#x0201c;relu&#x0201d;) %&#x0003e;%</p><p>layer_dropout(rate = 0.3) %&#x0003e;% ####Hidden layer 2</p><p>layer_dense(units = 1) ####Output layer</p><p>model %&#x0003e;% compile(</p><p>loss = &#x0201c;mean_squared_error&#x0201d;,</p><p>optimizer = optimizer_adam(),</p><p>metrics = c(&#x0201c;mean_squared_error&#x0201d;))</p><p>history &#x0003c;- model %&#x0003e;% fit(</p><p>X_tr, y_tr, epochs = epochs_M, batch_size = 30,</p><p>verbose = FALSE)</p><p>#######Evaluating the performance of the model###################</p><p>pf = model %&#x0003e;% evaluate(x = X_ts, y = y_ts, verbose = 0)</p><p>y_p = model %&#x0003e;% predict(X_ts)</p><p>y_p=y_p*SD_trn+ Mean_trn</p><p>y_ts=y_ts</p><p>y_ts=y_ts*SD_trn+ Mean_trn</p><p>###############Observed and predicted values of the testing set#</p><p>Y_all_tst = data.frame(cbind(y_ts, y_p))</p><p>cor(Y_all_tst[,1],Y_all_tst[,2])</p><p>plot(Y_all_tst)</p></sec></app><app><title>APPENDIX B</title><sec id="s28"><title/><table-wrap id="tblB1" position="anchor"><label>Table B1.</label><caption><p>Standard errors (SE) for the average Pearson&#x02019;s correlation (APC) for each environment in each of the 9 data sets. I denotes with and WI denotes without the (G&#x000d7;E) interaction term</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1">Env</th><th align="center" rowspan="1" colspan="1">Method</th><th align="center" rowspan="1" colspan="1">Interaction</th><th align="center" rowspan="1" colspan="1">Maize 1</th><th align="center" rowspan="1" colspan="1">Maize 2</th><th align="center" rowspan="1" colspan="1">Maize 3</th><th align="center" rowspan="1" colspan="1">Wheat 4</th><th align="center" rowspan="1" colspan="1">Wheat 5</th><th align="center" rowspan="1" colspan="1">Wheat 6</th><th align="center" rowspan="1" colspan="1">Wheat 7</th><th align="center" rowspan="1" colspan="1">Wheat 8</th><th align="center" rowspan="1" colspan="1">Wheat 9</th></tr></thead><tbody valign="top"><tr><td rowspan="1" colspan="1">Env1</td><td rowspan="1" colspan="1">GBLUP</td><td rowspan="1" colspan="1">I</td><td align="char" char="." rowspan="1" colspan="1">0.040</td><td align="char" char="." rowspan="1" colspan="1">0.030</td><td align="char" char="." rowspan="1" colspan="1">0.058</td><td align="char" char="." rowspan="1" colspan="1">0.008</td><td align="char" char="." rowspan="1" colspan="1">0.015</td><td align="char" char="." rowspan="1" colspan="1">0.014</td><td align="char" char="." rowspan="1" colspan="1">0.021</td><td align="char" char="." rowspan="1" colspan="1">0.011</td><td align="char" char="." rowspan="1" colspan="1">0.008</td></tr><tr><td rowspan="1" colspan="1">Env2</td><td rowspan="1" colspan="1">GBLUP</td><td rowspan="1" colspan="1">I</td><td align="char" char="." rowspan="1" colspan="1">0.042</td><td align="char" char="." rowspan="1" colspan="1">0.036</td><td align="char" char="." rowspan="1" colspan="1">0.031</td><td align="char" char="." rowspan="1" colspan="1">0.015</td><td align="char" char="." rowspan="1" colspan="1">0.024</td><td align="char" char="." rowspan="1" colspan="1">0.019</td><td align="char" char="." rowspan="1" colspan="1">0.019</td><td align="char" char="." rowspan="1" colspan="1">0.012</td><td align="char" char="." rowspan="1" colspan="1">0.008</td></tr><tr><td rowspan="1" colspan="1">Env3</td><td rowspan="1" colspan="1">GBLUP</td><td rowspan="1" colspan="1">I</td><td align="char" char="." rowspan="1" colspan="1">0.044</td><td align="char" char="." rowspan="1" colspan="1">0.032</td><td align="char" char="." rowspan="1" colspan="1">0.023</td><td align="char" char="." rowspan="1" colspan="1">0.019</td><td align="char" char="." rowspan="1" colspan="1">0.015</td><td align="char" char="." rowspan="1" colspan="1">0.007</td><td align="char" char="." rowspan="1" colspan="1">0.022</td><td align="char" char="." rowspan="1" colspan="1">0.016</td><td align="center" rowspan="1" colspan="1">-</td></tr><tr><td rowspan="1" colspan="1">Env4</td><td rowspan="1" colspan="1">GBLUP</td><td rowspan="1" colspan="1">I</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="char" char="." rowspan="1" colspan="1">0.011</td><td align="char" char="." rowspan="1" colspan="1">0.015</td><td align="char" char="." rowspan="1" colspan="1">0.014</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td></tr><tr><td rowspan="1" colspan="1">Env5</td><td rowspan="1" colspan="1">GBLUP</td><td rowspan="1" colspan="1">I</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="char" char="." rowspan="1" colspan="1">0.013</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td></tr><tr><td rowspan="1" colspan="1">Env1</td><td rowspan="1" colspan="1">DL</td><td rowspan="1" colspan="1">I</td><td align="char" char="." rowspan="1" colspan="1">0.036</td><td align="char" char="." rowspan="1" colspan="1">0.036</td><td align="char" char="." rowspan="1" colspan="1">0.037</td><td align="char" char="." rowspan="1" colspan="1">0.016</td><td align="char" char="." rowspan="1" colspan="1">0.017</td><td align="char" char="." rowspan="1" colspan="1">0.017</td><td align="char" char="." rowspan="1" colspan="1">0.034</td><td align="char" char="." rowspan="1" colspan="1">0.030</td><td align="char" char="." rowspan="1" colspan="1">0.005</td></tr><tr><td rowspan="1" colspan="1">Env2</td><td rowspan="1" colspan="1">DL</td><td rowspan="1" colspan="1">I</td><td align="char" char="." rowspan="1" colspan="1">0.040</td><td align="char" char="." rowspan="1" colspan="1">0.028</td><td align="char" char="." rowspan="1" colspan="1">0.033</td><td align="char" char="." rowspan="1" colspan="1">0.013</td><td align="char" char="." rowspan="1" colspan="1">0.033</td><td align="char" char="." rowspan="1" colspan="1">0.013</td><td align="char" char="." rowspan="1" colspan="1">0.021</td><td align="char" char="." rowspan="1" colspan="1">0.010</td><td align="char" char="." rowspan="1" colspan="1">0.016</td></tr><tr><td rowspan="1" colspan="1">Env3</td><td rowspan="1" colspan="1">DL</td><td rowspan="1" colspan="1">I</td><td align="char" char="." rowspan="1" colspan="1">0.083</td><td align="char" char="." rowspan="1" colspan="1">0.066</td><td align="char" char="." rowspan="1" colspan="1">0.028</td><td align="char" char="." rowspan="1" colspan="1">0.021</td><td align="char" char="." rowspan="1" colspan="1">0.027</td><td align="char" char="." rowspan="1" colspan="1">0.013</td><td align="char" char="." rowspan="1" colspan="1">0.025</td><td align="char" char="." rowspan="1" colspan="1">0.016</td><td align="center" rowspan="1" colspan="1">-</td></tr><tr><td rowspan="1" colspan="1">Env4</td><td rowspan="1" colspan="1">DL</td><td rowspan="1" colspan="1">I</td><td rowspan="1" colspan="1">-</td><td rowspan="1" colspan="1">-</td><td rowspan="1" colspan="1">-</td><td align="char" char="." rowspan="1" colspan="1">0.036</td><td align="char" char="." rowspan="1" colspan="1">0.024</td><td align="char" char="." rowspan="1" colspan="1">0.030</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td></tr><tr><td rowspan="1" colspan="1">Env5</td><td rowspan="1" colspan="1">DL</td><td rowspan="1" colspan="1">I</td><td rowspan="1" colspan="1">-</td><td rowspan="1" colspan="1">-</td><td rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="char" char="." rowspan="1" colspan="1">0.031</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td></tr><tr><td rowspan="1" colspan="1">Env1</td><td rowspan="1" colspan="1">GBLUP</td><td rowspan="1" colspan="1">WI</td><td align="char" char="." rowspan="1" colspan="1">0.048</td><td align="char" char="." rowspan="1" colspan="1">0.038</td><td align="char" char="." rowspan="1" colspan="1">0.073</td><td align="char" char="." rowspan="1" colspan="1">0.010</td><td align="char" char="." rowspan="1" colspan="1">0.010</td><td align="char" char="." rowspan="1" colspan="1">0.035</td><td align="char" char="." rowspan="1" colspan="1">0.049</td><td align="char" char="." rowspan="1" colspan="1">0.010</td><td align="char" char="." rowspan="1" colspan="1">0.007</td></tr><tr><td rowspan="1" colspan="1">Env2</td><td rowspan="1" colspan="1">GBLUP</td><td rowspan="1" colspan="1">WI</td><td align="char" char="." rowspan="1" colspan="1">0.079</td><td align="char" char="." rowspan="1" colspan="1">0.042</td><td align="char" char="." rowspan="1" colspan="1">0.066</td><td align="char" char="." rowspan="1" colspan="1">0.017</td><td align="char" char="." rowspan="1" colspan="1">0.026</td><td align="char" char="." rowspan="1" colspan="1">0.050</td><td align="char" char="." rowspan="1" colspan="1">0.013</td><td align="char" char="." rowspan="1" colspan="1">0.011</td><td align="char" char="." rowspan="1" colspan="1">0.015</td></tr><tr><td rowspan="1" colspan="1">Env3</td><td rowspan="1" colspan="1">GBLUP</td><td rowspan="1" colspan="1">WI</td><td align="char" char="." rowspan="1" colspan="1">0.044</td><td align="char" char="." rowspan="1" colspan="1">0.036</td><td align="char" char="." rowspan="1" colspan="1">0.028</td><td align="char" char="." rowspan="1" colspan="1">0.014</td><td align="char" char="." rowspan="1" colspan="1">0.024</td><td align="char" char="." rowspan="1" colspan="1">0.032</td><td align="char" char="." rowspan="1" colspan="1">0.020</td><td align="char" char="." rowspan="1" colspan="1">0.016</td><td align="center" rowspan="1" colspan="1">-</td></tr><tr><td rowspan="1" colspan="1">Env4</td><td rowspan="1" colspan="1">GBLUP</td><td rowspan="1" colspan="1">WI</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="char" char="." rowspan="1" colspan="1">0.016</td><td align="char" char="." rowspan="1" colspan="1">0.020</td><td align="char" char="." rowspan="1" colspan="1">0.022</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td></tr><tr><td rowspan="1" colspan="1">Env5</td><td rowspan="1" colspan="1">GBLUP</td><td rowspan="1" colspan="1">WI</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="char" char="." rowspan="1" colspan="1">0.033</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td></tr><tr><td rowspan="1" colspan="1">Env1</td><td rowspan="1" colspan="1">DL</td><td rowspan="1" colspan="1">WI</td><td align="char" char="." rowspan="1" colspan="1">0.029</td><td align="char" char="." rowspan="1" colspan="1">0.028</td><td align="char" char="." rowspan="1" colspan="1">0.040</td><td align="char" char="." rowspan="1" colspan="1">0.018</td><td align="char" char="." rowspan="1" colspan="1">0.018</td><td align="char" char="." rowspan="1" colspan="1">0.021</td><td align="char" char="." rowspan="1" colspan="1">0.052</td><td align="char" char="." rowspan="1" colspan="1">0.020</td><td align="char" char="." rowspan="1" colspan="1">0.015</td></tr><tr><td rowspan="1" colspan="1">Env2</td><td rowspan="1" colspan="1">DL</td><td rowspan="1" colspan="1">WI</td><td align="char" char="." rowspan="1" colspan="1">0.039</td><td align="char" char="." rowspan="1" colspan="1">0.032</td><td align="char" char="." rowspan="1" colspan="1">0.039</td><td align="char" char="." rowspan="1" colspan="1">0.019</td><td align="char" char="." rowspan="1" colspan="1">0.024</td><td align="char" char="." rowspan="1" colspan="1">0.028</td><td align="char" char="." rowspan="1" colspan="1">0.030</td><td align="char" char="." rowspan="1" colspan="1">0.008</td><td align="char" char="." rowspan="1" colspan="1">0.017</td></tr><tr><td rowspan="1" colspan="1">Env3</td><td rowspan="1" colspan="1">DL</td><td rowspan="1" colspan="1">WI</td><td align="char" char="." rowspan="1" colspan="1">0.063</td><td align="char" char="." rowspan="1" colspan="1">0.041</td><td align="char" char="." rowspan="1" colspan="1">0.036</td><td align="char" char="." rowspan="1" colspan="1">0.028</td><td align="char" char="." rowspan="1" colspan="1">0.018</td><td align="char" char="." rowspan="1" colspan="1">0.010</td><td align="char" char="." rowspan="1" colspan="1">0.025</td><td align="char" char="." rowspan="1" colspan="1">0.009</td><td align="center" rowspan="1" colspan="1">-</td></tr><tr><td rowspan="1" colspan="1">Env4</td><td rowspan="1" colspan="1">DL</td><td rowspan="1" colspan="1">WI</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="char" char="." rowspan="1" colspan="1">0.019</td><td align="char" char="." rowspan="1" colspan="1">0.020</td><td align="char" char="." rowspan="1" colspan="1">0.038</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td></tr><tr><td rowspan="1" colspan="1">Env5</td><td rowspan="1" colspan="1">DL</td><td rowspan="1" colspan="1">WI</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="char" char="." rowspan="1" colspan="1">0.030</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">-</td></tr></tbody></table></table-wrap></sec></app></app-group></back></article>