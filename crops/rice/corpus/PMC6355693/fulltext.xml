<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName journalpublishing.dtd?><?SourceDTD.Version 2.3?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Psychol</journal-id><journal-id journal-id-type="iso-abbrev">Front Psychol</journal-id><journal-id journal-id-type="publisher-id">Front. Psychol.</journal-id><journal-title-group><journal-title>Frontiers in Psychology</journal-title></journal-title-group><issn pub-type="epub">1664-1078</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6355693</article-id><article-id pub-id-type="doi">10.3389/fpsyg.2019.00058</article-id><article-categories><subj-group subj-group-type="heading"><subject>Psychology</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>CROCUFID: A Cross-Cultural Food Image Database for Research on Food Elicited Affective Responses</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Toet</surname><given-names>Alexander</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="c001"><sup>*</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/27359/overview"/></contrib><contrib contrib-type="author"><name><surname>Kaneko</surname><given-names>Daisuke</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/478797/overview"/></contrib><contrib contrib-type="author"><name><surname>de Kruijf</surname><given-names>Inge</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/577997/overview"/></contrib><contrib contrib-type="author"><name><surname>Ushiama</surname><given-names>Shota</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib><contrib contrib-type="author"><name><surname>van Schaik</surname><given-names>Martin G.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/121625/overview"/></contrib><contrib contrib-type="author"><name><surname>Brouwer</surname><given-names>Anne-Marie</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/11316/overview"/></contrib><contrib contrib-type="author"><name><surname>Kallen</surname><given-names>Victor</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/568197/overview"/></contrib><contrib contrib-type="author"><name><surname>van Erp</surname><given-names>Jan B. F.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff5"><sup>5</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/11317/overview"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Perceptual and Cognitive Systems, Netherlands Organisation for Applied Scientific Research (TNO)</institution>, <addr-line>Soesterberg</addr-line>, <country>Netherlands</country></aff><aff id="aff2"><sup>2</sup><institution>Kikkoman Europe R&#x00026;D Laboratory B.V.</institution>, <addr-line>Wageningen</addr-line>, <country>Netherlands</country></aff><aff id="aff3"><sup>3</sup><institution>Microbiology &#x00026; Systems Biology, Netherlands Organisation for Applied Scientific Research (TNO)</institution>, <addr-line>Zeist</addr-line>, <country>Netherlands</country></aff><aff id="aff4"><sup>4</sup><institution>Department of Research &#x00026; Development Division, Kikkoman Corporation</institution>, <addr-line>Noda</addr-line>, <country>Japan</country></aff><aff id="aff5"><sup>5</sup><institution>Research Group Human Media Interaction, University of Twente</institution>, <addr-line>Enschede</addr-line>, <country>Netherlands</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Boris C. Rodr&#x000ed;guez-Mart&#x000ed;n, Fundaci&#x000f3;n Recal, Spain</p></fn><fn fn-type="edited-by"><p>Reviewed by: Martin Yeomans, University of Sussex, United Kingdom; Davide Giacalone, University of Southern Denmark, Denmark</p></fn><corresp id="c001">*Correspondence: Alexander Toet, <email>lex.toet@tno.nl</email></corresp><fn fn-type="other" id="fn002"><p>This article was submitted to Eating Behavior, a section of the journal Frontiers in Psychology</p></fn></author-notes><pub-date pub-type="epub"><day>25</day><month>1</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>10</volume><elocation-id>58</elocation-id><history><date date-type="received"><day>16</day><month>10</month><year>2018</year></date><date date-type="accepted"><day>09</day><month>1</month><year>2019</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2019 Toet, Kaneko, de Kruijf, Ushiama, van Schaik, Brouwer, Kallen and van Erp.</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Toet, Kaneko, de Kruijf, Ushiama, van Schaik, Brouwer, Kallen and van Erp</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>We present CROCUFID: a CROss-CUltural Food Image Database that currently contains 840 images, including 479 food images with detailed metadata and 165 images of non-food items. The database includes images of sweet, savory, natural, and processed food from Western and Asian cuisines. To create sufficient variability in valence and arousal we included images of food with different degrees of appetitiveness (fresh, unfamiliar, molded or rotten, spoiled, and partly consumed). We used a standardized photographing protocol, resulting in high resolution images depicting all food items on a standard background (a white plate), seen from a fixed viewing (45&#x000b0;) angle. CROCUFID is freely available under the CC-By Attribution 4.0 International license and hosted on the OSF repository. The advantages of the CROCUFID database over other databases are its (1) free availability, (2) full coverage of the valence &#x02013; arousal space, (3) use of standardized recording methods, (4) inclusion of multiple cuisines and unfamiliar foods, (5) availability of normative and demographic data, (6) high image quality and (7) capability to support future (e.g., virtual and augmented reality) applications. Individuals from the United Kingdom (<italic>N</italic> = 266), North-America (<italic>N</italic> = 275), and Japan (<italic>N</italic> = 264) provided normative ratings of valence, arousal, perceived healthiness, and desire-to-eat using visual analog scales (VAS). In addition, for each image we computed 17 characteristics that are known to influence affective observer responses (e.g., texture, regularity, complexity, and colorfulness). Significant differences between groups and significant correlations between image characteristics and normative ratings were in accordance with previous research, indicating the validity of CROCUFID. We expect that CROCUFID will facilitate comparability across studies and advance experimental research on the determinants of food-elicited emotions. We plan to extend CROCUFID in the future with images of food from a wide range of different cuisines and with non-food images (for applications in for instance neuro-physiological studies). We invite researchers from all parts of the world to contribute to this effort by creating similar image sets that can be linked to this collection, so that CROCUFID will grow into a truly multicultural food database.</p></abstract><kwd-group><kwd>food pictures</kwd><kwd>food image database</kwd><kwd>valence</kwd><kwd>arousal</kwd><kwd>color</kwd><kwd>complexity</kwd><kwd>healthiness</kwd><kwd>desire to eat</kwd></kwd-group><counts><fig-count count="8"/><table-count count="4"/><equation-count count="0"/><ref-count count="145"/><page-count count="21"/><word-count count="0"/></counts></article-meta></front><body><sec><title>Introduction</title><p>Visual cues constitute a primary sensory input that allows predictions about the edibility and palatability of food. Through learning, visual food characteristics can become secondary reinforcers that affect eating-related behavior. The human brain has specific regions involved in the appetitive and affective processing of visual presentations of food stimuli (<xref rid="B56" ref-type="bibr">Killgore et al., 2003</xref>). The sight of food elicits a wide range of physiological, emotional and cognitive responses (<xref rid="B130" ref-type="bibr">van der Laan et al., 2011</xref>). Previous research has shown that viewing pictures of food not only activates the visual cortex, but also brain areas that code how food actually tastes (the insula/operculum) and the reward values of tasting it (the orbitofrontal cortex; <xref rid="B106" ref-type="bibr">Simmons et al., 2005</xref>). Food images are therefore typically considered as useful proxies for the real thing (e.g., <xref rid="B36" ref-type="bibr">Foroni et al., 2013</xref>; <xref rid="B10" ref-type="bibr">Blechert et al., 2014b</xref>; <xref rid="B77" ref-type="bibr">Miccoli et al., 2014</xref>, <xref rid="B76" ref-type="bibr">2016</xref>). Food pictures are increasingly used as stimuli in research on the factors underlying affective and appetitive responses to foods (e.g., in online experiments <xref rid="B10" ref-type="bibr">Blechert et al., 2014b</xref>; <xref rid="B53" ref-type="bibr">Jensen et al., 2016</xref>), especially when the experimental paradigm limits actual consumption (e.g., experiments involving physiological measures such as EEG and fMRI; <xref rid="B56" ref-type="bibr">Killgore et al., 2003</xref>; <xref rid="B92" ref-type="bibr">Piqueras-Fiszman et al., 2014</xref>).</p><p>The sensory characteristics and evoked emotions of food are crucial factors in predicting a consumer&#x02019;s food preference and therefore in developing new products (<xref rid="B29" ref-type="bibr">Dalenberg et al., 2014</xref>; <xref rid="B43" ref-type="bibr">Gutjar et al., 2015</xref>). Hedonic ratings alone do not predict food choice behavior accurately (<xref rid="B143" ref-type="bibr">Zandstra and El-Deredy, 2011</xref>; <xref rid="B42" ref-type="bibr">Griffioen-Roose et al., 2013</xref>). Emotions have an incremental predictive value over hedonic ratings alone, so that the best prediction can be obtained by combining both measures (<xref rid="B29" ref-type="bibr">Dalenberg et al., 2014</xref>; <xref rid="B102" ref-type="bibr">Samant and Seo, 2018</xref>). The relationship between food and emotions appears to be bidirectional: emotions influence eating behavior, while eating behavior also affects the consumer&#x02019;s emotional state (<xref rid="B30" ref-type="bibr">Desmet and Schifferstein, 2008</xref>). Assessing emotional responses to foods could reveal product attributes that are a valuable source of information for product development and marketing going beyond traditional sensory and acceptability measurements (<xref rid="B124" ref-type="bibr">Thomson et al., 2010</xref>). Given the high failure rate of new food products in the market (between 60 and 80%: <xref rid="B62" ref-type="bibr">K&#x000f6;ster and Mojet, 2007</xref>) and the fact that there is often only little difference in quality, price and design of different food products, knowledge about the affective responses of food products appears to be important for the food industry (<xref rid="B61" ref-type="bibr">K&#x000f6;ster, 2003</xref>; <xref rid="B103" ref-type="bibr">Schifferstein et al., 2013</xref>). Therefore, it is important to obtain valid and reliable measurements of food-evoked emotions. Pictures are easy to present and have been widely used to investigate the different factors underlying affective responses in general (<xref rid="B64" ref-type="bibr">Lang et al., 1993</xref>) and appetitive responses to foods (<xref rid="B9" ref-type="bibr">Blechert et al., 2014a</xref>; <xref rid="B48" ref-type="bibr">Hebert et al., 2015</xref>). To facilitate multi-disciplinary and cross-cultural studies on food-related emotions we present a publicly available database containing images of a wide range of various foods from different cuisines, together with normative observer ratings.</p><p>Despite their recognized importance for research, only a few food image databases have been made publicly available to support research on human eating behavior (<xref rid="B21" ref-type="bibr">Chen et al., 2009</xref>; <xref rid="B36" ref-type="bibr">Foroni et al., 2013</xref>; <xref rid="B10" ref-type="bibr">Blechert et al., 2014b</xref>; <xref rid="B77" ref-type="bibr">Miccoli et al., 2014</xref>; <xref rid="B19" ref-type="bibr">Charbonnier et al., 2016</xref>) and they fall short on one or more of the following important criteria: (1) the public availability and ease of reproducibility, (2) the coverage of the full valence and arousal space, (3) the use of standardized recording methods, (4) the use of multiple cuisines and unfamiliar foods, (5) the availability of normative and demographic data (6) the image quality and capability for future applications. We will discuss the importance of these criteria in the following sections.</p><sec><title>Public Availability and Ease of Reproducibility</title><p>Researchers often select food images based on their personal preferences. As a result, the image sets are typically highly variable and difficult to compare, re-use or replicate (<xref rid="B89" ref-type="bibr">Pashler and Wagenmakers, 2012</xref>; <xref rid="B19" ref-type="bibr">Charbonnier et al., 2016</xref>). While some studies used their own set of images (<xref rid="B70" ref-type="bibr">Manzocco et al., 2013</xref>; <xref rid="B92" ref-type="bibr">Piqueras-Fiszman et al., 2014</xref>; <xref rid="B18" ref-type="bibr">Charbonnier et al., 2015</xref>), others collected stimuli from the internet (<xref rid="B36" ref-type="bibr">Foroni et al., 2013</xref>; <xref rid="B10" ref-type="bibr">Blechert et al., 2014b</xref>; <xref rid="B77" ref-type="bibr">Miccoli et al., 2014</xref>; <xref rid="B48" ref-type="bibr">Hebert et al., 2015</xref>). Also, most studies report little information about the used pictures and do not make the used image sets available to other researchers. It has been recognized that the comparability of research findings across studies using food pictures would benefit from the availability of public databases with standardized images (<xref rid="B36" ref-type="bibr">Foroni et al., 2013</xref>; <xref rid="B10" ref-type="bibr">Blechert et al., 2014b</xref>; <xref rid="B19" ref-type="bibr">Charbonnier et al., 2016</xref>).</p></sec><sec><title>Coverage of the Affective Space</title><p>Existing food image databases typically focus on appetitive and positively valenced food pictures and lack aversive or negatively valenced ones. As a result, they cover only a limited part of the valence-arousal space, which significantly reduces their value for studies on food-evoked emotions. This is a result of the fact that food-related research has typically investigated positive and familiar food products while neglecting negative and unfamiliar ones, thus ignoring emotions and behaviors like disgust and rejection. This may suffice in product development and consumer research, but not in research on emotional responses to less liked and disliked food (<xref rid="B92" ref-type="bibr">Piqueras-Fiszman et al., 2014</xref>). Although positive food related emotional states are far more common, it is unavoidable that consumers sometimes experience negative food-related emotions (<xref rid="B30" ref-type="bibr">Desmet and Schifferstein, 2008</xref>; <xref rid="B57" ref-type="bibr">King et al., 2010</xref>; <xref rid="B16" ref-type="bibr">Cardello et al., 2012</xref>; <xref rid="B70" ref-type="bibr">Manzocco et al., 2013</xref>; <xref rid="B82" ref-type="bibr">Ng et al., 2013</xref>; <xref rid="B117" ref-type="bibr">Spinelli et al., 2015</xref>). Image databases intended to support research on human food-related behavior and to understand the full variety of emotional responses to food should therefore also include images of negatively valenced and unfamiliar products.</p></sec><sec><title>Standardized Images</title><p>Standardization across image characteristics (e.g., background, color, brightness, contrast, size etc.) will enhance the reliability and validity of experimental results. However, most of the current databases contain unstandardized images of food items, sometimes simply collected from the internet and pasted on white backgrounds. The resulting large variation in image characteristics (e.g., contrast and brightness), the different angles at which foods are depicted, the variation in magnifications, the lack of a visual reference such as a plate together with the lack of shadows may compromise the reliability and validity of the experimental results from studies in which these images are used (<xref rid="B19" ref-type="bibr">Charbonnier et al., 2016</xref>).</p></sec><sec><title>Multiple Cuisines and Unfamiliar Foods</title><p>Cultural background and familiarity are other factors that strongly influence food-related emotional responses (<xref rid="B132" ref-type="bibr">van Zyl and Meiselman, 2015</xref>) and determine food choice (<xref rid="B101" ref-type="bibr">Rozin, 1996</xref>). Databases should preferably include images representing a wide range of different food types (natural and processed) from a variety of cuisines. Existing food image databases include only a small range of (typically Western) cuisines, which limits their value for cross-cultural studies. Hence, there is a need for a food image database that includes food from different cuisines and has been validated across different nations, such as Latin-American, African, and Middle-Eastern.</p></sec><sec><title>Normative and Demographic Data</title><p>The inclusion of different food types in a food image database enables the selection of food with different colors, caloric content, macronutrients, readiness to eat, flavor, nutritional composition, healthiness, and familiarity and the distinction of different classes such as vegetables, meat-containing dishes, fruits, and snacks (e.g., <xref rid="B10" ref-type="bibr">Blechert et al., 2014b</xref>).</p><p>A validated food-image database should preferably also include data on individual differences that are known to influence normative subjective ratings like age (<xref rid="B51" ref-type="bibr">Hoogeveen et al., 2015</xref>), gender (e.g., <xref rid="B17" ref-type="bibr">Cepeda-Benito et al., 2003</xref>; <xref rid="B125" ref-type="bibr">Toepel et al., 2012</xref>), body mass index (BMI; e.g., <xref rid="B8" ref-type="bibr">Berthoud and Zheng, 2012</xref>; <xref rid="B125" ref-type="bibr">Toepel et al., 2012</xref>), food preferences or diets (e.g., <xref rid="B119" ref-type="bibr">Stockburger et al., 2009</xref>) and psychophysiological state (e.g., <xref rid="B50" ref-type="bibr">Hoefling et al., 2009</xref>). Also, the measures included in the validation of the database should make a careful distinction between liking (the hedonic appraisal of food) and wanting (the motivation to consume: <xref rid="B7" ref-type="bibr">Berridge, 2004</xref>). In addition, the inclusion of images of artificial non-food objects (either related to food &#x02013; like tableware or cutlery &#x02013; or unrelated to food &#x02013; like office supplies or toys), and organic non-food objects (like plants and flowers) is recommended to support brain studies (<xref rid="B130" ref-type="bibr">van der Laan et al., 2011</xref>).</p></sec><sec><title>Image Quality and Suitability for Different Applications</title><p>Item-only (transparent) food images can be used in different experimental techniques like simulations of different ambient environments and multisensory cues. Recent studies have shown that the sensorial and hedonic experience of food is significantly influenced by the color (<xref rid="B39" ref-type="bibr">Genschow et al., 2012</xref>; <xref rid="B90" ref-type="bibr">Piqueras-Fiszman et al., 2012</xref>; <xref rid="B91" ref-type="bibr">Piqueras-Fiszman et al., 2013</xref>; <xref rid="B118" ref-type="bibr">Stewart and Goss, 2013</xref>; <xref rid="B79" ref-type="bibr">Michel et al., 2015b</xref>; <xref rid="B128" ref-type="bibr">Tu et al., 2016</xref>; <xref rid="B23" ref-type="bibr">Chen et al., 2018</xref>), shape (<xref rid="B90" ref-type="bibr">Piqueras-Fiszman et al., 2012</xref>; <xref rid="B118" ref-type="bibr">Stewart and Goss, 2013</xref>; <xref rid="B23" ref-type="bibr">Chen et al., 2018</xref>) and size (<xref rid="B131" ref-type="bibr">Van Ittersum and Wansink, 2012</xref>; <xref rid="B138" ref-type="bibr">Wansink and van Ittersum, 2013</xref>) of the plateware that is used for serving. In real-world settings, contextual factors such as ambience (<xref rid="B120" ref-type="bibr">Stroebele and De Castro, 2004</xref>), room color (<xref rid="B75" ref-type="bibr">Mel&#x000e9;ndez-Mart&#x000ed;nez et al., 2005</xref>; <xref rid="B85" ref-type="bibr">Oberfeld et al., 2009</xref>; <xref rid="B115" ref-type="bibr">Spence et al., 2014</xref>; <xref rid="B104" ref-type="bibr">Schifferstein et al., 2017</xref>), background sounds and music (<xref rid="B73" ref-type="bibr">McCarron and Tierney, 1989</xref>; <xref rid="B114" ref-type="bibr">Spence and Shankar, 2010</xref>; <xref rid="B27" ref-type="bibr">Crisinel et al., 2012</xref>; <xref rid="B34" ref-type="bibr">Fiegel et al., 2014</xref>), ambient temperature (<xref rid="B49" ref-type="bibr">Herman, 1993</xref>), the color and intensity of ambient lighting (<xref rid="B85" ref-type="bibr">Oberfeld et al., 2009</xref>; <xref rid="B121" ref-type="bibr">Suk et al., 2012</xref>; <xref rid="B45" ref-type="bibr">Hasenbeck et al., 2014</xref>; <xref rid="B115" ref-type="bibr">Spence et al., 2014</xref>; <xref rid="B24" ref-type="bibr">Cho et al., 2015</xref>) and ambient scents (<xref rid="B109" ref-type="bibr">Spence, 2015a</xref>) are also known to modulate the assessment and consumption of food and drinks (for reviews see <xref rid="B113" ref-type="bibr">Spence and Piqueras-Fiszman, 2014</xref>; <xref rid="B111" ref-type="bibr">Spence, 2018</xref>). It has been suggested that these effects reflect a transfer of sensations through cross-modal correspondences (<xref rid="B108" ref-type="bibr">Spence, 2011</xref>). However, the exact nature of these effects and the way in which they interact is still a topic of research. Acquiring further knowledge about the way in which multisensory contextual and ambient cues interact and affect human food related behavior will be of great value in retail and restaurant settings and may help to improve food experience and consumption behavior (e.g., to fight obesity, to enhance the consumption experience of elderly or people in hospital and care facilities, etc.). Virtual Reality (VR: <xref rid="B40" ref-type="bibr">Gorini et al., 2010</xref>; <xref rid="B84" ref-type="bibr">Nordbo et al., 2015</xref>; <xref rid="B129" ref-type="bibr">Ung et al., 2018</xref>), Augmented Reality (AR: <xref rid="B81" ref-type="bibr">Narumi et al., 2012</xref>; <xref rid="B87" ref-type="bibr">Pallavicini et al., 2016</xref>), Mixed Reality (MR: <xref rid="B52" ref-type="bibr">Huisman et al., 2016</xref>) or other immersive technologies (<xref rid="B4" ref-type="bibr">Bangcuyo et al., 2015</xref>; <xref rid="B66" ref-type="bibr">Liu et al., 2018</xref>) appear to be promising tools for this kind of research (see also <xref rid="B113" ref-type="bibr">Spence and Piqueras-Fiszman, 2014</xref>). The extension of VR and AR systems with novel multisensory (taste, smell, and tactile) interfaces can further enhance the perceived reality of food imagery (<xref rid="B14" ref-type="bibr">Braun et al., 2016</xref>) and will provide researchers control over the various inputs that determine a given food experience (<xref rid="B134" ref-type="bibr">Velasco et al., 2018</xref>). This may significantly increase the effectiveness of these systems for studies on food-related emotions and behavior, personal health and wellbeing (<xref rid="B25" ref-type="bibr">Comber et al., 2014</xref>; <xref rid="B86" ref-type="bibr">Obrist et al., 2016</xref>). Multisensory HCI systems may for instance be used to match the visual, auditory and olfactory characteristics of a simulated table or restaurant setting to food (e.g., <xref rid="B97" ref-type="bibr">Reinoso Carvalho et al., 2016</xref>). Transparent item-only images can be overlaid on images of plates with different shapes, colors and textures to study the effects of the visual characteristics of the plate on the appraisal of food. To study the effects of the visual characteristics of the environment, these plated food images can in turn be overlaid on images, movies or VR renderings of different backgrounds (e.g., images showing plated food placed on tables covered with different tablecloths or on different natural surfaces, movies showing environments with different lighting characteristics, etc.). To study the effects of ambient sound, (dynamic) lighting or social presence, the plated food images can be overlaid on movies showing dynamic environments with different ambient characteristics. Adding smells, tastes or tactile stimulation to the image presentation may serve to further enhance the realism of this type of studies (<xref rid="B134" ref-type="bibr">Velasco et al., 2018</xref>). Using item-only images in augmented reality settings will provide an efficient way to study the effects of ambient characteristics on human response to many different types of food (<xref rid="B83" ref-type="bibr">Nishizawa et al., 2016</xref>).</p></sec><sec><title>The CROCUFID Approach</title><p>To complement the currently available food image databases and to further support systematic neuroscientific and behavioral studies on the emotional impact of food we present CROCUFID: a CROss-CUltural Food Image Database with high-resolution images of different types of food from various (currently mainly Western and Asian) cuisines, together with normative ratings (by participants from the United Kingdom, North-America, and Japan) of valence, arousal, desire-to-eat, perceived healthiness, and familiarity, complemented with computational measures of physical food properties that are known (or <italic>a priori</italic> considered likely) to influence food experience. To make it useful for brain (e.g., fMRI) studies, the dataset also contains images of non-food objects. To cover the full valence-arousal space, CROCUFID includes images of food with different degrees of appetitiveness (fresh, unfamiliar, molded or rotten, contaminated, and partly consumed). The inclusion of food types from different (Western and Asian) cuisines allows the images to be used in a culturally diverse population. The images resemble the viewing of a plate of food on a table during meal time and will therefore be a useful tool to assess emotional responses in studies that limit actual consumption (e.g., experiments involving physiological measures such as EEG and fMRI). To afford the use of CROCUFID in human&#x02013;computer interaction (HCI) studies, all images in the dataset are also provided as item-only (transparent PNG) images. CROCUFID complements the F4H image collection since both were registered using the same photographing protocol (<xref rid="B19" ref-type="bibr">Charbonnier et al., 2016</xref>).</p></sec></sec><sec><title>Related Work</title><p>In this section we first describe the characteristics of some currently and publicly available food image databases that have been designed to support neuroscientific and behavioral research on human eating behavior and preferences. Then we review the databases that have been constructed to develop and train automatic food recognition and ingredient or recipe retrieval algorithms. To the best of our knowledge, these databases are currently the only ones publicly available that contain images of a wide range of different cuisines. However, they generally appear to be unsuitable for systematic research on human food-related behavior since they typically contain real-life images with largely varying backgrounds, taken from different points of view, with different scales and rotation angles and under varying lighting conditions. Next, we describe the value of CROCUFID for studies on the effects of environmental characteristics and background context on human food experience. Finally, we discuss how CROCUFID can be used to perform cross-cultural food studies.</p><sec><title>Food Image Databases for Human Observer Studies</title><p>Table <xref rid="T1" ref-type="table">1</xref> provides an overview of publicly available food image databases for human observer studies.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p>Overview of food image databases for human observer studies.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1">Databases</th><th valign="top" align="left" rowspan="1" colspan="1">Coverage of affective space</th><th valign="top" align="left" rowspan="1" colspan="1">Recording methods</th><th valign="top" align="left" rowspan="1" colspan="1">Cuisines</th><th valign="top" align="left" rowspan="1" colspan="1">Availability of normative (and demographic) data</th><th valign="top" align="left" rowspan="1" colspan="1">Remarks</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">FRIDa (<xref rid="B36" ref-type="bibr">Foroni et al., 2013</xref>)</td><td valign="top" align="left" rowspan="1" colspan="1">Mainly positive valence</td><td valign="top" align="left" rowspan="1" colspan="1">Not standardized</td><td valign="top" align="left" rowspan="1" colspan="1">Mainly Western</td><td valign="top" align="left" rowspan="1" colspan="1">Valence, Arousal, Familiarity (Italian)</td><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Low resolution (530 pixels &#x000d7; 530 pixels)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Collected from Internet</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Includes non-food images</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Food-Pics (<xref rid="B10" ref-type="bibr">Blechert et al., 2014b</xref>)</td><td valign="top" align="left" rowspan="1" colspan="1">Mainly positive valence</td><td valign="top" align="left" rowspan="1" colspan="1">Not standardized</td><td valign="top" align="left" rowspan="1" colspan="1">Mainly Western</td><td valign="top" align="left" rowspan="1" colspan="1">Valence, Arousal, Familiarity, Recognizability, Complexity, Palatability (German and North American)</td><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Low resolution (600 pixels &#x000d7; 450 pixels)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Collected from Internet No fixed background</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">OLAF (<xref rid="B77" ref-type="bibr">Miccoli et al., 2014</xref>, <xref rid="B76" ref-type="bibr">2016</xref>)</td><td valign="top" align="left" rowspan="1" colspan="1">Mainly positive valence</td><td valign="top" align="left" rowspan="1" colspan="1">Not standardized</td><td valign="top" align="left" rowspan="1" colspan="1">Mainly Western</td><td valign="top" align="left" rowspan="1" colspan="1">Valence, Arousal, Dominance, Food Craving (Spanish)</td><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; High resolution (4000 pixels &#x000d7; 3000 pixels)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Includes some low valence images from IAPS</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Includes non-food images</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">F4H (<xref rid="B19" ref-type="bibr">Charbonnier et al., 2016</xref>)</td><td valign="top" align="left" rowspan="1" colspan="1">Mainly positive valence</td><td valign="top" align="left" rowspan="1" colspan="1">Standardized</td><td valign="top" align="left" rowspan="1" colspan="1">Mainly Western</td><td valign="top" align="left" rowspan="1" colspan="1">Liking, Healthiness, Recognizability, Perceived Calories (Greek, Dutch, Scottish, German, Hungary, and Swedish)</td><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Resolution (3872 pixels &#x000d7; 2592 pixels)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; All images registered by the authors</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Includes non-food images</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/></tr></tbody></table></table-wrap><p>The Foodcast Research Image Database (FRIDa<sup><xref ref-type="fn" rid="fn01">1</xref></sup>: <xref rid="B36" ref-type="bibr">Foroni et al., 2013</xref>) contains images of predominantly Western natural, transformed, and rotten food, natural and artificial non-food objects, animals, flowers and scenes, along with a description of several physical product properties (e.g., size, brightness and spatial frequency content) and normative ratings (by Italian participants) on several dimensions (including valence, arousal, and familiarity). The items were collected from the internet, pasted on a white background and have a low resolution (530 pixels &#x000d7; 530 pixels).</p><p>The Food-pics database<sup><xref ref-type="fn" rid="fn02">2</xref></sup> (<xref rid="B10" ref-type="bibr">Blechert et al., 2014b</xref>) contains images of predominantly Western food types, together with normative ratings (by participants from German speaking countries and North America) on familiarity, recognizability, complexity, valence, arousal, palatability, and desire to eat. The items were collected from the internet, pasted on a white background and have a low resolution (600 pixels &#x000d7; 450 pixels). Food-pics has been designed to support experimental research on food perception and eating behavior in general.</p><p>The Open Library of Affective Foods (OLAF<sup><xref ref-type="fn" rid="fn03">3</xref></sup>: <xref rid="B77" ref-type="bibr">Miccoli et al., 2014</xref>, <xref rid="B76" ref-type="bibr">2016</xref>) is a database of food pictures representing four different types of Western food (vegetables, fruit, sweet and salty high-fat foods), along with normative ratings (by Spanish students) of valence, arousal, dominance and food craving. The images have a high resolution (up to 4000 pixels &#x000d7; 3000 pixels) and include food served in restaurants and homemade meals, and display non-food items in the background to increase their ecological value and to resemble the appearance of images from the International Affective Picture System (IAPS: <xref rid="B63" ref-type="bibr">Lang et al., 2005</xref>). The four selected food categories focus on the extremes of the low-calorie/high-calorie food axis. Although OLAF was specifically compiled to be used in studies on the affective and appetitive effects of food, it contains no food images with negative valence. To remedy the lack of negative valence images and to provide affective anchors, OLAF was extended with 36 non-food images from the IAPS (12 from each of the three valence categories pleasant, neutral, unpleasant) that cover the full valence-arousal space.</p><p>The Full4Health Image Collection (F4H<sup><xref ref-type="fn" rid="fn04">4</xref></sup>) contains 228 images of Western food types of different caloric content, together with normative ratings (by adults from Greece, Netherlands and Scotland, and by children from Germany, Hungary, and Sweden) on recognizability, liking, healthiness and perceived number of calories. In addition, F4H also includes images of 73 non-food items. The images have a high resolution (3872 pixels &#x000d7; 2592 pixels) and were registered according to a standardized photographing protocol (<xref rid="B19" ref-type="bibr">Charbonnier et al., 2016</xref>). F4H has been designed for health-related studies in which (perceived) caloric content is of interest and contains no food pictures with negative valence.</p></sec><sec><title>Food Image Databases for Automatic Recognition Studies</title><p>Table <xref rid="T2" ref-type="table">2</xref> provides an overview of publicly available food image databases for automatic image recognition studies.</p><table-wrap id="T2" position="float"><label>Table 2</label><caption><p>Overview of food image databases for autonomic recognition studies.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1">Databases</th><th valign="top" align="left" rowspan="1" colspan="1">Coverage of affective space</th><th valign="top" align="left" rowspan="1" colspan="1">Recording methods</th><th valign="top" align="left" rowspan="1" colspan="1">Cuisines</th><th valign="top" align="left" rowspan="1" colspan="1">Availability of normative (and demographic) data</th><th valign="top" align="left" rowspan="1" colspan="1">Remarks</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">PFID (<xref rid="B21" ref-type="bibr">Chen et al., 2009</xref>)</td><td valign="top" align="left" rowspan="1" colspan="1">A small part of valence and arousal space</td><td valign="top" align="left" rowspan="1" colspan="1">Not standardized</td><td valign="top" align="left" rowspan="1" colspan="1">Mainly Western</td><td valign="top" align="left" rowspan="1" colspan="1">Not available</td><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; High resolution (2592 &#x000d7; 1944 pixels)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Collected by the authors</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; No fixed background</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">NU FOOD (<xref rid="B123" ref-type="bibr">Takahashi et al., 2017</xref>)</td><td valign="top" align="left" rowspan="1" colspan="1">Mainly positive valence</td><td valign="top" align="left" rowspan="1" colspan="1">Standardized</td><td valign="top" align="left" rowspan="1" colspan="1">Some Asian, Some Western</td><td valign="top" align="left" rowspan="1" colspan="1">Not available</td><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Only 10 different cuisines (six Asian and four Western cuisines)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; No resolution specified</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">ChineseFoodNet (<xref rid="B22" ref-type="bibr">Chen et al., 2017</xref>)</td><td valign="top" align="left" rowspan="1" colspan="1">Mainly positive valence</td><td valign="top" align="left" rowspan="1" colspan="1">Not standardized</td><td valign="top" align="left" rowspan="1" colspan="1">Only Chinese</td><td valign="top" align="left" rowspan="1" colspan="1">Not available</td><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Variable resolution</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Collected from Internet (185,628 images)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; No fixed background</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">UNICT Food Dataset 889 (<xref rid="B32" ref-type="bibr">Farinella et al., 2015</xref>)</td><td valign="top" align="left" rowspan="1" colspan="1">Mainly positive valence</td><td valign="top" align="left" rowspan="1" colspan="1">Not standardized</td><td valign="top" align="left" rowspan="1" colspan="1">Italian, English, Thailand, Indian, Japanese etc.</td><td valign="top" align="left" rowspan="1" colspan="1">Not available</td><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Variable resolution</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Collected with smartphones (3,583 images)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; No fixed background</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">UEC-Food 100 (<xref rid="B72" ref-type="bibr">Matsuda et al., 2012</xref>) UEC-Food 256 (<xref rid="B55" ref-type="bibr">Kawano and Yanai, 2015</xref>)</td><td valign="top" align="left" rowspan="1" colspan="1">Mainly positive valence</td><td valign="top" align="left" rowspan="1" colspan="1">Not standardized</td><td valign="top" align="left" rowspan="1" colspan="1">France, Italy, United States, China, Thailand, Vietnam, Japan, Indonesia, etc.</td><td valign="top" align="left" rowspan="1" colspan="1">Not available</td><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Variable resolution</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Collected from Internet</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; No fixed background</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">UPMCFOOD-101 (<xref rid="B137" ref-type="bibr">Wang et al., 2015</xref>) ETHZFOOD-101 (<xref rid="B12" ref-type="bibr">Bossard et al., 2014</xref>)</td><td valign="top" align="left" rowspan="1" colspan="1">Mainly positive valence</td><td valign="top" align="left" rowspan="1" colspan="1">Not standardized</td><td valign="top" align="left" rowspan="1" colspan="1">More than 101 international food categories</td><td valign="top" align="left" rowspan="1" colspan="1">Not available</td><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Variable resolution</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Collected from Internet</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; No fixed background</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">VIREO-172 (<xref rid="B20" ref-type="bibr">Chen and Ngo, 2016</xref>)</td><td valign="top" align="left" rowspan="1" colspan="1">Mainly positive valence</td><td valign="top" align="left" rowspan="1" colspan="1">Not standardized</td><td valign="top" align="left" rowspan="1" colspan="1">Only Chinese</td><td valign="top" align="left" rowspan="1" colspan="1">Not available</td><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Variable resolution</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; Collected from Internet (110,241 images)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">&#x02022; No fixed background</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/></tr></tbody></table></table-wrap><p>The Pittsburgh Fast-Food Image Dataset (PFID<sup><xref ref-type="fn" rid="fn05">5</xref></sup>: <xref rid="B21" ref-type="bibr">Chen et al., 2009</xref>) contains still images, stereo pairs, 360-degrees videos and videos of Western fast-food and eating events, acquired in both restaurant environments and laboratory settings. The dataset represents 101 different foods with information on caloric content and is primarily intended for research on automated visual food recognition for dietary assessment. Although the images are registered at a high resolution (2592 pixels &#x000d7; 1944 pixels), the products that are shown occupy only a small region of the image, while the luminance, structure and shadowing of the background vary largely across the image set (due to undulations in the gray cloth in the background). Since the database only contains images of fast-food, it covers only a small part of the valence-arousal space and is therefore not suitable for systematically studying the emotional impact of food. Also, the database is significantly (Western) cultural specific, implying the previously mentioned cross-cultural restrictions for this dataset as well.</p><p>The NU FOOD 360&#x000d7;10 database<sup><xref ref-type="fn" rid="fn06">6</xref></sup> (<xref rid="B123" ref-type="bibr">Takahashi et al., 2017</xref>) is a small food image database containing images of 10 different types of food, each shot at three elevation angles (30, 60, and 90 degrees) from 12 different angles (30 degree spacing). Six of the 10 foods are typically Asian (Sashimi, Curry and rice, Eel rice-bowl, Tempura rice-bowl, Fried pork rice-bowl, and Tuna rice-bowl), while the remaining four represent Western food (Beef stew, Hamburger steak, Cheese burger, and Fish burger). The food categories were selected considering the variation of the appearance in both color and shape. However, for reasons of convenience and reproducibility plastic food samples were used instead of real ones. This may degrade the perceived naturalness of the images.</p><p>The ChineseFoodNet<sup><xref ref-type="fn" rid="fn07">7</xref></sup> (<xref rid="B22" ref-type="bibr">Chen et al., 2017</xref>) contains over 185,628 images of 208 Chinese food categories. The images in this database are collected through the internet and taken in real world under unconstrained conditions. The database is intended for the development and training of automatic food recognition algorithms.</p><p>The UNICT Food Dataset 889 (UNICT-FD889<sup><xref ref-type="fn" rid="fn08">8</xref></sup>, <xref rid="B32" ref-type="bibr">Farinella et al., 2015</xref>) contains 3,583 images of 889 distinct real meals of different nationalities (e.g., Italian, English, Thai, Indian, Japanese, etc.). The images are acquired with smartphones both with and without flash. Although it is an extended, cross-cultural database, images are not standardized and provided with emotional scores of, e.g., valence and arousal. Additionally the technical quality of the presented images consequently fluctuates. This is most likely by design as the database is intended for the development of automatic image retrieval algorithms.</p><p>The UEC-Food100 (<xref rid="B72" ref-type="bibr">Matsuda et al., 2012</xref>) and UEC-Food256<sup><xref ref-type="fn" rid="fn09">9</xref></sup> (<xref rid="B55" ref-type="bibr">Kawano and Yanai, 2015</xref>) are both Japanese food image datasets, containing 100 and 256 food categories, respectively, from various countries such as France, Italy, United States, China, Thailand, Vietnam, Japan, and Indonesia. The dataset was compiled to develop algorithms that automatically retrieve food images from the internet. The images have widely varying backgrounds (e.g., different compositions and lighting of plates etc.), implying that this has limited value for human neurophysiological food-related studies.</p><p>The UPMCFOOD-101 (<xref rid="B137" ref-type="bibr">Wang et al., 2015</xref>) and ETHZFOOD-101<sup><xref ref-type="fn" rid="fn010">10</xref></sup> (<xref rid="B12" ref-type="bibr">Bossard et al., 2014</xref>) datasets are twin datasets with the same 101 international food categories but different real-world images, all collected through the internet. The images of UPMCFOOD-101 are annotated with recipe information, and the images of ETHZ-FOOD-101 are selfies. The datasets were compiled to develop automatic systems for recipe recognition, an exercise that requires significantly other pictorial features than applications that intent to evoke discriminative, though well-defined, emotional responses.</p><p>VIREO-172<sup><xref ref-type="fn" rid="fn011">11</xref></sup> (<xref rid="B20" ref-type="bibr">Chen and Ngo, 2016</xref>) is a dataset containing 110,241 images of popular Chinese dishes from 172 categories, annotated with 353 ingredient labels. The images are retrieved from the internet and have widely varying backgrounds, implying the associated diversity in technical quality. Like some previously mentioned databases, this database is intended to develop automatic cooking recipe retrieval algorithms with ingredient recognition.</p></sec></sec><sec><title>The Crocufid Database</title><sec><title>Recording Material and Protocol</title><p>The CROCUFID images were created following the photographing protocol presented by <xref rid="B19" ref-type="bibr">Charbonnier et al. (2016)</xref>, as shown in Figure <xref ref-type="fig" rid="F1">1</xref>. The images were taken with a Canon EOS 1300D high-resolution digital single lens mirror reflex camera that was mounted on a tripod and stored both in RAW (CR2) and JPEG format. The focal length used was 34.0 mm, the shutter speed was 1/50 s and the aperture value was 4.5 for each picture. In two pictures (image number 20 and 88) a blue plate was used to enable the study of color contrast effects (we plan to extend the database with more pictures of food arranged on plates with different colors in the future). Consequently, all other (food and non-food) items were placed on a white IKEA plate (type F&#x000c4;RGRIK<sup><xref ref-type="fn" rid="fn012">12</xref></sup>) with a diameter of 27 cm. The plate itself was placed in a 38 cm &#x000d7; 38 cm &#x000d7; 38 cm Foldio2 photo studio (a cubic photo tent made from white plastic sheets that soften and reflect the light from two LED strips above and below<sup><xref ref-type="fn" rid="fn013">13</xref></sup>). The camera was placed on a tripod. The Foldio2 studio and the camera were both rigidly attached to a common baseplate. The optical center of the camera was 39.5 cm from the center of the Foldio2 studio and 38 cm above the base plane. The viewing angle of the lens was approximately 45&#x000b0; downward, which resembles the viewing of a plate of food on a table during mealtime. A thin (5 mm thickness) gray foam board with a circular hole with the same diameter as the base of the IKEA plate was placed inside the Foldio2 studio, to ensure that the plate could easily be replaced in the same position for each image registration. The placement of the plate was checked by projecting a preregistered reference image of an empty plate over the actual plate using the &#x02018;Show Overlay&#x02019; function of the camera. The foam board had a light gray color to ensure sufficient luminance contrast of the plate with its background, which enables automatic digital background correction (e.g., using Adobe Photoshop or Matlab). At the start of each food-registration session a picture of an X-Rite ColorChecker Passport<sup><xref ref-type="fn" rid="fn014">14</xref></sup> was made to enable post-registration white-balance correction and reproduction of the original colors in each image (using Adobe Photoshop or the accompanying ColorChecker Camera Calibration software). The images were partly registered in the Netherlands (<italic>N</italic> = 426; images 1&#x02013;357 and 565&#x02013;633) and partly in Japan (<italic>N</italic> = 414; images 358&#x02013;564 and 634&#x02013;840).</p><fig id="F1" position="float"><label>FIGURE 1</label><caption><p>Standardized photographing protocol set-up (see: <xref rid="B19" ref-type="bibr">Charbonnier et al., 2016</xref>). A laptop <bold>(left)</bold> was used to control the camera <bold>(middle)</bold> settings and take the pictures of the plate with food in the Foldio2 photo studio <bold>(right)</bold>.</p></caption><graphic xlink:href="fpsyg-10-00058-g001"/></fig><p>The plate&#x02019;s background was standardized in all images as follows. First, a binary mask image was created in Photoshop by segmenting the reference image of the empty plate into a plate and background area followed by thresholding. Then, this mask image was used in Matlab 2018b<sup><xref ref-type="fn" rid="fn015">15</xref></sup> to segment the plate from each of the 840 images and combine it with the segmented background from the reference image. Some smoothing was applied to the edges of the plate to prevent abrupt (noticeable) luminance transitions. In addition, the luminance of the (visible part of the) IKEA plate was equalized throughout the image set.</p><p>The CROCUFID database currently comprises 840 images: 675 food pictures (for some examples see Figure <xref ref-type="fig" rid="F2">2</xref>) and 165 non-food pictures (Figure <xref ref-type="fig" rid="F3">3</xref>). Detailed metadata are provided for the first 479 food images. All images in CROCUFID are high-quality standardized 8 bits color pictures with a resolution of 5184 pixels &#x000d7; 3456 pixels. For the validation studies reported in this paper the images were reduced in size to 1037 pixels &#x000d7; 691 pixels. A corresponding set of item-only (food or non-food) images representing the displayed items on a transparent background (for some examples see Figure <xref ref-type="fig" rid="F4">4</xref>) was created by using the PhotoScissors background removal tool<sup><xref ref-type="fn" rid="fn016">16</xref></sup> for the initial separation of the food from the background, followed by the magic wand and color range selection tools in Adobe Photoshop to remove small remaining background sections inside the convex hull of the food area. These transparent item-only images can for instance be used to study the effects of background characteristics (e.g., plate size, color, texture, etc., see e.g., <xref rid="B90" ref-type="bibr">Piqueras-Fiszman et al., 2012</xref>, <xref rid="B91" ref-type="bibr">2013</xref>; <xref rid="B131" ref-type="bibr">Van Ittersum and Wansink, 2012</xref>; <xref rid="B118" ref-type="bibr">Stewart and Goss, 2013</xref>) and plating arrangements (e.g., centered on a plate or off-center: <xref rid="B78" ref-type="bibr">Michel et al., 2015a</xref>; <xref rid="B133" ref-type="bibr">Velasco et al., 2016</xref>) by simply superimposing them onto images of different plates or even video clips of dynamic background textures (as in <xref rid="B126" ref-type="bibr">Toet, 2016</xref>). After thresholding and binarization the item-only PNG images can also serve as masks to restrict the calculation of computational measures to the area of the item on the plate.</p><fig id="F2" position="float"><label>FIGURE 2</label><caption><p>Representative images of four typical food categories (Universal, Unappealing, Western, Asian).</p></caption><graphic xlink:href="fpsyg-10-00058-g002"/></fig><fig id="F3" position="float"><label>FIGURE 3</label><caption><p>Representative images of three typical non-food categories (objects related or unrelated to food, flowers).</p></caption><graphic xlink:href="fpsyg-10-00058-g003"/></fig><fig id="F4" position="float"><label>FIGURE 4</label><caption><p>Some examples of item-only food images.</p></caption><graphic xlink:href="fpsyg-10-00058-g004"/></fig></sec><sec><title>Image Features</title><p>Objective features like visual texture (<xref rid="B67" ref-type="bibr">Lucassen et al., 2011</xref>), complexity (<xref rid="B38" ref-type="bibr">Forsythe et al., 2011</xref>; <xref rid="B71" ref-type="bibr">Marin and Leder, 2013</xref>) and colorfulness (<xref rid="B15" ref-type="bibr">Cano et al., 2009</xref>) influence affective observer responses (measures of valence and arousal) to visual scenes in general (e.g., natural textures: <xref rid="B127" ref-type="bibr">Toet et al., 2012</xref>). Since our first sensory contact with food is typically through our eyes (<xref rid="B130" ref-type="bibr">van der Laan et al., 2011</xref>; <xref rid="B112" ref-type="bibr">Spence et al., 2015</xref>) it is not surprising that these visual cues also affect our responses to food (<xref rid="B144" ref-type="bibr">Zellner et al., 2010</xref>; <xref rid="B136" ref-type="bibr">Wadhera and Capaldi-Phillips, 2014</xref>). It has for instance been found that our appraisal and consumption of food depends on visual impressions like perceived regularity or randomness (<xref rid="B145" ref-type="bibr">Zellner et al., 2011</xref>; <xref rid="B142" ref-type="bibr">Zampollo et al., 2012</xref>), complexity (<xref rid="B80" ref-type="bibr">Mielby et al., 2012</xref>), spatial layout (<xref rid="B142" ref-type="bibr">Zampollo et al., 2012</xref>; <xref rid="B78" ref-type="bibr">Michel et al., 2015a</xref>; <xref rid="B122" ref-type="bibr">Szocs and Lefebvre, 2017</xref>), area of the plate covered (<xref rid="B131" ref-type="bibr">Van Ittersum and Wansink, 2012</xref>), and color content (<xref rid="B144" ref-type="bibr">Zellner et al., 2010</xref>; <xref rid="B142" ref-type="bibr">Zampollo et al., 2012</xref>; <xref rid="B93" ref-type="bibr">Piqueras-Fiszman and Spence, 2014</xref>; <xref rid="B110" ref-type="bibr">Spence, 2015b</xref>; <xref rid="B37" ref-type="bibr">Foroni et al., 2016</xref>; <xref rid="B59" ref-type="bibr">K&#x000f6;nig and Renner, 2018</xref>). We therefore complemented the dataset with several computational measures that evaluate visual features (derived from the image luminance and chrominance distribution) that are known (or <italic>a priori</italic> likely) to influence affective image appraisal. These measures allow the user to select CROCUFID images based on their physical properties.</p><p>The first seven measures characterize the perceived image texture (i.e., the low-level spatial arrangement of color or intensities in an image). Five of these measures are only defined for grayscale images. To enable the computation of these measures the color images were therefore first converted to grayscale images with the MATLAB <italic>rgb2gray</italic> function before calculating these texture measures.</p><p><italic>Entropy</italic> (S) is a statistical measure that characterizes the degree of randomness of the input image texture: entropy is 0 if all pixels have the same intensity value. Entropy was calculated with the standard MATLAB function <italic>entropy</italic>.</p><p><italic>Power</italic> (P) is the average of the power spectral density over the image support (in decibels), computed from the discrete 2D Fourier transform of the image. This measure reflects the mean variations in image intensity.</p><p>The remaining three grayscale image texture measures (contrast, energy, and homogeneity) were computed from the Gray Level Co-occurrence Matrix (GLCM, which is a classic technique used for image texture analysis and classification: <xref rid="B44" ref-type="bibr">Haralick et al., 1976</xref>) using the MATLAB function <italic>graycoprops</italic>.</p><p><italic>Contrast</italic> (C) measures the intensity contrast between a pixel and its neighbors averaged over the whole image and is equal to 0 for a constant valued image (<xref rid="B26" ref-type="bibr">Corchs et al., 2016</xref>).</p><p><italic>Energy</italic> (E) is the sum of squared elements in the GLCM and is equal to 1 for a constant valued image (<xref rid="B26" ref-type="bibr">Corchs et al., 2016</xref>).</p><p><italic>Homogeneity</italic> (H) measures the closeness of the distribution of elements in the GLCM with respect to the GLCM diagonal and is equal to 1 for a diagonal GLCM (<xref rid="B26" ref-type="bibr">Corchs et al., 2016</xref>).</p><p>We also computed two measures that describe the texture of color images. These measures are based on the Pyramid Histogram of Oriented Gradients (PHOG) image representation that was originally developed for object recognition and classification (<xref rid="B11" ref-type="bibr">Bosch et al., 2007</xref>) and have been used to characterize the aesthetics and liking of images and artworks (<xref rid="B96" ref-type="bibr">Redies et al., 2012</xref>; <xref rid="B13" ref-type="bibr">Braun et al., 2013</xref>; <xref rid="B47" ref-type="bibr">Hayn-Leichsenring et al., 2017</xref>). The PHOG descriptors are global feature vectors based on a pyramidal subdivision of an image into sub-images, for which Histograms of Oriented Gradients (HOG: <xref rid="B28" ref-type="bibr">Dalal and Triggs, 2005</xref>) are computed.</p><p><italic>Self-similarity</italic> (SS) is computed using the Histogram Intersection Kernel (HIK: <xref rid="B5" ref-type="bibr">Barla et al., 2002</xref>) to determine the similarity between Hog features at the individual levels of the PHOG (for details see <xref rid="B96" ref-type="bibr">Redies et al., 2012</xref>; <xref rid="B13" ref-type="bibr">Braun et al., 2013</xref>). Images of natural (growth) patterns typically have a highly self-similar (fractal) structure, whereas artificial (man-made) structures typically have a low self-similarity.</p><p><italic>Anisotropy</italic> (AN) describes how the gradient strength varies across the orientations in an image. Low anisotropy means that the strengths of the orientations are uniform across orientations and high anisotropy means that orientations differ in their overall prominence (<xref rid="B96" ref-type="bibr">Redies et al., 2012</xref>).</p><p>The next six measures quantify the structural image complexity. The complexity of an image depends on the number of its structural components, their heterogeneity, (e.g., a single shape repeated vs. multiple distinct shapes), their regularity (e.g., simple polygons vs. more abstract shapes) and the regularity of the arrangement of elements (e.g., symmetry, distribution characteristics; see Figure 1 of <xref rid="B6" ref-type="bibr">Berlyne, 1958</xref>).</p><p>The <italic>Compression Ratio</italic> (CR) between the original (uncompressed) and (JPEG or GIF format) compressed file sizes is a computational measure that is positively correlated with ratings of subjective image complexity (<xref rid="B71" ref-type="bibr">Marin and Leder, 2013</xref>). The file size of a digitized image is a measure of its structural information content (<xref rid="B31" ref-type="bibr">Donderi, 2006</xref>). Compression algorithms use image redundancy or predictability to reduce the file size, such that more complex (or less predictable) images need more elements. Using the lossless JPEG compression mode of the MATLAB <italic>imwrite</italic> function we computed the JPEG based compression ratio (CRjpeg) which has been shown to be a reliable measure of subjective complexity across various image domains (<xref rid="B71" ref-type="bibr">Marin and Leder, 2013</xref>). We also computed the GIF based compression ratio (CRgif) using Adobe Photoshop (settings: palette local selective, colors 256, forced black-white colors, no transparency, dither diffusion 75%, exact colors and normal order of lines, see: <xref rid="B71" ref-type="bibr">Marin and Leder, 2013</xref>).</p><p><italic>Feature Congestion</italic> (FC) is a visual clutter measure that implicitly captures the notion of spatial disorder by computing a weighted average of the local feature (color, orientation, and luminance) contrast covariance over multiple (typically three) spatial scales (<xref rid="B99" ref-type="bibr">Rosenholtz et al., 2007</xref>). Larger FC values correspond to higher levels of visual clutter. The FC measure was calculated using the MATLAB code provided<sup><xref ref-type="fn" rid="fn017">17</xref></sup> by <xref rid="B99" ref-type="bibr">Rosenholtz et al. (2007)</xref>.</p><p><italic>Subband Entropy</italic> (SE) is a clutter measure that encodes the image information content (or redundancy) by computing a weighted sum of the entropies of the luminance and chrominance image subbands (<xref rid="B99" ref-type="bibr">Rosenholtz et al., 2007</xref>). Larger SE values correspond to higher levels of visual clutter. The SE measure was calculated using the MATLAB code provided<sup><xref ref-type="fn" rid="fn018">18</xref></sup> by <xref rid="B99" ref-type="bibr">Rosenholtz et al. (2007)</xref>.</p><p>The <italic>Number of Proto-Objects</italic> (NPO) is the number of image segments or super-pixels with similar intensity, color and gradient orientation features (the proto-objects: <xref rid="B141" ref-type="bibr">Yu et al., 2014</xref>). Larger NPO values correspond to higher levels of visual clutter. The NPO measure was calculated using the MATLAB code provided by the authors<sup><xref ref-type="fn" rid="fn019">19</xref></sup>.</p><p>The <italic>Mean Information Gain</italic> (MIG) is defined as the difference between the spatial heterogeneity (i.e., the joint entropy among neighboring pixels) and the non-spatial heterogeneity (i.e., the probability of observing a pixel value independently of its location in the image) of an image (<xref rid="B1" ref-type="bibr">Andrienko et al., 2000</xref>; <xref rid="B94" ref-type="bibr">Proulx and Parrott, 2008</xref>). The MIG does not require knowledge of the &#x02018;maximal&#x02019; entropy of the image (which is sometimes hard to compute or even to define), and accounts for the inherent spatial correlations. The MIG increases monotonously with spatial randomness and ranges over 0&#x02013;1: MIG = 0 for uniform patterns and MIG = 1 for random patterns. The MIG index is a well-known complexity measure in statistical physics (<xref rid="B135" ref-type="bibr">Wackerbauer et al., 1994</xref>) and has successfully been used to quantify the complexity of two-dimensional patterns (<xref rid="B1" ref-type="bibr">Andrienko et al., 2000</xref>) and ecological habitats (<xref rid="B94" ref-type="bibr">Proulx and Parrott, 2008</xref>). The images were transformed to HSV format and the pixel value range was normalized from 0 to 10 before calculating MIG values independently for the color (Hue: MIGh), chroma (Saturation: MIGs), and intensity (Value: MIGv) components of each image. The MIG measure was calculated using the MATLAB code provided by <xref rid="B94" ref-type="bibr">Proulx and Parrott (2008)</xref> at <ext-link ext-link-type="uri" xlink:href="http://complexity.ok.ubc.ca/projects/measuring-complexity">http://complexity.ok.ubc.ca/projects/measuring-complexity</ext-link>.</p><p>The <italic>Mean Gradient Strength</italic> (MGS) or mean edge strength is a valid measure of subjective image complexity (<xref rid="B13" ref-type="bibr">Braun et al., 2013</xref>) and is based on the observation that the subjectively perceived level of image complexity increases with its number of edges. To compute the MGS we first transformed the images from RGB to Lab format (using the MATLAB function <italic>rgb2lab</italic>). Then the MATLAB function <italic>gradient</italic> was applied to each of the three image channels and a single gradient image was obtained by taking the pixelwise maximum of the three individual gradient images. Finally, the mean of the resulting gradient image was adopted as an overall measure of image complexity (<xref rid="B13" ref-type="bibr">Braun et al., 2013</xref>).</p><p>The following five measures characterize the image color distribution.</p><p>The <italic>Number of Colors</italic> (NC) represents the number of distinct colors in the RGB image. For each image this number was obtained as the size of the color map resulting from the application of the MATLAB <italic>rgb2ind</italic> function (with the minimum variance quantization and dithering options) to each original RGB image.</p><p><italic>Colorfulness</italic> is the sensation that an image appears to be more or less chromatic. Local colorfulness has been defined as a linear combination of the mean and standard deviation of the local chrominance values in color opponent space (<xref rid="B46" ref-type="bibr">Hasler and S&#x000fc;sstrunk, 2003</xref>). Note that colorfulness is not strictly related to the numbers of colors: an image can be more colorful even when its contains less different colors (<xref rid="B88" ref-type="bibr">Palus, 2005</xref>). A global image Colorfulness (CF) metric was computed as the mean value of the local colorfulness over a set of subwindows covering the entire image support (<xref rid="B46" ref-type="bibr">Hasler and S&#x000fc;sstrunk, 2003</xref>). CF varies from 0 (grayscale image) to 1 (most colorful image). CF was computed using the MATLAB code obtained from <ext-link ext-link-type="uri" xlink:href="https://gist.github.com/zabela/8539116">https://gist.github.com/zabela/8539116</ext-link>.</p><p>The proportional contribution of the Red (R), Green (G), and Blue (B) color channels was computed from the transparent PNG images to characterize the overall color of the (food or non-food) items (as in <xref rid="B10" ref-type="bibr">Blechert et al., 2014b</xref>).</p><p>Finally, the <italic>Fraction</italic> of the <italic>Plate Covered</italic> (FPC) by the (food or non-food) item on the plate was calculated for each image as follows. The total number of pixels over the area of the plate was calculated as the number of non-zero pixels in binary mask image of the empty plate. The total number of pixels covered by the item on the plate was calculated as the number of non-zero pixels in the binary image resulting after thresholding and binarization of the corresponding transparent PNG version of the image (note that this operation yields a binary mask of the area actually covered by the item). The FPC was then obtained as the ratio of both these numbers (i.e., the number of pixels representing the item divided by the number of pixels representing the plate). Note that the FPC can also be adopted as a measure for stimulus size (see <xref rid="B36" ref-type="bibr">Foroni et al., 2013</xref>) since the plate has the same size in all images.</p></sec><sec><title>CROCUFID Availability and Use</title><p>The CROCUFID database is publicly available from the Open Science Framework repository (OSF) at <ext-link ext-link-type="uri" xlink:href="https://osf.io/5jtqx">https://osf.io/5jtqx</ext-link> with doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/5JTQX">10.17605/OSF.IO/5JTQX</ext-link> under the CC-By Attribution 4.0 International license. Use is only allowed after complying with the following two conditions: (1) a credit line in publications and presentations reading: &#x0201c;<italic>Standardized images were taken from the CROCUFID database available from the OSF repository at <ext-link ext-link-type="uri" xlink:href="https://osf.io/5jtqx">https://osf.io/5jtqx</ext-link>,&#x0201d;</italic> and (2) a citation to the current article in any publication.</p><p>The database includes: the CROCUFID pictures; contact sheets providing a concise overview of all images in the dataset; documents (both in Adobe Acrobat and in Powerpoint) containing screen shots of the entire observer validation experiment; an SPSS file with the raw observer data (each participant&#x02019;s code and demographic data, followed by valence, arousal, perceived healthiness and desire-to-eat ratings); an Excel file listing for each image some classifiers, the computational measures, and the mean and standard deviations of the observer ratings for the food images. The full-scene images (showing the plated items on a gray background) in the CROCUFID database are stored in JPEG format, while the item-only images (isolated items displayed on a transparent background) are in PNG format. The original RAW images and the calibration images showing the X-Rite ColorChecker Passport are available (in CR2 or DNG format) from the authors on request. The CROCUFID image database complements the F4H image collection<sup><xref ref-type="fn" rid="fn020">20</xref></sup> since both were registered using the same protocol (<xref rid="B19" ref-type="bibr">Charbonnier et al., 2016</xref>).</p></sec></sec><sec><title>Cross-Cultural Evaluation Study</title><p>In our global economy cross-cultural research is becoming increasingly relevant in sensory and consumer science (<xref rid="B74" ref-type="bibr">Meiselman, 2013</xref>). Culture is one of the main factors determining human food-related behavior (<xref rid="B100" ref-type="bibr">Rozin, 1988</xref>). Internet-based research provides almost instantaneous world-wide access to large consumer samples, enabling researchers to conduct cross-cultural studies at relatively low-cost and in short time frames (<xref rid="B107" ref-type="bibr">Slater and Yani-de-Soriano, 2010</xref>; <xref rid="B2" ref-type="bibr">Ares, 2018</xref>). Online cross-cultural studies using images of food have for instance successfully been performed to study the effects of spatial arrangement and color composition of meals on food preference (<xref rid="B142" ref-type="bibr">Zampollo et al., 2012</xref>). Also, we recently used CROCUFID images in an online cross-cultural validation study of a new affective self-report tool (<xref rid="B54" ref-type="bibr">Kaneko et al., 2018</xref>). The item-only images in CROCUFID enable cross-cultural (online) HCI studies on the effects of environment and context on food experience (see previous section). The CROCUFID food image database is a useful tool for food-related cross-cultural research since it contains food pictures from different cuisines. Therefore, we conducted a first cross-cultural study as outlined below.</p><sec><title>Methods</title><sec><title>Participants</title><p>Three groups completed an online anonymous survey to provide normative data for the food images in CROCUFID. The total number of participants was 805.</p><p>Two groups consisted of English speaking participants that were recruited through the Prolific survey site<sup><xref ref-type="fn" rid="fn021">21</xref></sup>. The text of the experiment posted on Prolific was in English. The first group (UK) comprised 266 participants from the UK. The second group (US) comprised 275 US participants. The third group (JP) consisted of Japanese speaking participants that were recruited through the Crowdworks survey site<sup><xref ref-type="fn" rid="fn022">22</xref></sup>. The text of the experiment was translated into Japanese for this sample. The validity of the translation was checked using the back-translation technique (<xref rid="B116" ref-type="bibr">Sperber, 2004</xref>). The third group comprised 264 participants. Exclusion criteria for all participants were color blindness and age (younger than 18 or older than 70).</p><p>All participants signed an informed consent form before taking part in the study and received a small financial compensation after completing the study. The experimental protocol was reviewed and approved by the TNO Ethics Committee (Ethical Approval Ref: 2017-016-EM) and was in accordance with the Helsinki Declaration of 1975, as revised in 2013 (<xref rid="B140" ref-type="bibr">World Medical Association, 2013</xref>).</p></sec><sec><title>Stimuli</title><p>The compilation of the CROCUFID database continued in parallel during this entire study. For the observer validation experiments reported in this study we used only the 479 food images that were already available at the start of the experiments. The remaining 361 images in CROCUFID were registered during and after the observer experiments. In the present study the stimuli were classified into four categories: a Universal food category (consisting of universally well-known fruits and vegetables etc.: <italic>N</italic> = 110), a typical Western food category (consisting of sandwiches, cookies, cheeses, cakes etc.: <italic>N</italic> = 119), a typical Asian food category (consisting of sushi, sashimi, rice-bowl, noodles etc.: <italic>N</italic> = 209) and an Unappealing food category (consisting of unfamiliar, rotten, molded or contaminated food: <italic>N</italic> = 41). Unfamiliar food was classified as unappealing since people typically evaluate (e.g., liking and willingness to try) novel foods more negative compared to familiar foods (<xref rid="B95" ref-type="bibr">Raudenbush and Frank, 1999</xref>).</p></sec><sec><title>Demographics and State Variables</title><p>Participants were asked to report their personal characteristics. They provided gender, age, height, weight, nationality and eating habits, such as dieting and food allergies. Participants were not requested to refrain from eating prior to taking part in the experiment, but their psychophysiological state was registered by asking them how hungry and thirsty they were feeling at the time of the experiment and how much time had passed since their last food consumption. Their state of hunger and thirst were measured with visual analog scales (VAS) labeled from &#x0201c;<italic>very hungry/thirsty</italic>&#x0201d; to &#x0201c;<italic>not hungry/thirsty at all</italic>.&#x0201d; The VAS was displayed as a solid horizontal bar, and participants responded by clicking with the mouse on the appropriate location of the line. Responses were analyzed by converting distances along the bar to a scale ranging from 0 to 100 although this was not explicitly displayed to the participants. BMI was also calculated based on participants&#x02019; reported height and weight.</p></sec><sec><title>Subjective Image Measures</title><p>For all images, valence, arousal, perceived healthiness and desire-to-eat were measured using a VAS ranging from 0 to 100 (again, this was not explicitly displayed to the participants). This is a valid method to measure food elicited affective responses (<xref rid="B35" ref-type="bibr">Flint et al., 2000</xref>) that has previously been used to validate food image databases (<xref rid="B36" ref-type="bibr">Foroni et al., 2013</xref>; <xref rid="B10" ref-type="bibr">Blechert et al., 2014b</xref>). Valence expresses the pleasantness of an image. The question was: &#x0201c;<italic>How pleasant is the item presented in the image</italic>?&#x0201d; and the extremes of the scale were labeled as &#x0201c;<italic>Very unpleasant</italic>&#x0201d; (0) and &#x0201c;<italic>Very pleasant</italic>&#x0201d; (100). The instructions for the participants explained the concept of valence by presenting some related terms for both &#x0201c;<italic>Very unpleasant</italic>&#x0201d; (bad, disliked, disgusting, unsatisfied, and annoyed) and &#x0201c;<italic>Very pleasant</italic>&#x0201d; (good, liked, delicious, satisfied, and pleased). Arousal measures the excitement (or intensity) that was experienced while viewing a food picture. The question was: &#x0201c;<italic>How arousing is the item presented in the image</italic>?&#x0201d; and the extremes of the scale were labeled as &#x0201c;<italic>Not at all</italic>&#x0201d; and &#x0201c;<italic>Extremely</italic>.&#x0201d; The instructions for the participants explained the concept of arousal by presenting some related terms for both the lower (calming and relaxing) and upper (stimulating and energizing) ends of the scale. Perceived healthiness was assessed with the question: &#x0201c;<italic>How healthy do you think the item presented in this image is</italic>?&#x0201d; and the extremes of the scale were labeled as &#x0201c;<italic>Very unhealthy</italic>&#x0201d; and &#x0201c;<italic>Very healthy.&#x0201d;</italic> Desire-to-eat was assessed with the question: &#x0201c;<italic>How much would you like to eat this food right now if it was in front of you</italic>?&#x0201d; and the extremes of the scale were labeled as &#x0201c;<italic>Not at all</italic>&#x0201d; and &#x0201c;<italic>Extremely</italic>.&#x0201d; The degree of recognition of the food images was also assessed by asking the question: &#x0201c;<italic>Have you ever eaten the product in this image</italic>?&#x0201d; that could be rated on a trichotomous scale: &#x0201c;<italic>Yes</italic>&#x0201d;, &#x0201c;<italic>No, but I know what it is (Know)</italic>&#x0201d; or &#x0201c;<italic>No, I&#x02019;ve never seen it before (No)</italic>.&#x0201d;</p></sec><sec><title>Analysis</title><p>Normative observer data for the food images in CROCUFID were obtained through an anonymous online survey. As participants could not be expected to reliably rate all 479 food images, each participant rated only a subset of 60 images to avoid fatigue and early dropout. On average, each image was rated by approximately 100 participants including all participants from three different countries (United Kingdom, United States, and Japan).</p><p>Data was collected using Perl scripts<sup><xref ref-type="fn" rid="fn023">23</xref></sup> and analyzed with IBM SPSS Statistics 23<sup><xref ref-type="fn" rid="fn024">24</xref></sup>. Exploratory analyses were conducted between all demographics (i.e., age, gender, nationality, height, weight, diet, and allergies), state variables (hunger, thirstiness, and time since last food intake) and subjective self-report ratings (valence, arousal, perceived healthiness, and desire-to-eat) of four classified food image categories (Universal foods, Unappealing foods, typical Western foods, and typical Asian foods) from three different nationalities (United Kingdom, United States, and Japan). In addition, the degree of recognition of four different food image categories was calculated from UK, US, and JP participants. Participants who answered &#x0201c;<italic>Yes</italic>&#x0201d; or &#x0201c;<italic>No, but I know what it is (Know)</italic>&#x0201d; were categorized as &#x02018;recognizing&#x02019; participants, and participants who answered &#x0201c;<italic>No, I&#x02019;ve never seen it before</italic>&#x0201d; were categorized as &#x02018;non-recognizing&#x02019; participants.</p><p>Intraclass correlation coefficient (ICC) estimates and their 95% confident intervals were calculated based on a mean-rating (<italic>k</italic> = 3), absolute-agreement, 2-way mixed-effects model (<xref rid="B105" ref-type="bibr">Shrout and Fleiss, 1979</xref>; <xref rid="B60" ref-type="bibr">Koo and Li, 2016</xref>). ICC values less than 0.50 are indicative of poor reliability, values between 0.50 and 0.75 indicate moderate reliability, values between 0.75 and 0.90 indicate good reliability, while values greater than 0.90 indicate excellent reliability (<xref rid="B60" ref-type="bibr">Koo and Li, 2016</xref>).</p></sec><sec><title>Procedures</title><p>Participants took part in an anonymous online survey. Although internet surveys typically provide less control over the experimental conditions, they typically yield similar results as lab studies (e.g., <xref rid="B41" ref-type="bibr">Gosling et al., 2004</xref>; <xref rid="B139" ref-type="bibr">Woods et al., 2015</xref>; <xref rid="B69" ref-type="bibr">Majima et al., 2017</xref>) while they limiting several disadvantages associated with central location studies.</p><p>The experiment was programmed in the Java script language, and the survey itself was hosted on a web server. The time stamps of the different events (onset stimulus presentation, response clicks) and the display size and operating system of the participants were logged. This enabled us to check that participants did indeed view the stimuli on larger displays and not on mobile devices with low resolution screens. The resolution of the devices used by the participants in this study varied between 1280 &#x000d7; 720 and 3440 &#x000d7; 1440 (the average resolution was 1538 pixels &#x000d7; 904 pixels across participants, with standard deviations of 330 pixels &#x000d7; 165 pixels). We could not verify if the browser window was indeed maximized.</p><p>The survey commenced by presenting general information about the experiment and thanking participants for their interest. Then, the participants were informed that they would see 60 different food images during the experiment, and they were instructed to rate their first impression of each image without worrying about calories. It was emphasized that there were no correct or incorrect answers, and that it was important to respond seriously and intuitively. Subsequently, the participants signed an informed consent by clicking &#x0201c;<italic>I agree to participate in this study</italic>,&#x0201d; affirming that they were at least 18 years old and voluntarily participated in the study. The survey then continued with an assessment of the demographics and the current physical state of the participants.</p><p>Next, the participants were shown the VAS response tools together with an explanation how these could be used to report their ratings (valence, arousal, perceived healthiness, and desire to eat) for each image. Then, they performed two practice trials to further familiarize them with the use of the VAS tools. Immediately after these practice trials, the actual experiment started. During the actual experiment the participants rated 60 pseudo-randomly selected food images. The selection procedure was such that 14 images were randomly selected from the 110 Universal images, 5 from the 41 Unappealing/spoiled images, 26 from the 209 typical Asian images, and 15 from the 119 typical Western images. This selection procedure ensured that all images were rated by approximately the same number of participants and all participants viewed the same number of images from each category. The percentage of images from each category that each participant actually viewed was equal to the percentage of that particular category in the total stimulus set.</p><p>On each trial the screen displayed a food image together with the recognizability question and the four VAS tools to rate valence, arousal, perceived healthiness and desire-to-eat (for a screenshot of the experiment see Figure <xref ref-type="fig" rid="F5">5</xref>). Since this experiment was an online study with a (small) financial compensation and without any personal contact or interaction with the participants, there might be issues with motivation and seriousness. Therefore, the experiment concluded with a validated seriousness check (<xref rid="B3" ref-type="bibr">Aust et al., 2013</xref>) asking participants the question: &#x0201c;<italic>It would be very helpful if you could tell us at this point whether you have taken part seriously, so that we can use your answers for our scientific analysis, or whether you were just clicking through to take a look at the survey?</italic>&#x0201d;. Participants could select one of two answers: &#x0201c;<italic>I have taken part seriously</italic>&#x0201d; or &#x0201c;<italic>I have just clicked through, please throw my data away</italic>.&#x0201d; As an extra incentive, the following sentence was added to the instructions that were presented at the start of the experiment: &#x0201c;<italic>It&#x02019;s important for us that you are motivated and answer all questions seriously</italic>.&#x0201d; After completing the experiment, participants received a small financial compensation and were thanked for their participation. The whole session lasted about 20 min.</p><fig id="F5" position="float"><label>FIGURE 5</label><caption><p>Screenshot of the display during the image rating task.</p></caption><graphic xlink:href="fpsyg-10-00058-g005"/></fig></sec></sec><sec><title>Results</title><p>There were no participants that (1) completed the study in a time span that was evidently too short in comparison to the estimated total survey duration (suggesting that they did not participate seriously) or that (2) responded negatively to the question about their seriousness were removed from further analysis. Hence, all data collected were included in the analysis.</p><sec><title>Demographic Information</title><p>The average age, hunger, thirst, time since last food intake and BMI (weight in kilograms divided by height in meters squared), and the number of each gender for the three different participant groups (UK, US, and JP) is summarized in Table <xref rid="T3" ref-type="table">3</xref>. There was a significant difference between the average age of the UK and US participants. The average time since last food-intake was significantly shorter for JP participants than for UK and US participants, which is reflected in the result that JP participants were significantly less hungry than UK and US participants. There was no significant difference between three nationality groups regarding thirst.</p><table-wrap id="T3" position="float"><label>Table 3</label><caption><p>The summary of participants&#x02019; demographics from three different nationalities (United Kingdom, United States, and Japan).</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><td valign="top" align="left" rowspan="1" colspan="1"/><th valign="top" align="center" rowspan="1" colspan="1">United Kingdom</th><th valign="top" align="center" rowspan="1" colspan="1">United States</th><th valign="top" align="center" rowspan="1" colspan="1">Japan</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Number of participants</td><td valign="top" align="center" rowspan="1" colspan="1">266</td><td valign="top" align="center" rowspan="1" colspan="1">275</td><td valign="top" align="center" rowspan="1" colspan="1">264</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Age</td><td valign="top" align="center" rowspan="1" colspan="1">36.68 (&#x000b1;11.26)</td><td valign="top" align="center" rowspan="1" colspan="1">33.04 (&#x000b1;11.38)</td><td valign="top" align="center" rowspan="1" colspan="1">35.17 (&#x000b1;8.95)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Gender male(female)</td><td valign="top" align="center" rowspan="1" colspan="1">83 (183)</td><td valign="top" align="center" rowspan="1" colspan="1">141 (134)</td><td valign="top" align="center" rowspan="1" colspan="1">101 (163)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Hunger</td><td valign="top" align="center" rowspan="1" colspan="1">42.04 (&#x000b1;27.63)</td><td valign="top" align="center" rowspan="1" colspan="1">41.34 (&#x000b1;26.85)</td><td valign="top" align="center" rowspan="1" colspan="1">31.52 (&#x000b1;25.55)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Thirst</td><td valign="top" align="center" rowspan="1" colspan="1">45.96 (&#x000b1;24.39)</td><td valign="top" align="center" rowspan="1" colspan="1">44.54 (&#x000b1;24.13)</td><td valign="top" align="center" rowspan="1" colspan="1">42.63 (&#x000b1;22.31)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Time since last food-intake (hr.)</td><td valign="top" align="center" rowspan="1" colspan="1">4.20 (&#x000b1;4.59)</td><td valign="top" align="center" rowspan="1" colspan="1">4.97 (&#x000b1;4.66)</td><td valign="top" align="center" rowspan="1" colspan="1">3.40 (&#x000b1;2.33)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">BMI</td><td valign="top" align="center" rowspan="1" colspan="1">28.94 (&#x000b1;12.54)</td><td valign="top" align="center" rowspan="1" colspan="1">27.27 (&#x000b1;9.37)</td><td valign="top" align="center" rowspan="1" colspan="1">21.47 (&#x000b1;3.18)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/></tr></tbody></table></table-wrap></sec><sec><title>Data Reliability</title><p>To quantify the agreement between the mean subjective image ratings provided by participants from the three different countries (Japan, United Kingdom, and United States), we computed the intraclass correlation coefficient (ICC) between each pair of countries, for each of the four subjective ratings (valence, arousal, desire-to-eat, and perceived healthiness). As shown in Figure <xref ref-type="fig" rid="F6">6</xref>, there is excellent agreement (all ICC values are larger than 0.90) between the UK and US groups for each of the four subjective ratings. The JP group shows moderate (between 0.50 and 0.75) to good (between 0.75 and 0.90) agreement with the UK and US groups. Overall, there appears to be a trend that the JP group agrees more with the US group than with the UK group.</p><fig id="F6" position="float"><label>FIGURE 6</label><caption><p>Intraclass correlation between the mean subjective (valence, arousal, desire-to-eat and perceived healthiness) ratings for each of the three different nations (United Kingdom, United States, and Japan). Error bars represent the 95% confidence intervals.</p></caption><graphic xlink:href="fpsyg-10-00058-g006"/></fig></sec><sec><title>Validation Ratings</title><p>For all 479 food images, we computed the average rating scores of valence, arousal, perceived healthiness, and desire-to-eat, both over all groups and within groups (UK, US, and JP). The results are provided as additional material with this paper. The food images were categorized in four different categories: Universal food (food that is globally available), Unappealing food (i.e., unfamiliar, rotten, molded, or contaminated food), typical Western food, and typical Asian food. For all three groups tested, the rating scores for Universal food images are higher than those for Unappealing food images.</p><sec><title>Degree of recognition on food images across groups</title><p>We also evaluated the degree of recognition for each of the four categorized food image groups across the three different groups (UK, US, and JP). Figure <xref ref-type="fig" rid="F7">7</xref> shows for each population the average percentage who answered &#x0201c;<italic>Yes</italic>,&#x0201d; &#x0201c;<italic>No, but I know what it is (Know)</italic>,&#x0201d; and &#x0201c;<italic>No, I&#x02019;ve never seen it before (No)</italic>&#x0201d; for each food image group. Over all three populations, 94.9% of participants recognized Universal food images on average (separately 96.6% of UK, 95.3% of US, and 93.0% of JP). UK and US participants recognized significantly more Western than Asian food images. JP participants recognized most of the Asian food images (94.6%), while only 48.0% of UK and 52.7% of US participants recognized them. There was a significant difference in the degree of recognition of Asian food images between US and JP participants, and between UK and JP participants. US participants recognized significantly more Asian images than UK participants.</p><fig id="F7" position="float"><label>FIGURE 7</label><caption><p>The average degree of recognition of four categories of food images rated by JP participants, UK participants, and US participants.</p></caption><graphic xlink:href="fpsyg-10-00058-g007"/></fig></sec><sec><title>Comparison of rating scores on Asian and Western food categories across groups (valence, arousal, perceived healthiness, and desire to eat)</title><p>Next, we compared the average rating scores of valence, arousal, healthiness and desire-to-eat for Western (Figure <xref ref-type="fig" rid="F8">8A</xref>) and Asian (Figure <xref ref-type="fig" rid="F8">8B</xref>) food categories between UK and JP and between US and JP participants. US and UK participants rated Western food significantly higher on valence than JP participants. There was no significant difference between the rating scores on arousal and desire-to-eat for Western food and for each of the groups tested (US and UK and JP). JP participants rated Asian food images significantly higher than UK and US participants on all items.</p><fig id="F8" position="float"><label>FIGURE 8</label><caption><p>The comparison of the average rated scores of valence, arousal, healthiness and desire-to-eat on Western <bold>(A)</bold> and Asian <bold>(B)</bold> food category between UK and JP and between US and JP participants. <sup>&#x02217;</sup>Indicates a significant difference between groups.</p></caption><graphic xlink:href="fpsyg-10-00058-g008"/></fig></sec></sec><sec><title>Computational Image Measures</title><p>The main purpose of providing computational image characteristics with CROCUFID is to allow the user to select CROCUFID images based on their physical properties. Since the selected features are known (or <italic>a priori</italic> likely) to influence image appraisal we also computed the correlation between the computational metrics and the mean observer ratings. Table <xref rid="T4" ref-type="table">4</xref> lists the significant Pearson correlations between the mean observer ratings (overall and for each of the three groups individually) and each of the computational image measures.</p><table-wrap id="T4" position="float"><label>Table 4</label><caption><p>Pearson correlations between the mean observer ratings (overall and for each of the three groups individually) and the computational image measures.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1"/><th valign="top" align="center" colspan="22" rowspan="1">Computational measure</th></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" colspan="22" rowspan="1"><hr/></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><th valign="top" align="center" colspan="7" rowspan="1">Texture</th><th valign="top" align="center" colspan="8" rowspan="1">Complexity</th><th valign="top" align="center" colspan="7" rowspan="1">Color</th></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" colspan="22" rowspan="1"><hr/></td></tr><tr><th valign="top" align="center" colspan="2" rowspan="1">Mean observer rating</th><th valign="top" align="center" rowspan="1" colspan="1">S</th><th valign="top" align="center" rowspan="1" colspan="1">P</th><th valign="top" align="center" rowspan="1" colspan="1">C</th><th valign="top" align="center" rowspan="1" colspan="1">E</th><th valign="top" align="center" rowspan="1" colspan="1">H</th><th valign="top" align="center" rowspan="1" colspan="1">SS</th><th valign="top" align="center" rowspan="1" colspan="1">AN</th><th valign="top" align="center" rowspan="1" colspan="1">CRjpeg</th><th valign="top" align="center" rowspan="1" colspan="1">CRgif</th><th valign="top" align="center" rowspan="1" colspan="1">FC</th><th valign="top" align="center" rowspan="1" colspan="1">SE</th><th valign="top" align="center" rowspan="1" colspan="1">NPO</th><th valign="top" align="center" rowspan="1" colspan="1">MIGh</th><th valign="top" align="center" rowspan="1" colspan="1">MIGs</th><th valign="top" align="center" rowspan="1" colspan="1">MIGv</th><th valign="top" align="center" rowspan="1" colspan="1">MGS</th><th valign="top" align="center" rowspan="1" colspan="1">NC</th><th valign="top" align="center" rowspan="1" colspan="1">CF</th><th valign="top" align="center" rowspan="1" colspan="1">R</th><th valign="top" align="center" rowspan="1" colspan="1">G</th><th valign="top" align="center" rowspan="1" colspan="1">B</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Valence</td><td valign="top" align="center" rowspan="1" colspan="1">All</td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.238</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">-0.153<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.201</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.151<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.137<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.133<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.177<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.109<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.268<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.353<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">UK</td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.264</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">-0.179<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.095<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.300</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-<bold>0.329</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.243</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.226<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.295<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1">0.099<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.130<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.152<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.122<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.134<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.290<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.406<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">US</td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.273</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">-0.170<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.256</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-<bold>0.253</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.200</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.194<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.<bold>254</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.125<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.119<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.103<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.278<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.366<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">JP</td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">-0.124<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.133<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">.118<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.242</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.117<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.100<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.124<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.151<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">-0.149<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.100<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.137<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.148<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/></tr><tr><td valign="top" align="left" colspan="23" rowspan="1"><hr/></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Arousal</td><td valign="top" align="center" rowspan="1" colspan="1">All</td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0</bold>.<bold>266</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">-0.190<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.219</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.187<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.190<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.170<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.204<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.158<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.144<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.122<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.345<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.375<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">UK</td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.260<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">-0.189<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.119<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.294</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-<bold>0.328</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.258<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.230<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.288<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1">0.120<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.167<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.179<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.148<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.159<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.328<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.402<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">US</td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.295</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">-0.199<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.112<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.257</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-<bold>0.269</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.236<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.214<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.262<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1">0.109<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.170<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.168<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.118<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.153<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.344<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.365<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">JP</td><td valign="top" align="center" rowspan="1" colspan="1">0.133<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">-0.109<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.161<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.238<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.207<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/></tr><tr><td valign="top" align="left" colspan="23" rowspan="1"><hr/></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Healthiness</td><td valign="top" align="center" rowspan="1" colspan="1">All</td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.200</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.092<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.148<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.312</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.204</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.134<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.119<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.148<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">UK</td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.239</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.189<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.317</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.243</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.167<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.125<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.150<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">US</td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.233</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.208</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.314</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.223</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.172<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.115<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.155<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">JP</td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">-0.136<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.124<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.160<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.189<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.172<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.132<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>-0.200</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.255</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">-0.101<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.170<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.120<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.099<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">-0.098<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.113<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/></tr><tr><td valign="top" align="left" colspan="23" rowspan="1"><hr/></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Desire-to-eat</td><td valign="top" align="center" rowspan="1" colspan="1">All</td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.244</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">-0.153<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.214</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.161<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.170<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.167<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.209</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.105<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.109<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.244</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.353</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">UK</td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.275</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.097<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.184<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.141<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.334</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>-0.382</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.300</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.283</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.352</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.144<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.122<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.186<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.179<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.176<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.261<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.410<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">US</td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.280</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">-0.188<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.111<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.267</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>-0.282</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.249</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.237</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.300</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.103<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.123<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.157<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.123<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.143<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.259<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.374<sup>&#x02217;&#x02217;</sup></bold></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">JP</td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">-0.125<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.137<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">0.124<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.275</bold><sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.128<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.107<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.132<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.158<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">-0.155<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">-0.109<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.107<sup>&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1">0.119<sup>&#x02217;&#x02217;</sup></td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/></tr></tbody></table><table-wrap-foot><attrib><italic><italic><sup>&#x02217;</sup>Correlation is significant at the 0.05 level (2-tailed). <sup>&#x02217;&#x02217;</sup>Correlation is significant at the 0.01 level (2-tailed). Values above 0.2 are printed in bold. Only significant correlations are reported.</italic></italic></attrib></table-wrap-foot></table-wrap><p>Although most correlations are small (between 0 and 0.3), there is still an appreciable number of medium sized correlations (between 0.3 and 0.5), particularly for the texture metrics AN and SS, and the color metrics NC and CF.</p><sec><title>Image texture</title><p>The image texture metrics S, SS, and AN appear to have the largest absolute overall correlation with mean observer ratings for valence, arousal and desire-to-eat. For the UK and US groups, S and SS are positively correlated with all four of the mean observer ratings, meaning that for these groups valence, arousal, perceived healthiness and desire-to-eat all increase with increasing randomness (S) and self-similarity (SS) of the food items. For the JP group, S is only weakly positively correlated with arousal, while SS is not significantly correlated with any of the mean ratings, suggesting that randomness and self-similarity do not strongly affect the appraisal of food items for this group. AN (which is inversely related to disorder) correlates negatively with each of the four mean ratings for the UK and US groups, while it correlates positively with each of the mean ratings for the JP group. This means that increasing variations in local orientations over the area of a food item decreases each of the four mean ratings for the UK and US groups, while it increases these ratings for the JP group.</p></sec><sec><title>Complexity</title><p>The complexity metrics CR<sub>jpeg</sub>, CR<sub>gif</sub>, and FC appear to have the largest absolute positive overall correlation with mean observer ratings for valence, arousal and desire-to-eat. This correlation is positive for the UK and US groups, but negative for the JP group, meaning that perceived valence, arousal and desire-to-eat increase with increasing complexity for the UK and US groups, while they decrease with complexity for the JP group. NPO is the only complexity measure that systematically correlates positively with all four measures for all three groups. This means that all groups rate perceived healthiness higher when food items are composed of a larger number of distinguishable components.</p></sec><sec><title>Color</title><p>The color metrics NC and CF are positively associated with all four measures for all three groups. However, compared to the UK and US groups, this correlation is quite weak for the JP group. Hence, it seems that valence, arousal, perceived healthiness and desire-to-eat all increase with increasing number of colors and colorfulness of the food items, but more so for the UK and US groups than for the JP group.</p></sec><sec><title>Cultural differences</title><p>The results listed in Table <xref rid="T4" ref-type="table">4</xref> show that the relation between the mean subjective image ratings and each of the most predictive image metrics differs consistently between the JP groups and the UK and US groups: the variation of the mean (valence, arousal, desire-to-eat, and perceived healthiness) ratings of the JP group with each of the texture, complexity and color metrics is consistently smaller or even opposite to those of the UK and US groups. A graphical overview of these results is provided in the <xref ref-type="supplementary-material" rid="SM1">Supplementary Material</xref> with this article.</p><p>Arousal, valence, and desire-to-eat consistently increase with increasing structural food complexity (i.e., with increasing values of the texture, complexity and color metrics) for the UK and US groups (see Table <xref rid="T4" ref-type="table">4</xref>). On these three subjective measures, the JP group shows an opposite behavior to the UK and US groups for the texture measure AN and for each of the three complexity measures CR<sub>jpeg</sub>, CR<sub>gif</sub>, and FC. Like the UK and US groups, the mean arousal ratings by the JP group also increase with increasing food item colorfulness or number of colors, though less strongly. Perceived healthiness increases with the number of colors (NC), distinguishable components (NPO) and randomness (S) of the food items, though less strongly for the JP group than for the UK and US groups.</p></sec></sec></sec></sec><sec><title>Discussion</title><p>The CROCUFID database contains high-quality images that are registered according to standardized protocol, covering the full valence and arousal space. CROCUFID includes multiple cuisines and unfamiliar foods and provides normative and demographic data. The database is hosted in the OSF repository<sup><xref ref-type="fn" rid="fn025">25</xref></sup> and is freely available under the CC-By Attribution 4.0 International license. We plan to extend this image collection with food images from different cuisines in addition to the Western and Asian foods that are currently included. Also, the CROCUFID image collection can easily be extended by linking it to similar data sets. Researchers who are interested in contributing to this effort are kindly invited to provide the authors with a link to their image dataset. The images should preferably be documented and registered according to the protocol presented by <xref rid="B19" ref-type="bibr">Charbonnier et al. (2016)</xref>.</p><p>As expected, the present results of our cross-cultural study show that the JP group recognized significantly more Asian food compared to UK and US groups, while the UK and US groups recognized significantly more Western food than the JP groups, suggesting that Asian food images seem to have a strong cultural &#x0201c;bias&#x0201d;: for Western group (US and UK) it is hard to distinguish different food images. As also expected, JP group who has a higher degree of recognition of Asian food images rated significantly higher scores of valence, arousal, healthiness, and desire-to-eat than Western groups (US and UK). Regarding the rating scores on Western food images, Western groups (US and UK) rated significantly higher scores of valence than JP group. There was no significant difference in the rating scores of arousal and desire-to-eat between the western group and the JP group. The results of valence ratings on Asian and Western food by the JP group and the Western groups (UK and US) agree with the previous findings of <xref rid="B58" ref-type="bibr">Knaapila et al. (2017)</xref> that the familiarity of spice odors the participants have was positively associated with pleasantness. <xref rid="B33" ref-type="bibr">Fenko et al. (2015)</xref> also found similar results using soy products, where familiar products were evaluated more positively than unfamiliar products with German participants.</p><p>Our current result that valence, arousal, desire-to-eat and perceived healthiness are positively associated with all texture measures (spatial disorder) for the UK and US groups and less strongly with some of them for the JP group agrees with the previous finding of <xref rid="B142" ref-type="bibr">Zampollo et al. (2012)</xref> that participants from the United States and Italy tend to prefer disorganized food presentations, while Japanese participants do not have a significant preference.</p><p>The present result that valence, arousal, desire-to-eat and perceived healthiness are positively associated with all measures of complexity for the UK and US groups agrees with the results of many previous studies that reported a positive association between visual complexity and affective (i.e., valence and arousal) ratings: complex stimuli are typically perceived as more pleasant and more arousing than less complex ones (for an overview see <xref rid="B68" ref-type="bibr">Madan et al., 2018</xref>). However, the JP group shows the opposite behavior: mean valence, arousal and desire-to-eat ratings are all negatively associated with complexity for this group.</p><p>Our finding that the number of identifiable components (NPO) is positively associated with perceived healthiness agrees with previous observations that (1) people prefer servings composed of multiple pieces to single-piece ones (<xref rid="B136" ref-type="bibr">Wadhera and Capaldi-Phillips, 2014</xref>), and (2) food cut in smaller pieces is preferred to larger chunks (<xref rid="B136" ref-type="bibr">Wadhera and Capaldi-Phillips, 2014</xref>), probably because segregating food into multiple components increases the perceived variety of the foods and is therefore perceived as more rewarding (<xref rid="B65" ref-type="bibr">Levitsky et al., 2012</xref>). It has also been suggested that perceived variety (e.g., through multiple colors or components) may be preferred in food since it typically delays the sensory-specific satiety (<xref rid="B136" ref-type="bibr">Wadhera and Capaldi-Phillips, 2014</xref>).</p><p>Our finding that a higher intensity of the green color channel is positively associated with perceived healthiness, while a higher intensity of the red color channel is negatively correlated with healthiness, agrees with previous reports that a higher intensity of the green channel was positively associated with lower concentrations of protein, fat and carbohydrates <xref rid="B10" ref-type="bibr">Blechert et al., 2014b</xref>) and with a lower (perceived) number of calories (<xref rid="B10" ref-type="bibr">Blechert et al., 2014b</xref>; <xref rid="B37" ref-type="bibr">Foroni et al., 2016</xref>), while a higher intensity of the red channel was positively associated with energy density (<xref rid="B37" ref-type="bibr">Foroni et al., 2016</xref>).</p><p>Finally, the result that the number of colors (NC) and colorfulness (CF) are positively associated with that the mean valence, arousal, desire-to-eat and perceived healthiness ratings agrees with previous reports in the literature that multicolored food is rated higher in attractiveness (<xref rid="B144" ref-type="bibr">Zellner et al., 2010</xref>) and pleasantness (<xref rid="B98" ref-type="bibr">Rolls et al., 1982</xref>) than single-colored food, and that colorfulness is typically positively associated with healthiness (<xref rid="B59" ref-type="bibr">K&#x000f6;nig and Renner, 2018</xref>).</p><p>This study also has some limitations. First, since the validation rating internet survey was conducted during the construction of CROCUFID, not all food (non-food) images have been rated yet. Second, since all participants were randomly collected from the UK, US, and Japan, there is no information about their exact region of origin. Hence, additional validation survey is needed to complete the database and specify cross-cultural effects within each country.</p><p>In conclusion, CROCUFID currently contains 840 food images (675 food images and 165 non-food images), from different (Western and Asian) cuisines and with different degrees of valence. CROCUFID also includes computational image characteristics regarding visual texture, complexity, and colorfulness, that may be used to select images for different applications (e.g., food research, automatic image interpretation, and VR/AR applications). The accompanying validation data are derived from a total of 805 participants (ranging in age from of 18&#x02013;70) with different cultural backgrounds (UK, US, and JP). CROCUFID may also be used to conduct (neuro)physiological studies because all items are shown in the same background context and are also available as transparent overlays. Currently many different food cuisines are still lacking from CROCUFID. We hope to extend the collection with images of food from other cuisines. Researchers from different parts of the world are kindly invited to contribute to this effort by creating similar image sets that can be linked to this collection, so that CROCUFID will grow into a truly multicultural food database.</p></sec><sec><title>Author Contributions</title><p>AT, IdK, DK, A-MB, VK, and JvE conceived and designed the study. IdK and SU registered and compiled the image database. IdK and DK performed the online validation study. AT processed the imagery, calculated the computational image measures, and curated the data. IdK and MvS performed the statistical analysis of the results. AT, DK, and IdK wrote the manuscript. All authors contributed to manuscript revision, read and approved the submitted version.</p></sec><sec><title>Conflict of Interest Statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></body><back><fn-group><fn fn-type="financial-disclosure"><p><bold>Funding.</bold> This work was supported by Kikkoman Europe R&#x00026;D Laboratory B.V.</p></fn></fn-group><ack><p>The authors thank prof. Dr. Christoph Redies (Experimental Aesthetics Group, Institute of Anatomy, University of Jena School of Medicine, Jena University Hospital, Jena, Germany) for sharing his MATLAB software to calculate the PHOG based (self-similarity, complexity, and anisotropy) measures.</p></ack><fn-group><fn id="fn01"><label>1</label><p><ext-link ext-link-type="uri" xlink:href="https://foodcast.sissa.it/neuroscience">https://foodcast.sissa.it/neuroscience</ext-link></p></fn><fn id="fn02"><label>2</label><p><ext-link ext-link-type="uri" xlink:href="http://www.eat.sbg.ac.at/resources/food-pics">http://www.eat.sbg.ac.at/resources/food-pics</ext-link></p></fn><fn id="fn03"><label>3</label><p><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.10202">https://doi.org/10.5281/zenodo.10202</ext-link></p></fn><fn id="fn04"><label>4</label><p><ext-link ext-link-type="uri" xlink:href="http://nutritionalneuroscience.eu">http://nutritionalneuroscience.eu</ext-link></p></fn><fn id="fn05"><label>5</label><p><ext-link ext-link-type="uri" xlink:href="http://pfid.rit.albany.edu">http://pfid.rit.albany.edu</ext-link></p></fn><fn id="fn06"><label>6</label><p><ext-link ext-link-type="uri" xlink:href="http://www.murase.is.i.nagoya-u.ac.jp/nufood">http://www.murase.is.i.nagoya-u.ac.jp/nufood</ext-link></p></fn><fn id="fn07"><label>7</label><p><ext-link ext-link-type="uri" xlink:href="https://sites.google.com/view/chinesefoodnet/">https://sites.google.com/view/chinesefoodnet/</ext-link></p></fn><fn id="fn08"><label>8</label><p><ext-link ext-link-type="uri" xlink:href="http://iplab.dmi.unict.it/UNICT-FD889">http://iplab.dmi.unict.it/UNICT-FD889</ext-link></p></fn><fn id="fn09"><label>9</label><p><ext-link ext-link-type="uri" xlink:href="http://foodcam.mobi/dataset.html">http://foodcam.mobi/dataset.html</ext-link></p></fn><fn id="fn010"><label>10</label><p><ext-link ext-link-type="uri" xlink:href="https://www.vision.ee.ethz.ch/datasets_extra/food-101/">https://www.vision.ee.ethz.ch/datasets_extra/food-101/</ext-link></p></fn><fn id="fn011"><label>11</label><p><ext-link ext-link-type="uri" xlink:href="http://vireo.cs.cityu.edu.hk/VireoFood172/">http://vireo.cs.cityu.edu.hk/VireoFood172/</ext-link></p></fn><fn id="fn012"><label>12</label><p><ext-link ext-link-type="uri" xlink:href="http://www.ikea.com">www.ikea.com</ext-link></p></fn><fn id="fn013"><label>13</label><p><ext-link ext-link-type="uri" xlink:href="http://orangemonkie.com/foldio2">http://orangemonkie.com/foldio2</ext-link></p></fn><fn id="fn014"><label>14</label><p><ext-link ext-link-type="uri" xlink:href="http://xritephoto.com">http://xritephoto.com</ext-link></p></fn><fn id="fn015"><label>15</label><p><ext-link ext-link-type="uri" xlink:href="http://www.mathworks.com">http://www.mathworks.com</ext-link></p></fn><fn id="fn016"><label>16</label><p><ext-link ext-link-type="uri" xlink:href="http://www.photoscissors.com">www.photoscissors.com</ext-link></p></fn><fn id="fn017"><label>17</label><p><ext-link ext-link-type="uri" xlink:href="http://www.mit.edu/~yzli/clutter.htm">http://www.mit.edu/&#x0223c;yzli/clutter.htm</ext-link></p></fn><fn id="fn018"><label>18</label><p><ext-link ext-link-type="uri" xlink:href="http://www.mit.edu/~{}yzli/clutter.htm">http://www.mit.edu/&#x0223c;{}yzli/clutter.htm</ext-link></p></fn><fn id="fn019"><label>19</label><p><ext-link ext-link-type="uri" xlink:href="http://www.chenpingyu.org/projects/proto-objects.html">http://www.chenpingyu.org/projects/proto-objects.html</ext-link></p></fn><fn id="fn020"><label>20</label><p><ext-link ext-link-type="uri" xlink:href="http://nutritionalneuroscience.eu">http://nutritionalneuroscience.eu</ext-link></p></fn><fn id="fn021"><label>21</label><p><ext-link ext-link-type="uri" xlink:href="http://www.prolific.ac">www.prolific.ac</ext-link></p></fn><fn id="fn022"><label>22</label><p><ext-link ext-link-type="uri" xlink:href="https://crowdworks.jp">https://crowdworks.jp</ext-link></p></fn><fn id="fn023"><label>23</label><p><ext-link ext-link-type="uri" xlink:href="http://www.perl.com">www.perl.com</ext-link></p></fn><fn id="fn024"><label>24</label><p><ext-link ext-link-type="uri" xlink:href="http://www.ibm.com">www.ibm.com</ext-link></p></fn><fn id="fn025"><label>25</label><p><ext-link ext-link-type="uri" xlink:href="https://osf.io/5jtqx">https://osf.io/5jtqx</ext-link></p></fn></fn-group><sec sec-type="supplementary material"><title>Supplementary Material</title><p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00058/full#supplementary-material">https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00058/full#supplementary-material</ext-link></p><supplementary-material content-type="local-data" id="SM1"><media xlink:href="Data_Sheet_1.PDF"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andrienko</surname><given-names>Y. A.</given-names></name><name><surname>Brilliantov</surname><given-names>N. V.</given-names></name><name><surname>Kurths</surname><given-names>J.</given-names></name></person-group> (<year>2000</year>). <article-title>Complexity of two-dimensional patterns.</article-title>
<source><italic>Eur. Phys. J. B</italic></source>
<volume>15</volume>
<fpage>539</fpage>&#x02013;<lpage>546</lpage>. <pub-id pub-id-type="doi">10.1007/s100510051157</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ares</surname><given-names>G.</given-names></name></person-group> (<year>2018</year>). <article-title>Methodological issues in cross-cultural sensory and consumer research.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>64(Suppl. C)</volume>, <fpage>253</fpage>&#x02013;<lpage>263</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2016.10.007</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aust</surname><given-names>F.</given-names></name><name><surname>Diedenhofen</surname><given-names>B.</given-names></name><name><surname>Ullrich</surname><given-names>S.</given-names></name><name><surname>Musch</surname><given-names>J.</given-names></name></person-group> (<year>2013</year>). <article-title>Seriousness checks are useful to improve data validity in online research.</article-title>
<source><italic>Behav. Res. Methods</italic></source>
<volume>45</volume>
<fpage>527</fpage>&#x02013;<lpage>535</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-012-0265-2</pub-id>
<?supplied-pmid 23055170?><pub-id pub-id-type="pmid">23055170</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bangcuyo</surname><given-names>R. G.</given-names></name><name><surname>Smith</surname><given-names>K. J.</given-names></name><name><surname>Zumach</surname><given-names>J. L.</given-names></name><name><surname>Pierce</surname><given-names>A. M.</given-names></name><name><surname>Guttman</surname><given-names>G. A.</given-names></name><name><surname>Simons</surname><given-names>C. T.</given-names></name></person-group> (<year>2015</year>). <article-title>The use of immersive technologies to improve consumer testing: the role of ecological validity, context and engagement in evaluating coffee.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>41</volume>
<fpage>84</fpage>&#x02013;<lpage>95</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2014.11.017</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barla</surname><given-names>A.</given-names></name><name><surname>Franceschi</surname><given-names>E.</given-names></name><name><surname>Odone</surname><given-names>F.</given-names></name><name><surname>Verri</surname><given-names>A.</given-names></name></person-group> (<year>2002</year>). <article-title>&#x0201c;Image Kernels,&#x0201d; in</article-title>
<source><italic>Pattern Recognition with Support Vector Machines: First International Workshop, SVM 2002 Niagara Falls, Canada, August 10</italic> 2002 Proceedings</source>, <role>eds</role>
<person-group person-group-type="editor"><name><surname>Lee</surname><given-names>S.-W.</given-names></name><name><surname>Verri</surname><given-names>A.</given-names></name></person-group> (<publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>83</fpage>&#x02013;<lpage>96</lpage>. <pub-id pub-id-type="doi">10.1007/3-540-45665-1_7</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berlyne</surname><given-names>D. E.</given-names></name></person-group> (<year>1958</year>). <article-title>The influence of complexity and novelty in visual figures on orienting responses.</article-title>
<source><italic>J. Exp. Psychol.</italic></source>
<volume>55</volume>
<fpage>289</fpage>&#x02013;<lpage>296</lpage>. <pub-id pub-id-type="doi">10.1037/h0043555</pub-id>
<?supplied-pmid 13513951?><pub-id pub-id-type="pmid">13513951</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berridge</surname><given-names>K. C.</given-names></name></person-group> (<year>2004</year>). <article-title>Motivation concepts in behavioral neuroscience.</article-title>
<source><italic>Physiol. Behav.</italic></source>
<volume>81</volume>
<fpage>179</fpage>&#x02013;<lpage>209</lpage>. <pub-id pub-id-type="doi">10.1016/j.physbeh.2004.02.004</pub-id>
<?supplied-pmid 15159167?><pub-id pub-id-type="pmid">15159167</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berthoud</surname><given-names>H.-R.</given-names></name><name><surname>Zheng</surname><given-names>H.</given-names></name></person-group> (<year>2012</year>). <article-title>Modulation of taste responsiveness and food preference by obesity and weight loss.</article-title>
<source><italic>Physiol. Behav.</italic></source>
<volume>107</volume>
<fpage>527</fpage>&#x02013;<lpage>532</lpage>. <pub-id pub-id-type="doi">10.1016/j.physbeh.2012.04.004</pub-id>
<?supplied-pmid 22521912?><pub-id pub-id-type="pmid">22521912</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blechert</surname><given-names>J.</given-names></name><name><surname>Goltsche</surname><given-names>J. E.</given-names></name><name><surname>Herbert</surname><given-names>B. M.</given-names></name><name><surname>Wilhelm</surname><given-names>F. H.</given-names></name></person-group> (<year>2014a</year>). <article-title>Eat your troubles away: electrocortical and experiential correlates of food image processing are related to emotional eating style and emotional state.</article-title>
<source><italic>Biol. Psychol.</italic></source>
<volume>96</volume>
<fpage>94</fpage>&#x02013;<lpage>101</lpage>. <pub-id pub-id-type="doi">10.1016/j.biopsycho.2013.12.007</pub-id>
<?supplied-pmid 24361542?><pub-id pub-id-type="pmid">24361542</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blechert</surname><given-names>J.</given-names></name><name><surname>Meule</surname><given-names>A.</given-names></name><name><surname>Busch</surname><given-names>N. A.</given-names></name><name><surname>Ohla</surname><given-names>K.</given-names></name></person-group> (<year>2014b</year>). <article-title>Food-pics: an image database for experimental research on eating and appetite.</article-title>
<source><italic>Front. Psychol.</italic></source>
<volume>5</volume>:<issue>617</issue>. <pub-id pub-id-type="doi">10.3389/fpsyg.2014.00617</pub-id>
<?supplied-pmid 25009514?><pub-id pub-id-type="pmid">25009514</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bosch</surname><given-names>A.</given-names></name><name><surname>Zisserman</surname><given-names>A.</given-names></name><name><surname>Munoz</surname><given-names>X.</given-names></name></person-group> (<year>2007</year>). <article-title>&#x0201c;Representing shape with a spatial pyramid kernel,&#x0201d; in</article-title>
<source><italic>Proceedings of the 6th ACM International Conference on Image and Video Retrieval</italic></source>, (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name>), <fpage>401</fpage>&#x02013;<lpage>408</lpage>. <pub-id pub-id-type="doi">10.1145/1282280.1282340</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bossard</surname><given-names>L.</given-names></name><name><surname>Guillaumin</surname><given-names>M.</given-names></name><name><surname>Van Gool</surname><given-names>L.</given-names></name></person-group> (<year>2014</year>). <source><italic>Food-101 &#x02013; Mining Discriminative Components with Random Forests.</italic></source>
<publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>, <fpage>446</fpage>&#x02013;<lpage>461</lpage>.</mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braun</surname><given-names>J.</given-names></name><name><surname>Amirshahi</surname><given-names>S. A.</given-names></name><name><surname>Denzler</surname><given-names>J.</given-names></name><name><surname>Redies</surname><given-names>C.</given-names></name></person-group> (<year>2013</year>). <article-title>Statistical image properties of print advertisements, visual artworks and images of architecture.</article-title>
<source><italic>Front. Psychol.</italic></source>
<volume>4</volume>:<issue>808</issue>. <pub-id pub-id-type="doi">10.3389/fpsyg.2013.00808</pub-id>
<?supplied-pmid 24204353?><pub-id pub-id-type="pmid">24204353</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braun</surname><given-names>M. H.</given-names></name><name><surname>Pradana</surname><given-names>G. A.</given-names></name><name><surname>Buchanan</surname><given-names>G.</given-names></name><name><surname>Cheok</surname><given-names>A. D.</given-names></name><name><surname>Velasco</surname><given-names>C.</given-names></name><name><surname>Spence</surname><given-names>C.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Emotional priming of digital images through mobile telesmell and virtual food.</article-title>
<source><italic>Int. J. Food Des.</italic></source>
<volume>1</volume>
<fpage>29</fpage>&#x02013;<lpage>45</lpage>. <pub-id pub-id-type="doi">10.1386/ijfd.1.1.29_1</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cano</surname><given-names>M. E.</given-names></name><name><surname>Class</surname><given-names>Q. A.</given-names></name><name><surname>Polich</surname><given-names>J.</given-names></name></person-group> (<year>2009</year>). <article-title>Affective valence, stimulus attributes, and P300: color vs. black/white and normal vs. scrambled images.</article-title>
<source><italic>Int. J. Psychophysiol.</italic></source>
<volume>71</volume>
<fpage>17</fpage>&#x02013;<lpage>24</lpage>. <pub-id pub-id-type="doi">10.1016/j.ijpsycho.2008.07.016</pub-id>
<?supplied-pmid 18708099?><pub-id pub-id-type="pmid">18708099</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cardello</surname><given-names>A. V.</given-names></name><name><surname>Meiselman</surname><given-names>H. L.</given-names></name><name><surname>Schutz</surname><given-names>H. G.</given-names></name><name><surname>Craig</surname><given-names>C.</given-names></name><name><surname>Given</surname><given-names>Z.</given-names></name><name><surname>Lesher</surname><given-names>L. L.</given-names></name><etal/></person-group> (<year>2012</year>). <article-title>Measuring emotional responses to foods and food names using questionnaires.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>24</volume>
<fpage>243</fpage>&#x02013;<lpage>250</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2011.12.002</pub-id>
<?supplied-pmid 28784478?><pub-id pub-id-type="pmid">28784478</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cepeda-Benito</surname><given-names>A.</given-names></name><name><surname>Fernandez</surname><given-names>M. C.</given-names></name><name><surname>Moreno</surname><given-names>S.</given-names></name></person-group> (<year>2003</year>). <article-title>Relationship of gender and eating disorder symptoms to reported cravings for food: construct validation of state and trait craving questionnaires in Spanish.</article-title>
<source><italic>Appetite</italic></source>
<volume>40</volume>
<fpage>47</fpage>&#x02013;<lpage>54</lpage>. <pub-id pub-id-type="doi">10.1016/S0195-6663(02)00145-9</pub-id>
<?supplied-pmid 12631504?><pub-id pub-id-type="pmid">12631504</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Charbonnier</surname><given-names>L.</given-names></name><name><surname>van der Laan</surname><given-names>L. N.</given-names></name><name><surname>Viergever</surname><given-names>M. A.</given-names></name><name><surname>Smeets</surname><given-names>P. A. M.</given-names></name></person-group> (<year>2015</year>). <article-title>Functional MRI of challenging food choices: forced choice between equally liked high- and low-calorie foods in the absence of hunger.</article-title>
<source><italic>PLoS One</italic></source>
<volume>10</volume>:<issue>e0131727</issue>. <pub-id pub-id-type="doi">10.1371/journal.pone.0131727</pub-id>
<?supplied-pmid 26167916?><pub-id pub-id-type="pmid">26167916</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Charbonnier</surname><given-names>L.</given-names></name><name><surname>van Meer</surname><given-names>F.</given-names></name><name><surname>van der Laan</surname><given-names>L. N.</given-names></name><name><surname>Viergever</surname><given-names>M. A.</given-names></name><name><surname>Smeets</surname><given-names>P. A. M.</given-names></name></person-group> (<year>2016</year>). <article-title>Standardized food images: a photographing protocol and image database.</article-title>
<source><italic>Appetite</italic></source>
<volume>96</volume>
<fpage>166</fpage>&#x02013;<lpage>173</lpage>. <pub-id pub-id-type="doi">10.1016/j.appet.2015.08.041</pub-id>
<?supplied-pmid 26344127?><pub-id pub-id-type="pmid">26344127</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Ngo</surname><given-names>C.-W.</given-names></name></person-group> (<year>2016</year>). <article-title>&#x0201c;Deep-based ingredient recognition for cooking recipe retrieval,&#x0201d; in</article-title>
<source><italic>Proceedings of the 2016 ACM on Multimedia Conference</italic></source>, (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name>), <fpage>32</fpage>&#x02013;<lpage>41</lpage>. <pub-id pub-id-type="doi">10.1145/2964284.2964315</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>M.</given-names></name><name><surname>Dhingra</surname><given-names>K.</given-names></name><name><surname>Wu</surname><given-names>W.</given-names></name><name><surname>Yang</surname><given-names>L.</given-names></name><name><surname>Sukthankar</surname><given-names>R.</given-names></name><name><surname>Yang</surname><given-names>J.</given-names></name></person-group> (<year>2009</year>). <article-title>&#x0201c;PFID: Pittsburgh fast-food image dataset,&#x0201d; in</article-title>
<source><italic>Proceedings of the 6th IEEE International Conference on Image Processing</italic></source>, <publisher-loc>Cairo</publisher-loc>, <fpage>289</fpage>&#x02013;<lpage>292</lpage>. <pub-id pub-id-type="doi">10.1109/ICIP.2009.5413511</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X.</given-names></name><name><surname>Zhou</surname><given-names>H.</given-names></name><name><surname>Diao</surname><given-names>L.</given-names></name></person-group> (<year>2017</year>). <article-title>ChineseFoodNet: a large-scale image dataset for Chinese food recognition.</article-title>
<source><italic>arXiv</italic></source>
<comment>[Preprint]</comment>
<ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1705.02743">arXiv:1705.02743</ext-link></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y.-C.</given-names></name><name><surname>Woods</surname><given-names>A. T.</given-names></name><name><surname>Spence</surname><given-names>C.</given-names></name></person-group> (<year>2018</year>). <article-title>Sensation transference from plateware to food: the sounds and tastes of plates.</article-title>
<source><italic>Int. J. Food Des.</italic></source>
<volume>3</volume>
<fpage>41</fpage>&#x02013;<lpage>62</lpage>. <pub-id pub-id-type="doi">10.1386/ijfd.3.1.41_1</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cho</surname><given-names>S.</given-names></name><name><surname>Han</surname><given-names>A.</given-names></name><name><surname>Taylor</surname><given-names>M. H.</given-names></name><name><surname>Huck</surname><given-names>A. C.</given-names></name><name><surname>Mishler</surname><given-names>A. M.</given-names></name><name><surname>Mattal</surname><given-names>K. L.</given-names></name><etal/></person-group> (<year>2015</year>). <article-title>Blue lighting decreases the amount of food consumed in men, but not in women.</article-title>
<source><italic>Appetite</italic></source>
<volume>85</volume>
<fpage>111</fpage>&#x02013;<lpage>117</lpage>. <pub-id pub-id-type="doi">10.1016/j.appet.2014.11.020</pub-id>
<?supplied-pmid 25447013?><pub-id pub-id-type="pmid">25447013</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Comber</surname><given-names>R.</given-names></name><name><surname>Choi</surname><given-names>J. H.-J.</given-names></name><name><surname>Hoonhout</surname><given-names>J.</given-names></name><name><surname>O&#x02019;Hara</surname><given-names>K.</given-names></name></person-group> (<year>2014</year>). <article-title>Editorial: designing for human-food interaction: an introduction to the special issue on &#x02018;food and interaction design&#x02019;.</article-title>
<source><italic>Int. J. Hum. Comput. Stud.</italic></source>
<volume>72</volume>
<fpage>181</fpage>&#x02013;<lpage>184</lpage>. <pub-id pub-id-type="doi">10.1016/j.ijhcs.2013.09.001</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corchs</surname><given-names>S. E.</given-names></name><name><surname>Ciocca</surname><given-names>G.</given-names></name><name><surname>Bricolo</surname><given-names>E.</given-names></name><name><surname>Gasparini</surname><given-names>F.</given-names></name></person-group> (<year>2016</year>). <article-title>Predicting complexity perception of real world images.</article-title>
<source><italic>PLoS One</italic></source>
<volume>11</volume>:<issue>e0157986</issue>. <pub-id pub-id-type="doi">10.1371/journal.pone.0157986</pub-id>
<?supplied-pmid 27336469?><pub-id pub-id-type="pmid">27336469</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crisinel</surname><given-names>A.-S.</given-names></name><name><surname>Cosser</surname><given-names>S.</given-names></name><name><surname>King</surname><given-names>S.</given-names></name><name><surname>Jones</surname><given-names>R.</given-names></name><name><surname>Petrie</surname><given-names>J.</given-names></name><name><surname>Spence</surname><given-names>C.</given-names></name></person-group> (<year>2012</year>). <article-title>A bittersweet symphony: systematically modulating the taste of food by changing the sonic properties of the soundtrack playing in the background.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>24</volume>
<fpage>201</fpage>&#x02013;<lpage>204</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2011.08.009</pub-id></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dalal</surname><given-names>N.</given-names></name><name><surname>Triggs</surname><given-names>B.</given-names></name></person-group> (<year>2005</year>). <article-title>&#x0201c;Histograms of oriented gradients for human detection,&#x0201d; in</article-title>
<source><italic>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</italic></source>, (<publisher-loc>Piscataway, NJ</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>886</fpage>&#x02013;<lpage>893</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2005.177</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dalenberg</surname><given-names>J. R.</given-names></name><name><surname>Gutjar</surname><given-names>S.</given-names></name><name><surname>ter Horst</surname><given-names>G. J.</given-names></name><name><surname>de Graaf</surname><given-names>K.</given-names></name><name><surname>Renken</surname><given-names>R. J.</given-names></name><name><surname>Jager</surname><given-names>G.</given-names></name></person-group> (<year>2014</year>). <article-title>Evoked emotions predict food choice.</article-title>
<source><italic>PLoS One</italic></source>
<volume>9</volume>:<issue>e115388</issue>. <pub-id pub-id-type="doi">10.1371/journal.pone.0115388</pub-id>
<?supplied-pmid 25521352?><pub-id pub-id-type="pmid">25521352</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desmet</surname><given-names>P. M. A.</given-names></name><name><surname>Schifferstein</surname><given-names>H. N. J.</given-names></name></person-group> (<year>2008</year>). <article-title>Sources of positive and negative emotions in food experience.</article-title>
<source><italic>Appetite</italic></source>
<volume>50</volume>
<fpage>290</fpage>&#x02013;<lpage>301</lpage>. <pub-id pub-id-type="doi">10.1016/j.appet.2007.08.003</pub-id>
<?supplied-pmid 17945385?><pub-id pub-id-type="pmid">17945385</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donderi</surname><given-names>D.</given-names></name></person-group> (<year>2006</year>). <article-title>An information theory analysis of visual complexity and dissimilarity.</article-title>
<source><italic>Perception</italic></source>
<volume>35</volume>
<fpage>823</fpage>&#x02013;<lpage>835</lpage>. <pub-id pub-id-type="doi">10.1068/p5249</pub-id>
<?supplied-pmid 16836047?><pub-id pub-id-type="pmid">16836047</pub-id></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Farinella</surname><given-names>G. M.</given-names></name><name><surname>Allegra</surname><given-names>D.</given-names></name><name><surname>Stanco</surname><given-names>F.</given-names></name></person-group> (<year>2015</year>). <source><italic>A Benchmark Dataset to Study the Representation of Food Images.</italic></source>
<publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>, <fpage>584</fpage>&#x02013;<lpage>599</lpage>.</mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fenko</surname><given-names>A.</given-names></name><name><surname>Backhaus</surname><given-names>B. W.</given-names></name><name><surname>van Hoof</surname><given-names>J. J.</given-names></name></person-group> (<year>2015</year>). <article-title>The influence of product- and person-related factors on consumer hedonic responses to soy products.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>41</volume>
<fpage>30</fpage>&#x02013;<lpage>40</lpage>. <pub-id pub-id-type="doi">10.1016/j.chb.2014.12.001</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiegel</surname><given-names>A.</given-names></name><name><surname>Meullenet</surname><given-names>J. F.</given-names></name><name><surname>Harrington</surname><given-names>R. J.</given-names></name><name><surname>Humble</surname><given-names>R.</given-names></name><name><surname>Seo</surname><given-names>H. S.</given-names></name></person-group> (<year>2014</year>). <article-title>Background music genre can modulate flavor pleasantness and overall impression of food stimuli.</article-title>
<source><italic>Appetite</italic></source>
<volume>76</volume>
<fpage>144</fpage>&#x02013;<lpage>152</lpage>. <pub-id pub-id-type="doi">10.1016/j.appet.2014.01.079</pub-id>
<?supplied-pmid 24530691?><pub-id pub-id-type="pmid">24530691</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flint</surname><given-names>A.</given-names></name><name><surname>Raben</surname><given-names>A.</given-names></name><name><surname>Blundell</surname><given-names>J. E.</given-names></name><name><surname>Astrup</surname><given-names>A.</given-names></name></person-group> (<year>2000</year>). <article-title>Reproducibility, power and validity of visual analogue scales in assessment of appetite sensations in single test meal studies.</article-title>
<source><italic>Int. J. Obes. Relat. Metab. Disord.</italic></source>
<volume>24</volume>
<fpage>38</fpage>&#x02013;<lpage>48</lpage>. <pub-id pub-id-type="doi">10.1038/sj.ijo.0801083</pub-id>
<?supplied-pmid 10702749?><pub-id pub-id-type="pmid">10702749</pub-id></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foroni</surname><given-names>F.</given-names></name><name><surname>Pergola</surname><given-names>G.</given-names></name><name><surname>Argiris</surname><given-names>G.</given-names></name><name><surname>Rumiati</surname><given-names>R.</given-names></name></person-group> (<year>2013</year>). <article-title>The Food Cast research image database (FRIDa).</article-title>
<source><italic>Front. Hum. Neurosci.</italic></source>
<volume>7</volume>:<issue>51</issue>. <pub-id pub-id-type="doi">10.3389/fnhum.2013.00051</pub-id>
<?supplied-pmid 23459781?><pub-id pub-id-type="pmid">23459781</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foroni</surname><given-names>F.</given-names></name><name><surname>Pergola</surname><given-names>G.</given-names></name><name><surname>Rumiati</surname><given-names>R. I.</given-names></name></person-group> (<year>2016</year>). <article-title>Food color is in the eye of the beholder: the role of human trichromatic vision in food evaluation.</article-title>
<source><italic>Sci. Rep.</italic></source>
<volume>6</volume>:<issue>37034</issue>. <pub-id pub-id-type="doi">10.1038/srep37034</pub-id>
<?supplied-pmid 27841327?><pub-id pub-id-type="pmid">27841327</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Forsythe</surname><given-names>A.</given-names></name><name><surname>Nadal</surname><given-names>M.</given-names></name><name><surname>Sheehy</surname><given-names>N.</given-names></name><name><surname>Cela-Conde</surname><given-names>C. J.</given-names></name><name><surname>Sawey</surname><given-names>M.</given-names></name></person-group> (<year>2011</year>). <article-title>Predicting beauty: fractal dimension and visual complexity in art.</article-title>
<source><italic>Br. J. Psychol.</italic></source>
<volume>102</volume>
<fpage>49</fpage>&#x02013;<lpage>70</lpage>. <pub-id pub-id-type="doi">10.1348/000712610X498958</pub-id>
<?supplied-pmid 21241285?><pub-id pub-id-type="pmid">21241285</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Genschow</surname><given-names>O.</given-names></name><name><surname>Reutner</surname><given-names>L.</given-names></name><name><surname>W&#x000e4;nke</surname><given-names>M.</given-names></name></person-group> (<year>2012</year>). <article-title>The color red reduces snack food and soft drink intake.</article-title>
<source><italic>Appetite</italic></source>
<volume>58</volume>
<fpage>699</fpage>&#x02013;<lpage>702</lpage>. <pub-id pub-id-type="doi">10.1016/j.appet.2011.12.023</pub-id>
<?supplied-pmid 22245725?><pub-id pub-id-type="pmid">22245725</pub-id></mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorini</surname><given-names>A.</given-names></name><name><surname>Griez</surname><given-names>E.</given-names></name><name><surname>Petrova</surname><given-names>A.</given-names></name><name><surname>Riva</surname><given-names>G.</given-names></name></person-group> (<year>2010</year>). <article-title>Assessment of the emotional responses produced by exposure to real food, virtual food and photographs of food in patients affected by eating disorders.</article-title>
<source><italic>Ann. Gen. Psychiatry</italic></source>
<volume>9</volume>
<fpage>1</fpage>&#x02013;<lpage>10</lpage>. <pub-id pub-id-type="doi">10.1186/1744-859x-9-30</pub-id>
<?supplied-pmid 20602749?><pub-id pub-id-type="pmid">20602749</pub-id></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gosling</surname><given-names>S. D.</given-names></name><name><surname>Vazire</surname><given-names>S.</given-names></name><name><surname>Srivastava</surname><given-names>S.</given-names></name><name><surname>John</surname><given-names>O. P.</given-names></name></person-group> (<year>2004</year>). <article-title>Should we trust web-based studies? A comparative analysis of six preconceptions about internet questionnaires.</article-title>
<source><italic>Am. Psychol.</italic></source>
<volume>59</volume>
<fpage>93</fpage>&#x02013;<lpage>104</lpage>. <pub-id pub-id-type="doi">10.1037/0003-066X.59.2.93</pub-id>
<?supplied-pmid 14992636?><pub-id pub-id-type="pmid">14992636</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffioen-Roose</surname><given-names>S.</given-names></name><name><surname>Smeets</surname><given-names>P. A. M.</given-names></name><name><surname>Weijzen</surname><given-names>P. L. G.</given-names></name><name><surname>van Rijn</surname><given-names>I.</given-names></name><name><surname>van den Bosch</surname><given-names>I.</given-names></name><name><surname>de Graaf</surname><given-names>C.</given-names></name></person-group> (<year>2013</year>). <article-title>Effect of replacing sugar with non-caloric sweeteners in beverages on the reward value after repeated exposure.</article-title>
<source><italic>PLoS One</italic></source>
<volume>8</volume>:<issue>e81924</issue>. <pub-id pub-id-type="doi">10.1371/journal.pone.0081924</pub-id>
<?supplied-pmid 24312382?><pub-id pub-id-type="pmid">24312382</pub-id></mixed-citation></ref><ref id="B43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutjar</surname><given-names>S.</given-names></name><name><surname>de Graaf</surname><given-names>C.</given-names></name><name><surname>Kooijman</surname><given-names>V.</given-names></name><name><surname>de Wijk</surname><given-names>R. A.</given-names></name><name><surname>Nys</surname><given-names>A.</given-names></name><name><surname>ter Horst</surname><given-names>G. J.</given-names></name><etal/></person-group> (<year>2015</year>). <article-title>The role of emotions in food choice and liking.</article-title>
<source><italic>Food Res. Int.</italic></source>
<volume>76(Part 2)</volume>, <fpage>216</fpage>&#x02013;<lpage>223</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodres.2014.12.022</pub-id></mixed-citation></ref><ref id="B44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haralick</surname><given-names>R. M.</given-names></name><name><surname>Shanmugam</surname><given-names>K.</given-names></name><name><surname>Dinstein</surname><given-names>I.</given-names></name></person-group> (<year>1976</year>). <article-title>Textural features for image classification.</article-title>
<source><italic>IEEE Trans. Syst. Man Cybern.</italic></source>
<volume>3</volume>
<fpage>610</fpage>&#x02013;<lpage>621</lpage>. <pub-id pub-id-type="doi">10.1109/TSMC.1973.4309314</pub-id></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasenbeck</surname><given-names>A.</given-names></name><name><surname>Cho</surname><given-names>S.</given-names></name><name><surname>Meullenet</surname><given-names>J. F.</given-names></name><name><surname>Tokar</surname><given-names>T.</given-names></name><name><surname>Yang</surname><given-names>F.</given-names></name><name><surname>Huddleston</surname><given-names>E. A.</given-names></name><etal/></person-group> (<year>2014</year>). <article-title>Color and illuminance level of lighting can modulate willingness to eat bell peppers.</article-title>
<source><italic>J. Sci. Food Agric.</italic></source>
<volume>94</volume>
<fpage>2049</fpage>&#x02013;<lpage>2056</lpage>. <pub-id pub-id-type="doi">10.1002/jsfa.6523</pub-id>
<?supplied-pmid 24318125?><pub-id pub-id-type="pmid">24318125</pub-id></mixed-citation></ref><ref id="B46"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hasler</surname><given-names>D.</given-names></name><name><surname>S&#x000fc;sstrunk</surname><given-names>S. E.</given-names></name></person-group> (<year>2003</year>). <article-title>&#x0201c;Measuring colorfulness in natural images,&#x0201d; in</article-title>
<source><italic>Human Vision and Electronic Imaging VIII</italic></source>, <role>eds</role>
<person-group person-group-type="editor"><name><surname>Rogowitz</surname><given-names>B. E.</given-names></name><name><surname>Thrasyvoulos</surname><given-names>N. P.</given-names></name></person-group> (<publisher-loc>Santa Clara, CA</publisher-loc>: <publisher-name>The International Society for Optical Engineering</publisher-name>), <fpage>87</fpage>&#x02013;<lpage>95</lpage>. <pub-id pub-id-type="doi">10.1117/12.477378</pub-id></mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayn-Leichsenring</surname><given-names>G. U.</given-names></name><name><surname>Lehmann</surname><given-names>T.</given-names></name><name><surname>Redies</surname><given-names>C.</given-names></name></person-group> (<year>2017</year>). <article-title>Subjective ratings of beauty and aesthetics: correlations with statistical image properties in western oil paintings.</article-title>
<source><italic>Iperception</italic></source>
<volume>8</volume>:<issue>2041669517715474</issue>. <pub-id pub-id-type="doi">10.1177/2041669517715474</pub-id>
<?supplied-pmid 28694958?><pub-id pub-id-type="pmid">28694958</pub-id></mixed-citation></ref><ref id="B48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebert</surname><given-names>K. R.</given-names></name><name><surname>Valle-Incl&#x000e1;n</surname><given-names>F.</given-names></name><name><surname>Hackley</surname><given-names>S. A.</given-names></name></person-group> (<year>2015</year>). <article-title>Modulation of eyeblink and postauricular reflexes during the anticipation and viewing of food images.</article-title>
<source><italic>Psychophysiology</italic></source>
<volume>52</volume>
<fpage>509</fpage>&#x02013;<lpage>517</lpage>. <?supplied-pmid 25336280?><pub-id pub-id-type="pmid">25336280</pub-id></mixed-citation></ref><ref id="B49"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Herman</surname><given-names>C. P.</given-names></name></person-group> (<year>1993</year>). <article-title>&#x0201c;Effects of heat on appetite,&#x0201d; in</article-title>
<source><italic>Nutritional Needs in Hot Environments: Applications for Military Personnel in Field Operations</italic></source>, <role>ed.</role>
<person-group person-group-type="editor"><name><surname>Marriott</surname><given-names>B. M.</given-names></name></person-group> (<publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Academy Press</publisher-name>), <fpage>187</fpage>&#x02013;<lpage>214</lpage>.</mixed-citation></ref><ref id="B50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoefling</surname><given-names>A.</given-names></name><name><surname>Likowski</surname><given-names>K. U.</given-names></name><name><surname>Deutsch</surname><given-names>R.</given-names></name><name><surname>H&#x000e4;fner</surname><given-names>M.</given-names></name><name><surname>Seibt</surname><given-names>B.</given-names></name><name><surname>M&#x000fc;hlberger</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2009</year>). <article-title>When hunger finds no fault with moldy corn: food deprivation reduces food-related disgust.</article-title>
<source><italic>Emotion</italic></source>
<volume>9</volume>
<fpage>50</fpage>&#x02013;<lpage>58</lpage>. <pub-id pub-id-type="doi">10.1037/a0014449</pub-id>
<?supplied-pmid 19186916?><pub-id pub-id-type="pmid">19186916</pub-id></mixed-citation></ref><ref id="B51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoogeveen</surname><given-names>H. R.</given-names></name><name><surname>Dalenberg</surname><given-names>J. R.</given-names></name><name><surname>Renken</surname><given-names>R. J.</given-names></name><name><surname>ter Horst</surname><given-names>G. J.</given-names></name><name><surname>Lorist</surname><given-names>M. M.</given-names></name></person-group> (<year>2015</year>). <article-title>Neural processing of basic tastes in healthy young and older adults - an fMRI study.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>119</volume>
<fpage>1</fpage>&#x02013;<lpage>12</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.06.017</pub-id>
<?supplied-pmid 26072251?><pub-id pub-id-type="pmid">26072251</pub-id></mixed-citation></ref><ref id="B52"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Huisman</surname><given-names>G.</given-names></name><name><surname>Bruijnes</surname><given-names>M.</given-names></name><name><surname>Heylen</surname><given-names>D. K. J.</given-names></name></person-group> (<year>2016</year>). <article-title>&#x0201c;A moving feast: effects of color, shape and animation on taste associations and taste perceptions,&#x0201d; in</article-title>
<source><italic>Proceedings of the 13th International Conference on Advances in Computer Entertainment Technology</italic></source>, (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name>), <fpage>1</fpage>&#x02013;<lpage>12</lpage>. <pub-id pub-id-type="doi">10.1145/3001773.3001776</pub-id></mixed-citation></ref><ref id="B53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>C. D.</given-names></name><name><surname>Duraccio</surname><given-names>K. M.</given-names></name><name><surname>Barnett</surname><given-names>K. A.</given-names></name><name><surname>Stevens</surname><given-names>K. S.</given-names></name></person-group> (<year>2016</year>). <article-title>Appropriateness of the food-pics image database for experimental eating and appetite research with adolescents.</article-title>
<source><italic>Eat. Behav.</italic></source>
<volume>23</volume>
<fpage>195</fpage>&#x02013;<lpage>199</lpage>. <pub-id pub-id-type="doi">10.1016/j.eatbeh.2016.10.007</pub-id>
<?supplied-pmid 27842263?><pub-id pub-id-type="pmid">27842263</pub-id></mixed-citation></ref><ref id="B54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaneko</surname><given-names>D.</given-names></name><name><surname>Toet</surname><given-names>A.</given-names></name><name><surname>Ushiama</surname><given-names>S.</given-names></name><name><surname>Brouwer</surname><given-names>A. M.</given-names></name><name><surname>Kallen</surname><given-names>V.</given-names></name><name><surname>van Erp</surname><given-names>J. B. F.</given-names></name></person-group> (<year>2018</year>). <article-title>EmojiGrid: a 2D pictorial scale for cross-cultural emotion assessment of negatively and positively valenced food.</article-title>
<source><italic>Food Res. Int.</italic></source>
<volume>115</volume>
<fpage>541</fpage>&#x02013;<lpage>551</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodres.2018.09.049</pub-id>
<?supplied-pmid 30599977?><pub-id pub-id-type="pmid">30599977</pub-id></mixed-citation></ref><ref id="B55"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kawano</surname><given-names>Y.</given-names></name><name><surname>Yanai</surname><given-names>K.</given-names></name></person-group> (<year>2015</year>). <article-title>&#x0201c;Automatic expansion of a food image dataset leveraging existing categories with domain adaptation,&#x0201d; in</article-title>
<source><italic>Computer Vision - ECCV 2014 Workshops</italic></source>, <role>eds</role>
<person-group person-group-type="editor"><name><surname>Agapito</surname><given-names>L.</given-names></name><name><surname>Bronstein</surname><given-names>M. M.</given-names></name><name><surname>Rother</surname><given-names>C.</given-names></name></person-group> (<publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>), <fpage>3</fpage>&#x02013;<lpage>17</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-16199-0_1</pub-id></mixed-citation></ref><ref id="B56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Killgore</surname><given-names>W. D. S.</given-names></name><name><surname>Young</surname><given-names>A. D.</given-names></name><name><surname>Femia</surname><given-names>L. A.</given-names></name><name><surname>Bogorodzki</surname><given-names>P.</given-names></name><name><surname>Rogowska</surname><given-names>J.</given-names></name><name><surname>Yurgelun-Todd</surname><given-names>D. A.</given-names></name></person-group> (<year>2003</year>). <article-title>Cortical and limbic activation during viewing of high- versus low-calorie foods.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>19</volume>
<fpage>1381</fpage>&#x02013;<lpage>1394</lpage>. <pub-id pub-id-type="doi">10.1016/S1053-8119(03)00191-5</pub-id>
<?supplied-pmid 12948696?><pub-id pub-id-type="pmid">12948696</pub-id></mixed-citation></ref><ref id="B57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>S. C.</given-names></name><name><surname>Meiselman</surname><given-names>H. L.</given-names></name><name><surname>Carr</surname><given-names>B. T.</given-names></name></person-group> (<year>2010</year>). <article-title>Measuring emotions associated with foods in consumer testing.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>21</volume>
<fpage>1114</fpage>&#x02013;<lpage>1116</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2010.08.004</pub-id></mixed-citation></ref><ref id="B58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knaapila</surname><given-names>A.</given-names></name><name><surname>Laaksonen</surname><given-names>O.</given-names></name><name><surname>Virtanen</surname><given-names>M.</given-names></name><name><surname>Yang</surname><given-names>B.</given-names></name><name><surname>Lagstr&#x000f6;m</surname><given-names>H.</given-names></name><name><surname>Sandell</surname><given-names>M.</given-names></name></person-group> (<year>2017</year>). <article-title>Pleasantness, familiarity, and identification of spice odors are interrelated and enhanced by consumption of herbs and food neophilia.</article-title>
<source><italic>Appetite</italic></source>
<volume>109</volume>
<fpage>190</fpage>&#x02013;<lpage>200</lpage>. <pub-id pub-id-type="doi">10.1016/j.appet.2016.11.025</pub-id>
<?supplied-pmid 27884762?><pub-id pub-id-type="pmid">27884762</pub-id></mixed-citation></ref><ref id="B59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>K&#x000f6;nig</surname><given-names>L. M.</given-names></name><name><surname>Renner</surname><given-names>B.</given-names></name></person-group> (<year>2018</year>). <article-title>Colourful = healthy? Exploring meal colour variety and its relation to food consumption.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>64(Suppl. C)</volume>, <fpage>66</fpage>&#x02013;<lpage>71</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2017.10.011</pub-id></mixed-citation></ref><ref id="B60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koo</surname><given-names>T. K.</given-names></name><name><surname>Li</surname><given-names>M. Y.</given-names></name></person-group> (<year>2016</year>). <article-title>A guideline of selecting and reporting intraclass correlation coefficients for reliability research.</article-title>
<source><italic>J. Chiropr. Med.</italic></source>
<volume>15</volume>
<fpage>155</fpage>&#x02013;<lpage>163</lpage>. <pub-id pub-id-type="doi">10.1016/j.jcm.2016.02.012</pub-id>
<?supplied-pmid 29276468?><pub-id pub-id-type="pmid">27330520</pub-id></mixed-citation></ref><ref id="B61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>K&#x000f6;ster</surname><given-names>E. P.</given-names></name></person-group> (<year>2003</year>). <article-title>The psychology of food choice: some often encountered fallacies.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>14</volume>
<fpage>359</fpage>&#x02013;<lpage>373</lpage>. <pub-id pub-id-type="doi">10.1016/S0950-3293(03)00017-X</pub-id></mixed-citation></ref><ref id="B62"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>K&#x000f6;ster</surname><given-names>E. P.</given-names></name><name><surname>Mojet</surname><given-names>J.</given-names></name></person-group> (<year>2007</year>). <article-title>&#x0201c;Boredom and the reasons why some new products fail,&#x0201d; in</article-title>
<source><italic>Consumer&#x02014;Led Food Product Development</italic></source>, <role>ed.</role>
<person-group person-group-type="editor"><name><surname>MacFie</surname><given-names>H.</given-names></name></person-group> (<publisher-loc>Cambridge</publisher-loc>: <publisher-name>Woodhead</publisher-name>), <fpage>262</fpage>&#x02013;<lpage>280</lpage>. <pub-id pub-id-type="doi">10.1533/9781845693381.2.262</pub-id></mixed-citation></ref><ref id="B63"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lang</surname><given-names>P. J.</given-names></name><name><surname>Bradley</surname><given-names>M. M.</given-names></name><name><surname>Cuthbert</surname><given-names>B. N.</given-names></name></person-group> (<year>2005</year>). <source><italic>International Affective Picture System (IAPS): Instruction Manual and Affective Ratings.</italic></source>
<publisher-loc>Gainesville, FL</publisher-loc>: <publisher-name>University of Florida</publisher-name>.</mixed-citation></ref><ref id="B64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lang</surname><given-names>P. J.</given-names></name><name><surname>Greenwald</surname><given-names>M. K.</given-names></name><name><surname>Bradley</surname><given-names>M. M.</given-names></name><name><surname>Hamm</surname><given-names>A. O.</given-names></name></person-group> (<year>1993</year>). <article-title>Looking at pictures: affective, facial, visceral, and behavioral reactions.</article-title>
<source><italic>Psychophysiology</italic></source>
<volume>30</volume>
<fpage>261</fpage>&#x02013;<lpage>273</lpage>. <pub-id pub-id-type="doi">10.1111/j.1469-8986.1993.tb03352.x</pub-id>
<?supplied-pmid 8497555?><pub-id pub-id-type="pmid">8497555</pub-id></mixed-citation></ref><ref id="B65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levitsky</surname><given-names>D. A.</given-names></name><name><surname>Iyer</surname><given-names>S.</given-names></name><name><surname>Pacanowski</surname><given-names>C. R.</given-names></name></person-group> (<year>2012</year>). <article-title>Number of foods available at a meal determines the amount consumed.</article-title>
<source><italic>Eat. Behav.</italic></source>
<volume>13</volume>
<fpage>183</fpage>&#x02013;<lpage>187</lpage>. <pub-id pub-id-type="doi">10.1016/j.eatbeh.2012.01.006</pub-id>
<?supplied-pmid 22664394?><pub-id pub-id-type="pmid">22664394</pub-id></mixed-citation></ref><ref id="B66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>R.</given-names></name><name><surname>Hannum</surname><given-names>M.</given-names></name><name><surname>Simons</surname><given-names>C. T.</given-names></name></person-group> (<year>2018</year>). <article-title>Using immersive technologies to explore the effects of congruent and incongruent contextual cues on context recall, product evaluation time, and preference and liking during consumer hedonic testing.</article-title>
<source><italic>Food Res. Int.</italic></source>
<pub-id pub-id-type="doi">10.1016/j.foodres.2018.04.024</pub-id></mixed-citation></ref><ref id="B67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lucassen</surname><given-names>M. P.</given-names></name><name><surname>Gevers</surname><given-names>T.</given-names></name><name><surname>Gijsenij</surname><given-names>A.</given-names></name></person-group> (<year>2011</year>). <article-title>Texture affects color emotion.</article-title>
<source><italic>Color Res. Appl.</italic></source>
<volume>36</volume>
<fpage>426</fpage>&#x02013;<lpage>436</lpage>. <pub-id pub-id-type="doi">10.1002/col.20647</pub-id></mixed-citation></ref><ref id="B68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madan</surname><given-names>C. R.</given-names></name><name><surname>Bayer</surname><given-names>J.</given-names></name><name><surname>Gamer</surname><given-names>M.</given-names></name><name><surname>Lonsdorf</surname><given-names>T. B.</given-names></name><name><surname>Sommer</surname><given-names>T.</given-names></name></person-group> (<year>2018</year>). <article-title>Visual complexity and affect: ratings reflect more than meets the eye.</article-title>
<source><italic>Front. Psychol.</italic></source>
<volume>8</volume>:<issue>2368</issue>. <pub-id pub-id-type="doi">10.3389/fpsyg.2017.02368</pub-id>
<?supplied-pmid 29403412?><pub-id pub-id-type="pmid">29403412</pub-id></mixed-citation></ref><ref id="B69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Majima</surname><given-names>Y.</given-names></name><name><surname>Nishiyama</surname><given-names>K.</given-names></name><name><surname>Nishihara</surname><given-names>A.</given-names></name><name><surname>Hata</surname><given-names>R.</given-names></name></person-group> (<year>2017</year>). <article-title>Conducting online behavioral research using crowdsourcing services in Japan.</article-title>
<source><italic>Front. Psychol.</italic></source>
<volume>8</volume>:<issue>378</issue>. <pub-id pub-id-type="doi">10.3389/fpsyg.2017.00378</pub-id>
<?supplied-pmid 28382006?><pub-id pub-id-type="pmid">28382006</pub-id></mixed-citation></ref><ref id="B70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manzocco</surname><given-names>L.</given-names></name><name><surname>Rumignani</surname><given-names>A.</given-names></name><name><surname>Lagazio</surname><given-names>C.</given-names></name></person-group> (<year>2013</year>). <article-title>Emotional response to fruit salads with different visual quality.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>28</volume>
<fpage>17</fpage>&#x02013;<lpage>22</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2012.08.014</pub-id></mixed-citation></ref><ref id="B71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marin</surname><given-names>M. M.</given-names></name><name><surname>Leder</surname><given-names>H.</given-names></name></person-group> (<year>2013</year>). <article-title>Examining complexity across domains: relating subjective and objective measures of affective environmental scenes, paintings and music.</article-title>
<source><italic>PLoS One</italic></source>
<volume>8</volume>:<issue>e72412</issue>. <pub-id pub-id-type="doi">10.1371/journal.pone.0072412</pub-id>
<?supplied-pmid 23977295?><pub-id pub-id-type="pmid">23977295</pub-id></mixed-citation></ref><ref id="B72"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Matsuda</surname><given-names>Y.</given-names></name><name><surname>Hoashi</surname><given-names>H.</given-names></name><name><surname>Yanai</surname><given-names>K.</given-names></name></person-group> (<year>2012</year>). <article-title>&#x0201c;Recognition of multiple-food images by detecting candidate regions,&#x0201d; in</article-title>
<source><italic>Proceedings of the 2012 IEEE International Conference on Multimedia and Expo</italic></source>, <publisher-loc>Melbourne</publisher-loc>, <fpage>25</fpage>&#x02013;<lpage>30</lpage>. <pub-id pub-id-type="doi">10.1109/ICME.2012.157</pub-id></mixed-citation></ref><ref id="B73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCarron</surname><given-names>A.</given-names></name><name><surname>Tierney</surname><given-names>K. J.</given-names></name></person-group> (<year>1989</year>). <article-title>The effect of auditory stimulation on the consumption of soft drinks.</article-title>
<source><italic>Appetite</italic></source>
<volume>13</volume>
<fpage>155</fpage>&#x02013;<lpage>159</lpage>. <pub-id pub-id-type="doi">10.1016/0195-6663(89)90112-8</pub-id>
<?supplied-pmid 2802594?><pub-id pub-id-type="pmid">2802594</pub-id></mixed-citation></ref><ref id="B74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meiselman</surname><given-names>H. L.</given-names></name></person-group> (<year>2013</year>). <article-title>The future in sensory/consumer research: &#x02026;&#x02026;&#x02026;..evolving to a better science.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>27</volume>
<fpage>208</fpage>&#x02013;<lpage>214</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2012.03.002</pub-id></mixed-citation></ref><ref id="B75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mel&#x000e9;ndez-Mart&#x000ed;nez</surname><given-names>A. J.</given-names></name><name><surname>Vicario</surname><given-names>I. M.</given-names></name><name><surname>Heredia</surname><given-names>F. J.</given-names></name></person-group> (<year>2005</year>). <article-title>Correlation between visual and instrumental colour measurements of orange juice dilutions: effect of the background.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>16</volume>
<fpage>471</fpage>&#x02013;<lpage>478</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2004.09.003</pub-id></mixed-citation></ref><ref id="B76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miccoli</surname><given-names>L.</given-names></name><name><surname>Delgado</surname><given-names>R.</given-names></name><name><surname>Guerra</surname><given-names>P.</given-names></name><name><surname>Versace</surname><given-names>F.</given-names></name><name><surname>Rodr&#x000ed;guez-Ruiz</surname><given-names>S.</given-names></name><name><surname>Fern&#x000e1;ndez-Santaella</surname><given-names>M. C.</given-names></name></person-group> (<year>2016</year>). <article-title>Affective pictures and the Open Library of Affective Foods (OLAF): tools to investigate emotions toward food in adults.</article-title>
<source><italic>PLoS One</italic></source>
<volume>11</volume>:<issue>e0158991</issue>. <pub-id pub-id-type="doi">10.1371/journal.pone.0158991</pub-id>
<?supplied-pmid 27513636?><pub-id pub-id-type="pmid">27513636</pub-id></mixed-citation></ref><ref id="B77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miccoli</surname><given-names>L.</given-names></name><name><surname>Delgado</surname><given-names>R.</given-names></name><name><surname>Rodr&#x000ed;guez-Ruiz</surname><given-names>S.</given-names></name><name><surname>Guerra</surname><given-names>P.</given-names></name><name><surname>Garc&#x000ed;a-M&#x000e1;rmol</surname><given-names>E.</given-names></name><name><surname>Fern&#x000e1;ndez-Santaella</surname><given-names>M. C.</given-names></name></person-group> (<year>2014</year>). <article-title>Meet OLAF, a good friend of the IAPS! The Open Library of Affective Foods: a tool to investigate the emotional impact of food in adolescents.</article-title>
<source><italic>PLoS One</italic></source>
<volume>9</volume>:<issue>e114515</issue>. <pub-id pub-id-type="doi">10.1371/journal.pone.0114515</pub-id>
<?supplied-pmid 25490404?><pub-id pub-id-type="pmid">25490404</pub-id></mixed-citation></ref><ref id="B78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michel</surname><given-names>C.</given-names></name><name><surname>Velasco</surname><given-names>C.</given-names></name><name><surname>Fraemohs</surname><given-names>P.</given-names></name><name><surname>Spence</surname><given-names>C.</given-names></name></person-group> (<year>2015a</year>). <article-title>Studying the impact of plating on ratings of the food served in a naturalistic dining context.</article-title>
<source><italic>Appetite</italic></source>
<volume>90</volume>
<fpage>45</fpage>&#x02013;<lpage>50</lpage>. <pub-id pub-id-type="doi">10.1016/j.appet.2015.02.030</pub-id>
<?supplied-pmid 25728885?><pub-id pub-id-type="pmid">25728885</pub-id></mixed-citation></ref><ref id="B79"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michel</surname><given-names>C.</given-names></name><name><surname>Velasco</surname><given-names>C.</given-names></name><name><surname>Spence</surname><given-names>C.</given-names></name></person-group> (<year>2015b</year>). <article-title>Cutlery matters: heavy cutlery enhances diners&#x02019; enjoyment of the food served in a realistic dining environment.</article-title>
<source><italic>Flavour</italic></source>
<volume>4</volume>:<issue>26</issue>
<pub-id pub-id-type="doi">10.1186/s13411-015-0036-y</pub-id></mixed-citation></ref><ref id="B80"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mielby</surname><given-names>L. H.</given-names></name><name><surname>Kildegaard</surname><given-names>H.</given-names></name><name><surname>Gabrielsen</surname><given-names>G.</given-names></name><name><surname>Edelenbos</surname><given-names>M.</given-names></name><name><surname>Thybo</surname><given-names>A. K.</given-names></name></person-group> (<year>2012</year>). <article-title>Adolescent and adult visual preferences for pictures of fruit and vegetable mixes &#x02013; Effect of complexity.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>26</volume>
<fpage>188</fpage>&#x02013;<lpage>195</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2012.04.014</pub-id></mixed-citation></ref><ref id="B81"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Narumi</surname><given-names>T.</given-names></name><name><surname>Ban</surname><given-names>Y.</given-names></name><name><surname>Kajinami</surname><given-names>T.</given-names></name><name><surname>Tanikawa</surname><given-names>T.</given-names></name><name><surname>Hirose</surname><given-names>M.</given-names></name></person-group> (<year>2012</year>). <article-title>&#x0201c;Augmented perception of satiety: controlling food consumption by changing apparent size of food with augmented reality,&#x0201d; in</article-title>
<source><italic>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</italic></source>, (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name>), <fpage>109</fpage>&#x02013;<lpage>118</lpage>. <pub-id pub-id-type="doi">10.1145/2207676.2207693</pub-id></mixed-citation></ref><ref id="B82"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ng</surname><given-names>M.</given-names></name><name><surname>Chaya</surname><given-names>C.</given-names></name><name><surname>Hort</surname><given-names>J.</given-names></name></person-group> (<year>2013</year>). <article-title>Beyond liking: comparing the measurement of emotional response using EsSense Profile and consumer defined check-all-that-apply methodologies.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>28</volume>
<fpage>193</fpage>&#x02013;<lpage>205</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2012.08.012</pub-id></mixed-citation></ref><ref id="B83"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nishizawa</surname><given-names>M.</given-names></name><name><surname>Jiang</surname><given-names>W.</given-names></name><name><surname>Okajima</surname><given-names>K.</given-names></name></person-group> (<year>2016</year>). <article-title>&#x0201c;Projective-AR system for customizing the appearance and taste of food,&#x0201d; in</article-title>
<source><italic>Proceedings of the 2016 Workshop on Multimodal Virtual and Augmented Reality</italic></source>, (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name>), <fpage>1</fpage>&#x02013;<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1145/3001959.3001966</pub-id></mixed-citation></ref><ref id="B84"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nordbo</surname><given-names>K.</given-names></name><name><surname>Milne</surname><given-names>D.</given-names></name><name><surname>Calvo</surname><given-names>R. A.</given-names></name><name><surname>Allman-Farinelli</surname><given-names>M.</given-names></name></person-group> (<year>2015</year>). <article-title>&#x0201c;Virtual food court: a VR environment to assess people&#x02019;s food choices,&#x0201d; in</article-title>
<source><italic>Proceedings of the Annual Meeting of the Australian Special Interest Group for Computer Human Interaction</italic></source>, (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name>), <fpage>69</fpage>&#x02013;<lpage>72</lpage>. <pub-id pub-id-type="doi">10.1145/2838739.2838817</pub-id></mixed-citation></ref><ref id="B85"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oberfeld</surname><given-names>D.</given-names></name><name><surname>Hecht</surname><given-names>H.</given-names></name><name><surname>Allendorf</surname><given-names>U.</given-names></name><name><surname>Wickelmaier</surname><given-names>F.</given-names></name></person-group> (<year>2009</year>). <article-title>Ambient lighting modifies the flavor of wine.</article-title>
<source><italic>J. Sens. Stud.</italic></source>
<volume>24</volume>
<fpage>797</fpage>&#x02013;<lpage>832</lpage>. <pub-id pub-id-type="doi">10.1111/j.1745-459X.2009.00239.x</pub-id></mixed-citation></ref><ref id="B86"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obrist</surname><given-names>M.</given-names></name><name><surname>Velasco</surname><given-names>C.</given-names></name><name><surname>Vi</surname><given-names>C.</given-names></name><name><surname>Ranasinghe</surname><given-names>N.</given-names></name><name><surname>Israr</surname><given-names>A.</given-names></name><name><surname>Cheok</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Sensing the future of HCI: touch, taste, and smell user interfaces.</article-title>
<source><italic>Interactions</italic></source>
<volume>23</volume>
<fpage>40</fpage>&#x02013;<lpage>49</lpage>. <pub-id pub-id-type="doi">10.1145/2973568</pub-id></mixed-citation></ref><ref id="B87"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pallavicini</surname><given-names>F.</given-names></name><name><surname>Serino</surname><given-names>S.</given-names></name><name><surname>Cipresso</surname><given-names>P.</given-names></name><name><surname>Pedroli</surname><given-names>E.</given-names></name><name><surname>Chicchi Giglioli</surname><given-names>I. A.</given-names></name><name><surname>Chirico</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Testing augmented reality for cue exposure in obese patients: an exploratory study.</article-title>
<source><italic>Cyberpsychol. Behav. Soc. Netw.</italic></source>
<volume>19</volume>
<fpage>107</fpage>&#x02013;<lpage>114</lpage>. <pub-id pub-id-type="doi">10.1089/cyber.2015.0235</pub-id>
<?supplied-pmid 26882325?><pub-id pub-id-type="pmid">26882325</pub-id></mixed-citation></ref><ref id="B88"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Palus</surname><given-names>H.</given-names></name></person-group> (<year>2005</year>). <article-title>&#x0201c;Colourfulness of the image and its application in image filtering,&#x0201d; in</article-title>
<source><italic>Proceedings of the Fifth IEEE International Symposium on Signal Processing and Information Technology</italic></source>, (<publisher-loc>Piscataway, NJ</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>884</fpage>&#x02013;<lpage>889</lpage>. <pub-id pub-id-type="doi">10.1109/ISSPIT.2005.1577216</pub-id></mixed-citation></ref><ref id="B89"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pashler</surname><given-names>H.</given-names></name><name><surname>Wagenmakers</surname><given-names>E. J.</given-names></name></person-group> (<year>2012</year>). <article-title>Editors&#x02019; introduction to the special section on replicability in psychological science: a crisis of confidence?</article-title>
<source><italic>Perspect. Psychol. Sci.</italic></source>
<volume>7</volume>
<fpage>528</fpage>&#x02013;<lpage>530</lpage>. <pub-id pub-id-type="doi">10.1177/1745691612465253</pub-id>
<?supplied-pmid 26168108?><pub-id pub-id-type="pmid">26168108</pub-id></mixed-citation></ref><ref id="B90"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piqueras-Fiszman</surname><given-names>B.</given-names></name><name><surname>Alcaide</surname><given-names>J.</given-names></name><name><surname>Roura</surname><given-names>E.</given-names></name><name><surname>Spence</surname><given-names>C.</given-names></name></person-group> (<year>2012</year>). <article-title>Is it the plate or is it the food? Assessing the influence of the color (black or white) and shape of the plate on the perception of the food placed on it.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>24</volume>
<fpage>205</fpage>&#x02013;<lpage>208</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2011.08.011</pub-id></mixed-citation></ref><ref id="B91"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piqueras-Fiszman</surname><given-names>B.</given-names></name><name><surname>Giboreau</surname><given-names>A.</given-names></name><name><surname>Spence</surname><given-names>C.</given-names></name></person-group> (<year>2013</year>). <article-title>Assessing the influence of the color of the plate on the perception of a complex food in a restaurant setting.</article-title>
<source><italic>Flavour</italic></source>
<volume>2</volume>:<issue>24</issue>
<pub-id pub-id-type="doi">10.1186/2044-7248-2-24</pub-id></mixed-citation></ref><ref id="B92"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piqueras-Fiszman</surname><given-names>B.</given-names></name><name><surname>Kraus</surname><given-names>A. A.</given-names></name><name><surname>Spence</surname><given-names>C.</given-names></name></person-group> (<year>2014</year>). <article-title>&#x0201c;Yummy&#x0201d; versus &#x0201c;Yucky&#x0201d;! Explicit and implicit approach&#x02013;avoidance motivations towards appealing and disgusting foods.</article-title>
<source><italic>Appetite</italic></source>
<volume>78</volume>
<fpage>193</fpage>&#x02013;<lpage>202</lpage>. <pub-id pub-id-type="doi">10.1016/j.appet.2014.03.029</pub-id>
<?supplied-pmid 24709484?><pub-id pub-id-type="pmid">24709484</pub-id></mixed-citation></ref><ref id="B93"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piqueras-Fiszman</surname><given-names>B.</given-names></name><name><surname>Spence</surname><given-names>C.</given-names></name></person-group> (<year>2014</year>). <article-title>Colour, pleasantness, and consumption behaviour within a meal.</article-title>
<source><italic>Appetite</italic></source>
<volume>75</volume>
<fpage>165</fpage>&#x02013;<lpage>172</lpage>. <pub-id pub-id-type="doi">10.1016/j.appet.2014.01.004</pub-id>
<?supplied-pmid 24462488?><pub-id pub-id-type="pmid">24462488</pub-id></mixed-citation></ref><ref id="B94"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proulx</surname><given-names>R.</given-names></name><name><surname>Parrott</surname><given-names>L.</given-names></name></person-group> (<year>2008</year>). <article-title>Measures of structural complexity in digital images for monitoring the ecological signature of an old-growth forest ecosystem.</article-title>
<source><italic>Ecol. Indic.</italic></source>
<volume>8</volume>
<fpage>270</fpage>&#x02013;<lpage>284</lpage>. <pub-id pub-id-type="doi">10.1016/j.ecolind.2007.02.005</pub-id></mixed-citation></ref><ref id="B95"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raudenbush</surname><given-names>B.</given-names></name><name><surname>Frank</surname><given-names>R. A.</given-names></name></person-group> (<year>1999</year>). <article-title>Assessing food neophobia: the role of stimulus familiarity.</article-title>
<source><italic>Appetite</italic></source>
<volume>32</volume>
<fpage>261</fpage>&#x02013;<lpage>271</lpage>. <pub-id pub-id-type="doi">10.1006/appe.1999.0229</pub-id>
<?supplied-pmid 10097030?><pub-id pub-id-type="pmid">10097030</pub-id></mixed-citation></ref><ref id="B96"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Redies</surname><given-names>C.</given-names></name><name><surname>Amirshahi</surname><given-names>S. A.</given-names></name><name><surname>Koch</surname><given-names>M.</given-names></name><name><surname>Denzler</surname><given-names>J.</given-names></name></person-group> (<year>2012</year>). <article-title>&#x0201c;PHOG-Derived aesthetic measures applied to color photographs of artworks, natural scenes and objects,&#x0201d; in</article-title>
<source><italic>Computer Vision &#x02013; ECCV 2012. Workshops and Demonstrations: Florence, Italy, October 7-13</italic> 2012 Proceedings, Part I</source>, <role>eds</role>
<person-group person-group-type="editor"><name><surname>Fusiello</surname><given-names>A.</given-names></name><name><surname>Murino</surname><given-names>V.</given-names></name><name><surname>Cucchiara</surname><given-names>R.</given-names></name></person-group> (<publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>522</fpage>&#x02013;<lpage>531</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-642-33863-2_54</pub-id></mixed-citation></ref><ref id="B97"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Reinoso Carvalho</surname><given-names>F.</given-names></name><name><surname>Van Ee</surname><given-names>R.</given-names></name><name><surname>Rychtarikova</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>). <article-title>&#x0201c;Matching soundscapes and music with food types,&#x0201d; in</article-title>
<source><italic>Proceedings of Euroregio 2016</italic></source>, (<publisher-loc>Porto</publisher-loc>: <publisher-name>Sociedade Portuguesa de Ac&#x000fa;stica</publisher-name>), <fpage>178</fpage>&#x02013;<lpage>186</lpage>.</mixed-citation></ref><ref id="B98"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>B. J.</given-names></name><name><surname>Rowe</surname><given-names>E. A.</given-names></name><name><surname>Rolls</surname><given-names>E. T.</given-names></name></person-group> (<year>1982</year>). <article-title>How sensory properties of foods affect human feeding behavior.</article-title>
<source><italic>Physiol. Behav.</italic></source>
<volume>29</volume>
<fpage>409</fpage>&#x02013;<lpage>417</lpage>. <pub-id pub-id-type="doi">10.1016/0031-9384(82)90259-1</pub-id><pub-id pub-id-type="pmid">7178247</pub-id></mixed-citation></ref><ref id="B99"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenholtz</surname><given-names>R.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Nakano</surname><given-names>T.</given-names></name></person-group> (<year>2007</year>). <article-title>Measuring visual clutter.</article-title>
<source><italic>J. Vis.</italic></source>
<volume>7</volume>
<fpage>1</fpage>&#x02013;<lpage>22</lpage>. <pub-id pub-id-type="doi">10.1167/7.2.17</pub-id>
<?supplied-pmid 18217832?><pub-id pub-id-type="pmid">18217832</pub-id></mixed-citation></ref><ref id="B100"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rozin</surname><given-names>P.</given-names></name></person-group> (<year>1988</year>). <article-title>&#x0201c;Cultural approaches to human food preferences,&#x0201d; in</article-title>
<source><italic>Nutritional Modulation of Neural Function</italic></source>, <role>eds</role>
<person-group person-group-type="editor"><name><surname>Morley</surname><given-names>J. E.</given-names></name><name><surname>Sterman</surname><given-names>M. B.</given-names></name><name><surname>Walsh</surname><given-names>J. H.</given-names></name></person-group> (<publisher-loc>San Diego, CA</publisher-loc>: <publisher-name>Academic Press</publisher-name>), <fpage>137</fpage>&#x02013;<lpage>153</lpage>.</mixed-citation></ref><ref id="B101"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rozin</surname><given-names>P.</given-names></name></person-group> (<year>1996</year>). <article-title>Towards a psychology of food and eating: from motivation to module to model to marker, morality, meaning, and metaphor.</article-title>
<source><italic>Curr. Dir. Psychol. Sci.</italic></source>
<volume>5</volume>
<fpage>18</fpage>&#x02013;<lpage>24</lpage>. <pub-id pub-id-type="doi">10.1111/1467-8721.ep10772690</pub-id></mixed-citation></ref><ref id="B102"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samant</surname><given-names>S. S.</given-names></name><name><surname>Seo</surname><given-names>H.-S.</given-names></name></person-group> (<year>2018</year>). <article-title>Using both emotional responses and sensory attribute intensities to predict consumer liking and preference toward vegetable juice products.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>73</volume>
<fpage>75</fpage>&#x02013;<lpage>85</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2018.12.006</pub-id></mixed-citation></ref><ref id="B103"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schifferstein</surname><given-names>H. N. J.</given-names></name><name><surname>Fenko</surname><given-names>A.</given-names></name><name><surname>Desmet</surname><given-names>P. M. A.</given-names></name><name><surname>Labbe</surname><given-names>D.</given-names></name><name><surname>Martin</surname><given-names>N.</given-names></name></person-group> (<year>2013</year>). <article-title>Influence of package design on the dynamics of multisensory and emotional food experience.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>27</volume>
<fpage>18</fpage>&#x02013;<lpage>25</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2012.06.003</pub-id></mixed-citation></ref><ref id="B104"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schifferstein</surname><given-names>H. N. J.</given-names></name><name><surname>Howell</surname><given-names>B. F.</given-names></name><name><surname>Pont</surname><given-names>S. C.</given-names></name></person-group> (<year>2017</year>). <article-title>Colored backgrounds affect the attractiveness of fresh produce, but not it&#x02019;s perceived color.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>56(Part A)</volume>, <fpage>173</fpage>&#x02013;<lpage>180</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2016.10.011</pub-id></mixed-citation></ref><ref id="B105"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shrout</surname><given-names>P. E.</given-names></name><name><surname>Fleiss</surname><given-names>J. L.</given-names></name></person-group> (<year>1979</year>). <article-title>Intraclass correlations: uses in assessing rater reliability.</article-title>
<source><italic>Psychol. Bull.</italic></source>
<volume>86</volume>
<fpage>420</fpage>&#x02013;<lpage>428</lpage>. <pub-id pub-id-type="doi">10.1037/0033-2909.86.2.420</pub-id>
<?supplied-pmid 18839484?><pub-id pub-id-type="pmid">18839484</pub-id></mixed-citation></ref><ref id="B106"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simmons</surname><given-names>W. K.</given-names></name><name><surname>Martin</surname><given-names>A.</given-names></name><name><surname>Barsalou</surname><given-names>L. W.</given-names></name></person-group> (<year>2005</year>). <article-title>Pictures of appetizing foods activate gustatory cortices for taste and reward.</article-title>
<source><italic>Cereb. Cortex</italic></source>
<volume>15</volume>
<fpage>1602</fpage>&#x02013;<lpage>1608</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhi038</pub-id>
<?supplied-pmid 15703257?><pub-id pub-id-type="pmid">15703257</pub-id></mixed-citation></ref><ref id="B107"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Slater</surname><given-names>S.</given-names></name><name><surname>Yani-de-Soriano</surname><given-names>M.</given-names></name></person-group> (<year>2010</year>). <article-title>Researching consumers in multicultural societies: emerging methodological issues.</article-title>
<source><italic>J. Mark. Manag.</italic></source>
<volume>26</volume>
<fpage>1143</fpage>&#x02013;<lpage>1160</lpage>. <pub-id pub-id-type="doi">10.1080/0267257X.2010.509581</pub-id></mixed-citation></ref><ref id="B108"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spence</surname><given-names>C.</given-names></name></person-group> (<year>2011</year>). <article-title>Crossmodal correspondences: a tutorial review.</article-title>
<source><italic>Attent. Percept. Psychophys.</italic></source>
<volume>73</volume>
<fpage>971</fpage>&#x02013;<lpage>995</lpage>. <pub-id pub-id-type="doi">10.3758/s13414-010-0073-7</pub-id>
<?supplied-pmid 21264748?><pub-id pub-id-type="pmid">21264748</pub-id></mixed-citation></ref><ref id="B109"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spence</surname><given-names>C.</given-names></name></person-group> (<year>2015a</year>). <article-title>Leading the consumer by the nose: on the commercialization of olfactory design for the food and beverage sector.</article-title>
<source><italic>Flavour</italic></source>
<volume>4</volume>:<issue>31</issue>
<pub-id pub-id-type="doi">10.1186/s13411-015-0041-1</pub-id></mixed-citation></ref><ref id="B110"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spence</surname><given-names>C.</given-names></name></person-group> (<year>2015b</year>). <article-title>On the psychological impact of food colour.</article-title>
<source><italic>Flavour</italic></source>
<volume>4</volume>:<issue>21</issue>
<pub-id pub-id-type="doi">10.1186/s13411-015-0031-3</pub-id></mixed-citation></ref><ref id="B111"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spence</surname><given-names>C.</given-names></name></person-group> (<year>2018</year>). <article-title>Background colour &#x00026; its impact on food perception &#x00026; behaviour.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>68</volume>
<fpage>156</fpage>&#x02013;<lpage>166</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2018.02.012</pub-id></mixed-citation></ref><ref id="B112"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spence</surname><given-names>C.</given-names></name><name><surname>Okajima</surname><given-names>K.</given-names></name><name><surname>Cheok</surname><given-names>A. D.</given-names></name><name><surname>Petit</surname><given-names>O.</given-names></name><name><surname>Michel</surname><given-names>C.</given-names></name></person-group> (<year>2015</year>). <article-title>Eating with our eyes: from visual hunger to digital satiation.</article-title>
<source><italic>Brain Cogn.</italic></source>
<volume>110</volume>
<fpage>53</fpage>&#x02013;<lpage>63</lpage>. <pub-id pub-id-type="doi">10.1016/j.bandc.2015.08.006</pub-id>
<?supplied-pmid 26432045?><pub-id pub-id-type="pmid">26432045</pub-id></mixed-citation></ref><ref id="B113"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Spence</surname><given-names>C.</given-names></name><name><surname>Piqueras-Fiszman</surname><given-names>B.</given-names></name></person-group> (<year>2014</year>). <source><italic>The Perfect Meal: the Multisensory Science of Food and Dining.</italic></source>
<publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>John Wiley &#x00026; Sons</publisher-name>.</mixed-citation></ref><ref id="B114"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spence</surname><given-names>C.</given-names></name><name><surname>Shankar</surname><given-names>M. U.</given-names></name></person-group> (<year>2010</year>). <article-title>The influence of auditory cues on the perception of, and responses to, food and drink.</article-title>
<source><italic>J. Sens. Stud.</italic></source>
<volume>25</volume>
<fpage>406</fpage>&#x02013;<lpage>430</lpage>. <pub-id pub-id-type="doi">10.1111/j.1745-459X.2009.00267.x</pub-id></mixed-citation></ref><ref id="B115"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spence</surname><given-names>C.</given-names></name><name><surname>Velasco</surname><given-names>C.</given-names></name><name><surname>Knoeferle</surname><given-names>K.</given-names></name></person-group> (<year>2014</year>). <article-title>A large sample study on the influence of the multisensory environment on the wine drinking experience.</article-title>
<source><italic>Flavour</italic></source>
<volume>3</volume>:<issue>8</issue>
<pub-id pub-id-type="doi">10.1186/2044-7248-3-8</pub-id></mixed-citation></ref><ref id="B116"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sperber</surname><given-names>A. D.</given-names></name></person-group> (<year>2004</year>). <article-title>Translation and validation of study instruments for cross-cultural research.</article-title>
<source><italic>Gastroenterology</italic></source>
<volume>126</volume>
<fpage>S124</fpage>&#x02013;<lpage>S128</lpage>. <pub-id pub-id-type="doi">10.1053/j.gastro.2003.10.016</pub-id><pub-id pub-id-type="pmid">14978648</pub-id></mixed-citation></ref><ref id="B117"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spinelli</surname><given-names>S.</given-names></name><name><surname>Masi</surname><given-names>C.</given-names></name><name><surname>Zoboli</surname><given-names>G. P.</given-names></name><name><surname>Prescott</surname><given-names>J.</given-names></name><name><surname>Monteleone</surname><given-names>E.</given-names></name></person-group> (<year>2015</year>). <article-title>Emotional responses to branded and unbranded foods.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>42</volume>
<fpage>1</fpage>&#x02013;<lpage>11</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2014.12.009</pub-id></mixed-citation></ref><ref id="B118"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stewart</surname><given-names>P. C.</given-names></name><name><surname>Goss</surname><given-names>E.</given-names></name></person-group> (<year>2013</year>). <article-title>Plate shape and colour interact to influence taste and quality judgments.</article-title>
<source><italic>Flavour</italic></source>
<volume>2</volume>:<issue>27</issue>
<pub-id pub-id-type="doi">10.1186/2044-7248-2-27</pub-id></mixed-citation></ref><ref id="B119"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stockburger</surname><given-names>J.</given-names></name><name><surname>Renner</surname><given-names>B.</given-names></name><name><surname>Weike</surname><given-names>A. I.</given-names></name><name><surname>Hamm</surname><given-names>A. O.</given-names></name><name><surname>Schupp</surname><given-names>H. T.</given-names></name></person-group> (<year>2009</year>). <article-title>Vegetarianism and food perception. Selective visual attention to meat pictures.</article-title>
<source><italic>Appetite</italic></source>
<volume>52</volume>
<fpage>513</fpage>&#x02013;<lpage>516</lpage>. <pub-id pub-id-type="doi">10.1016/j.appet.2008.10.001</pub-id>
<?supplied-pmid 18996158?><pub-id pub-id-type="pmid">18996158</pub-id></mixed-citation></ref><ref id="B120"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stroebele</surname><given-names>N.</given-names></name><name><surname>De Castro</surname><given-names>J. M.</given-names></name></person-group> (<year>2004</year>). <article-title>Effect of ambience on food intake and food choice.</article-title>
<source><italic>Nutrition</italic></source>
<volume>20</volume>
<fpage>821</fpage>&#x02013;<lpage>838</lpage>. <pub-id pub-id-type="doi">10.1016/j.nut.2004.05.012</pub-id>
<?supplied-pmid 15325695?><pub-id pub-id-type="pmid">15325695</pub-id></mixed-citation></ref><ref id="B121"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suk</surname><given-names>H.-J.</given-names></name><name><surname>Park</surname><given-names>G.-L.</given-names></name><name><surname>Kim</surname><given-names>Y.</given-names></name></person-group> (<year>2012</year>). <article-title>Bon App&#x000e9;tit! An investigation about the best and worst color combinations of lighting and food.</article-title>
<source><italic>J. Literature Art Stud.</italic></source>
<volume>2</volume>
<fpage>559</fpage>&#x02013;<lpage>566</lpage>.</mixed-citation></ref><ref id="B122"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szocs</surname><given-names>C.</given-names></name><name><surname>Lefebvre</surname><given-names>S.</given-names></name></person-group> (<year>2017</year>). <article-title>Spread or stacked? Vertical versus horizontal food presentation, portion size perceptions, and consumption.</article-title>
<source><italic>J. Bus. Res.</italic></source>
<volume>75</volume>
<fpage>249</fpage>&#x02013;<lpage>257</lpage>. <pub-id pub-id-type="doi">10.1016/j.jbusres.2016.07.022</pub-id></mixed-citation></ref><ref id="B123"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Takahashi</surname><given-names>K.</given-names></name><name><surname>Doman</surname><given-names>K.</given-names></name><name><surname>Kawanishi</surname><given-names>Y.</given-names></name><name><surname>Hirayama</surname><given-names>T.</given-names></name><name><surname>Ide</surname><given-names>I.</given-names></name><name><surname>Deguchi</surname><given-names>D.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>&#x0201c;Estimation of the attractiveness of food photography focusing on main ingredients,&#x0201d; in</article-title>
<source><italic>Proceedings of the 9th Workshop on Multimedia for Cooking and Eating Activities, in conjunction with The 2017 International Joint Conference on Artificial Intelligence</italic></source>, (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name>), <fpage>1</fpage>&#x02013;<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1145/3106668.3106670</pub-id></mixed-citation></ref><ref id="B124"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thomson</surname><given-names>D. M. H.</given-names></name><name><surname>Crocker</surname><given-names>C.</given-names></name><name><surname>Marketo</surname><given-names>C. G.</given-names></name></person-group> (<year>2010</year>). <article-title>Linking sensory characteristics to emotions: an example using dark chocolate.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>21</volume>
<fpage>1117</fpage>&#x02013;<lpage>1125</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2010.04.011</pub-id></mixed-citation></ref><ref id="B125"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toepel</surname><given-names>U.</given-names></name><name><surname>Knebel</surname><given-names>J.-F.</given-names></name><name><surname>Hudry</surname><given-names>J.</given-names></name><name><surname>le Coutre</surname><given-names>J.</given-names></name><name><surname>Murray</surname><given-names>M. M.</given-names></name></person-group> (<year>2012</year>). <article-title>Gender and weight shape brain dynamics during food viewing.</article-title>
<source><italic>PLoS One</italic></source>
<volume>7</volume>:<issue>e36778</issue>. <pub-id pub-id-type="doi">10.1371/journal.pone.0036778</pub-id>
<?supplied-pmid 22590605?><pub-id pub-id-type="pmid">22590605</pub-id></mixed-citation></ref><ref id="B126"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toet</surname><given-names>A.</given-names></name></person-group> (<year>2016</year>). <article-title>Natural dynamic backgrounds affect perceived facial dominance.</article-title>
<source><italic>Matters</italic></source>
<volume>2</volume>:<issue>e201610000018</issue>
<pub-id pub-id-type="doi">10.19185/matters.201610000018</pub-id></mixed-citation></ref><ref id="B127"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toet</surname><given-names>A.</given-names></name><name><surname>Henselmans</surname><given-names>M.</given-names></name><name><surname>Lucassen</surname><given-names>M. P.</given-names></name><name><surname>Gevers</surname><given-names>T.</given-names></name></person-group> (<year>2012</year>). <article-title>Emotional effects of dynamic textures.</article-title>
<source><italic>Iperception</italic></source>
<volume>2</volume>
<fpage>969</fpage>&#x02013;<lpage>991</lpage>. <pub-id pub-id-type="doi">10.1068/i0477</pub-id>
<?supplied-pmid 23145257?><pub-id pub-id-type="pmid">23145257</pub-id></mixed-citation></ref><ref id="B128"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tu</surname><given-names>Y.</given-names></name><name><surname>Yang</surname><given-names>Z.</given-names></name><name><surname>Ma</surname><given-names>C.</given-names></name></person-group> (<year>2016</year>). <article-title>The taste of plate: how the spiciness of food is affected by the color of the plate used to serve It.</article-title>
<source><italic>J. Sens. Stud.</italic></source>
<volume>31</volume>
<fpage>50</fpage>&#x02013;<lpage>60</lpage>. <pub-id pub-id-type="doi">10.1111/joss.12190</pub-id></mixed-citation></ref><ref id="B129"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ung</surname><given-names>C.-Y.</given-names></name><name><surname>Menozzi</surname><given-names>M.</given-names></name><name><surname>Hartmann</surname><given-names>C.</given-names></name><name><surname>Siegrist</surname><given-names>M.</given-names></name></person-group> (<year>2018</year>). <article-title>Innovations in consumer research: the virtual food buffet.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>63(Suppl. C)</volume>, <fpage>12</fpage>&#x02013;<lpage>17</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2017.07.007</pub-id></mixed-citation></ref><ref id="B130"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Laan</surname><given-names>L. N.</given-names></name><name><surname>de Ridder</surname><given-names>D. T. D.</given-names></name><name><surname>Viergever</surname><given-names>M. A.</given-names></name><name><surname>Smeets</surname><given-names>P. A. M.</given-names></name></person-group> (<year>2011</year>). <article-title>The first taste is always with the eyes: a meta-analysis on the neural correlates of processing visual food cues.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>55</volume>
<fpage>296</fpage>&#x02013;<lpage>303</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.11.055</pub-id>
<?supplied-pmid 21111829?><pub-id pub-id-type="pmid">21111829</pub-id></mixed-citation></ref><ref id="B131"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Ittersum</surname><given-names>K.</given-names></name><name><surname>Wansink</surname><given-names>B.</given-names></name></person-group> (<year>2012</year>). <article-title>Plate size and color suggestibility: the Delboeuf Illusion&#x02019;s bias on serving and eating behavior.</article-title>
<source><italic>J. Consum. Res.</italic></source>
<volume>39</volume>
<fpage>215</fpage>&#x02013;<lpage>228</lpage>. <pub-id pub-id-type="doi">10.1086/662615</pub-id></mixed-citation></ref><ref id="B132"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Zyl</surname><given-names>H.</given-names></name><name><surname>Meiselman</surname><given-names>H. L.</given-names></name></person-group> (<year>2015</year>). <article-title>The roles of culture and language in designing emotion lists: comparing the same language in different English and Spanish speaking countries.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>41(Suppl. C)</volume>, <fpage>201</fpage>&#x02013;<lpage>213</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2014.12.003</pub-id></mixed-citation></ref><ref id="B133"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Velasco</surname><given-names>C.</given-names></name><name><surname>Michel</surname><given-names>C.</given-names></name><name><surname>Woods</surname><given-names>A. T.</given-names></name><name><surname>Spence</surname><given-names>C.</given-names></name></person-group> (<year>2016</year>). <article-title>On the importance of balance to aesthetic plating.</article-title>
<source><italic>Int. J. Gastron. Food Sci.</italic></source>
<fpage>5</fpage>&#x02013;<lpage>6</lpage>, <fpage>10</fpage>&#x02013;<lpage>16</lpage>. <pub-id pub-id-type="doi">10.1016/j.ijgfs.2016.08.001</pub-id></mixed-citation></ref><ref id="B134"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Velasco</surname><given-names>C.</given-names></name><name><surname>Obrist</surname><given-names>M.</given-names></name><name><surname>Petit</surname><given-names>O.</given-names></name><name><surname>Spence</surname><given-names>C.</given-names></name></person-group> (<year>2018</year>). <article-title>Multisensory technology for flavor augmentation: a mini review.</article-title>
<source><italic>Front. Psychol.</italic></source>
<volume>9</volume>:<issue>26</issue>. <pub-id pub-id-type="doi">10.3389/fpsyg.2018.00026</pub-id>
<?supplied-pmid 29441030?><pub-id pub-id-type="pmid">29441030</pub-id></mixed-citation></ref><ref id="B135"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wackerbauer</surname><given-names>R.</given-names></name><name><surname>Witt</surname><given-names>A.</given-names></name><name><surname>Atmanspacher</surname><given-names>H.</given-names></name><name><surname>Kurths</surname><given-names>J.</given-names></name><name><surname>Scheingraber</surname><given-names>H.</given-names></name></person-group> (<year>1994</year>). <article-title>A comparative classification of complexity measures.</article-title>
<source><italic>Chaos Solitons Fractals</italic></source>
<volume>4</volume>
<fpage>133</fpage>&#x02013;<lpage>173</lpage>. <pub-id pub-id-type="doi">10.1016/0960-0779(94)90023-X</pub-id></mixed-citation></ref><ref id="B136"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wadhera</surname><given-names>D.</given-names></name><name><surname>Capaldi-Phillips</surname><given-names>E. D.</given-names></name></person-group> (<year>2014</year>). <article-title>A review of visual cues associated with food on food acceptance and consumption.</article-title>
<source><italic>Eat. Behav.</italic></source>
<volume>15</volume>
<fpage>132</fpage>&#x02013;<lpage>143</lpage>. <pub-id pub-id-type="doi">10.1016/j.eatbeh.2013.11.003</pub-id>
<?supplied-pmid 24411766?><pub-id pub-id-type="pmid">24411766</pub-id></mixed-citation></ref><ref id="B137"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Kumar</surname><given-names>D.</given-names></name><name><surname>Thome</surname><given-names>M.</given-names></name><name><surname>Cord</surname><given-names>M.</given-names></name><name><surname>Precioso</surname><given-names>F.</given-names></name></person-group> (<year>2015</year>). <article-title>&#x0201c;Recipe recognition with large multimodal food dataset,&#x0201d; in</article-title>
<source><italic>Proceedings of the 2015 IEEE International Conference on Multimedia &#x00026; Expo Workshops (ICMEW)</italic></source>, <publisher-loc>Turin</publisher-loc>, <fpage>1</fpage>&#x02013;<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1109/ICMEW.2015.7169757</pub-id></mixed-citation></ref><ref id="B138"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wansink</surname><given-names>B.</given-names></name><name><surname>van Ittersum</surname><given-names>K.</given-names></name></person-group> (<year>2013</year>). <article-title>Portion size me: plate-size induced consumption norms and win-win solutions for reducing food intake and waste.</article-title>
<source><italic>J. Exp. Psychol. Appl.</italic></source>
<volume>19</volume>
<fpage>320</fpage>&#x02013;<lpage>332</lpage>. <pub-id pub-id-type="doi">10.1037/a0035053</pub-id>
<?supplied-pmid 24341317?><pub-id pub-id-type="pmid">24341317</pub-id></mixed-citation></ref><ref id="B139"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woods</surname><given-names>A. T.</given-names></name><name><surname>Velasco</surname><given-names>C.</given-names></name><name><surname>Levitan</surname><given-names>C. A.</given-names></name><name><surname>Wan</surname><given-names>X.</given-names></name><name><surname>Spence</surname><given-names>C.</given-names></name></person-group> (<year>2015</year>). <article-title>Conducting perception research over the internet: a tutorial review.</article-title>
<source><italic>PeerJ</italic></source>
<volume>3</volume>:<issue>e1058</issue>. <pub-id pub-id-type="doi">10.7717/peerj.1058</pub-id>
<?supplied-pmid 26244107?><pub-id pub-id-type="pmid">26244107</pub-id></mixed-citation></ref><ref id="B140"><mixed-citation publication-type="journal"><collab>World Medical Association</collab> (<year>2013</year>). <article-title>World Medical Association declaration of Helsinki: ethical principles for medical research involving human subjects.</article-title>
<source><italic>J. Am. Med. Assoc.</italic></source>
<volume>310</volume>
<fpage>2191</fpage>&#x02013;<lpage>2194</lpage>. <pub-id pub-id-type="doi">10.1001/jama.2013.281053</pub-id>
<?supplied-pmid 24141714?><pub-id pub-id-type="pmid">24141714</pub-id></mixed-citation></ref><ref id="B141"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>C. P.</given-names></name><name><surname>Samaras</surname><given-names>D.</given-names></name><name><surname>Zelinsky</surname><given-names>G. J.</given-names></name></person-group> (<year>2014</year>). <article-title>Modeling visual clutter perception using proto-object segmentation.</article-title>
<source><italic>J. Vis.</italic></source>
<volume>14</volume>
<fpage>1</fpage>&#x02013;<lpage>16</lpage>. <pub-id pub-id-type="doi">10.1167/14.7.4</pub-id>
<?supplied-pmid 24904121?><pub-id pub-id-type="pmid">24904121</pub-id></mixed-citation></ref><ref id="B142"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zampollo</surname><given-names>F.</given-names></name><name><surname>Wansink</surname><given-names>B.</given-names></name><name><surname>Kniffin</surname><given-names>K. M.</given-names></name><name><surname>Shimizu</surname><given-names>M.</given-names></name><name><surname>Omori</surname><given-names>A.</given-names></name></person-group> (<year>2012</year>). <article-title>Looks good enough to eat: how food plating preferences differ across cultures and continents.</article-title>
<source><italic>Cross Cult. Res.</italic></source>
<volume>46</volume>
<fpage>31</fpage>&#x02013;<lpage>49</lpage>. <pub-id pub-id-type="doi">10.1177/1069397111418428</pub-id></mixed-citation></ref><ref id="B143"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zandstra</surname><given-names>E. H.</given-names></name><name><surname>El-Deredy</surname><given-names>W.</given-names></name></person-group> (<year>2011</year>). <article-title>Effects of energy conditioning on food preferences and choice.</article-title>
<source><italic>Appetite</italic></source>
<volume>57</volume>
<fpage>45</fpage>&#x02013;<lpage>49</lpage>. <pub-id pub-id-type="doi">10.1016/j.appet.2011.03.007</pub-id>
<?supplied-pmid 21440593?><pub-id pub-id-type="pmid">21440593</pub-id></mixed-citation></ref><ref id="B144"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zellner</surname><given-names>D. A.</given-names></name><name><surname>Lankford</surname><given-names>M.</given-names></name><name><surname>Ambrose</surname><given-names>L.</given-names></name><name><surname>Locher</surname><given-names>P.</given-names></name></person-group> (<year>2010</year>). <article-title>Art on the plate: effect of balance and color on attractiveness of, willingness to try and liking for food.</article-title>
<source><italic>Food Qual. Prefer.</italic></source>
<volume>21</volume>
<fpage>575</fpage>&#x02013;<lpage>578</lpage>. <pub-id pub-id-type="doi">10.1016/j.foodqual.2010.02.007</pub-id></mixed-citation></ref><ref id="B145"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zellner</surname><given-names>D. A.</given-names></name><name><surname>Siemers</surname><given-names>E.</given-names></name><name><surname>Teran</surname><given-names>V.</given-names></name><name><surname>Conroy</surname><given-names>R.</given-names></name><name><surname>Lankford</surname><given-names>M.</given-names></name><name><surname>Agrafiotis</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2011</year>). <article-title>Neatness counts. How plating affects liking for the taste of food.</article-title>
<source><italic>Appetite</italic></source>
<volume>57</volume>
<fpage>642</fpage>&#x02013;<lpage>648</lpage>. <pub-id pub-id-type="doi">10.1016/j.appet.2011.08.004</pub-id>
<?supplied-pmid 21855585?><pub-id pub-id-type="pmid">21855585</pub-id></mixed-citation></ref></ref-list></back></article>