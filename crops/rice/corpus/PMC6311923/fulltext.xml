<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Genomics</journal-id><journal-id journal-id-type="iso-abbrev">BMC Genomics</journal-id><journal-title-group><journal-title>BMC Genomics</journal-title></journal-title-group><issn pub-type="epub">1471-2164</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6311923</article-id><article-id pub-id-type="pmid">30598079</article-id><article-id pub-id-type="publisher-id">5283</article-id><article-id pub-id-type="doi">10.1186/s12864-018-5283-8</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group></article-categories><title-group><article-title>Deep learning for DNase I hypersensitive sites identification</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Lyu</surname><given-names>Chuqiao</given-names></name><address><email>2220170862@bit.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Lei</given-names></name><address><email>2220170862@bit.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Zhang</surname><given-names>Juhua</given-names></name><address><email>jhzhang@bit.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 8841 6246</institution-id><institution-id institution-id-type="GRID">grid.43555.32</institution-id><institution>School of Life Science, Beijing Institute of Technology, </institution></institution-wrap>South Zhongguancun Street, Beijing, 100081 China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 8841 6246</institution-id><institution-id institution-id-type="GRID">grid.43555.32</institution-id><institution>Key Laboratory of Convergence Medical Engineering System and Healthcare Technology the Ministry of Industry and Information Technology, Beijing Institute of Technology, </institution></institution-wrap>Beijing, China </aff></contrib-group><pub-date pub-type="epub"><day>31</day><month>12</month><year>2018</year></pub-date><pub-date pub-type="pmc-release"><day>31</day><month>12</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>19</volume><issue>Suppl 10</issue><issue-sponsor>Publication of this supplement has not been supported by sponsorship. Information about the source of funding for publication charges can be found in the individual articles. The articles have undergone the journal's standard peer review process for supplements. YZ was not involved in the review of their own authored paper. The Supplement Editors declare that they have no other competing interests.</issue-sponsor><elocation-id>905</elocation-id><permissions><copyright-statement>&#x000a9; The Author(s) 2018</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p></license></permissions><abstract id="Abs1"><sec><title>Background</title><p>The DNase I hypersensitive sites (DHSs) are associated with the cis-regulatory DNA elements. An efficient method of identifying DHSs can enhance the understanding on the accessibility of chromatin. Despite a multitude of resources available on line including experimental datasets and computational tools, the complex language of DHSs remains incompletely understood.</p></sec><sec><title>Methods</title><p>Here, we address this challenge using an approach based on a state-of-the-art machine learning method. We present a novel convolutional neural network (CNN) which combined Inception like networks with a gating mechanism for the response of multiple patterns and longterm association in DNA sequences to predict multi-scale DHSs in Arabidopsis, rice and Homo sapiens.</p></sec><sec><title>Results</title><p>Our method obtains 0.961 area under curve (AUC) on Arabidopsis, 0.969 AUC on rice and 0.918 AUC on Homo sapiens.</p></sec><sec><title>Conclusions</title><p>Our method provides an efficient and accurate way to identify multi-scale DHSs sequences by deep learning.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>DNase I hypersensitive sites</kwd><kwd>Deep learning</kwd><kwd>Convolutional neural network</kwd></kwd-group><conference xlink:href="http://datamining-web.it.uts.edu.au/giw2018/"><conf-name>29th&#x000a0;International Conference on Genome Informatics</conf-name><conf-loc>Yunnan, China</conf-loc><conf-date>3-5 December 2018</conf-date></conference><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2018</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background</title><p>Thirty years ago, it was confirmed that DNA bound by proteins was not degenerated by the DNase I [<xref ref-type="bibr" rid="CR1">1</xref>]. The early study [<xref ref-type="bibr" rid="CR2">2</xref>] also showed that there are many highly sensitive nucleotide fragments on the chromosome to DNase I digestion, and they have a high influence on the transcription of the gene. Nucleotide regions that are extremely sensitive to the DNase I are referred to as DNase I hypersensitive sites (DHSs). Some research attempts that DHSs can be precisely coupled with the cis-regulatory elements, including enhancers, promoters, silencers, and locus control regions [<xref ref-type="bibr" rid="CR3">3</xref>]. Some other research [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>] have that many DHSs appear around the highly expressed genes, and few DHSs appear near the low-expressed genes.</p><p>Benefit from the improvement of high-throughput sequencing technologies, some new techniques have been applicated to detect DHSs, such as ChIP-seq [<xref ref-type="bibr" rid="CR6">6</xref>] and DNase-seq [<xref ref-type="bibr" rid="CR7">7</xref>]. Scientists have detected the DHSs from the human genome and stored them in the public dataset [<xref ref-type="bibr" rid="CR8">8</xref>]. At the same time, in the field of plant genomes, a large number of DHSs has been detected in plants and be established in a website to visualize these data [<xref ref-type="bibr" rid="CR9">9</xref>]. There also have a single cell DNase I sequencing (scDNase-seq) [<xref ref-type="bibr" rid="CR10">10</xref>] method that can identify genome-wide DHSs in a single cell type or less than 1000 cell types. These estimable experimental methods collected many valuable data. It contributes important suggestions for studying the activity of the DNase I, the accessibility of chromatin and gene expression. However, the experimental methods are not only expensive but also takes a lot of time and effort to achieve a complete sequencing, which hinders the progress of subsequent experiments. Covering more and more experimental data, it is still meaningful to design a clever, fast and efficient calculation method to recognize DHSs.</p><p>A reasonable dataset of DHSs established and published in 2005, which included 280 DHSs and 737 non-DHSs from erythroid cells [<xref ref-type="bibr" rid="CR11">11</xref>]. In the next decade, some researchers applied this data to create many useful algorithms for recognize DHSs based on DNA sequences. Support vector machine (SVM) was used to extract dinucleotide features in the sequence [<xref ref-type="bibr" rid="CR11">11</xref>]. The iDHS-EL [<xref ref-type="bibr" rid="CR12">12</xref>] use three random forests(RF) to extract different nucleotide sequence features to recognize DHSs. However, due to the imbalance of positive and negative samples, conventional algorithms always get a high false positive rate and not accurate enough for applications. So both gkm-SVM [<xref ref-type="bibr" rid="CR13">13</xref>] and BIRD [<xref ref-type="bibr" rid="CR14">14</xref>] use the human genome data to calculate the DNase I hypersensity with regression methods, which have been proved in practice. However, the process of manual feature extraction and design is relatively complex, which requires a lot of patience, and is not conducive to the generalization of the model. As everyone knows, DHSs are both tissue-specific and cell-specific. It was reported that 34% of human DHSs were specifically appear in one cell line, 66% were appear in both cell lines, and only 0.09<italic>%</italic> can be detected in all cell lines, analyzing high throughput sequencing results of 125 human cell lines [<xref ref-type="bibr" rid="CR8">8</xref>]. The proportion of DHSs in human exons is only 1/2 of rice [<xref ref-type="bibr" rid="CR15">15</xref>]. However, both of their proportion of DHSs in the intergenic region is coincident [<xref ref-type="bibr" rid="CR16">16</xref>]. In other words, the activity of DHSs is closely related to the epigenetic factors. In Arabidopsis thaliana, once a hypermethylated DNA fragment loses its methylation, the sensitivity of DNase I will be greatly increased [<xref ref-type="bibr" rid="CR17">17</xref>]. Histone modifications also affect chromatin sensitivity of DNase I in varying degrees [<xref ref-type="bibr" rid="CR18">18</xref>&#x02013;<xref ref-type="bibr" rid="CR21">21</xref>]. The previous calculation methods have obtained less than ideal results, due to these fundamental reasons, and it could be almost impossible that the accuracy rate of recognition of DHSs were further improved with previous algorithms simply based on DNA sequences in a single cell type.</p><p>In order to avoid the limitations of the artificial feature, we try to use the deep learning algorithm [<xref ref-type="bibr" rid="CR22">22</xref>] to actualize the classification of DHSs and turn in the direction of DHSs combined in a large number of cell types. The deep learning algorithm has unique advantages in feature extraction, even if it can explore some features that cannot be visualized by the original data. For example, in the field of natural language processing (NLP), recurrent neural network (RNN) [<xref ref-type="bibr" rid="CR23">23</xref>] can mine contextual information [<xref ref-type="bibr" rid="CR24">24</xref>] from a text, understand the emotions it expresses, and even answer the questions [<xref ref-type="bibr" rid="CR25">25</xref>]. In the field of image recognition, convolutional neural network (CNN) [<xref ref-type="bibr" rid="CR26">26</xref>] can understand the pixel value from the local to the whole image and accomplish detection and segmentation [<xref ref-type="bibr" rid="CR27">27</xref>] of the target. It is very different from other statistical analysis methods. In deep learning models, the network structure is established to complete the understanding of the original data layer by layer, and both of feature extraction and classification in models are completed automatically.</p><p>But deep learning also has its weaknesses. Firstly, the structure of the model is a black box which cannot be described. Secondly, a large number of labeled data supplies are required during the training process of a supervised model. But considering the unique expressiveness of deep learning algorithms, they are still excellent choices in all existing calculation methods. In recent years, the deep methods has granted the computational power to resolve genomics research questions. Some researchers [<xref ref-type="bibr" rid="CR28">28</xref>] have proved the validity of CNN, RNN and their mixture models in gene sequence classification. DeepBind [<xref ref-type="bibr" rid="CR29">29</xref>], DeepSEA [<xref ref-type="bibr" rid="CR30">30</xref>] and Basset [<xref ref-type="bibr" rid="CR31">31</xref>] used CNN to predict protein binding sites, non-coding regions and the functional activity of DNA sequences, respectively. ProLanGO [<xref ref-type="bibr" rid="CR32">32</xref>], DeepNano [<xref ref-type="bibr" rid="CR33">33</xref>] and DanQ [<xref ref-type="bibr" rid="CR34">34</xref>] used RNN to predict protein expression, base recognition and non coding DNA, respectively. Deep GDashboard [<xref ref-type="bibr" rid="CR28">28</xref>] and BiRen [<xref ref-type="bibr" rid="CR35">35</xref>] used the CNN-RNN hybrid framework to predict the locations and enhancers of transcription factor binding, respectively. All of these methods have achieved good results. RNN can recognize different length sequences according to its loop structure and understand the characteristics of long-term association. However, it can not carry out parallel computing, which needs a lot of time for training. CNN can only handle the sequence of fixed length and broken segments, but it runs fast. Most of the hybrid architectures only stacks the CNN and the RNN, without considering combining the advantages of them. So here we make a new model that combines the speed advantages of CNN and effectively understands the long-range association of sequences, to support training of indefinite long sequence, and we established Arabidopsis, rice and Homo sapiens datasets to verify our model. Finally our model achieved state-of-the-art results on the datasets of Arabidopsis and rice, also achieved ambitious results on Homo sapiens.</p></sec><sec id="Sec2"><title>Methods</title><sec id="Sec3"><title>Model establishing</title><p>Because of the novel gate layer of LeNup [<xref ref-type="bibr" rid="CR36">36</xref>], it is straightforward to learn the association of the long ranges of nucleotide fragments. We contemplated that all the active DNA fragments have three-dimensional structures. With the predominant feature extraction capability of CNN network, the entry gate control can make the organization of DNA in the three-dimensional structure to a feature. So LeNup has a good performance in nucleosome positioning. We also made experiments shown in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>, that the gate layer structure is still valid in DHSs. It indicates that the design of DNA recognition by gated layers is effective. So, on the basis of the DHSs classification model, we fine-tuning the first five convolution layers of LeNup and then changing the last pooling layer to special pyramid pooling (SPP) layer. Finally, we uses the LeakyReLU function to activate the entire network. The most important of these adjustments is the SPP layer, which enables the model to support the variable-length nucleotide segments as input in a reasonable range, while the other adjustments are designed to prevent the gradient disappearance and improve the speed of training.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Comparison of DHSs predictions in different species</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Species</th><th align="left">Interval(bp)</th><th align="left"><italic>S</italic><sub><italic>n</italic></sub>(<italic>%</italic>)</th><th align="left"><italic>S</italic><sub><italic>p</italic></sub>(<italic>%</italic>)</th><th align="left"><italic>A</italic><italic>C</italic><italic>C</italic>(<italic>%</italic>)</th><th align="left">
<italic>MCC</italic>
</th><th align="left">
<italic>AUC</italic>
</th></tr></thead><tbody><tr><td align="left">Arabidopsis</td><td align="left">50</td><td align="left">90.64</td><td align="left">90.78</td><td align="left">90.70</td><td align="left">0.813</td><td align="left">0.961</td></tr><tr><td align="left"/><td align="left">100</td><td align="left">88.27</td><td align="left">92.94</td><td align="left">90.46</td><td align="left">0.810</td><td align="left">0.956</td></tr><tr><td align="left"/><td align="left">200</td><td align="left">86.23</td><td align="left">93.54</td><td align="left">89.66</td><td align="left">0.796</td><td align="left">0.953</td></tr><tr><td align="left">Rice</td><td align="left">50</td><td align="left">89.30</td><td align="left">94.44</td><td align="left">91.87</td><td align="left">0.838</td><td align="left">0.969</td></tr><tr><td align="left"/><td align="left">100</td><td align="left">89.88</td><td align="left">93.68</td><td align="left">91.78</td><td align="left">0.836</td><td align="left">0.962</td></tr><tr><td align="left"/><td align="left">200</td><td align="left">82.73</td><td align="left">95.66</td><td align="left">89.19</td><td align="left">0.790</td><td align="left">0.959</td></tr><tr><td align="left">Human</td><td align="left">50</td><td align="left">86.99</td><td align="left">85.31</td><td align="left">86.15</td><td align="left">0.723</td><td align="left">0.918</td></tr><tr><td align="left"/><td align="left">100</td><td align="left">82.26</td><td align="left">88.73</td><td align="left">85.51</td><td align="left">0.711</td><td align="left">0.911</td></tr><tr><td align="left"/><td align="left">200</td><td align="left">77.31</td><td align="left">89.96</td><td align="left">83.65</td><td align="left">0.678</td><td align="left">0.849</td></tr></tbody></table></table-wrap></p><p><bold>The special pyramid pooling layer:</bold> The DHSs are variable-large-length nucleotide segments (from tens bp to thousands of bp). Deep learning models normally support a fixed-length input (LeNup network only supports a 147bp length of nucleotide segments). Because, first of all, the convolution layer is insensitive to the scale of input as long as the scale does not exceed the computation range. But the output of convolution layer needs to pass through the full connection layer, where the connection parameters are fixed. In the field of image recognition, the usual way to solve this problem of multi-scale input is to normalize the pictures to the combined dimension by scaling and clipping. However, the nucleotide sequences are different with the images. Because the length of DHSs is longer than the wide of an image, and the initial information will miss if we cut the nucleotide sequence. Therefore, we added the SPP layer between the convolution layer and the full connection layer of LeNup in order to allow the models to operate the DHSs sequence information. This method (first proposed in 2015 [<xref ref-type="bibr" rid="CR37">37</xref>]) was used to solve the problem of multi-size of images in CNN. SPP layer applies several multi-size pooling layers to replace one pooling layer between the last convolution layer and the first full connection layer. Firstly, in this paper, we encoded the DNA sequences to the one-hot numbers, whose fragment was converted into a two-dimensional matrix of <italic>n</italic>&#x000d7;4. The DNA sequence is similar to the multi-scale image, but the DNA encoding only changes one dimension (length), while the image is changed in two dimensions (wide and height). Secondly, we used the SPP layer separates the output which from the last convolution layer into 1, 2, and 4 parts, and recorded the average value of every part. Finally, we stacked all values as the output of the SPP layer. Through the SPP layer, the dimension of uncertain <italic>n</italic>&#x000d7;4 (<italic>n</italic>&#x0003e;4, in this article) of output can be modified into 7&#x000d7;4 (1+2+4=7), and then the fixed size output can enter the full connection layer to do classification prediction. The structure of the SPP layer is shown as illustrated (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>).
<fig id="Fig1"><label>Fig. 1</label><caption><p>The internal structure of the SPP layer: Number of 384 describes the number of the fifth convolution layer. The fifth layer convolution is the last layer. The output obtained from the fifth layer convolution layer are pooled through the pooling layer of <italic>n</italic>/4, <italic>n</italic>/2, and <italic>n</italic>/1 dimension respectively, then the 4, 2, and 1 features are obtained. Finally, the 4+2+1=7 features are gathered into the full connection layer</p></caption><graphic xlink:href="12864_2018_5283_Fig1_HTML" id="MO1"/></fig></p><p><bold>The leaky rectified linear unit</bold>[<xref ref-type="bibr" rid="CR38">38</xref>]: It shows that the number of convolution layers of CNN model is positively related to the performance. If the model is deeper, the problem of gradient disappearance is more serious. Considering the nucleotides have less information than words or pixels (only four nucleotides of A, C, G, and T will be used), the problem of gradient disappearance is particularly visible when the DHSs classification model is trained. Here we desire that the gradient of the model does not disappear when the model with five convolution layers is being trained. So we used the leaky rectified linear unit (LeakyReLU) [<xref ref-type="bibr" rid="CR39">39</xref>] to fix this problem. The LeakyReLU function is mathematically given by: 
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ y_{i} = \left\{ \begin{array}{ccc} x_{i} &#x00026; if &#x00026; x_{i} \ge 0,\\ \frac{x_{i}}{a_{i}} &#x00026; if &#x00026; x_{i} &#x0003c; 0. \end{array} \right.  $$ \end{document}</tex-math><mml:math id="M2"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="" open="{" separators=""><mml:mrow><mml:mtable class="array" columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mtext mathvariant="italic">if</mml:mtext></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mtext mathvariant="italic">if</mml:mtext></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:mn>0</mml:mn><mml:mi>.</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12864_2018_5283_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p><p><italic>a</italic><sub><italic>i</italic></sub> indicates the range correction parameter (this model takes 100.0). <italic>x</italic><sub><italic>i</italic></sub> indicates the input of LeakyReLU layer, and <italic>y</italic><sub><italic>i</italic></sub> indicates the output after activation. To a certain extent the LeakyReLU layer can effectively prevent the gradient from vanishing. Although the LeakyReLU layer cannot provid a striking increase in accuracy of prediction, but the problem of gradient vanishing was effectively prevented during the training process, and the robustness of the model was increased.</p><p><bold>Other layers:</bold> In addition to modifying the pooling layer and the activation function, we also fine-tuning the convolution layers of LeNup. The structural parameters of each layer are depicted in Figs.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>, <xref rid="Fig3" ref-type="fig">3</xref>, <xref rid="Fig4" ref-type="fig">4</xref>, we add <xref rid="Fig1" ref-type="fig">1</xref>, <xref rid="Fig3" ref-type="fig">3</xref>, <xref rid="Fig5" ref-type="fig">5</xref>, and <xref rid="Fig7" ref-type="fig">7</xref> filters in the Gate-inception-A. The Gate-inception-B and the Gate-inception-C module are consistent with LeNup. Every convolution kernel of the model gets a gated convolution operation. The model is finally depicted in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Gated Inception-A block</p></caption><graphic xlink:href="12864_2018_5283_Fig2_HTML" id="MO2"/></fig>
<fig id="Fig3"><label>Fig. 3</label><caption><p>Gated Inception-B block</p></caption><graphic xlink:href="12864_2018_5283_Fig3_HTML" id="MO3"/></fig>
<fig id="Fig4"><label>Fig. 4</label><caption><p>Gated Inception-C block</p></caption><graphic xlink:href="12864_2018_5283_Fig4_HTML" id="MO4"/></fig>
<fig id="Fig5"><label>Fig. 5</label><caption><p>The figure shows that the overall construction of the model, which including 5 convolution layers, 4 maximum pooling layers, 1 SPP layer, and 3 fully connected layers. In addition to these visible structures, every layer is activated by LeakyReLU and followed by the dropout layer (the parameter is 0.3), and each fully connected layer is normalized by the batch normalization (BN, whose parameter is 0.5), which can speed up the convergence of the network. Dropout layers and BN layers are not depicted in the diagram</p></caption><graphic xlink:href="12864_2018_5283_Fig5_HTML" id="MO5"/></fig></p><p><bold>The establishment of data sets:</bold> After setting up the training model, we need the appropriate datasets to test its performance. Firstly, we downloaded the DHSs data of aiabidopsis and rice from the website (<ext-link ext-link-type="uri" xlink:href="http://www.plantdhs.org/Download">http://www.plantdhs.org/Download</ext-link>). The DHSs data of human was also obtained from ENCODE. The different species have different length distribution of DNA fragments, and the expression level of the DHSs in the different cell lines of one specie is different. So we used the range of DHSs derived from all the DHSs known in the whole genome of the species. Secondly, in order to ensure the stability of the model, we used the length from 200 to 800bp for each DNA fragments. At the same time, we selected an equal length DNA fragment for each DHSs in the non-DHSs region of the same chromosome as a negative sample. The length distribution of the non-DHSs in the obtained data is exactly same as that of the DHSs. Finally, we used cd-hit [<xref ref-type="bibr" rid="CR40">40</xref>] software to remove the higher identity sequence in both positive and negative samples. Through these methods, we setted up three datasets of Arabidopsis, rice and Homo sapiens respectively (Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>). All of the data are only selected from euchromosomes. At the same time, in order to test the reliability of datasets, we accompanied these three datasets with benchmark datasets [<xref ref-type="bibr" rid="CR11">11</xref>]. The results are shown in Figs.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> and <xref rid="Fig7" ref-type="fig">7</xref>.
<fig id="Fig6"><label>Fig. 6</label><caption><p>Analysis of Arabidopsis, rice, and human data shows that the number of DHSs decreases with increasing length. However, in the benchmark dataset, because of the tiny size, it cannot display the trend. It is embarrassing to enhance the prediction capacity of the model, so it is easy to fall into overfitting</p></caption><graphic xlink:href="12864_2018_5283_Fig6_HTML" id="MO6"/></fig>
<fig id="Fig7"><label>Fig. 7</label><caption><p>It is obvious from the graph that the difference of nucleotide ratios between DHSs and non-DHSs will decreases with reducing of the complexity of species. Benchmark dataset quiet has a weak coverage of sample space, because of the small amount of data. At the same time, a larger number of non-DHSs (benchmark dataset has 280 DHSs and 737 non-DHSs) were more likely to lead to overfitting of the model on the non-DHSs. It can also be seen that there is little distinction the proportion of MNC and DNC in arabidopsis&#x02019;s DHSs and non-DHSs, which indicates that it is challenging to use the feature of nucleotide site training model on the benchmark dataset</p></caption><graphic xlink:href="12864_2018_5283_Fig7_HTML" id="MO7"/></fig>
<table-wrap id="Tab2"><label>Table 2</label><caption><p>The statistical results of three datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Species</th><th align="left">Positive(P)</th><th align="left">Negative(N)</th><th align="left">Avg(bp)</th><th align="left">Ratio(P:N)</th></tr></thead><tbody><tr><td align="left">Arabidopsis</td><td align="left">26399</td><td align="left">23112</td><td align="left">403</td><td align="left">1.14</td></tr><tr><td align="left">Rice</td><td align="left">56033</td><td align="left">46201</td><td align="left">376</td><td align="left">1.21</td></tr><tr><td align="left">Human</td><td align="left">943681</td><td align="left">742476</td><td align="left">326</td><td align="left">1.27</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec4"><title>Multi-scale training strategy</title><p><bold>The distribution of datasets:</bold> We used random extraction and cross validation method to partition training datasets and testing datasets. 5-fold cross validation was used in both Arabidopsis and rice, and 10-fold in Homo sapiens. Because the number of Homo sapiens DHSs was too large, so the ratio of training data and testing data was bound to 1:9. Our original purpose was that the model can receive multi-scale input during the training process. Theoretically, the above model can accept input from arbitrary dimensions. However, the graphic processing unit (GPU) in computers can only receive fixed-length inputs when parallel computing. Here we hope to give full play to the advantages of GPU. So during the training process, we divided the training data into multiple parts of the training data in accordance the range of their length (for example, the length of datasets will be divided into 200bp-400bp, 400bp-600bp, and 600bp-800bp of the length according to the interval of 200bp). Since each data was selected from the true existing chromosomes (whether it is a positive or negative sample), we extended it into the longest BP from both ends (for example, we extend all the length of the DHSs in the 200bp-400bp part to 400bp length). From this way, we got many new DHSs datasets, but the length of nucleotide sequences in each dataset was same, and the ratio of positive and negative samples was approximately 1:1. It was also possible to ensure that there was a complete DHS in every positive sample after extension. The advantage of this method is that the length of the sequence extended to two ends can be controlled by ourself. And the training datasets will be amplified if we change the length of the left or right extensed fragments. (It depending on the number of samples, the number of Arabidopsis training data was amplified by 3 times, and the number of rice and the number of Homo sapiens are sufficient, so they didn&#x02019;t be expanded). Training with the amplified data can increase the accuracy on the testing data (give an average increase of about 1%). But it also introduced some noise in the positive sample. Therefore, in order to get faster convergence speed, it is unavoidable to sacrifice the accuracy of some models. However, it should be noted that the fragment length of the testing data was still arbitrary, and the quality of the model was only determined by the performance on the testing. The training datasets selection method and the generated noise are shown in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref> (we only select the human DHSs at intervals of 100bp as example).
<fig id="Fig8"><label>Fig. 8</label><caption><p>The figure above shows the ratio of the training data to the non-extended data. Firstly, the DHSs is sorted according to the length of the fragments. The vertical axis indicates the length of each fragment. The horizontal axis indicates the number of the fragment in the entire dataset. The blue line indicates the non-extended data, and the area underneath it indicates the number of bases in the whole non-extended data. The red dotted line indicates the training data, whose bottom area indicates the base number of the whole training data. The gray area indicates the extra fragments, which is also used to speed up training and extended by us</p></caption><graphic xlink:href="12864_2018_5283_Fig8_HTML" id="MO8"/></fig></p><p>After filtering, We got several datasets of various scales. In training phase, these training sets share the same parameters of one model. Because of the presence of SPP layer, the dimensions of parameters in the model will not change. Here referring to the multi-scale training method [<xref ref-type="bibr" rid="CR37">37</xref>]. For example we took the interval as 200bp to split the training datasets. So we obtained three training datasets which had the lenght of 400bp, 600bp and 800bp respectively. Firstly, we initialized the network parameters, and the 400bp&#x02019;s set was used to train a complete epoch. Secondly, we retained the parameters of the model, and trained a complete epoch with 600bp&#x02019;s set. and then 800bp&#x02019;s. When all data was trained once, it would be recorded as a complete iteration. This training method allows the model to learn input information from different dimensions, and retain the advantages of GPU. During the training process, we found the convergence rate of the segmented training loss was similar from the single length training, and only cost a slight time in the process of converting input length.</p></sec><sec id="Sec5"><title>Training parameters</title><p>We have trained our models running on a single NVidia Quadro P6000 with stochastic gradient descent with momentum in pytorch. PyTorch is a handy deep learning library that extends Python. The training step used momentum with a decay of 0.98, a learning rate of 0.002, and decayed every epoch using an exponential rate of 0.97. We also used a mini-batch size of 128 samples and trained the model for 100 iterations. Each iteration taked about one minute. The well trained model size was about 12.5 megabyte, and the number of parameters was 3,077,382.</p></sec></sec><sec id="Sec6" sec-type="results"><title>Results</title><p>We selected the sensitivity (Sn), the specificity (Sp), the accuracy (ACC) and the Matthew&#x02019;s correlation coefficient (MCC) to evaluate the analogous method. These were generally used in identifying the consequences of models. They are defined as follows: 
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} S_{n}&#x00026;=\frac{T_{p}}{T_{p}+F_{n}} \\ S_{p}&#x00026;=\frac{T_{n}}{T_{n}+F_{p}} \\ ACC&#x00026;=\frac{T_{p}+T_{n}}{T_{p}+F_{n}+T_{n}+F_{p}} \\ MCC&#x00026;=\frac{T_{p}\times T_{n}-F_{p}\times F_{n}}{\sqrt{(T_{n}\,+\,F_{n})\times\!(T_{n}+F_{p})\times\!(T_{p}+F_{n})\times\!(T_{p}+F_{p})}} \end{aligned}  $$ \end{document}</tex-math><mml:math id="M4"><mml:mspace width="-16.0pt"/><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext mathvariant="italic">ACC</mml:mtext></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext mathvariant="italic">MCC</mml:mtext></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.3em"/><mml:mo>+</mml:mo><mml:mspace width="0.3em"/><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>&#x000d7;</mml:mo><mml:mspace width="0.3em"/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>&#x000d7;</mml:mo><mml:mspace width="0.3em"/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>&#x000d7;</mml:mo><mml:mspace width="0.3em"/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12864_2018_5283_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p><sec id="Sec7"><title>Multi-scale training in different species</title><p>In order to verify the best segmental training effect, we tested the interval of 50bp, 100bp, and 200bp to divide the training data, and evaluated the accuracy on the testing data of Arabidopsis, rice and human respectively. The ROC curves are shown in Fig.&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref>. After testing, it is easy to find that reducing the interval in three species sets can improve the accuracy of the model. With the increase of intervals, the proportion of noise also increase, which lead to the bias of the model lean to negative samples. It is shown in the Arabidopsis dataset that <italic>S</italic><sub><italic>n</italic></sub> decreases and <italic>S</italic><sub><italic>p</italic></sub> increases with increase of interval. This indicates that the noise can lead to over fitting of models on negative samples. But the lower interval can not get better results on all evaluation indicators. There is not a large difference between the 50bp interval and the 100bp interval training method on rice. For Homo sapiens, the 50bp interval brought a very low AUC. This indicates that the model may incline to positive samples. Therefore, the synthesis of the three datasets proves that there is a reasonable and no lose of model capability to divide datasets with 100bp interval. At the same time, in the process of cross validation, we got a very stable rate of accuracy on each testing data. Arabidopsis got 90&#x000b1;2<italic>%</italic>, rice got 91&#x000b1;2<italic>%</italic>, and Homo sapiens got 86&#x000b1;2<italic>%</italic>. The compared results are shown in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>.
<fig id="Fig9"><label>Fig. 9</label><caption><p>Two ROC curves obtained from 5-fold cross-validation tests using the genome dataset of Arabidopsis, Rice; One ROC curve obtained from 10-fold cross-validation tests using the genome dataset of Human</p></caption><graphic xlink:href="12864_2018_5283_Fig9_HTML" id="MO9"/></fig></p></sec><sec id="Sec8"><title>Performance comparison with single-scale training</title><p>In order to compare with other methods, we referred to other methods [<xref ref-type="bibr" rid="CR41">41</xref>]. The author had done a lot of work on the choice of non-DHSs, and they also had established a DHSs classification model based on their database called pDHS-ELM. We used their datasets which can be downloaded from the website (<ext-link ext-link-type="uri" xlink:href="https://github.com/wesd778/dhsNet/tree/master/raw">https://github.com/wesd778/dhsNet/tree/master/raw</ext-link>) to train our model. However, it is important to note that the author chooses the non-DHS locus randomly in 100bp-600bp, which makes the length distribution of negative samples inconsistent with positive samples. If we use the multi-scale training method, the proportion of negative samples will be very high (For example, in 500bp-600bp, the number of non-DHSs will ten times with DHSs). Therefore, in order to compared the reasonable experimental results, we gave up the multi-scale inputs and extended all the DNA sequences to 600bp. The training method also uses 5-fold cross validation, and without completely changing the structure and training parameters of the model. The final results are listed in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>. We also downloaded the single-scale (600pb) human dataset in the published research using Basset [<xref ref-type="bibr" rid="CR31">31</xref>]. Basset had three convolution layers and two fully connected layers, which was powerful in DHSs identification. We got the mean-auc value of 0.890 (0.780 for gkm-svm, 0.895 for basset) without completely changing the structure and training parameters of the model, which was slightly worse than that of multi-scale training (0.918, Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>). It also proves that the method of multi-scale training of DHSs is effective.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Our mothod performance measured by 5-fold cross validation</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Methods</th><th align="left"><italic>S</italic><sub><italic>n</italic></sub>(<italic>%</italic>)</th><th align="left"><italic>S</italic><sub><italic>p</italic></sub>(<italic>%</italic>)</th><th align="left"><italic>A</italic><italic>C</italic><italic>C</italic>(<italic>%</italic>)</th><th align="left">
<italic>MCC</italic>
</th></tr></thead><tbody><tr><td align="left">SVM-Revchmer [<xref ref-type="bibr" rid="CR42">42</xref>]</td><td align="left">82.54</td><td align="left">79.78</td><td align="left">81.66</td><td align="left">0.634</td></tr><tr><td align="left">PseDNC-SVM [<xref ref-type="bibr" rid="CR43">43</xref>]</td><td align="left">81.30</td><td align="left">78.91</td><td align="left">80.11</td><td align="left">0.602</td></tr><tr><td align="left">iDHS-EL [<xref ref-type="bibr" rid="CR44">44</xref>]</td><td align="left">81.24</td><td align="left">76.11</td><td align="left">78.61</td><td align="left">0.572</td></tr><tr><td align="left">Unb-PseTNC [<xref ref-type="bibr" rid="CR45">45</xref>]</td><td align="left">86.48</td><td align="left">83.74</td><td align="left">85.11</td><td align="left">0.702</td></tr><tr><td align="left">pDHS-ELM [<xref ref-type="bibr" rid="CR41">41</xref>]</td><td align="left">89.17</td><td align="left">87.78</td><td align="left">88.48</td><td align="left">0.717</td></tr><tr><td align="left">ours</td><td align="left">88.25</td><td align="left">96.49</td><td align="left">92.88</td><td align="left">0.856</td></tr></tbody></table><table-wrap-foot><p>Note: The datasets were downloaded from [<xref ref-type="bibr" rid="CR41">41</xref>]</p></table-wrap-foot></table-wrap></p></sec></sec><sec id="Sec9" sec-type="discussion"><title>Discussion</title><p>After comparison, we found that the new network structure shows a surprising result on a single-scale dataset. It also proved that given too much emphasis on the proportion of single nucleotides or polynucleotides in DNA fragments would make a large limitation on the results of model. By combining the gate layers and the inception layers in deep learning model, the features of the DHSs could be more accurately captured. In a sense, it was very similar to sentiment analysis in natural language processing (NLP).</p></sec><sec id="Sec10" sec-type="conclusion"><title>Conclusions</title><p>The experimental conclusions illustrate that CNN network can effectively extract features from nucleotide sequences and be used for genome-wide DHSs classification. We can not prove that the DHSs are completely related with DNA sequence, because they have specific expression in different cell lines. However, as a result, the new model can be used as a tool for detecting DHSs, only to give the sequencing data of the corresponding cell lines and the DHSs from it for training. After the model converges, the nucleotide fragments in the same cell line can be assessed in a very powerful accuracy rate. Moreover, based on this model, it produces a good solution for the problem of DNA segment classification with uncertain length. If there are provide adequate datasets, such as regulatory units, cancer genes, and so on, we believed that the sequence-based flexible classification model will be more widely used.</p></sec></body><back><glossary><title>Abbreviations</title><def-list><def-item><term>ACC</term><def><p>Accuracy</p></def></def-item><def-item><term>AUC</term><def><p>Area under curve</p></def></def-item><def-item><term>CNN</term><def><p>Convolutional neural network</p></def></def-item><def-item><term>DHSs</term><def><p>DNase I hypersensitive sites</p></def></def-item><def-item><term>GPU</term><def><p>Graphic processing unit</p></def></def-item><def-item><term>LeakyReLU</term><def><p>Leaky rectified linear unit</p></def></def-item><def-item><term>MCC</term><def><p>Matthew&#x02019;s correlation coefficient</p></def></def-item><def-item><term>NLP</term><def><p>Natural language processing</p></def></def-item><def-item><term>RNN</term><def><p>Recurrent neural network</p></def></def-item><def-item><term>Sn</term><def><p>Sensitivity</p></def></def-item><def-item><term>SPP</term><def><p>Special pyramid pooling</p></def></def-item><def-item><term>Sp</term><def><p>Specificity</p></def></def-item></def-list></glossary><ack><title>Acknowledgements</title><p>The authors would like to thank the anonymous reviews for their constructive comments.</p><sec id="d29e1660"><title>Funding</title><p>Publication costs were funded by the open project of Key Laboratory of Convergence Medical Engineering System and Healthcare Technology of the Ministry of Industry and Information Technology, Beijing institute of technology, China.</p></sec><sec id="d29e1665"><title>Availability of data and materials</title><p>Source code and experimental datasets are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/wesd778/dhsNet">https://github.com/wesd778/dhsNet</ext-link>.</p></sec><sec id="d29e1675"><title>About this supplement</title><p>This article has been published as part of <italic>BMC Genomics Volume 19 Supplement 10, 2018: Proceedings of the 29th International Conference on Genome Informatics (GIW 2018): genomics</italic>. The full contents of the supplement are available online at <ext-link ext-link-type="uri" xlink:href="https://bmcgenomics.biomedcentral.com/articles/supplements/volume-19-supplement-10">https://bmcgenomics.biomedcentral.com/articles/supplements/volume-19-supplement-10</ext-link>.</p></sec></ack><notes notes-type="author-contribution"><title>Authors&#x02019; contributions</title><p>The following authors have made substantial contributions to conception and design of the study (CL, JZ, LW), the acquisition of data (CL, LW), the implementation of code(CL), the analysis of result (CL, JZ), the drafting or critical revising the manuscript (CL, LW, JZ). All authors read and approved the final manuscript.</p></notes><notes notes-type="COI-statement"><sec><title>Ethics approval and consent to participate</title><p>Not applicable.</p></sec><sec><title>Consent for publication</title><p>Not applicable.</p></sec><sec><title>Competing interests</title><p>The authors have read and understood the BMC policy on declaration of interests and no relevant interests to declare.</p></sec><sec><title>Publisher&#x02019;s Note</title><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></sec></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maniatis</surname><given-names>T</given-names></name><name><surname>Ptashne</surname><given-names>M</given-names></name></person-group><article-title>Structure of the <italic>&#x003bb;</italic> operators</article-title><source>Nature</source><year>1973</year><volume>246</volume><issue>5429</issue><fpage>133</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1038/246133a0</pub-id><pub-id pub-id-type="pmid">4586104</pub-id></element-citation></ref><ref id="CR2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>C</given-names></name><name><surname>Bingham</surname><given-names>PM</given-names></name><name><surname>Livak</surname><given-names>KJ</given-names></name><name><surname>Holmgren</surname><given-names>R</given-names></name><name><surname>Elgin</surname><given-names>SC</given-names></name></person-group><article-title>The chromatin structure of specific genes: I. evidence for higher order domains of defined dna sequence</article-title><source>Cell</source><year>1979</year><volume>16</volume><issue>4</issue><fpage>797</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1016/0092-8674(79)90095-3</pub-id><pub-id pub-id-type="pmid">455449</pub-id></element-citation></ref><ref id="CR3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felsenfeld</surname><given-names>G</given-names></name></person-group><article-title>Chromatin as an essential part of the transcriptional mechanim</article-title><source>Nature</source><year>1992</year><volume>355</volume><issue>6357</issue><fpage>219</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1038/355219a0</pub-id><pub-id pub-id-type="pmid">1731219</pub-id></element-citation></ref><ref id="CR4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iida</surname><given-names>K</given-names></name><name><surname>Kawaguchi</surname><given-names>S</given-names></name><name><surname>Kobayashi</surname><given-names>N</given-names></name><name><surname>Yoshida</surname><given-names>Y</given-names></name><name><surname>Ishii</surname><given-names>M</given-names></name><name><surname>Harada</surname><given-names>E</given-names></name><name><surname>Hanada</surname><given-names>K</given-names></name><name><surname>Matsui</surname><given-names>A</given-names></name><name><surname>Okamoto</surname><given-names>M</given-names></name><name><surname>Ishida</surname><given-names>J</given-names></name></person-group><article-title>Artade2db: improved statistical inferences for arabidopsis gene functions and structure predictions by dynamic structure-based dynamic expression (dsde) analyses</article-title><source>Plant Cell Physiol</source><year>2011</year><volume>52</volume><issue>2</issue><fpage>254</fpage><pub-id pub-id-type="doi">10.1093/pcp/pcq202</pub-id><pub-id pub-id-type="pmid">21227933</pub-id></element-citation></ref><ref id="CR5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okamoto</surname><given-names>M</given-names></name><name><surname>Tatematsu</surname><given-names>K</given-names></name><name><surname>Matsui</surname><given-names>A</given-names></name><name><surname>Morosawa</surname><given-names>T</given-names></name><name><surname>Ishida</surname><given-names>J</given-names></name><name><surname>Tanaka</surname><given-names>M</given-names></name><name><surname>Endo</surname><given-names>TA</given-names></name><name><surname>Mochizuki</surname><given-names>Y</given-names></name><name><surname>Toyoda</surname><given-names>T</given-names></name><name><surname>Kamiya</surname><given-names>Y</given-names></name></person-group><article-title>Genome-wide analysis of endogenous abscisic acid-mediated transcription in dry and imbibed seeds of arabidopsis using tiling arrays</article-title><source>Plant J Cell Mol Biol</source><year>2010</year><volume>62</volume><issue>1</issue><fpage>39</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1111/j.1365-313X.2010.04135.x</pub-id></element-citation></ref><ref id="CR6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crawford</surname><given-names>GE</given-names></name><name><surname>Holt</surname><given-names>IE</given-names></name><name><surname>Whittle</surname><given-names>J</given-names></name><name><surname>Webb</surname><given-names>BD</given-names></name><name><surname>Tai</surname><given-names>D</given-names></name><name><surname>Davis</surname><given-names>S</given-names></name><name><surname>Margulies</surname><given-names>EH</given-names></name><name><surname>Chen</surname><given-names>YD</given-names></name><name><surname>Bernat</surname><given-names>JA</given-names></name><name><surname>Ginsburg</surname><given-names>D</given-names></name></person-group><article-title>Genome-wide mapping of dnase hypersensitive sites using massively parallel signature sequencing (mpss)</article-title><source>Genome Res</source><year>2006</year><volume>16</volume><issue>1</issue><fpage>123</fpage><pub-id pub-id-type="doi">10.1101/gr.4074106</pub-id><pub-id pub-id-type="pmid">16344561</pub-id></element-citation></ref><ref id="CR7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>DS</given-names></name><name><surname>Mortazavi</surname><given-names>A</given-names></name><name><surname>Myers</surname><given-names>RM</given-names></name><name><surname>Wold</surname><given-names>B</given-names></name></person-group><article-title>Genome-wide mapping of in vivo protein-dna interactions</article-title><source>Science</source><year>2007</year><volume>316</volume><issue>5830</issue><fpage>1497</fpage><pub-id pub-id-type="doi">10.1126/science.1141319</pub-id><pub-id pub-id-type="pmid">17540862</pub-id></element-citation></ref><ref id="CR8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thurman</surname><given-names>RE</given-names></name><name><surname>Rynes</surname><given-names>E</given-names></name><name><surname>Humbert</surname><given-names>R</given-names></name><name><surname>Vierstra</surname><given-names>J</given-names></name><name><surname>Maurano</surname><given-names>MT</given-names></name><name><surname>Haugen</surname><given-names>E</given-names></name><name><surname>Sheffield</surname><given-names>NC</given-names></name><name><surname>Stergachis</surname><given-names>AB</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Vernot</surname><given-names>B</given-names></name></person-group><article-title>The accessible chromatin landscape of the human genome</article-title><source>Nature</source><year>2011</year><volume>489</volume><issue>7414</issue><fpage>75</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1038/nature11232</pub-id></element-citation></ref><ref id="CR9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>T</given-names></name><name><surname>Marand</surname><given-names>AP</given-names></name><name><surname>Jiang</surname><given-names>J</given-names></name></person-group><article-title>Plantdhs: a database for dnase i hypersensitive sites in plants</article-title><source>Nucleic Acids Res</source><year>2016</year><volume>44</volume><issue>Database issue</issue><fpage>1148</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1093/nar/gkv962</pub-id></element-citation></ref><ref id="CR10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cooper</surname><given-names>J</given-names></name><name><surname>Ding</surname><given-names>Y</given-names></name><name><surname>Song</surname><given-names>J</given-names></name><name><surname>Zhao</surname><given-names>K</given-names></name></person-group><article-title>Genome-wide mapping of dnase i hypersensitive sites in rare cell populations using single-cell dnase sequencing</article-title><source>Nat Protoc</source><year>2017</year><volume>12</volume><issue>11</issue><fpage>2342</fpage><pub-id pub-id-type="doi">10.1038/nprot.2017.099</pub-id></element-citation></ref><ref id="CR11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noble</surname><given-names>WS</given-names></name><name><surname>Kuehn</surname><given-names>S</given-names></name><name><surname>Thurman</surname><given-names>R</given-names></name><name><surname>Stamatoyannopoulos</surname><given-names>J</given-names></name><name><surname>Stamatoyannopoulos</surname><given-names>J</given-names></name></person-group><article-title>Predicting the in vivo signature of human gene regulatory sequences</article-title><source>Bioinformatics</source><year>2005</year><volume>21 Suppl 1</volume><issue>1</issue><fpage>338</fpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bti1047</pub-id><pub-id pub-id-type="pmid">15347572</pub-id></element-citation></ref><ref id="CR12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B</given-names></name><name><surname>Long</surname><given-names>R</given-names></name><name><surname>Chou</surname><given-names>KC</given-names></name></person-group><article-title>idhs-el: Identifying dnase i hypersensitive-sites by fusing three different modes of pseu-do nucleotide composition into an ensemble learning framework</article-title><source>Bioinformatics</source><year>2016</year><volume>28</volume><issue>2</issue><fpage>250</fpage><lpage>7</lpage></element-citation></ref><ref id="CR13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Gorkin</surname><given-names>DU</given-names></name><name><surname>Baker</surname><given-names>M</given-names></name><name><surname>Strober</surname><given-names>BJ</given-names></name><name><surname>Asoni</surname><given-names>AL</given-names></name><name><surname>Mccallion</surname><given-names>AS</given-names></name><name><surname>Beer</surname><given-names>MA</given-names></name></person-group><article-title>A method to predict the impact of regulatory variants from dna sequence</article-title><source>Nat Genet</source><year>2015</year><volume>47</volume><issue>8</issue><fpage>955</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1038/ng.3331</pub-id><pub-id pub-id-type="pmid">26075791</pub-id></element-citation></ref><ref id="CR14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>W</given-names></name><name><surname>Sherwood</surname><given-names>B</given-names></name><name><surname>Ji</surname><given-names>Z</given-names></name><name><surname>Xue</surname><given-names>Y</given-names></name><name><surname>Du</surname><given-names>F</given-names></name><name><surname>Bai</surname><given-names>J</given-names></name><name><surname>Ying</surname><given-names>M</given-names></name><name><surname>Ji</surname><given-names>H</given-names></name></person-group><article-title>Genome-wide prediction of dnase i hypersensitivity using gene expression</article-title><source>Nat Commun</source><year>2017</year><volume>8</volume><issue>1</issue><fpage>1038</fpage><pub-id pub-id-type="doi">10.1038/s41467-017-01188-x</pub-id><pub-id pub-id-type="pmid">29051481</pub-id></element-citation></ref><ref id="CR15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boyle</surname><given-names>AP</given-names></name><name><surname>Davis</surname><given-names>S</given-names></name><name><surname>Shulha</surname><given-names>HP</given-names></name><name><surname>Meltzer</surname><given-names>P</given-names></name><name><surname>Margulies</surname><given-names>EH</given-names></name><name><surname>Weng</surname><given-names>Z</given-names></name><name><surname>Furey</surname><given-names>TS</given-names></name><name><surname>Crawford</surname><given-names>GE</given-names></name></person-group><article-title>High-resolution mapping and characterization of open chromatin across the genome</article-title><source>Cell</source><year>2008</year><volume>132</volume><issue>2</issue><fpage>311</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2007.12.014</pub-id><pub-id pub-id-type="pmid">18243105</pub-id></element-citation></ref><ref id="CR16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ullah</surname><given-names>F</given-names></name><name><surname>Hamilton</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>ASN</given-names></name><name><surname>Benhur</surname><given-names>A</given-names></name></person-group><article-title>Exploring the relationship between intron retention and chromatin accessibility in plants</article-title><source>Bmc Genomics</source><year>2018</year><volume>19</volume><issue>1</issue><fpage>21</fpage><pub-id pub-id-type="doi">10.1186/s12864-017-4393-z</pub-id><pub-id pub-id-type="pmid">29304739</pub-id></element-citation></ref><ref id="CR17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lippman</surname><given-names>Z</given-names></name><name><surname>Gendrel</surname><given-names>AV</given-names></name><name><surname>Black</surname><given-names>M</given-names></name><name><surname>Vaughn</surname><given-names>MW</given-names></name><name><surname>Dedhia</surname><given-names>N</given-names></name><name><surname>Mccombie</surname><given-names>WR</given-names></name><name><surname>Lavine</surname><given-names>K</given-names></name><name><surname>Mittal</surname><given-names>V</given-names></name><name><surname>May</surname><given-names>B</given-names></name><name><surname>Kasschau</surname><given-names>KD</given-names></name></person-group><article-title>Role of transposable elements in heterochromatin and epigenetic control</article-title><source>Nature</source><year>2004</year><volume>430</volume><issue>6998</issue><fpage>471</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1038/nature02651</pub-id><pub-id pub-id-type="pmid">15269773</pub-id></element-citation></ref><ref id="CR18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Germann</surname><given-names>S</given-names></name><name><surname>Blus</surname><given-names>BJ</given-names></name><name><surname>Khorasanizadeh</surname><given-names>S</given-names></name><name><surname>Gaudin</surname><given-names>V</given-names></name><name><surname>Jacobsen</surname><given-names>SE</given-names></name></person-group><article-title>The arabidopsis lhp1 protein colocalizes with histone h3 lys27 trimethylation</article-title><source>Nat Struct Mol Biol</source><year>2007</year><volume>14</volume><issue>9</issue><fpage>869</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1038/nsmb1283</pub-id><pub-id pub-id-type="pmid">17676062</pub-id></element-citation></ref><ref id="CR19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zilberman</surname><given-names>D</given-names></name><name><surname>Coleman-Derr</surname><given-names>D</given-names></name><name><surname>Ballinger</surname><given-names>T</given-names></name><name><surname>Henikoff</surname><given-names>S</given-names></name></person-group><article-title>Histone h2a.z and dna methylation are mutually antagonistic chromatin marks</article-title><source>Nature</source><year>2008</year><volume>456</volume><issue>7218</issue><fpage>125</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/nature07324</pub-id><pub-id pub-id-type="pmid">18815594</pub-id></element-citation></ref><ref id="CR20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Bernatavichute</surname><given-names>YV</given-names></name><name><surname>Cokus</surname><given-names>S</given-names></name><name><surname>Pellegrini</surname><given-names>M</given-names></name><name><surname>Jacobsen</surname><given-names>SE</given-names></name></person-group><article-title>Genome-wide analysis of mono-, di- and trimethylation of histone h3 lysine 4 in arabidopsis thaliana</article-title><source>Genome Biol</source><year>2009</year><volume>10</volume><issue>6</issue><fpage>62</fpage><pub-id pub-id-type="doi">10.1186/gb-2009-10-6-r62</pub-id></element-citation></ref><ref id="CR21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>He</surname><given-names>K</given-names></name><name><surname>Charron</surname><given-names>JBF</given-names></name><name><surname>Elling</surname><given-names>AA</given-names></name><name><surname>Xing</surname><given-names>WD</given-names></name></person-group><article-title>Genome-wide profiling of histone h3 lysine 9 acetylation and dimethylation in arabidopsis reveals correlation between multiple histone marks and gene expression</article-title><source>Plant Mol Biol</source><year>2010</year><volume>72</volume><issue>6</issue><fpage>585</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1007/s11103-009-9594-7</pub-id><pub-id pub-id-type="pmid">20054610</pub-id></element-citation></ref><ref id="CR22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lecun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><issue>7553</issue><fpage>436</fpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="CR23"><label>23</label><mixed-citation publication-type="other">Mikolov T, Karafi&#x000e1;t M, Burget L, Cernock&#x000fd; J, Khudanpur S. Recurrent neural network based language model. In: INTERSPEECH 2010, Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September: 2010. p. 1045&#x02013;8.</mixed-citation></ref><ref id="CR24"><label>24</label><mixed-citation publication-type="other">Dauphin YN, Fan A, Auli M, Grangier D. Language modeling with gated convolutional networks.2016.</mixed-citation></ref><ref id="CR25"><label>25</label><mixed-citation publication-type="other">Zhou X, Hu B, Chen Q, Wang X. Recurrent convolutional neural network for answer selection in community question answering. Neurocomputing. 2018;274:8&#x02013;18.</mixed-citation></ref><ref id="CR26"><label>26</label><mixed-citation publication-type="other">Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. In: International Conference on Neural Information Processing Systems.2012. p. 1097&#x02013;105.</mixed-citation></ref><ref id="CR27"><label>27</label><mixed-citation publication-type="other">Dai J, Li Y, He K, Sun J. R-fcn: Object detection via region-based fully convolutional networks.2016.</mixed-citation></ref><ref id="CR28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lanchantin</surname><given-names>J</given-names></name><name><surname>Singh</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>B</given-names></name><name><surname>Qi</surname><given-names>Y</given-names></name></person-group><article-title>Deep motif dashboard: Visualizing and understanding genomic sequences using deep neural networks</article-title><source>Pac Symp Biocomput Pac Symp Biocomput</source><year>2016</year><volume>22</volume><fpage>254</fpage></element-citation></ref><ref id="CR29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alipanahi</surname><given-names>B</given-names></name><name><surname>Delong</surname><given-names>A</given-names></name><name><surname>Weirauch</surname><given-names>MT</given-names></name><name><surname>Frey</surname><given-names>BJ</given-names></name></person-group><article-title>Predicting the sequence specificities of dna- and rna-binding proteins by deep learning</article-title><source>Nat Biotechnol</source><year>2015</year><volume>33</volume><issue>8</issue><fpage>831</fpage><pub-id pub-id-type="doi">10.1038/nbt.3300</pub-id><pub-id pub-id-type="pmid">26213851</pub-id></element-citation></ref><ref id="CR30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Hu</surname><given-names>H</given-names></name><name><surname>Gong</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Cheng</surname><given-names>C</given-names></name><name><surname>Zeng</surname><given-names>J</given-names></name></person-group><article-title>A deep learning framework for modeling structural features of rna-binding protein targets</article-title><source>Nucleic Acids Res</source><year>2015</year><volume>44</volume><issue>4</issue><fpage>32</fpage><pub-id pub-id-type="doi">10.1093/nar/gkv1025</pub-id></element-citation></ref><ref id="CR31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelley</surname><given-names>DR</given-names></name><name><surname>Snoek</surname><given-names>J</given-names></name><name><surname>Rinn</surname><given-names>J</given-names></name></person-group><article-title>Basset: Learning the regulatory code of the accessible genome with deep convolutional neural networks</article-title><source>Genome Res</source><year>2016</year><volume>26</volume><issue>7</issue><fpage>990</fpage><pub-id pub-id-type="doi">10.1101/gr.200535.115</pub-id><pub-id pub-id-type="pmid">27197224</pub-id></element-citation></ref><ref id="CR32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>R</given-names></name><name><surname>Freitas</surname><given-names>C</given-names></name><name><surname>Chan</surname><given-names>L</given-names></name><name><surname>Sun</surname><given-names>M</given-names></name><name><surname>Jiang</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name></person-group><article-title>Prolango: Protein function prediction using neural machine translation based on a recurrent neural network</article-title><source>Molecules</source><year>2017</year><volume>22</volume><issue>10</issue><fpage>1732</fpage><pub-id pub-id-type="doi">10.3390/molecules22101732</pub-id></element-citation></ref><ref id="CR33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bo&#x0017e;a</surname><given-names>V</given-names></name><name><surname>Brejov&#x000e1;</surname><given-names>B</given-names></name><name><surname>Vina&#x00159;</surname><given-names>T</given-names></name></person-group><article-title>Deepnano: Deep recurrent neural networks for base calling in minion nanopore reads</article-title><source>PLoS ONE</source><year>2017</year><volume>12</volume><issue>6</issue><fpage>0178751</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0178751</pub-id></element-citation></ref><ref id="CR34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daniel</surname><given-names>Q</given-names></name><name><surname>Xie</surname><given-names>X</given-names></name></person-group><article-title>Danq: a hybrid convolutional and recurrent deep neural network for quantifying the function of dna sequences:</article-title><source>Nucleic Acids Res</source><year>2016</year><volume>44</volume><issue>11</issue><fpage>107</fpage><pub-id pub-id-type="doi">10.1093/nar/gkw226</pub-id></element-citation></ref><ref id="CR35"><label>35</label><mixed-citation publication-type="other">Yang B, Liu F, Ren C, Ouyang Z, Xie Z, Bo X, Shu W. Biren: predicting enhancers with a deep-learning-based model using the dna sequence alone. Bioinformatics. 2017;33(13):1930&#x02013;1936.</mixed-citation></ref><ref id="CR36"><label>36</label><mixed-citation publication-type="other">Zhang J, Peng W, Wang L. Lenup: Learning nucleosome positioning from dna sequences with improved convolutional neural networks. Bioinformatics. 2018;34(10):1705&#x02013;1712.</mixed-citation></ref><ref id="CR37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><article-title>Spatial pyramid pooling in deep convolutional networks for visual recognition</article-title><source>IEEE Trans Pattern Anal Mach Intell</source><year>2015</year><volume>37</volume><issue>9</issue><fpage>1904</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2015.2389824</pub-id><pub-id pub-id-type="pmid">26353135</pub-id></element-citation></ref><ref id="CR38"><label>38</label><mixed-citation publication-type="other">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. Comput Sci. 2014.</mixed-citation></ref><ref id="CR39"><label>39</label><mixed-citation publication-type="other">Mass AL, Hannun AY, Ng AY. Rectifier nonlinearities improve neural network acoustic models. In: Procedings of the 30 Th International Conference on Machine Learning, vol. 30.2013.</mixed-citation></ref><ref id="CR40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>L</given-names></name><name><surname>Niu</surname><given-names>B</given-names></name><name><surname>Zhu</surname><given-names>Z</given-names></name><name><surname>Wu</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>W</given-names></name></person-group><article-title>Cd-hit: accelerated for clustering the next-generation sequencing data</article-title><source>Bioinformatics</source><year>2012</year><volume>28</volume><issue>23</issue><fpage>3150</fpage><lpage>2</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bts565</pub-id><pub-id pub-id-type="pmid">23060610</pub-id></element-citation></ref><ref id="CR41"><label>41</label><mixed-citation publication-type="other">Zhang S, Chang M, Zhou Z, Dai X, Xu Z. pdhs-elm: computational predictor for plant dnase i hypersensitive sites based on extreme learning machines. Mol Gen Genomics. 2018;293(4):1035&#x02013;1049.</mixed-citation></ref><ref id="CR42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noble</surname><given-names>WS</given-names></name><name><surname>Kuehn</surname><given-names>S</given-names></name><name><surname>Thurman</surname><given-names>R</given-names></name><name><surname>Yu</surname><given-names>M</given-names></name><name><surname>Stamatoyannopoulos</surname><given-names>J</given-names></name></person-group><article-title>Predicting the in vivo signature of human gene regulatory sequences</article-title><source>Bioinformatics</source><year>2005</year><volume>21 Suppl 1</volume><issue>1</issue><fpage>338</fpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bti1047</pub-id><pub-id pub-id-type="pmid">15347572</pub-id></element-citation></ref><ref id="CR43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>YX</given-names></name><name><surname>Shen</surname><given-names>HB</given-names></name></person-group><article-title>Predicting pupylation sites in prokaryotic proteins using pseudo-amino acid composition and extreme learning machine</article-title><source>Neurocomputing</source><year>2014</year><volume>128</volume><issue>5</issue><fpage>267</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2012.11.058</pub-id></element-citation></ref><ref id="CR44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B</given-names></name><name><surname>Ren</surname><given-names>L</given-names></name><name><surname>Chou</surname><given-names>KC</given-names></name></person-group><article-title>idhs-el: identifying dnase i hypersensitive sites by fusing three different modes of pseudo nucleotide composition into an ensemble learning framework</article-title><source>Bioinformatics</source><year>2016</year><volume>28</volume><issue>2</issue><fpage>250</fpage><lpage>7</lpage></element-citation></ref><ref id="CR45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kabir</surname><given-names>M</given-names></name><name><surname>Yu</surname><given-names>DJ</given-names></name></person-group><article-title>Predicting dnase i hypersensitive sites via un-biased pseudo trinucleotide composition</article-title><source>Chemometr Intell Lab Syst</source><year>2017</year><volume>167</volume><issue>15 August 2017</issue><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1016/j.chemolab.2017.05.001</pub-id></element-citation></ref></ref-list></back></article>