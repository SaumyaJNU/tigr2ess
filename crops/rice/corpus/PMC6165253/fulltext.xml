<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6165253</article-id><article-id pub-id-type="doi">10.3390/s18093139</article-id><article-id pub-id-type="publisher-id">sensors-18-03139</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Crop Classification Method Integrating GF-3 PolSAR and Sentinel-2A Optical Data in the Dongting Lake Basin</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Gao</surname><given-names>Han</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03139">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4461-068X</contrib-id><name><surname>Wang</surname><given-names>Changcheng</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03139">1</xref><xref ref-type="aff" rid="af2-sensors-18-03139">2</xref><xref rid="c1-sensors-18-03139" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Guanya</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03139">1</xref></contrib><contrib contrib-type="author"><name><surname>Zhu</surname><given-names>Jianjun</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03139">1</xref></contrib><contrib contrib-type="author"><name><surname>Tang</surname><given-names>Yuqi</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03139">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6691-1398</contrib-id><name><surname>Shen</surname><given-names>Peng</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03139">1</xref></contrib><contrib contrib-type="author"><name><surname>Zhu</surname><given-names>Ziwei</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03139">1</xref></contrib></contrib-group><aff id="af1-sensors-18-03139"><label>1</label>School of Geosciences and Info-Physics, Central South University, Changsha 410083, China; <email>dawnhan314@csu.edu.cn</email> (H.G.); <email>wangguanya@csu.edu.cn</email> (G.W.); <email>zjj@csu.edu.cn</email> (J.Z.); <email>yqtang@csu.edu.cn</email> (Y.T.); <email>shen-peng@csu.edu.cn</email> (P.S.); <email>zhumyrtle@csu.edu.cn</email> (Z.Z.)</aff><aff id="af2-sensors-18-03139"><label>2</label>Key Laboratory of Metallogenic Prediction of Nonferrous Metals and Geological Environment Monitoring, Ministry of Education, Central South University, Changsha 410083, China</aff><author-notes><corresp id="c1-sensors-18-03139"><label>*</label>Correspondence: <email>wangchangcheng@csu.edu.cn</email>; Tel.: +86-731-8883-6931</corresp></author-notes><pub-date pub-type="epub"><day>17</day><month>9</month><year>2018</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2018</year></pub-date><volume>18</volume><issue>9</issue><elocation-id>3139</elocation-id><history><date date-type="received"><day>30</day><month>6</month><year>2018</year></date><date date-type="accepted"><day>14</day><month>9</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; 2018 by the authors.</copyright-statement><copyright-year>2018</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>With the increasing of satellite sensors, more available multi-source data can be used for large-scale high-precision crop classification. Both polarimetric synthetic aperture radar (PolSAR) and multi-spectral optical data have been widely used for classification. However, it is difficult to combine the covariance matrix of PolSAR data with the spectral bands of optical data. Using Hoekman&#x02019;s method, this study solves the above problems by transforming the covariance matrix to an intensity vector that includes multiple intensity values on different polarization basis. In order to reduce the features redundancy, the principal component analysis (PCA) algorithm is adopted to select some useful polarimetric and optical features. In this study, the PolSAR data acquired by satellite Gaofen-3 (GF-3) on 19 July 2017 and the optical data acquired by Sentinel-2A on 17 July 2017 over the Dongting lake basin are selected for the validation experiment. The results show that the full feature integration method proposed in this study achieves an overall classification accuracy of 85.27%, higher than that of the single dataset method or some other feature integration modes.</p></abstract><kwd-group><kwd>GF-3</kwd><kwd>PolSAR data</kwd><kwd>Sentinel-2A</kwd><kwd>optical data</kwd><kwd>data integration</kwd><kwd>crop classification</kwd><kwd>Dongting lake basin</kwd></kwd-group></article-meta></front><body><sec id="sec1-sensors-18-03139"><title>1. Introduction</title><p>As for the demand of large-scale and high-efficiency crop mapping, remote sensing technology can substitute for the traditional field measurement and it can observe the same area many times in a short revisit time. Nowadays, optical data and polarimetric synthetic aperture radar (PolSAR) data are often used for crops&#x02019; monitoring and the integration of multi-source data sets can help to achieve high-precision classification results. However, in the integrated classification, some effective features extracted from data of different sensors cannot be used at the same time, so that the potential of integrated datasets cannot be fully explored. Particularly, the covariance matrix of PolSAR data is difficult to be combined with multi-spectral optical data for classification. Considering the covariance matrix contains rich polarimetric information, this paper applies Hoekman&#x02019;s method [<xref rid="B1-sensors-18-03139" ref-type="bibr">1</xref>], the matrix can be transformed to an intensity vector, detailed in <xref ref-type="sec" rid="sec3dot2-sensors-18-03139">Section 3.2</xref>. Such intensity vector has nine bands, denoting the intensity values on different polarization bases, which has the similar data structure with the spectral bands of optical data, so it is easy to combine these two kinds of information. In addition, some other useful features are extracted, including the polarimetric features, as the radar vegetation index (RVI) and the decomposed Yamguichi four components, as well as some optical features as the normalized difference vegetation index (NDVI) and the information entropy describing the texture information. The spectral characteristics in the optical data are mainly used to indicate the changes in the moisture and chlorophyll content of the crop leaves [<xref rid="B2-sensors-18-03139" ref-type="bibr">2</xref>,<xref rid="B3-sensors-18-03139" ref-type="bibr">3</xref>]. In the PolSAR data, the backscatter information of the multiple polarimetric channels are used to describe the structure, orientation distribution and dielectric constant characteristics of crops [<xref rid="B4-sensors-18-03139" ref-type="bibr">4</xref>,<xref rid="B5-sensors-18-03139" ref-type="bibr">5</xref>,<xref rid="B6-sensors-18-03139" ref-type="bibr">6</xref>,<xref rid="B7-sensors-18-03139" ref-type="bibr">7</xref>,<xref rid="B8-sensors-18-03139" ref-type="bibr">8</xref>,<xref rid="B9-sensors-18-03139" ref-type="bibr">9</xref>]. Generally speaking, the optical and PolSAR data can characterize different properties of crops. These two data are mutually independent and complementary to each other. There are some methods developed for using each of these data set for crop classification, including the PolSAR classification methods [<xref rid="B10-sensors-18-03139" ref-type="bibr">10</xref>,<xref rid="B11-sensors-18-03139" ref-type="bibr">11</xref>,<xref rid="B12-sensors-18-03139" ref-type="bibr">12</xref>,<xref rid="B13-sensors-18-03139" ref-type="bibr">13</xref>,<xref rid="B14-sensors-18-03139" ref-type="bibr">14</xref>,<xref rid="B15-sensors-18-03139" ref-type="bibr">15</xref>,<xref rid="B16-sensors-18-03139" ref-type="bibr">16</xref>,<xref rid="B17-sensors-18-03139" ref-type="bibr">17</xref>,<xref rid="B18-sensors-18-03139" ref-type="bibr">18</xref>,<xref rid="B19-sensors-18-03139" ref-type="bibr">19</xref>] and the optical classification methods [<xref rid="B20-sensors-18-03139" ref-type="bibr">20</xref>,<xref rid="B21-sensors-18-03139" ref-type="bibr">21</xref>,<xref rid="B22-sensors-18-03139" ref-type="bibr">22</xref>,<xref rid="B23-sensors-18-03139" ref-type="bibr">23</xref>,<xref rid="B24-sensors-18-03139" ref-type="bibr">24</xref>]. However, the limited kinds of observation measurements by single type of satellite is hard to fully represent the characteristics of targets and the combination of multi-source data can be used for crop classification [<xref rid="B25-sensors-18-03139" ref-type="bibr">25</xref>,<xref rid="B26-sensors-18-03139" ref-type="bibr">26</xref>,<xref rid="B27-sensors-18-03139" ref-type="bibr">27</xref>,<xref rid="B28-sensors-18-03139" ref-type="bibr">28</xref>,<xref rid="B29-sensors-18-03139" ref-type="bibr">29</xref>,<xref rid="B30-sensors-18-03139" ref-type="bibr">30</xref>,<xref rid="B31-sensors-18-03139" ref-type="bibr">31</xref>,<xref rid="B32-sensors-18-03139" ref-type="bibr">32</xref>].</p><p>Nowadays, data fusion and data integration are two common combination modes of multi-source data. Particularly, compared with the data integration methods, there are more data fusion methods, as PCA fusion method [<xref rid="B33-sensors-18-03139" ref-type="bibr">33</xref>,<xref rid="B34-sensors-18-03139" ref-type="bibr">34</xref>], Brovey fusion method [<xref rid="B35-sensors-18-03139" ref-type="bibr">35</xref>,<xref rid="B36-sensors-18-03139" ref-type="bibr">36</xref>], Gram-Schmidt transform fusion method [<xref rid="B37-sensors-18-03139" ref-type="bibr">37</xref>,<xref rid="B38-sensors-18-03139" ref-type="bibr">38</xref>], wavelet transform method [<xref rid="B39-sensors-18-03139" ref-type="bibr">39</xref>,<xref rid="B40-sensors-18-03139" ref-type="bibr">40</xref>,<xref rid="B41-sensors-18-03139" ref-type="bibr">41</xref>,<xref rid="B42-sensors-18-03139" ref-type="bibr">42</xref>]. However, the dimension of feature sets extracted in data fusion is generally three, corresponding to the RGB channels for visual representation. Due to the number of feature sets extracted in data fusion is fewer than the data integration, the classification accuracy of data fusion method is lower [<xref rid="B43-sensors-18-03139" ref-type="bibr">43</xref>]. So, the data integration is applied in the classification.</p><p>Furthermore, the extracted feature sets can be applied into crop classification. Available classification algorithms include the maximum likelihood algorithm [<xref rid="B44-sensors-18-03139" ref-type="bibr">44</xref>], the support vector machine (SVM) [<xref rid="B29-sensors-18-03139" ref-type="bibr">29</xref>,<xref rid="B45-sensors-18-03139" ref-type="bibr">45</xref>], the neural network [<xref rid="B46-sensors-18-03139" ref-type="bibr">46</xref>], the deep learning algorithm [<xref rid="B47-sensors-18-03139" ref-type="bibr">47</xref>]. Among which, the maximum likelihood algorithm is based on the probability distribution of the characteristics of feature sets, which is simple and easy to be operated. But its classification accuracy is low, because the selected distribution model may not be suitable for all terrain types. Other three methods all belong to machine learning algorithms, which use training samples for iterative learning. And the classification rules can be generated to identify the unknown objects. The neural network and deep learning algorithm require a large number of training samples and the training process is time-consuming, caused by the high model complexity. Whereas, the SVM algorithm is to convert the feature sets into high dimensional space through a kernel function and to generate a classification plane. It needs only a few training samples and has low modeling complexity and good usability. So, it has been applied in many cases of classification and recognition of objects.</p><p>The paper is organized as follows. <xref ref-type="sec" rid="sec2-sensors-18-03139">Section 2</xref> illustrates the study area and datasets. <xref ref-type="sec" rid="sec3-sensors-18-03139">Section 3</xref> describes the main detailed steps of the proposed method, including data preprocessing, feature extraction and integration and SVM classification. <xref ref-type="sec" rid="sec4-sensors-18-03139">Section 4</xref> presents the experimental results. <xref ref-type="sec" rid="sec5-sensors-18-03139">Section 5</xref> makes some detailed discussions for the results. Finally, we draw some conclusions in <xref ref-type="sec" rid="sec6-sensors-18-03139">Section 6</xref>.</p></sec><sec id="sec2-sensors-18-03139"><title>2. Study Area and Dataset</title><p>The study area is located in the southeastern Dongting Lake basin, Hunan, China (<xref ref-type="fig" rid="sensors-18-03139-f001">Figure 1</xref>). The main crops there are rice, watermelon and lotus. With the steady stream of irrigation support from Dongting Lake, there grows the single-season rice (Rice1) and the two-season rice (Rice2). We selected the GF-3 polarimetric SAR data acquired on 19 July 2017 and Sentinel-2A optical data on 17 July 2017, for crop classification. The specific imaging parameters of GF-3 data and Sentinel-2A data are shown in <xref rid="sensors-18-03139-t001" ref-type="table">Table 1</xref> and <xref rid="sensors-18-03139-t002" ref-type="table">Table 2</xref>, respectively. Hereon, the research based on the satellite GF3 can expand the application of GF-3 data in agriculture. As the first C-band synthetic aperture radar (SAR) satellite in China, it owns 12 imaging modes with the highest spatial resolution of 1 m [<xref rid="B48-sensors-18-03139" ref-type="bibr">48</xref>]. GF-3 satellite is able to monitor the ocean and the land under any weather conditions. Moreover, its unique left and right side looking modes improve its ability of quick response to the emergence of disasters.</p><p>We collected the crop information through an in-situ survey. We kept a record for crop types and their growth stages. The crop types were identified through the regional agricultural expertise and farmers. Finally, the training samples and testing samples were separately selected (<xref ref-type="fig" rid="sensors-18-03139-f002">Figure 2</xref>) according to the basic sampling principle [<xref rid="B49-sensors-18-03139" ref-type="bibr">49</xref>,<xref rid="B50-sensors-18-03139" ref-type="bibr">50</xref>] and the detailed information of samples are listed in <xref rid="sensors-18-03139-t003" ref-type="table">Table 3</xref>.</p></sec><sec id="sec3-sensors-18-03139"><title>3. Methodology</title><p>The proposed method includes the following steps: data preprocessing, feature extraction and integration, SVM classification. The flowchart of the proposed method is shown in <xref ref-type="fig" rid="sensors-18-03139-f003">Figure 3</xref>.</p><sec id="sec3dot1-sensors-18-03139"><title>3.1. Data Preprocessing</title><p>In order to make the extracted features better used for classification, the careful data preprocessing is necessary. Firstly, the GF-3 data is polarimetric calibrated. Specifically, the backscattering amplitude information on different polarization channels should be corrected according to the calibration constants in the header file. Then, the polarimetric coherency matrix <italic>T</italic><sub>3</sub> is generated and the Non-Local filtering is used to reduce the speckle noise [<xref rid="B51-sensors-18-03139" ref-type="bibr">51</xref>,<xref rid="B52-sensors-18-03139" ref-type="bibr">52</xref>]. Finally, the area of interest is selected for subsequent experiments. As for the Sentinel-2A data, there are 13 bands, of which the selected four bands are commonly used for classification, including red (R), green (G), blue (B) and near infrared (NIR) bands.</p><p>Then the two data are registered into the same coordinate system for extracting and integrating features. Because the SAR acquisition is side looking, which is different from the central projection of optical data, the original optical data is registered into the SAR coordinate system for keeping target&#x02019;s backscattering characteristics. Details are shown in <xref ref-type="fig" rid="sensors-18-03139-f004">Figure 4</xref>. We choose the ground control points (GCPs) and then register data sets based on corresponding GCPs. Since the study area has a flat terrain, the SAR data has no obvious foreshortening, layover and shadow. So, the registration method based on the GCPs can achieve a high registered accuracy. At last, the optical data is cut into the interested area the same as GF-3 data.</p></sec><sec id="sec3dot2-sensors-18-03139"><title>3.2. Feature Extraction and Integration</title><p>To fully characterize different crops, we extract the backscattering intensity, backscattering type, canopy vegetation index from the GF-3 data and the spectral characteristics, spatial texture, canopy vegetation index from Sentinel-2A data. Since the intensity information is the most direct representation of the backscattering of radar waves in ground objects, it is extracted firstly.</p><p>We use the method proposed by Hoekman in 2003 [<xref rid="B1-sensors-18-03139" ref-type="bibr">1</xref>] to transform elements of covariance matrix <italic>C</italic><sub>3</sub> into multi-channel intensity vectors. Matrix <italic>B</italic> can be used to convert the elements of matrix <italic>C</italic><sub>3</sub> into an intensity vector <inline-formula><mml:math id="mm999"><mml:mrow><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, which can represent the backscattering intensity of crops in different polarimetric channels. The equation is shown specifically as follows:
<disp-formula id="FD1-sensors-18-03139"><label>(1)</label><mml:math id="mm3"><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo stretchy="false">&#x02329;</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">&#x0232a;</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo stretchy="false">&#x02329;</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">&#x0232a;</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo stretchy="false">&#x02329;</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">&#x0232a;</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mo stretchy="false">&#x02329;</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mo stretchy="false">&#x0232a;</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mo stretchy="false">&#x02329;</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mo stretchy="false">&#x0232a;</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mo stretchy="false">&#x02329;</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mo stretchy="false">&#x0232a;</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mo stretchy="false">&#x02329;</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mo stretchy="false">&#x0232a;</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mo stretchy="false">&#x02329;</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mo stretchy="false">&#x0232a;</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mo stretchy="false">&#x02329;</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mo stretchy="false">&#x0232a;</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>D</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>D</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>D</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>D</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>D</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>D</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>D</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>D</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>D</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mn>45</mml:mn><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>9</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD2-sensors-18-03139"><label>(2)</label><mml:math id="mm3999"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>4</mml:mn><mml:mi>&#x003c0;</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>3</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>3</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>3</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>3</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>3</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>5</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>3</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>3</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>5</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>3</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>9</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm4"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the intensity value and the subscripts denote the received and transmitted polarization bases: horizontal (<italic>h</italic>), vertical (<italic>v</italic>), left circular (<italic>l</italic>), right circular (<italic>r</italic>), 45&#x000b0; linear (+ or +45) and &#x02212;45&#x000b0; linear (&#x02212; or &#x02212;45).</p><p>It is worth noting that the backscattering intensity often contains a number of large magnitude values. For the normalization during the data combination, we transform the original intensity into the intensity with backscattering coefficient format (dB) by
<disp-formula id="FD3-sensors-18-03139"><label>(3)</label><mml:math id="mm5"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:msub><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD4-sensors-18-03139"><label>(4)</label><mml:math id="mm6"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mn>45</mml:mn><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>9</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm7"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> denotes the transformed intensity vector and its detailed values are presented in Formula (4). The subscripts in <inline-formula><mml:math id="mm8"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> are the same with <inline-formula><mml:math id="mm9"><mml:mrow><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. Although, the backscattering intensity information can be characterized by <inline-formula><mml:math id="mm10"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, the dimension of <inline-formula><mml:math id="mm11"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> in multi-source data integration is large and will lead to data redundancy. Such redundancy will reduce the classification accuracy and computational efficiency. The principal component analysis (PCA) algorithm can pick out one or two main eigenvalues to replace the total eigenvector, so as to increase the classification accuracy and computational efficiency. In this paper, the sum of the first two principal components&#x02019; variance values accounts for 98% of the total, which can be used to substitute for eigenvector in the calculation. In addition, such two principal component features <inline-formula><mml:math id="mm12"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula><sub>pca1</sub> and <inline-formula><mml:math id="mm13"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula><sub>pca2</sub> are extracted.</p><p>As for the backscattering type information, the corresponding polarimetric characteristics can be extracted by the Y4R decomposition method which is proposed by Yamguichi in 2005 [<xref rid="B53-sensors-18-03139" ref-type="bibr">53</xref>]. On the basis of the classical Freeman three-component decomposition, the Y4R decomposition method further considers the helix scattering mechanism, which makes the backscattering types of polarimetric decomposition closer to the real situation, so that the Y4R method has been widely used for PolSAR image classification.
<disp-formula id="FD5-sensors-18-03139"><label>(5)</label><mml:math id="mm14"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD6-sensors-18-03139"><label>(6)</label><mml:math id="mm15"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD7-sensors-18-03139"><label>(7)</label><mml:math id="mm16"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD8-sensors-18-03139"><label>(8)</label><mml:math id="mm17"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD9-sensors-18-03139"><label>(9)</label><mml:math id="mm18"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm19"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm20"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm21"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm22"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the scattering intensity of surface scattering, double scattering, volume scattering and helix scattering, respectively, <inline-formula><mml:math id="mm23"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm24"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm25"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm26"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the surface, double-bounce, volume, helix scattering contributions to <inline-formula><mml:math id="mm27"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm28"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm29"><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:math></inline-formula> denote the reals.</p><p>RVI extracted from PolSAR data can be used as the canopy vegetation index [<xref rid="B54-sensors-18-03139" ref-type="bibr">54</xref>] and it applies the power of different polarimetric channels to reflect the canopy vegetation characteristics of different phenological stages. The greater the power, the closer the crop canopy is to the forest canopy.
<disp-formula id="FD10-sensors-18-03139"><label>(10)</label><mml:math id="mm30"><mml:mrow><mml:mrow><mml:mi>RVI</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>8</mml:mn><mml:mo stretchy="false">&#x02329;</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">&#x0232a;</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">&#x02329;</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">&#x0232a;</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">&#x02329;</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">&#x0232a;</mml:mo><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">&#x02329;</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">&#x0232a;</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The characteristics of crop spectral information, spatial texture information and canopy vegetation index are extracted from the Sentinel-2A optical data. Multi-spectral information is more sensitive to moisture and the chlorophyll component of crop leaves, which can be used to identify the crop species. In this paper, four common spectral bands (R, G, B, NIR) are extracted to characterize the spectral information of crops and their corresponding feature vectors are also transformed by PCA algorithm. The first two principal components Opband<sub>pca1</sub> and Opband<sub>pca2</sub> are extracted, of which sum can contribute 99% of the overall variance of eigenvector.</p><p>Then the information entropy H of image on the red (R) band is used to characterize the spatial texture information of crops. The information entropy is an indicator of uncertainty measurement. The greater the value, the higher the uncertainty [<xref rid="B55-sensors-18-03139" ref-type="bibr">55</xref>]. As for the image on single spectral band, the uncertainty is often determined by the richness of texture. The richer the texture information, the higher the uncertainty.</p><p>At last, the normalized difference vegetation index (NDVI) is calculated from red and near infrared band images by Equation (11). NDVI is used to characterize the canopy properties of different crops, especially the changes of canopy density and biomass.
<disp-formula id="FD11-sensors-18-03139"><label>(11)</label><mml:math id="mm31"><mml:mrow><mml:mrow><mml:mi>NDVI</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>NIR</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>NIR</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Then the extracted features should be integrated before the SVM classification. In order to eliminate the effects of different features&#x02019; scale, this paper normalizes all these features&#x02019; range to (0~1). As shown in <xref ref-type="fig" rid="sensors-18-03139-f005">Figure 5</xref>, the imaging characteristics between PolSAR data and optical data are obviously different. The features obtained by such two kinds of data are independent and complementary to each other.</p></sec><sec id="sec3dot3-sensors-18-03139"><title>3.3. SVM Classification</title><p>Based on the integrated features, the support vector machine (SVM) method is applied to crop classification. The SVM classifier is an excellent two-class classification model, which can use the kernel function to map the multi-dimensional feature sets into higher dimensional space, to construct the classification plane and distinguish different categories. This method can efficiently get high-precision classification results with a few training samples. The SVM classifier has been successfully applied in many aspects, such as land use classification mapping, data mining. The kernel function adopted in this paper is the radial basis function (RBF), which can solve the linear non-separable problem in SVM classification by nonlinear mapping and it has only several parameters and low model complexity. After the SVM classification, the results with the SAR coordinate system will be transformed into the geographic coordinate system.</p></sec></sec><sec id="sec4-sensors-18-03139"><title>4. Experimental Results</title><p>As shown in <xref rid="sensors-18-03139-t004" ref-type="table">Table 4</xref>, the overall classification accuracy is 85.27% and the Kappa coefficient is 0.8306. As for the misclassification condition, the accuracy of water, lotus pond and vegetation has even reached 96% and that of the single-season rice, watermelon greenhouse, bare soil and grassland also reaches 80%. However, the misclassification rate of two-season rice is even higher than 54%. This is because the two-season rice has similar spectral characteristics as the single-season rice and vegetation. The omission rates of water, watermelon greenhouse and lotus pond are lower than 10% and that of bare soil and grassland is also lower than 20%. Besides, the omission rates of two kinds of rice are higher than the above five species, around 25%. While the omission rates of the vegetation are both over 30%. Although PolSAR can distinguish rice in different growing seasons, the classification accuracy is low, since there are nearly 1/4 of the two-season rice was misclassified as single-season rice. This could be resulted from the small number of available data. If the multi-temporal images are available, such two kinds of rice could be distinguished with the temporal information. And the omitted vegetation pixels here are mainly classified as the two-season rice and grassland. The reason is that the vegetation mostly grows in undulated mountains, where the speckle noise is stronger in PolSAR images and reduce the classification accuracy.</p></sec><sec id="sec5-sensors-18-03139"><title>5. Discussion</title><sec id="sec5dot1-sensors-18-03139"><title>5.1. Comparison with Different Datasets</title><p>To validate the proposed full feature integration method, this section compares the results generated from the integrated data and that from single GF-3 data as well as from the single Sentinel-2A data (<xref ref-type="fig" rid="sensors-18-03139-f006">Figure 6</xref>). We also assessed the classification accuracies. The evaluated indicators are the rates of true positive (TP), false negative (FN), true negative (TN) and false positive (FP). These indicators can fairly evaluate result on each class no matter how many samples are used [<xref rid="B56-sensors-18-03139" ref-type="bibr">56</xref>]. We present these indicators by histograms. The sum of TP&#x02019;s rate and FN&#x02019;s rate equals to 1, which can be shown in one bar of the histogram (<xref ref-type="fig" rid="sensors-18-03139-f007">Figure 7</xref>). And the case is the same for the TN&#x02019;s rate and FP&#x02019;s rate (<xref ref-type="fig" rid="sensors-18-03139-f008">Figure 8</xref>). It can be seen that the overall classification accuracy of the integrated data is the highest, followed by the single optical data, then the single PolSAR data. The GF-3 PolSAR data alone can distinguish single-season rice from two-season rice but it will misclassify bare soil, grassland and watermelon greenhouse mainly with the surface scattering. While the Sentinel-2A data alone performs oppositely to GF-3 PolSAR data. It shows better classification ability for bare soil, grassland and watermelon greenhouse, because the spectral information of these three land covers varies greatly. But it cannot classify the single-season rice and two-season rice as well as the GF-3 data, providing a classification accuracy of two-season rice of as low as 28%. The proposed integration method takes the advantages of both two data, so the results have the highest classification accuracy.</p></sec><sec id="sec5dot2-sensors-18-03139"><title>5.2. Comparison with Different Feature Integration Modes</title><p>This section aims to validate the advantage of full feature integration proposed by this paper. Traditional data fusion methods think that both the intensity values of SAR data and the spectral information of optical data into classification at the same time, leading to data redundancy. But the intensity of SAR data is different from the spectral information of optical data. The former denotes the backscattering characteristics, whereas the latter denotes the reflection of sunlight. The classification results under different feature integration modes will be discussed and the details are shown in <xref rid="sensors-18-03139-t005" ref-type="table">Table 5</xref>. In this study, we used three feature integration modes, including (1) GF-3 features (<inline-formula><mml:math id="mm34"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula><sub>pca1</sub>, <inline-formula><mml:math id="mm35"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula><sub>pca2</sub>, RVI, <italic>P<sub>s</sub></italic>, <italic>P<sub>d</sub></italic>, <italic>P<sub>h</sub></italic> and <italic>P<sub>v</sub></italic>) + Sentinel-2A features (Opband<sub>pca1</sub>, Opband<sub>pca2</sub>, NDVI and H); (2) GF-3 features (<inline-formula><mml:math id="mm36"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula><sub>pca1</sub>, <inline-formula><mml:math id="mm37"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula><sub>pca2</sub>, RVI, <italic>P<sub>s</sub></italic>, <italic>P<sub>d</sub></italic>, <italic>P<sub>h</sub></italic> and <italic>P<sub>v</sub></italic>) + Sentinel-2A features (NDVI and H); (3) GF-3 features (RVI, <italic>P<sub>s</sub></italic>, <italic>P<sub>d</sub></italic>, <italic>P<sub>h</sub></italic> and <italic>P<sub>v</sub></italic>) + Sentinel-2A features (Opband<sub>pca1</sub>, Opband<sub>pca2</sub>, NDVI and H). The classification results are shown in <xref ref-type="fig" rid="sensors-18-03139-f009">Figure 9</xref> and the accuracy assessments are shown in <xref ref-type="fig" rid="sensors-18-03139-f010">Figure 10</xref> and <xref ref-type="fig" rid="sensors-18-03139-f011">Figure 11</xref>. It can be concluded that, the full feature integration method has achieved the highest overall classification accuracy and larger Kappa coefficient. It is mainly owing to the improvement of the classification accuracy of vegetation and grassland. And the involvement of more features makes the classification more accurate and stable. In addition, it can be seen that when the PolSAR features are more involved (GF-3 (7 bands) + S2A (2 bands)), the classification accuracy of single-season rice and two-season rice is increased. However, when more optical features are involved (GF-3 (5 bands) + S2A (4 bands)), the classification accuracy of bare soil and watermelon greenhouse is improved. So, this conclusion is consistent with that of last section. To sum up, the full feature integration method proposed in this paper can get a higher classification accuracy.</p></sec><sec id="sec5dot3-sensors-18-03139"><title>5.3. Classification Ability of <inline-formula><mml:math id="mm42"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula></title><p>The Wishart supervised classification based on the covariance matrix <inline-formula><mml:math id="mm43"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> or the coherency matrix <inline-formula><mml:math id="mm44"><mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> has be widely used. In this study, we substituted the intensity vector <inline-formula><mml:math id="mm45"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> for covariance matrix to adapt to the SVM classifier. Input variables of the SVM classifier should be multiple independent bands. Hoekman has proved that the intensity vector <inline-formula><mml:math id="mm46"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> can represent the full polarimetric target characteristics by a covariance matrix [<xref rid="B1-sensors-18-03139" ref-type="bibr">1</xref>] and <inline-formula><mml:math id="mm47"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is more suitable to crop classification, because it can describe the biophysical parameter variations of crops. To clarify this point, we compare three polarimetric classification methods, including (1) Wishart supervised classification with <inline-formula><mml:math id="mm48"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, (2) SVM classification with <inline-formula><mml:math id="mm49"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and (3) SVM classification with the first two PCA components of <inline-formula><mml:math id="mm50"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. The results are presented in <xref ref-type="fig" rid="sensors-18-03139-f012">Figure 12</xref>. As the figure shows, the SVM classification with <inline-formula><mml:math id="mm51"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> has the highest overall accuracy and kappa coefficient in all methods. We also calculated the rates of TP, FN, TN and FP and made a comparison (<xref ref-type="fig" rid="sensors-18-03139-f013">Figure 13</xref> and <xref ref-type="fig" rid="sensors-18-03139-f014">Figure 14</xref>). The comparison shows that the SVM method with <inline-formula><mml:math id="mm52"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> performs better than the Wishart supervised method in most land covers but the Wishart method has the best performance in the watermelon greenhouse and the forest region among these three methods. The crop classification results of the SVM classification with <inline-formula><mml:math id="mm53"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> has the highest accuracy, verifying Hoekman&#x02019;s theory that <inline-formula><mml:math id="mm54"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is more suitable to describe crops. And for the crops, the first two PCA components of <inline-formula><mml:math id="mm55"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can achieve similar classification results as the whole <inline-formula><mml:math id="mm56"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. We can conclude that the intensity vector and its PCA components can be successfully applied into the polarimetric classification and get better results than the Wishart supervised classification in most crop cases.</p></sec></sec><sec id="sec6-sensors-18-03139"><title>6. Conclusions</title><p>The GF-3 PolSAR data is sensitive to the change of morphological structure during crop growth, whereas the Sentinel-2A optical data can show the change of moisture and chlorophyll content in crop leaves well. Integrating such two kinds of data can improve the accuracy of crop classification. However, some useful features cannot be used in the classification at the same time. Particularly, the covariance matrix of PolSAR data is hard to be combined with the spectral bands of optical data. To solve this problem, we used the Hoekman&#x02019;s method to transform the covariance matrix to an intensity vector. The PCA algorithm was applied to reduce the redundancy of feature sets. Then, the training samples were selected to do the SVM classification. The classification accuracy of the proposed method is higher than that of single data set method and other two feature integration modes and the intensity vector has a better performance than the covariance matrix for crop classification. In total, full feature integration method proposed by this paper is suitable for crop classification and can effectively improve the classification accuracy. Furthermore, this paper expands the application of GF-3 satellite in agriculture, proving the great potential in monitoring crops.</p></sec></body><back><ack><title>Acknowledgments</title><p>The Sentinel-2A data were downloaded from the Copernicus Open Access Hub website: <uri xlink:href="https://scihub.copernicus.eu/dhus">https://scihub.copernicus.eu/dhus</uri>.</p></ack><notes><title>Author Contributions</title><p>H.G. performed the experiments, wrote the paper; C.W. contributed the ideas, analyzed the experimental results and revised the paper; G.W. analyzed the experimental results and revised the paper; J.Z., Y.T., P.S. and Z.Z. contributed discussions for the results and revised the paper.</p></notes><notes><title>Funding</title><p>The work was supported by the National Natural Science Foundation of China (Nos. 41531068, 41671356 and 41371335), the Natural Science Foundation of Hunan Province, China (No. 2016JJ2141). The GF-3 data were provided by the National Satellite Ocean Application Service (NSOAS), China.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-18-03139"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoekman</surname><given-names>D.H.</given-names></name><name><surname>Vissers</surname><given-names>M.A.M.</given-names></name></person-group><article-title>A new polarimetric classification approach evaluated for agricultural crops</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2003</year><volume>41</volume><fpage>2881</fpage><lpage>2889</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2003.817795</pub-id></element-citation></ref><ref id="B2-sensors-18-03139"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haboudane</surname><given-names>D.</given-names></name><name><surname>Tremblay</surname><given-names>N.</given-names></name><name><surname>Miller</surname><given-names>J.R.</given-names></name><name><surname>Vigneault</surname><given-names>P.</given-names></name></person-group><article-title>Remote Estimation of Crop Chlorophyll Content Using Spectral Indices Derived From Hyperspectral Data</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2008</year><volume>46</volume><fpage>423</fpage><lpage>437</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2007.904836</pub-id></element-citation></ref><ref id="B3-sensors-18-03139"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cloutis</surname><given-names>E.A.</given-names></name><name><surname>Connery</surname><given-names>D.R.</given-names></name><name><surname>Major</surname><given-names>D.J.</given-names></name><name><surname>Dover</surname><given-names>F.J.</given-names></name></person-group><article-title>Airborne multi-spectral monitoring of agricultural crop status: Effect of time of year, crop type and crop condition parameter</article-title><source>Int. J. Remote Sens.</source><year>1996</year><volume>17</volume><fpage>2579</fpage><lpage>2601</lpage><pub-id pub-id-type="doi">10.1080/01431169608949094</pub-id></element-citation></ref><ref id="B4-sensors-18-03139"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steele-Dunne</surname><given-names>S.C.</given-names></name><name><surname>Mcnairn</surname><given-names>H.</given-names></name><name><surname>Monsivais-Huertero</surname><given-names>A.</given-names></name><name><surname>Judge</surname><given-names>J.</given-names></name><name><surname>Liu</surname><given-names>P.W.</given-names></name><name><surname>Papathanassiou</surname><given-names>K.</given-names></name></person-group><article-title>Radar Remote Sensing of Agricultural Canopies: A Review</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2017</year><volume>10</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2016.2639043</pub-id></element-citation></ref><ref id="B5-sensors-18-03139"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mcdonald</surname><given-names>A.J.</given-names></name><name><surname>Bennett</surname><given-names>J.C.</given-names></name><name><surname>Cookmartin</surname><given-names>G.</given-names></name><name><surname>Crossley</surname><given-names>S.</given-names></name><name><surname>Morrison</surname><given-names>K.</given-names></name><name><surname>Quegan</surname><given-names>S.</given-names></name></person-group><article-title>The effect of leaf geometry on the microwave backscatter from leaves</article-title><source>Int. J. Remote Sens.</source><year>2000</year><volume>21</volume><fpage>395</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1080/014311600210911</pub-id></element-citation></ref><ref id="B6-sensors-18-03139"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karam</surname><given-names>M.A.</given-names></name><name><surname>Fung</surname><given-names>A.K.</given-names></name><name><surname>Lang</surname><given-names>R.H.</given-names></name><name><surname>Chauhan</surname><given-names>N.S.</given-names></name></person-group><article-title>A Microwave Scattering Model for Layered Vegetation</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>1992</year><volume>30</volume><fpage>767</fpage><lpage>784</lpage><pub-id pub-id-type="doi">10.1109/36.158872</pub-id></element-citation></ref><ref id="B7-sensors-18-03139"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Q.</given-names></name><name><surname>Ballester-Berman</surname><given-names>J.</given-names></name><name><surname>Lopez-Sanchez</surname><given-names>J.</given-names></name><name><surname>Zhu</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>C.</given-names></name></person-group><article-title>On the Use of Generalized Volume Scattering Models for the Improvement of General Polarimetric Model-Based Decomposition</article-title><source>Remote Sens.</source><year>2017</year><volume>2</volume><elocation-id>117</elocation-id><pub-id pub-id-type="doi">10.3390/rs9020117</pub-id></element-citation></ref><ref id="B8-sensors-18-03139"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H.</given-names></name><name><surname>Wang</surname><given-names>C.</given-names></name><name><surname>Zhu</surname><given-names>J.</given-names></name><name><surname>Fu</surname><given-names>H.</given-names></name><name><surname>Xie</surname><given-names>Q.</given-names></name><name><surname>Shen</surname><given-names>P.</given-names></name></person-group><article-title>Forest Above-Ground Biomass Estimation Using Single-Baseline Polarization Coherence Tomography with P-Band PolInSAR Data</article-title><source>Forests</source><year>2018</year><volume>9</volume><elocation-id>163</elocation-id><pub-id pub-id-type="doi">10.3390/f9040163</pub-id></element-citation></ref><ref id="B9-sensors-18-03139"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>X.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>C.</given-names></name><name><surname>Fu</surname><given-names>H.</given-names></name><name><surname>Du</surname><given-names>Y.</given-names></name></person-group><article-title>A Maximum Likelihood Based Nonparametric Iterative Adaptive Method of Synthetic Aperture Radar Tomography and Its Application for Estimating Underlying Topography and Forest Height</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>2459</elocation-id><pub-id pub-id-type="doi">10.3390/s18082459</pub-id><?supplied-pmid 30061478?><pub-id pub-id-type="pmid">30061478</pub-id></element-citation></ref><ref id="B10-sensors-18-03139"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C.</given-names></name></person-group><article-title>Mapping paddy rice with multitemporal ALOS/PALSAR imagery in southeast China</article-title><source>Int. J. Remote Sens.</source><year>2009</year><volume>30</volume><fpage>6301</fpage><lpage>6315</lpage></element-citation></ref><ref id="B11-sensors-18-03139"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skriver</surname><given-names>H.</given-names></name></person-group><article-title>Crop Classification by Multitemporal C- and L-Band Single- and Dual-Polarization and Fully Polarimetric SAR</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2012</year><volume>50</volume><fpage>2138</fpage><lpage>2149</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2011.2172994</pub-id></element-citation></ref><ref id="B12-sensors-18-03139"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoekman</surname><given-names>D.H.</given-names></name><name><surname>Vissers</surname><given-names>M.A.M.</given-names></name><name><surname>Tran</surname><given-names>T.N.</given-names></name></person-group><article-title>Unsupervised Full-Polarimetric SAR Data Segmentation as a Tool for Classification of Agricultural Areas</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2011</year><volume>4</volume><fpage>402</fpage><lpage>411</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2010.2042280</pub-id></element-citation></ref><ref id="B13-sensors-18-03139"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cloude</surname><given-names>S.R.</given-names></name><name><surname>Pottier</surname><given-names>E.</given-names></name></person-group><article-title>An entropy based classification scheme for land applications of polarimetric SAR</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>1997</year><volume>35</volume><fpage>68</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1109/36.551935</pub-id></element-citation></ref><ref id="B14-sensors-18-03139"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ainsworth</surname><given-names>T.L.</given-names></name><name><surname>Kelly</surname><given-names>J.P.</given-names></name><name><surname>Lee</surname><given-names>J.S.</given-names></name></person-group><article-title>Classification comparisons between dual-pol, compact polarimetric and quad-pol SAR imagery</article-title><source>ISPRS J. Photogramm. Remote Sens.</source><year>2009</year><volume>64</volume><fpage>464</fpage><lpage>471</lpage><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2008.12.008</pub-id></element-citation></ref><ref id="B15-sensors-18-03139"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>W.</given-names></name><name><surname>Yang</surname><given-names>J.</given-names></name><name><surname>Ma</surname><given-names>W.</given-names></name></person-group><article-title>Land Cover Classification for Polarimetric SAR Images Based on Mixture Models</article-title><source>Remote Sens.</source><year>2014</year><volume>6</volume><fpage>3770</fpage><lpage>3790</lpage><pub-id pub-id-type="doi">10.3390/rs6053770</pub-id></element-citation></ref><ref id="B16-sensors-18-03139"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sonobe</surname><given-names>R.</given-names></name><name><surname>Tani</surname><given-names>H.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Kobayashi</surname><given-names>N.</given-names></name><name><surname>Shimamura</surname><given-names>H.</given-names></name></person-group><article-title>Discrimination of crop types with TerraSAR-X-derived information</article-title><source>Phys. Chem. Earth Parts A/B/C</source><year>2015</year><volume>83&#x02013;84</volume><fpage>2</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1016/j.pce.2014.11.001</pub-id></element-citation></ref><ref id="B17-sensors-18-03139"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiao</surname><given-names>X.</given-names></name><name><surname>Kovacs</surname><given-names>J.M.</given-names></name><name><surname>Shang</surname><given-names>J.</given-names></name><name><surname>Mcnairn</surname><given-names>H.</given-names></name><name><surname>Dan</surname><given-names>W.</given-names></name><name><surname>Ma</surname><given-names>B.</given-names></name><name><surname>Geng</surname><given-names>X.</given-names></name></person-group><article-title>Object-oriented crop mapping and monitoring using multi-temporal polarimetric RADARSAT-2 data</article-title><source>ISPRS J. Photogramm. Remote Sens.</source><year>2014</year><volume>96</volume><fpage>38</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2014.06.014</pub-id></element-citation></ref><ref id="B18-sensors-18-03139"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skriver</surname><given-names>H.</given-names></name><name><surname>Mattia</surname><given-names>F.</given-names></name><name><surname>Satalino</surname><given-names>G.</given-names></name><name><surname>Balenzano</surname><given-names>A.</given-names></name><name><surname>Pauwels</surname><given-names>V.R.N.</given-names></name><name><surname>Verhoest</surname><given-names>N.E.C.</given-names></name><name><surname>Davidson</surname><given-names>M.</given-names></name></person-group><article-title>Crop Classification Using Short-Revisit Multitemporal SAR Data</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2011</year><volume>4</volume><fpage>423</fpage><lpage>431</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2011.2106198</pub-id></element-citation></ref><ref id="B19-sensors-18-03139"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>H.</given-names></name><name><surname>Xu</surname><given-names>F.</given-names></name><name><surname>Jin</surname><given-names>Y.Q.</given-names></name></person-group><article-title>Polarimetric SAR Image Classification Using Deep Convolutional Neural Networks</article-title><source>IEEE Geosci. Remote Sens. Lett.</source><year>2017</year><volume>13</volume><fpage>1935</fpage><lpage>1939</lpage><pub-id pub-id-type="doi">10.1109/LGRS.2016.2618840</pub-id></element-citation></ref><ref id="B20-sensors-18-03139"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>Y.</given-names></name><name><surname>Guan</surname><given-names>K.</given-names></name><name><surname>Peng</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>S.</given-names></name><name><surname>Seifert</surname><given-names>C.</given-names></name><name><surname>Wardlow</surname><given-names>B.</given-names></name><name><surname>Li</surname><given-names>Z.</given-names></name></person-group><article-title>A high-performance and in-season classification system of field-level crop types using time-series Landsat data and a machine learning approach</article-title><source>Remote Sens. Environ.</source><year>2018</year><volume>210</volume><fpage>35</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1016/j.rse.2018.02.045</pub-id></element-citation></ref><ref id="B21-sensors-18-03139"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Massey</surname><given-names>R.</given-names></name><name><surname>Sankey</surname><given-names>T.T.</given-names></name><name><surname>Congalton</surname><given-names>R.G.</given-names></name><name><surname>Yadav</surname><given-names>K.</given-names></name><name><surname>Thenkabail</surname><given-names>P.S.</given-names></name><name><surname>Ozdogan</surname><given-names>M.</given-names></name><name><surname>S&#x000e1;nchez Meador</surname><given-names>A.J.</given-names></name></person-group><article-title>MODIS phenology-derived, multi-year distribution of conterminous U.S. crop types</article-title><source>Remote Sens. Environ.</source><year>2017</year><volume>198</volume><fpage>490</fpage><lpage>503</lpage><pub-id pub-id-type="doi">10.1016/j.rse.2017.06.033</pub-id></element-citation></ref><ref id="B22-sensors-18-03139"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>F.</given-names></name><name><surname>Anderson</surname><given-names>M.C.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Yang</surname><given-names>Z.</given-names></name><name><surname>Alfieri</surname><given-names>J.G.</given-names></name><name><surname>Kustas</surname><given-names>W.P.</given-names></name><name><surname>Mueller</surname><given-names>R.</given-names></name><name><surname>Johnson</surname><given-names>D.M.</given-names></name><name><surname>Prueger</surname><given-names>J.H.</given-names></name></person-group><article-title>Toward mapping crop progress at field scales through fusion of Landsat and MODIS imagery</article-title><source>Remote Sens. Environ.</source><year>2017</year><volume>188</volume><fpage>9</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1016/j.rse.2016.11.004</pub-id></element-citation></ref><ref id="B23-sensors-18-03139"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wardlow</surname><given-names>B.D.</given-names></name><name><surname>Egbert</surname><given-names>S.L.</given-names></name><name><surname>Kastens</surname><given-names>J.H.</given-names></name></person-group><article-title>Analysis of time-series MODIS 250 m vegetation index data for crop classification in the U.S. Central Great Plains</article-title><source>Remote Sens. Environ.</source><year>2007</year><volume>108</volume><fpage>290</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.1016/j.rse.2006.11.021</pub-id></element-citation></ref><ref id="B24-sensors-18-03139"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonneaux</surname><given-names>V.</given-names></name><name><surname>Duchemin</surname><given-names>B.</given-names></name><name><surname>Helson</surname><given-names>D.</given-names></name><name><surname>Er-Raki</surname><given-names>S.</given-names></name><name><surname>Olioso</surname><given-names>A.</given-names></name><name><surname>Chehbouni</surname><given-names>A.G.</given-names></name></person-group><article-title>The use of high-resolution image time series for crop classification and evapotranspiration estimate over an irrigated area in central Morocco</article-title><source>Int. J. Remote Sens.</source><year>2008</year><volume>29</volume><fpage>95</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1080/01431160701250390</pub-id></element-citation></ref><ref id="B25-sensors-18-03139"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blaes</surname><given-names>X.</given-names></name><name><surname>Vanhalle</surname><given-names>L.</given-names></name><name><surname>Defourny</surname><given-names>P.</given-names></name></person-group><article-title>Efficiency of crop identification based on optical and SAR image time series</article-title><source>Remote Sens. Environ.</source><year>2005</year><volume>96</volume><fpage>352</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1016/j.rse.2005.03.010</pub-id></element-citation></ref><ref id="B26-sensors-18-03139"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kussul</surname><given-names>N.</given-names></name><name><surname>Skakun</surname><given-names>S.</given-names></name><name><surname>Shelestov</surname><given-names>A.</given-names></name><name><surname>Kravchenko</surname><given-names>O.</given-names></name><name><surname>Kussul</surname><given-names>O.</given-names></name></person-group><article-title>Crop Classification in Ukraine Using Satellite Optical and SAR Images</article-title><source>Int. J. Inf. Model Anal.</source><year>2013</year><volume>2</volume><fpage>118</fpage><lpage>122</lpage></element-citation></ref><ref id="B27-sensors-18-03139"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haldar</surname><given-names>D.</given-names></name><name><surname>Patnaik</surname><given-names>C.</given-names></name></person-group><article-title>Synergistic use of multi-temporal Radarsat SAR and AWiFS data for Rabi rice identification</article-title><source>J. Indian Soc. Remote Sens.</source><year>2010</year><volume>38</volume><fpage>153</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1007/s12524-010-0006-x</pub-id></element-citation></ref><ref id="B28-sensors-18-03139"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dong</surname><given-names>J.</given-names></name><name><surname>Xiao</surname><given-names>X.</given-names></name><name><surname>Chen</surname><given-names>B.</given-names></name><name><surname>Torbick</surname><given-names>N.</given-names></name><name><surname>Jin</surname><given-names>C.</given-names></name><name><surname>Zhang</surname><given-names>G.</given-names></name><name><surname>Biradar</surname><given-names>C.</given-names></name></person-group><article-title>Mapping deciduous rubber plantations through integration of PALSAR and multi-temporal Landsat imagery</article-title><source>Remote Sens. Environ.</source><year>2013</year><volume>134</volume><fpage>392</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1016/j.rse.2013.03.014</pub-id></element-citation></ref><ref id="B29-sensors-18-03139"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waske</surname><given-names>B.</given-names></name><name><surname>Linden</surname><given-names>S.V.D.</given-names></name></person-group><article-title>Classifying Multilevel Imagery From SAR and Optical Sensors by Decision Fusion</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2008</year><volume>46</volume><fpage>1457</fpage><lpage>1466</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2008.916089</pub-id></element-citation></ref><ref id="B30-sensors-18-03139"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNairn</surname><given-names>H.</given-names></name><name><surname>Champagne</surname><given-names>C.</given-names></name><name><surname>Shang</surname><given-names>J.</given-names></name><name><surname>Holmstrom</surname><given-names>D.</given-names></name><name><surname>Reichert</surname><given-names>G.</given-names></name></person-group><article-title>Integration of optical and Synthetic Aperture Radar (SAR) imagery for delivering operational annual crop inventories</article-title><source>ISPRS J. Photogramm. Remote Sens.</source><year>2009</year><volume>64</volume><fpage>434</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2008.07.006</pub-id></element-citation></ref><ref id="B31-sensors-18-03139"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ianninia</surname><given-names>L.</given-names></name></person-group><article-title>Integration of multispectral and C-band SAR data for crop classification</article-title><source>Proc. SPIE</source><year>2013</year><volume>8887</volume><pub-id pub-id-type="doi">10.1117/12.2029330</pub-id></element-citation></ref><ref id="B32-sensors-18-03139"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Qiao</surname><given-names>C.</given-names></name><name><surname>Daneshfar</surname><given-names>B.</given-names></name><name><surname>Davidson</surname><given-names>A.</given-names></name><name><surname>Jarvis</surname><given-names>I.</given-names></name><name><surname>Liu</surname><given-names>T.</given-names></name><name><surname>Fisette</surname><given-names>T.</given-names></name></person-group><article-title>Integration of Optical and Polarimetric SAR Imagery for Locally Accurate Crop Classification</article-title><source>Proceedings of the Geoscience and Remote Sensing Symposium</source><conf-loc>Quebec City, QC, Canada</conf-loc><conf-date>13&#x02013;18 July 2014</conf-date><fpage>1485</fpage><lpage>1488</lpage></element-citation></ref><ref id="B33-sensors-18-03139"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turhan-Sayan</surname><given-names>G.</given-names></name></person-group><article-title>Real time electromagnetic target classification using a novel feature extraction technique with PCA-based fusion</article-title><source>IEEE Trans. Antenna Propag.</source><year>2005</year><volume>53</volume><fpage>766</fpage><lpage>776</lpage><pub-id pub-id-type="doi">10.1109/TAP.2004.841326</pub-id></element-citation></ref><ref id="B34-sensors-18-03139"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Understanding image fusion</article-title><source>Photogramm. Eng. Remote Sens.</source><year>2004</year><volume>70</volume><fpage>657</fpage><lpage>661</lpage></element-citation></ref><ref id="B35-sensors-18-03139"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>S.J.</given-names></name><name><surname>Qin</surname><given-names>Q.M.</given-names></name><name><surname>Wang</surname><given-names>W.J.</given-names></name></person-group><article-title>An Improvement of Brovey RS Image Fusion by Using Wavelet Signal Analysis</article-title><source>J. Inst. Surv. Mapp.</source><year>2004</year><volume>21</volume><fpage>118</fpage><lpage>120</lpage><comment>(in Chinese with English abstract)</comment></element-citation></ref><ref id="B36-sensors-18-03139"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cakir</surname><given-names>H.I.</given-names></name><name><surname>Khorram</surname><given-names>S.</given-names></name></person-group><article-title>Pixel Level Fusion of Panchromatic and Multispectral Images Based on Correspondence Analysis</article-title><source>Photogramm. Eng. Remote Sens.</source><year>2008</year><volume>74</volume><fpage>183</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.14358/PERS.74.2.183</pub-id></element-citation></ref><ref id="B37-sensors-18-03139"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tao</surname><given-names>Z.</given-names></name><name><surname>Liu</surname><given-names>J.</given-names></name><name><surname>Yang</surname><given-names>K.</given-names></name><name><surname>Luo</surname><given-names>W.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Fusion Algorithm for Hyperspectral Remote Sensing Image Combined with Harmonic Analysis and Gram-Schmidt Transform</article-title><source>Acta Geod. Cartogr. Sin.</source><year>2015</year><volume>44</volume><fpage>1042</fpage><lpage>1047</lpage></element-citation></ref><ref id="B38-sensors-18-03139"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>H.-Y.</given-names></name><name><surname>Yan</surname><given-names>B.K.</given-names></name><name><surname>Gan</surname><given-names>F.P.</given-names></name><name><surname>Chi</surname><given-names>W.X.</given-names></name><name><surname>Wu</surname><given-names>F.-D.</given-names></name></person-group><article-title>Hyperspectral Image Fusion by an Enhanced Gram Schmidt Spectral Transformation</article-title><source>Geogr. Geo-Inf. Sci.</source><year>2007</year><volume>23</volume><fpage>39</fpage><lpage>42</lpage><comment>(In Chinese with English Abstract)</comment></element-citation></ref><ref id="B39-sensors-18-03139"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Marcelino</surname><given-names>E.V.</given-names></name><name><surname>Fonseca</surname><given-names>L.M.G.</given-names></name><name><surname>Ventura</surname><given-names>F.</given-names></name><name><surname>Rosa</surname><given-names>A.</given-names></name></person-group><article-title>Evaluation of IHS, PCA and wavelet transform fusion techniques for the identification of landslide scars using satellite data</article-title><source>Proceedings of the IX Simp&#x000f3;sio Brasileiro de Sensoriamento Remoto</source><conf-loc>Belo Horizonte, Brazil</conf-loc><conf-date>5&#x02013;10 April 2003</conf-date><fpage>487</fpage><lpage>494</lpage></element-citation></ref><ref id="B40-sensors-18-03139"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mandhare</surname><given-names>R.A.</given-names></name><name><surname>Upadhyay</surname><given-names>P.</given-names></name><name><surname>Gupta</surname><given-names>S.</given-names></name></person-group><article-title>Pixel-Level Image Fusion Using Brovey Transforme and Wavelet Transform</article-title><source>Int. J. Adv. Res. Electr. Electron. Instrum. Eng.</source><year>2013</year><volume>2</volume><fpage>2690</fpage><lpage>2695</lpage></element-citation></ref><ref id="B41-sensors-18-03139"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J.</given-names></name></person-group><article-title>Multi-source remote sensing data fusion: Status and trends</article-title><source>Int. J. Image Data Fusion</source><year>2010</year><volume>1</volume><fpage>5</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1080/19479830903561035</pub-id></element-citation></ref><ref id="B42-sensors-18-03139"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>H.Q.</given-names></name><name><surname>Zhu</surname><given-names>J.J.</given-names></name><name><surname>Wang</surname><given-names>C.C.</given-names></name><name><surname>Wang</surname><given-names>H.Q.</given-names></name><name><surname>Zhao</surname><given-names>R.</given-names></name></person-group><article-title>A Wavelet Decomposition and Polynomial Fitting-Based Method for the Estimation of Time-Varying Residual Motion Error in Airborne Interferometric SAR</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2018</year><volume>56</volume><fpage>49</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2017.2727076</pub-id></element-citation></ref><ref id="B43-sensors-18-03139"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oliveirapereira</surname><given-names>L.</given-names></name><name><surname>Costafreitas</surname><given-names>C.</given-names></name><name><surname>Lu</surname><given-names>D.</given-names></name><name><surname>Moran</surname><given-names>E.</given-names></name></person-group><article-title>Optical and radar data integration for land use and land cover mapping in the Brazilian Amazon</article-title><source>Mapp. Sci. Remote Sens.</source><year>2013</year><volume>50</volume><fpage>301</fpage><lpage>321</lpage></element-citation></ref><ref id="B44-sensors-18-03139"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frery</surname><given-names>A.C.</given-names></name><name><surname>Correia</surname><given-names>A.H.</given-names></name><name><surname>Freitas</surname><given-names>C.D.C.</given-names></name></person-group><article-title>Classifying Multifrequency Fully Polarimetric Imagery With Multiple Sources of Statistical Evidence and Contextual Information</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2007</year><volume>45</volume><fpage>3098</fpage><lpage>3109</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2007.903828</pub-id></element-citation></ref><ref id="B45-sensors-18-03139"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waske</surname><given-names>B.</given-names></name><name><surname>Benediktsson</surname><given-names>J.A.</given-names></name></person-group><article-title>Fusion of Support Vector Machines for Classification of Multisensor Data</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2007</year><volume>45</volume><fpage>3858</fpage><lpage>3866</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2007.898446</pub-id></element-citation></ref><ref id="B46-sensors-18-03139"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simone</surname><given-names>G.</given-names></name><name><surname>Farina</surname><given-names>A.</given-names></name><name><surname>Morabito</surname><given-names>F.C.</given-names></name><name><surname>Serpico</surname><given-names>S.B.</given-names></name><name><surname>Bruzzone</surname><given-names>L.</given-names></name></person-group><article-title>Image fusion techniques for remote sensing applications</article-title><source>Inf. Fusion</source><year>2002</year><volume>3</volume><fpage>3</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1016/S1566-2535(01)00056-2</pub-id></element-citation></ref><ref id="B47-sensors-18-03139"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Lin</surname><given-names>Z.</given-names></name><name><surname>Zhao</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>G.</given-names></name><name><surname>Gu</surname><given-names>Y.</given-names></name></person-group><article-title>Deep Learning-Based Classification of Hyperspectral Data</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2017</year><volume>7</volume><fpage>2094</fpage><lpage>2107</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2014.2329330</pub-id></element-citation></ref><ref id="B48-sensors-18-03139"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Q.</given-names></name></person-group><article-title>System Design and Key Technologies of the GF-3 Satellite</article-title><source>Acta Geod. Cartogr. Sin.</source><year>2017</year><pub-id pub-id-type="doi">10.11947/j.AGCS.2017.20170049</pub-id></element-citation></ref><ref id="B49-sensors-18-03139"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Congalton</surname><given-names>R.G.</given-names></name></person-group><article-title>A review of assessing the accuracy of classifications of remotely sensed data</article-title><source>Remote Sens. Environ.</source><year>1991</year><volume>37</volume><fpage>35</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1016/0034-4257(91)90048-B</pub-id></element-citation></ref><ref id="B50-sensors-18-03139"><label>50.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Congalton</surname><given-names>R.G.</given-names></name><name><surname>Green</surname><given-names>K.</given-names></name></person-group><source>Assessing the Accuracy of Remotely Sensed Data: Principles and Practices</source><publisher-name>CRC Press</publisher-name><publisher-loc>Boca Raton, FL, USA</publisher-loc><year>2008</year></element-citation></ref><ref id="B51-sensors-18-03139"><label>51.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deledalle</surname><given-names>C.A.</given-names></name><name><surname>Tupin</surname><given-names>F.</given-names></name><name><surname>Denis</surname><given-names>L.</given-names></name></person-group><article-title>Polarimetric SAR estimation based on non-local means</article-title><source>Proceedings of the Geoscience and Remote Sensing Symposium</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>25&#x02013;30 July 2010</conf-date><fpage>2515</fpage><lpage>2518</lpage></element-citation></ref><ref id="B52-sensors-18-03139"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>P.</given-names></name><name><surname>Wang</surname><given-names>C.</given-names></name><name><surname>Gao</surname><given-names>H.</given-names></name><name><surname>Zhu</surname><given-names>J.</given-names></name></person-group><article-title>An Adaptive Nonlocal Mean Filter for PolSAR Data with Shape-Adaptive Patches Matching</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>2215</elocation-id><pub-id pub-id-type="doi">10.3390/s18072215</pub-id><?supplied-pmid 29996522?><pub-id pub-id-type="pmid">29996522</pub-id></element-citation></ref><ref id="B53-sensors-18-03139"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamaguchi</surname><given-names>Y.</given-names></name><name><surname>Moriyama</surname><given-names>T.</given-names></name><name><surname>Ishido</surname><given-names>M.</given-names></name><name><surname>Yamada</surname><given-names>H.</given-names></name></person-group><article-title>Four-component scattering model for polarimetric SAR image decomposition</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2005</year><volume>104</volume><fpage>1699</fpage><lpage>1706</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2005.852084</pub-id></element-citation></ref><ref id="B54-sensors-18-03139"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>Y.</given-names></name><name><surname>Zyl</surname><given-names>J.J.V.</given-names></name></person-group><article-title>A Time-Series Approach to Estimate Soil Moisture Using Polarimetric Radar Data</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2009</year><volume>47</volume><fpage>2519</fpage><lpage>2527</lpage></element-citation></ref><ref id="B55-sensors-18-03139"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Z.</given-names></name><name><surname>Huang</surname><given-names>P.</given-names></name></person-group><article-title>Quantitative measures for spatial information of maps</article-title><source>Int. J. Geogr. Inf. Syst.</source><year>2002</year><volume>16</volume><fpage>699</fpage><lpage>709</lpage><pub-id pub-id-type="doi">10.1080/13658810210149416</pub-id></element-citation></ref><ref id="B56-sensors-18-03139"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fawcett</surname><given-names>T.</given-names></name></person-group><article-title>An introduction to ROC analysis</article-title><source>Pattern Recognit. Lett.</source><year>2006</year><volume>27</volume><fpage>861</fpage><lpage>874</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2005.10.010</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="sensors-18-03139-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>The location of the study area and the used data coverage, the yellow and orange rectangle denotes the GF-3 PolSAR data and the Sentinel-2A optical data, respectively. The red rectangles outline the experimental area.</p></caption><graphic xlink:href="sensors-18-03139-g001"/></fig><fig id="sensors-18-03139-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>The training (<bold>left</bold>) and testing (<bold>right</bold>) samples in the study area.</p></caption><graphic xlink:href="sensors-18-03139-g002"/></fig><fig id="sensors-18-03139-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>The flowchart of the proposed method.</p></caption><graphic xlink:href="sensors-18-03139-g003"/></fig><fig id="sensors-18-03139-f004" orientation="portrait" position="float"><label>Figure 4</label><caption><p>The registration process of the proposed method.</p></caption><graphic xlink:href="sensors-18-03139-g004"/></fig><fig id="sensors-18-03139-f005" orientation="portrait" position="float"><label>Figure 5</label><caption><p>The features normalized to the range of [0,1]. (<bold>a</bold>&#x02013;<bold>d</bold>) Opband<sub>pca1</sub>, Opband<sub>pca2</sub>, NDVI and the information entropy H extracted from the Sentinel-2A data; (<bold>e</bold>&#x02013;<bold>k</bold>) <inline-formula><mml:math id="mm32"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula><sub>pca1</sub>, <inline-formula><mml:math id="mm33"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula><sub>pca2</sub>, RVI, <italic>P<sub>s</sub></italic>, <italic>P<sub>d</sub></italic>, <italic>P<sub>h</sub></italic> and <italic>P<sub>v</sub></italic> extracted from the GF-3 PolSAR data.</p></caption><graphic xlink:href="sensors-18-03139-g005"/></fig><fig id="sensors-18-03139-f006" orientation="portrait" position="float"><label>Figure 6</label><caption><p>The classification results generated from (<bold>a</bold>) the integrated data (<bold>b</bold>) the GF-3 data and (<bold>c</bold>) the Sentinel-2A data; (<bold>d</bold>) the testing sample.</p></caption><graphic xlink:href="sensors-18-03139-g006"/></fig><fig id="sensors-18-03139-f007" orientation="portrait" position="float"><label>Figure 7</label><caption><p>True positive (TP) rates and false negative (FN) rates of different land covers from different datasets. Wm means watermelon.</p></caption><graphic xlink:href="sensors-18-03139-g007"/></fig><fig id="sensors-18-03139-f008" orientation="portrait" position="float"><label>Figure 8</label><caption><p>True negative (TN) rates and false positive (FP) rates of different plants from different datasets. Wm means watermelon.</p></caption><graphic xlink:href="sensors-18-03139-g008"/></fig><fig id="sensors-18-03139-f009" orientation="portrait" position="float"><label>Figure 9</label><caption><p>The classification results of (<bold>a</bold>) integration with of all features of dataset; (<bold>b</bold>) GF-3 (7 bands) + S2A (2 bands) and (<bold>c</bold>) denotes GF-3 (5 bands) + S2A (4 bands); (<bold>d</bold>) the testing sample.</p></caption><graphic xlink:href="sensors-18-03139-g009"/></fig><fig id="sensors-18-03139-f010" orientation="portrait" position="float"><label>Figure 10</label><caption><p>The true positive (TP) rates and the false negative (FN) rates of different land covers generated from different combination of features. Wm means watermelon.</p></caption><graphic xlink:href="sensors-18-03139-g010"/></fig><fig id="sensors-18-03139-f011" orientation="portrait" position="float"><label>Figure 11</label><caption><p>The true negative (TN) rates and the false positive (FP) rates of different land covers generated from different combination of features. Wm means watermelon.</p></caption><graphic xlink:href="sensors-18-03139-g011"/></fig><fig id="sensors-18-03139-f012" orientation="portrait" position="float"><label>Figure 12</label><caption><p>The classification results of different polarimetric classification methods. (<bold>a</bold>) the Wishart supervised classification with <inline-formula><mml:math id="mm57"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>; (<bold>b</bold>) the SVM classification with <inline-formula><mml:math id="mm58"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and (<bold>c</bold>) the SVM classification with the first two PCA components of <inline-formula><mml:math id="mm59"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003c3;</mml:mi><mml:mo stretchy="false">&#x021c0;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>; (<bold>d</bold>) the testing sample.</p></caption><graphic xlink:href="sensors-18-03139-g012"/></fig><fig id="sensors-18-03139-f013" orientation="portrait" position="float"><label>Figure 13</label><caption><p>The true positive (TP) rates and the false negative (FN) rates of different polarimetric classification methods. Wm means watermelon.</p></caption><graphic xlink:href="sensors-18-03139-g013"/></fig><fig id="sensors-18-03139-f014" orientation="portrait" position="float"><label>Figure 14</label><caption><p>The true negative (TN) rates and the false positive (FP) rates of different polarimetric classification methods. Wm means watermelon.</p></caption><graphic xlink:href="sensors-18-03139-g014"/></fig><table-wrap id="sensors-18-03139-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03139-t001_Table 1</object-id><label>Table 1</label><caption><p>Main imaging parameters of GF-3 satellite.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Item</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Polarization mode</td><td align="center" valign="middle" rowspan="1" colspan="1">HH, HV, VH and VV</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Chirp Bandwidth (MHz)</td><td align="center" valign="middle" rowspan="1" colspan="1">40</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Centre frequency (GHz)</td><td align="center" valign="middle" rowspan="1" colspan="1">5.400012</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Band</td><td align="center" valign="middle" rowspan="1" colspan="1">C-band</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Range pixel spacing (m)</td><td align="center" valign="middle" rowspan="1" colspan="1">2.248443</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Azimuth pixel spacing (m)</td><td align="center" valign="middle" rowspan="1" colspan="1">4.733369</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Acquisition Type</td><td align="center" valign="middle" rowspan="1" colspan="1">Stripmap (QPSI)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Start time</td><td align="center" valign="middle" rowspan="1" colspan="1">2017-07-19, 22:26:57.615189</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Stop time</td><td align="center" valign="middle" rowspan="1" colspan="1">2017-07-19, 22:27:01.799853</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Incidence angle</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">38.16&#x000b0;</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-03139-t002" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03139-t002_Table 2</object-id><label>Table 2</label><caption><p>Main imaging parameters of Sentinel-2A satellite.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Item</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Swath (km)</td><td align="center" valign="middle" rowspan="1" colspan="1">290</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Acquisition time</td><td align="center" valign="middle" rowspan="1" colspan="1">2017-07-17, 11:05:41.26</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Spectral bands</td><td align="center" valign="middle" rowspan="1" colspan="1">R (Band 4), G (Band 3), B (Band 2), NIR (Band 8)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Centre Wavelength (nm)</td><td align="center" valign="middle" rowspan="1" colspan="1">R (665), G (560), B (490), NIR (842)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Bandwidth (nm)</td><td align="center" valign="middle" rowspan="1" colspan="1">R (30), G (35), B (65), NIR (115)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Spatial Resolution (m)</td><td align="center" valign="middle" rowspan="1" colspan="1">R (10), G (10), B (10), NIR ( 0)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Reference Radiances <italic>L<sub>ref</sub></italic><break/>(W m<sup>&#x02212;2</sup> sr<sup>&#x02212;1</sup> &#x000b5;m<sup>&#x02212;1</sup>)</td><td align="center" valign="middle" rowspan="1" colspan="1">R (108), G (128), B (128), NIR (103)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Signal-to-Noise Ratios @ <italic>L<sub>ref</sub></italic></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R (142), G (168), B (154), NIR (174)</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-03139-t003" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03139-t003_Table 3</object-id><label>Table 3</label><caption><p>Field data collected for classification training and testing.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Land Cover</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Training Samples</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Testing Samples</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of Pixels</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of Plots</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of Pixels</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of Plots</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Water</td><td align="center" valign="middle" rowspan="1" colspan="1">5118</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">241,174</td><td align="center" valign="middle" rowspan="1" colspan="1">86</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Rice (single-season)</td><td align="center" valign="middle" rowspan="1" colspan="1">3305</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">199,891</td><td align="center" valign="middle" rowspan="1" colspan="1">81</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Rice (two-season)</td><td align="center" valign="middle" rowspan="1" colspan="1">3572</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">122,269</td><td align="center" valign="middle" rowspan="1" colspan="1">91</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Watermelon</td><td align="center" valign="middle" rowspan="1" colspan="1">2679</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">106,678</td><td align="center" valign="middle" rowspan="1" colspan="1">52</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Lotus</td><td align="center" valign="middle" rowspan="1" colspan="1">4193</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">188,068</td><td align="center" valign="middle" rowspan="1" colspan="1">56</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Bare soil</td><td align="center" valign="middle" rowspan="1" colspan="1">2841</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">134,727</td><td align="center" valign="middle" rowspan="1" colspan="1">55</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Forest</td><td align="center" valign="middle" rowspan="1" colspan="1">1890</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">168,832</td><td align="center" valign="middle" rowspan="1" colspan="1">52</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Grass</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4336</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">208,945</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-03139-t004" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03139-t004_Table 4</object-id><label>Table 4</label><caption><p>Classification accuracy assessment of the integrated dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Pixels</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Water</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Rice1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Rice2</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Wm</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Lotus</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Bare Soil</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Forest</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Grass</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UA (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Water</td><td align="center" valign="middle" rowspan="1" colspan="1">237,449</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">35</td><td align="center" valign="middle" rowspan="1" colspan="1">1052</td><td align="center" valign="middle" rowspan="1" colspan="1">249</td><td align="center" valign="middle" rowspan="1" colspan="1">377</td><td align="center" valign="middle" rowspan="1" colspan="1">99.28</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Rice1</td><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">152,273</td><td align="center" valign="middle" rowspan="1" colspan="1">21,550</td><td align="center" valign="middle" rowspan="1" colspan="1">9</td><td align="center" valign="middle" rowspan="1" colspan="1">3014</td><td align="center" valign="middle" rowspan="1" colspan="1">882</td><td align="center" valign="middle" rowspan="1" colspan="1">6381</td><td align="center" valign="middle" rowspan="1" colspan="1">1965</td><td align="center" valign="middle" rowspan="1" colspan="1">81.83</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Rice2</td><td align="center" valign="middle" rowspan="1" colspan="1">124</td><td align="center" valign="middle" rowspan="1" colspan="1">44,750</td><td align="center" valign="middle" rowspan="1" colspan="1">97,382</td><td align="center" valign="middle" rowspan="1" colspan="1">13</td><td align="center" valign="middle" rowspan="1" colspan="1">392</td><td align="center" valign="middle" rowspan="1" colspan="1">1108</td><td align="center" valign="middle" rowspan="1" colspan="1">41935</td><td align="center" valign="middle" rowspan="1" colspan="1">13133</td><td align="center" valign="middle" rowspan="1" colspan="1">48.98</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Wm</td><td align="center" valign="middle" rowspan="1" colspan="1">624</td><td align="center" valign="middle" rowspan="1" colspan="1">34</td><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">98,113</td><td align="center" valign="middle" rowspan="1" colspan="1">68</td><td align="center" valign="middle" rowspan="1" colspan="1">13364</td><td align="center" valign="middle" rowspan="1" colspan="1">361</td><td align="center" valign="middle" rowspan="1" colspan="1">3812</td><td align="center" valign="middle" rowspan="1" colspan="1">84.30</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Lotus</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">104</td><td align="center" valign="middle" rowspan="1" colspan="1">13</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">179,632</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">270</td><td align="center" valign="middle" rowspan="1" colspan="1">1790</td><td align="center" valign="middle" rowspan="1" colspan="1">98.80</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Bare soil</td><td align="center" valign="middle" rowspan="1" colspan="1">2732</td><td align="center" valign="middle" rowspan="1" colspan="1">591</td><td align="center" valign="middle" rowspan="1" colspan="1">98</td><td align="center" valign="middle" rowspan="1" colspan="1">8074</td><td align="center" valign="middle" rowspan="1" colspan="1">92</td><td align="center" valign="middle" rowspan="1" colspan="1">113,877</td><td align="center" valign="middle" rowspan="1" colspan="1">384</td><td align="center" valign="middle" rowspan="1" colspan="1">3540</td><td align="center" valign="middle" rowspan="1" colspan="1">88.01</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Forest</td><td align="center" valign="middle" rowspan="1" colspan="1">125</td><td align="center" valign="middle" rowspan="1" colspan="1">342</td><td align="center" valign="middle" rowspan="1" colspan="1">2715</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">50</td><td align="center" valign="middle" rowspan="1" colspan="1">81</td><td align="center" valign="middle" rowspan="1" colspan="1">106,606</td><td align="center" valign="middle" rowspan="1" colspan="1">910</td><td align="center" valign="middle" rowspan="1" colspan="1">96.19</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Grass</td><td align="center" valign="middle" rowspan="1" colspan="1">115</td><td align="center" valign="middle" rowspan="1" colspan="1">1797</td><td align="center" valign="middle" rowspan="1" colspan="1">508</td><td align="center" valign="middle" rowspan="1" colspan="1">463</td><td align="center" valign="middle" rowspan="1" colspan="1">4785</td><td align="center" valign="middle" rowspan="1" colspan="1">4363</td><td align="center" valign="middle" rowspan="1" colspan="1">12642</td><td align="center" valign="middle" rowspan="1" colspan="1">183,418</td><td align="center" valign="middle" rowspan="1" colspan="1">88.14</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PA (%)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.65</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.98</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.51</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.52</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Overall Accuracy (%)</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">85.2745</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Kappa coefficient</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">0.8306</td></tr></tbody></table><table-wrap-foot><fn><p>Note: Wm denotes &#x0201c;Watermelon.&#x0201d; The user&#x02019;s accuracy (UA) indicates the misclassification condition, while the producer&#x02019;s accuracy (PA) indicates the omission condition.</p></fn></table-wrap-foot></table-wrap><table-wrap id="sensors-18-03139-t005" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03139-t005_Table 5</object-id><label>Table 5</label><caption><p>Details on different feature integration modes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Feature Integration Mode</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GF-3 Features</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sentinel-2A Features</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">GF-3 (7 bands) + S2A (4 bands)</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm38"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula><sub>pca1</sub>, <inline-formula><mml:math id="mm39"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula><sub>pca2</sub>, RVI, <italic>P<sub>s</sub></italic>, <italic>P<sub>d</sub></italic>, <italic>P<sub>h</sub></italic> and <italic>P<sub>v</sub></italic>.</td><td align="center" valign="middle" rowspan="1" colspan="1">Opband<sub>pca1</sub>, Opband<sub>pca2</sub>, NDVI and H</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GF-3 (7 bands) + S2A (2 bands)</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-formula><mml:math id="mm40"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula><sub>pca1</sub>, <inline-formula><mml:math id="mm41"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula><sub>pca2</sub>, RVI, <italic>P<sub>s</sub></italic>, <italic>P<sub>d</sub></italic>, <italic>P<sub>h</sub></italic> and <italic>P<sub>v</sub></italic>.</td><td align="center" valign="middle" rowspan="1" colspan="1">NDVI and H</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GF-3 (5 bands) + S2A (4 bands)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RVI, <italic>P<sub>s</sub></italic>, <italic>P<sub>d</sub></italic>, <italic>P<sub>h</sub></italic> and <italic>P<sub>v</sub></italic>.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Opband<sub>pca1</sub>, Opband<sub>pca2</sub>, NDVI and H</td></tr></tbody></table></table-wrap></floats-group></article>