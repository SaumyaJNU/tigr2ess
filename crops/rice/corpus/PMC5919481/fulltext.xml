<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 39.96?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">5919481</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0196302</article-id><article-id pub-id-type="publisher-id">PONE-D-18-00387</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Plants</subject><subj-group><subject>Weeds</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Plants</subject><subj-group><subject>Grasses</subject><subj-group><subject>Rice</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Experimental Organism Systems</subject><subj-group><subject>Plant and Algal Models</subject><subj-group><subject>Rice</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Agriculture</subject><subj-group><subject>Agrochemicals</subject><subj-group><subject>Herbicides</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Geoinformatics</subject><subj-group><subject>Remote Sensing Imagery</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Earth Sciences</subject><subj-group><subject>Geography</subject><subj-group><subject>Geoinformatics</subject><subj-group><subject>Remote Sensing Imagery</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject><subj-group><subject>Remote Sensing Imagery</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Remote Sensing</subject><subj-group><subject>Remote Sensing Imagery</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Agriculture</subject><subj-group><subject>Crop Science</subject><subj-group><subject>Crops</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Machine Learning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Transportation</subject><subj-group><subject>Aircraft</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>A fully convolutional network for weed mapping of unmanned aerial vehicle (UAV) imagery</article-title><alt-title alt-title-type="running-head">Weed mapping based on UAV imagery</alt-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6546-6501</contrib-id><name><surname>Huang</surname><given-names>Huasheng</given-names></name><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Software</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="aff" rid="aff002"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Deng</surname><given-names>Jizhong</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Funding acquisition</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Project administration</role><role content-type="http://credit.casrai.org/">Supervision</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="aff" rid="aff002"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Lan</surname><given-names>Yubin</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Funding acquisition</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Project administration</role><role content-type="http://credit.casrai.org/">Supervision</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="aff" rid="aff002"><sup>2</sup></xref><xref ref-type="corresp" rid="cor001">*</xref></contrib><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Aqing</given-names></name><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib><contrib contrib-type="author"><name><surname>Deng</surname><given-names>Xiaoling</given-names></name><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff002"><sup>2</sup></xref><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Lei</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff002"><sup>2</sup></xref><xref ref-type="aff" rid="aff004"><sup>4</sup></xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>College of Engineering, South China Agricultural University, Guangzhou, China</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>National Center for International Collaboration Research on Precision Agricultural Aviation Pesticide Spraying Technology, Guangzhou, China</addr-line></aff><aff id="aff003"><label>3</label>
<addr-line>College of Electronical Engineering, South China Agricultural University, Guangzhou, China</addr-line></aff><aff id="aff004"><label>4</label>
<addr-line>College of Agriculture, South China Agricultural University, Guangzhou, China</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Gonzalez-Andujar</surname><given-names>Jose Luis</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1"><addr-line>Instituto Agricultura Sostenible, SPAIN</addr-line></aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>ylan@scau.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>26</day><month>4</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>13</volume><issue>4</issue><elocation-id>e0196302</elocation-id><history><date date-type="received"><day>4</day><month>1</month><year>2018</year></date><date date-type="accepted"><day>10</day><month>4</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; 2018 Huang et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Huang et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0196302.pdf"/><abstract><p>Appropriate Site Specific Weed Management (SSWM) is crucial to ensure the crop yields. Within SSWM of large-scale area, remote sensing is a key technology to provide accurate weed distribution information. Compared with satellite and piloted aircraft remote sensing, unmanned aerial vehicle (UAV) is capable of capturing high spatial resolution imagery, which will provide more detailed information for weed mapping. The objective of this paper is to generate an accurate weed cover map based on UAV imagery. The UAV RGB imagery was collected in 2017 October over the rice field located in South China. The Fully Convolutional Network (FCN) method was proposed for weed mapping of the collected imagery. Transfer learning was used to improve generalization capability, and skip architecture was applied to increase the prediction accuracy. After that, the performance of FCN architecture was compared with Patch_based CNN algorithm and Pixel_based CNN method. Experimental results showed that our FCN method outperformed others, both in terms of accuracy and efficiency. The overall accuracy of the FCN approach was up to 0.935 and the accuracy for weed recognition was 0.883, which means that this algorithm is capable of generating accurate weed cover maps for the evaluated UAV imagery.</p></abstract><funding-group><award-group id="award001"><funding-source><institution>Science and Technology Planning Project of Guangdong Province, China</institution></funding-source><award-id>2017A020208046</award-id><principal-award-recipient><name><surname>Deng</surname><given-names>Jizhong</given-names></name></principal-award-recipient></award-group><award-group id="award002"><funding-source><institution>the National Key Research and Development Plan: High Efficient Ground and Aerial Spraying Technology and Intelligent Equipment, China</institution></funding-source><award-id>2016YFD0200700</award-id><principal-award-recipient><name><surname>Lan</surname><given-names>Yubin</given-names></name></principal-award-recipient></award-group><award-group id="award003"><funding-source><institution>The National Natural Science Fund, China</institution></funding-source><award-id>61675003</award-id><principal-award-recipient><name><surname>Deng</surname><given-names>Xiaoling</given-names></name></principal-award-recipient></award-group><funding-statement>This work was supported by: 1. the National Key Research and Development Plan: High Efficient Ground and Aerial Spraying Technology and Intelligent Equipment, China; grant number: 2016YFD0200700; <ext-link ext-link-type="uri" xlink:href="http://service.most.gov.cn/2015tztg_all/20160622/1108.html">http://service.most.gov.cn/2015tztg_all/20160622/1108.html</ext-link>; funding institution: Ministry of Agriculture of the People's Republic of China; the author receiving the funding: Yubin Lan; role of funder in this study: study design, decision to publish. 2: Science and Technology Planning Project of Guangdong Province, China; grant number: 2017A020208046; <ext-link ext-link-type="uri" xlink:href="http://www.gdstc.gov.cn/">http://www.gdstc.gov.cn/</ext-link>; funding institution: Department of Science and Technology of Guangdong Province, China; the author receiving the funding: Jizhong Deng; role of funder in this study: study design, decision to publish. 3: The National Natural Science Fund, China; grant number: 61675003; <ext-link ext-link-type="uri" xlink:href="http://www.nsfc.gov.cn/publish/portal0/">http://www.nsfc.gov.cn/publish/portal0/</ext-link>; funding institution: National Natural Science Foundation of China; the author receiving the funding: Xiaoling Deng; role of funder in this study: study design, preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="13"/><table-count count="4"/><page-count count="19"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>Data available from the Figshare Repository (DOIs: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.5739669">10.6084/m9.figshare.5739669</ext-link>), and the URLs is: <ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/The_dataset_of_the_manuscript_A_Fully_Convolutional_Network_for_weed_mapping_of_Unmanned_Aerial_Vehicle_UAV_Imagery_/5739669">https://figshare.com/articles/The_dataset_of_the_manuscript_A_Fully_Convolutional_Network_for_weed_mapping_of_Unmanned_Aerial_Vehicle_UAV_Imagery_/5739669</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>Data available from the Figshare Repository (DOIs: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.5739669">10.6084/m9.figshare.5739669</ext-link>), and the URLs is: <ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/The_dataset_of_the_manuscript_A_Fully_Convolutional_Network_for_weed_mapping_of_Unmanned_Aerial_Vehicle_UAV_Imagery_/5739669">https://figshare.com/articles/The_dataset_of_the_manuscript_A_Fully_Convolutional_Network_for_weed_mapping_of_Unmanned_Aerial_Vehicle_UAV_Imagery_/5739669</ext-link>.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>Many agricultural crops require the use of herbicides as essential tools for maintaining the quality and quantity of crop production [<xref rid="pone.0196302.ref001" ref-type="bibr">1</xref>]. However, the inappropriate use of herbicides could cause yield reduction and environmental pollution. The main reason for this problem is that, the usual practice of weed management is to broadcast herbicides over the entire field, even within the weed-free areas [<xref rid="pone.0196302.ref002" ref-type="bibr">2</xref>]. In the field of Site Specific Weed Management (SSWM), there is a need for developing a new strategy to solve the problem of current weed control practices.</p><p>To achieve this goal, it is necessary to generate an accurate weed cover map for precise spraying of herbicide. In the practice of SSWM, a weed cover map can be used to decide where the chemicals are needed most, least, or where it should not be used at all [<xref rid="pone.0196302.ref003" ref-type="bibr">3</xref>]. Usually, the weed cover map can be developed using remote sensing technology. Through image processing, remote sensing imagery can be converted to a weed cover map [<xref rid="pone.0196302.ref003" ref-type="bibr">3</xref>] which could be applied for accurate spraying.</p><p>In the past few years, piloted aircraft and satellite remote sensing have been employed for weed detection and mapping [<xref rid="pone.0196302.ref004" ref-type="bibr">4</xref>, <xref rid="pone.0196302.ref005" ref-type="bibr">5</xref>]. However, it is hard to obtain a satisfactory result because of the insufficient spatial resolution of the remote sensing imagery. Today, the insufficiency of spatial resolution can be appropriately solved by using UAV-based remote sensing technology. UAVs can fly at a low altitude and capture ultra-high spatial resolution imagery, which may significantly improve the overall accuracy of weed mapping. P&#x000e9;rez-Ortiz et al. [<xref rid="pone.0196302.ref006" ref-type="bibr">6</xref>] utilized the UAV imagery for weed mapping, and an excellent performance was obtained using a semi-supervised approach. Castaldi et al. [<xref rid="pone.0196302.ref007" ref-type="bibr">7</xref>] applied UAV multispectral imagery for maize/weed classification. The classification results used in the weed management of the maize field leaded to a decrease in the use of herbicide without negative consequences in terms of crop yield.</p><p>Weed detection in crop fields is a difficult task. This task is even complex in rice fields due to that the rice and weeds share similarities in spectral characteristics and general appearance, and due to the variability and changing conditions in rice fields. Even using OBIA method, accurate weed mapping is still a challenging task. In recent few years, machine learning methods have been successfully applied in weed mapping based of UAV imagery. Alexandridis et al. [<xref rid="pone.0196302.ref008" ref-type="bibr">8</xref>] applied four novelty detection classifiers for weed detection and mapping of Silybum marianum (S. marianum) weed based on UAV multispectral imagery, and the identification accuracy using One Class Support Vector Machine (OC-SVM) reached an overall accuracy of 96%. Tamouridou et al. [<xref rid="pone.0196302.ref009" ref-type="bibr">9</xref>] used the Multilayer Perceptron with Automatic Relevance Determination (MLP-ARD) to identify the S. marianum among other vegetation based on UAV remote sensing, and the S. marianum identification rate was up to 99.54%.</p><p>As the varieties of machine learning approaches, Convolutional Neural Network (CNN) is now the dominating method for most remote sensing applications [<xref rid="pone.0196302.ref010" ref-type="bibr">10</xref>&#x02013;<xref rid="pone.0196302.ref012" ref-type="bibr">12</xref>]. However, traditional CNN architecture is an &#x0201c;image-label&#x0201d; mode that maps an image into a 1-D probability distribution related to different classes [<xref rid="pone.0196302.ref013" ref-type="bibr">13</xref>], while what we expect in our application is a dense class map, which is a &#x0201c;pixel-label&#x0201d; pattern. In order to generate a weed cover map, the output image should have the same resolution with the input image, and the output map is a 2-D probability distribution. To perform the &#x0201c;pixel-label&#x0201d; classification task, Shelhamer et al.[<xref rid="pone.0196302.ref014" ref-type="bibr">14</xref>] proposed a Fully Convolutional Network (FCN) which is capable of providing dense class map in image analysis. Gang Fu et al.[<xref rid="pone.0196302.ref013" ref-type="bibr">13</xref>] applied FCN method for classification of satellite imagery and proved that this approach outperformed other existing methods in terms of accuracy. Sherrah et al.[<xref rid="pone.0196302.ref015" ref-type="bibr">15</xref>] used an adapted FCN without downsampling for semantic labeling of aerial imagery and proved that the proposed network yielded state-of-the-art accuracy for the ISPRS Vaihingen and Potsdam benchmark data sets. However, to the best of our knowledge, the potential of FCN method for weed mapping of UAV imagery is not yet been accessed.</p><p>The objectives of this paper are to: (1) Evaluate the feasibility of FCN method, which is usually used in computer vision field, on application of accurate weed mapping from the UAV imagery; (2) Apply transfer learning method to improve the generalization capability; (3) Use skip architecture to increase the prediction accuracy; (4) Compare the proposed method with other methods in the application of weed mapping.</p><p>The rest of this paper is organized as followed: Section 2 introduces the data collection and analyzing methods; Section 3 describes and discusses the experimental results; and Section 4 draws the conclusion and future perspective.</p></sec><sec sec-type="materials|methods" id="sec002"><title>Materials and methods</title><sec id="sec003"><title>Study site</title><p>The study site was located at Zengcheng Teaching and Research Bases, South China Agricultural University (Guangzhou city, Guangdong province, China, coordinates 23.240 N, 113.637 E, datum WGS84). The UAV data from a rice field was studied. This plot had an area of 0.54 ha, and was divided into 172 subplots of approximately the same size of 20 m<sup>2</sup> (<xref ref-type="fig" rid="pone.0196302.g001">Fig 1</xref>). These subplots were all irrigated and sown (Huahang No. 31 [<xref rid="pone.0196302.ref016" ref-type="bibr">16</xref>]) on August 21<sup>th</sup> 2017, at 60 Kg ha<sup>-1</sup> in rows spaced 50 cm apart. The fertilizer (50 Kg N ha<sup>-1</sup>, 50 Kg P<sub>2</sub>O<sub>5</sub> ha<sup>-1</sup> and 40 Kg K<sub>2</sub>0 ha<sup>-1</sup>) was drilled at the seedling stage, and N at 40 Kg ha<sup>-1</sup> was top-dressed at the active tillering stage. The studied field was naturally infested with <italic>Leptochloa chinensis Nees (L</italic>. <italic>chinensis)</italic> [<xref rid="pone.0196302.ref017" ref-type="bibr">17</xref>] and <italic>Cyperus iric</italic> [<xref rid="pone.0196302.ref018" ref-type="bibr">18</xref>] (<xref ref-type="fig" rid="pone.0196302.g002">Fig 2</xref>), and the weed species were mixed in the field.</p><fig id="pone.0196302.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0196302.g001</object-id><label>Fig 1</label><caption><title>The general location of the study site and the brief overview of the studied rice field.</title></caption><graphic xlink:href="pone.0196302.g001"/></fig><fig id="pone.0196302.g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0196302.g002</object-id><label>Fig 2</label><caption><title>In-field photograph of the study site.</title><p>(a) The cultivated rice and some patches of L. chinensis; (b) The cultivated rice and some patches of Cyperus iric.</p></caption><graphic xlink:href="pone.0196302.g002"/></fig></sec><sec id="sec004"><title>Data collection</title><p>A multi-rotor UAV (Phantom 4, SZ DJI Technology Co., Ltd., Shenzhen, China) was used to collect all of the imagery used in this study. The Phantom 4 camera uses 1-inch CMOS sensor to capture 12 megapixel imagery [<xref rid="pone.0196302.ref019" ref-type="bibr">19</xref>] with the resolution of 4000*3000 pixels. During the flight, a 3-axis gimbal on Phantom 4 provided a steady platform to keep the attached camera pointed close to nadir. An overview of the UAV and the captured imagery were shown in <xref ref-type="fig" rid="pone.0196302.g003">Fig 3</xref>. DJI GS PRO (also known as the ground station of UAV) is an Ipad app designed for DJI aircrafts, and was used to plan and control automatic flight [<xref rid="pone.0196302.ref020" ref-type="bibr">20</xref>] of UAV in this study.</p><fig id="pone.0196302.g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0196302.g003</object-id><label>Fig 3</label><caption><title>An overview of UAV and the collected imagery.</title><p>(a) The Phantom 4 UAV flying over the rice field; (b) Aerial imagery (with RGB channels) captured by the UAV at an altitude of 6 m, showing the cultivated rice and some patches of weeds.</p></caption><graphic xlink:href="pone.0196302.g003"/></fig><p>Field campaigns were conducted on October 2<sup>th</sup> 2017, just when the rice and the infested weeds were both in growing stage and the herbicide treatments were recommended. The flights were conducted with the permission from Prof. Guohua Zhong, who was the authority responsible for the experiment site. An rectangle region of 60 &#x02573; 50 m was selected as the experimental area to perform the flights. According to the results of ground investigation, the selected region was composed of 90 subplots (1800 m<sup>2</sup>) of rice, 8 subplots (160 m<sup>2</sup>) of weeds and 38 subplots (760 m<sup>2</sup>) of weed-rice combination with different weed densities, as shown in <xref ref-type="fig" rid="pone.0196302.g004">Fig 4</xref>. During the ground investigation, the weed density was obtained by visual judges under the instruction of agronomy experts. It can be seen from <xref ref-type="fig" rid="pone.0196302.g004">Fig 4</xref> that, the experimental site contained much variability in weed-rice combinations.</p><fig id="pone.0196302.g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0196302.g004</object-id><label>Fig 4</label><caption><title>An overview of the results from ground investigation.</title><p>(a) The ground truth map of the experimental site; (b) Sampling zone for division of the subplots in the rice field.</p></caption><graphic xlink:href="pone.0196302.g004"/></fig><p>The coordinate of each corner was collected as a boundary point and the flight plan was automatically planed by DJI GS PRO. During data collection, the flight altitude was kept 6 meters above the ground, and the spatial resolution was around 0.3 cm. Meanwhile, the forward-lap and side-lap were set to 60% and 50% respectively. In this work, 91 imagery were collected in total.</p><p>Since the resolution of our imagery is 4000 &#x02573; 3000 and much larger than other FCN experiments like [<xref rid="pone.0196302.ref014" ref-type="bibr">14</xref>, <xref rid="pone.0196302.ref015" ref-type="bibr">15</xref>], directly exporting the imagery into our FCN network can easily exhaust the GPU&#x02019;s memory. Therefore, each original imagery was divided into 12 tiles of size 1000 &#x02573; 1000, and 1092 tiles were obtained in total. Among the whole imagery tiles, 892 titles were randomly selected as training dataset, and the other 200 tiles were chosen as validation dataset.</p><p>In the UAV imagery, all the objects were divided into 3 classes: <italic>Rice</italic>, <italic>Weeds</italic> and <italic>Others</italic> (including cement ground, water, et al.). Since the resolution of imagery is high enough to visually distinguish different classes from the imagery, we manually label the Ground Truth (GT) labels using specified colors: color green for <italic>Rice</italic> class, color red for <italic>Weeds</italic> class, and color gray for <italic>Others</italic> class, as shown in <xref ref-type="fig" rid="pone.0196302.g005">Fig 5(b)</xref>.</p><fig id="pone.0196302.g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0196302.g005</object-id><label>Fig 5</label><caption><title>Demonstration for imagery labeling.</title><p>(a) UAV imagery; (b) GT labels corresponding to the imagery in (a).</p></caption><graphic xlink:href="pone.0196302.g005"/></fig></sec><sec id="sec005" sec-type="materials|methods"><title>Methods</title><p>Similar with other supervised machine learning approaches, our method could be divided into training and validation stage, as <xref ref-type="fig" rid="pone.0196302.g006">Fig 6</xref>. In the training stage, the image-labels pairs in the training set, with pixel-to-pixel corresponds, are input into the FCN network, as is shown in the upper part of <xref ref-type="fig" rid="pone.0196302.g006">Fig 6</xref>. The network maps the input image into a same sized output image, and the output image as well as the ground truth label (GT label) are used to compute the loss as an objective function. After that, the gradient of the objective function with respect to the weights of all the network modules are computed using the chain rule for derivatives, and then the network parameters are updated by gradient descend method. The above training process will be iterated until the loss is less than a threshold or the maximum iterations reach. In the validation stage, the trained FCN network will map the validation image into a prediction class map with per-pixel classification, as is shown in lower part of <xref ref-type="fig" rid="pone.0196302.g006">Fig 6</xref>.</p><fig id="pone.0196302.g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0196302.g006</object-id><label>Fig 6</label><caption><title>Overview of our methodology: The training stage and validation stage are illustrated in the upper and lower parts, respectively.</title></caption><graphic xlink:href="pone.0196302.g006"/></fig><p>The FCN network was proposed as the basic architecture for our dense prediction task. Based on the basic of FCN network, transfer learning is introduced to improve the generalization capability, and skip architecture is applied to increase the prediction accuracy.</p><sec id="sec006"><title>Fully convolutional network</title><p>The general structure of a typical CNN is composed of multiple convolutional layers interlaced with pooling layers [<xref rid="pone.0196302.ref021" ref-type="bibr">21</xref>], followed by some fully-connected layers in the end [<xref rid="pone.0196302.ref022" ref-type="bibr">22</xref>], as shown in the upper part of <xref ref-type="fig" rid="pone.0196302.g007">Fig 7</xref>. The convolutional layer automatically extract the hierarchical and high-level feature representation [<xref rid="pone.0196302.ref021" ref-type="bibr">21</xref>], while pooling layer reduces the image resolution and help achieve spatial invariance, and the fully-connected layer reduces the image dimension and outputs a 1-D distribution over classes. In general, the maximum output in the 1-D distribution corresponds to the predicted result, which classifies one image to one single label. This &#x0201c;image-label&#x0201d; mode has achieved success in the scene classification of high-resolution satellite imagery by the study of Hu et al [<xref rid="pone.0196302.ref011" ref-type="bibr">11</xref>].</p><fig id="pone.0196302.g007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0196302.g007</object-id><label>Fig 7</label><caption><title>Overview of architecture of CNN (upper part) and FCN (lower part).</title></caption><graphic xlink:href="pone.0196302.g007"/></fig><p>However, the goal of this work is not a single label of the input image, but a dense prediction map, which could generate a weed cover map for our UAV imagery. Therefore, the FCN method was proposed for our study. Compared with CNN, FCN network transforms all the fully-connected layers into convolutional layers and enables a classification net to output a heatmap [<xref rid="pone.0196302.ref014" ref-type="bibr">14</xref>]. Using this transformation, the 2-D spatial information of the original image is properly maintained, which contributes to output a dense prediction map. In the last convolutional layer, the number of feature maps corresponds to the quantity of classes, where each feature map represents the spatial probabilities with regard to a certain class, as shown in the lower part of <xref ref-type="fig" rid="pone.0196302.g007">Fig 7</xref>.</p><p>Though FCN network could maintain the 2-D spatial information, the size of output is typically reduced using the pooling operation. To compensate the resolution reduction, the deconvolution operation is applied to the coarse output, as introduced by Shelhamer et al.[<xref rid="pone.0196302.ref014" ref-type="bibr">14</xref>]. A deconvolutional layer is initialized using bilinear interpolation and learned for nonlinear regression during the training stage. As the upsampling approach, the deconvolutional layers restore the output to full resolution, thus an end-to-end, pixel-to-pixel prediction architecture was established, as shown in <xref ref-type="fig" rid="pone.0196302.g008">Fig 8</xref>. The channels of the last deconvolutional layer is equal to the quantity of classes, and each feature map represents the heat map for a certain class (the 3 feature maps corresponds to <italic>Rice</italic>, <italic>Weeds</italic> and <italic>Others</italic>, respectively). For every pixel in the prediction feature maps, the class with maximum probability along the whole channels is used as the predicted labels [<xref rid="pone.0196302.ref015" ref-type="bibr">15</xref>], and the predicted labels of all the pixels in the image form the output result.</p><fig id="pone.0196302.g008" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0196302.g008</object-id><label>Fig 8</label><caption><title>Overview of deconvolutional operation for per-pixel classification tasks.</title></caption><graphic xlink:href="pone.0196302.g008"/></fig><p>During the training process, the entropy-loss was calculated by comparing the target label vectors <italic>y</italic> and predicted label vectors <inline-formula id="pone.0196302.e001"><alternatives><graphic xlink:href="pone.0196302.e001.jpg" id="pone.0196302.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> (assuming there are <italic>n</italic> samples and <italic>T</italic> possible classes), as defined by (<xref ref-type="disp-formula" rid="pone.0196302.e002">1</xref>):
<disp-formula id="pone.0196302.e002"><alternatives><graphic xlink:href="pone.0196302.e002.jpg" id="pone.0196302.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mrow><mml:mtext>L</mml:mtext><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mspace width="4pt"/><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives><label>(1)</label></disp-formula></p><p>After the loss calculation, the gradient of the loss with respect to the weights of all modules in the network was computed using chain rules [<xref rid="pone.0196302.ref022" ref-type="bibr">22</xref>], and used for network parameter updating, as define by (<xref ref-type="disp-formula" rid="pone.0196302.e003">2</xref>) and (<xref ref-type="disp-formula" rid="pone.0196302.e004">3</xref>):
<disp-formula id="pone.0196302.e003"><alternatives><graphic xlink:href="pone.0196302.e003.jpg" id="pone.0196302.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:msup><mml:mrow><mml:mtext>W</mml:mtext></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mo>&#x00394;</mml:mo><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives><label>(2)</label></disp-formula>
<disp-formula id="pone.0196302.e004"><alternatives><graphic xlink:href="pone.0196302.e004.jpg" id="pone.0196302.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:mo>&#x00394;</mml:mo><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mfrac><mml:mrow><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:mo>&#x00394;</mml:mo><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives><label>(3)</label></disp-formula>
Where <italic>W</italic><sup>(<italic>n</italic>)</sup> and <italic>W</italic><sup>(<italic>n</italic>+1)</sup> denoted the previous weights and current weights, &#x00394;<italic>W</italic><sup>(<italic>n</italic>)</sup> and &#x00394;<italic>W</italic><sup>(<italic>n</italic>+1)</sup> were the previous increment and current increment, <italic>&#x003b7;</italic> was the learning rate, and <italic>d</italic><sub><italic>w</italic></sub> and <italic>m</italic> denoted the weight decay and momentum, respectively.</p></sec><sec id="sec007"><title>Transfer learning</title><p>As a fully-supervised method, FCN achieves great success in computer vision field as well as remote sensing. However, with limited training data, fully-supervised training will generally cause the problem of overfitting [<xref rid="pone.0196302.ref023" ref-type="bibr">23</xref>], resulting in pool generalization capability.</p><p>Transfer learning uses the existing weights from the model that is fully-trained in a specific dataset like ImageNet [<xref rid="pone.0196302.ref024" ref-type="bibr">24</xref>], and retrains it for new classes [<xref rid="pone.0196302.ref025" ref-type="bibr">25</xref>]. Previous studies have proven that transfer learning is effective in many remote sensing applications [<xref rid="pone.0196302.ref011" ref-type="bibr">11</xref>, <xref rid="pone.0196302.ref013" ref-type="bibr">13</xref>, <xref rid="pone.0196302.ref026" ref-type="bibr">26</xref>]. Generally, transfer learning method discards the last layer of the pre-trained model, and appends a fully-connected layer where the the neurons corresponds to the number of predicted classes. During the training stage, final layer is trained from scratch, while the others are initialized from the pre-trained model and updated by back-propagation rule or kept fixed.</p><p>In this work, the AlexNet [<xref rid="pone.0196302.ref027" ref-type="bibr">27</xref>] that won the ILSVRC-2012 champion was considered, as well as the VGGnet [<xref rid="pone.0196302.ref028" ref-type="bibr">28</xref>] and GoogLeNet [<xref rid="pone.0196302.ref029" ref-type="bibr">29</xref>] that showed great performance in the ILSVRC-2014. Despite its success, CNNs were largely forsaken by the machine learning communities until the emergence of AlexNet in 2012 [<xref rid="pone.0196302.ref022" ref-type="bibr">22</xref>]. When applied to the ImageNet in ILSVRC-2012, AlexNet halved the error rate of the state-of-art. The success came from the efficient use of GPU, the technique of Local Response Normalization, overlapping pooling, data augmentation and dropout strategy [<xref rid="pone.0196302.ref027" ref-type="bibr">27</xref>]. VGGnet was developed by increasing the depth to 16&#x02013;19 and using very small (3<bold>&#x02573;</bold>3) convolution filters [<xref rid="pone.0196302.ref028" ref-type="bibr">28</xref>], which showed a significant improvement on the classification accuracy and generalization capability. GoogLeNet was a 22 layers deep network that won the runner-up of ILSVRC-2014 classification and detection challenges. The hallmark of this architecture was the improved utilization of the computing resources inside the network, which allowed for increasing the depth and width while keeping the computation costs constant [<xref rid="pone.0196302.ref029" ref-type="bibr">29</xref>]. Compared with AlexNet and VGGnet, GoogLeNet achieved a similar accuracy in ILSVRC with less parameters. In the following experiments, the pre-trained AlexNet, VGGnet and GoogLeNet were adapted into FCN network for the weed mapping task by fine-tuning.</p></sec><sec id="sec008"><title>Skip architecture</title><p>The pooling layers in the FCN network could significantly reduce computation and introduces invariance to small translation of input image [<xref rid="pone.0196302.ref021" ref-type="bibr">21</xref>]. However, 2-D spatial precision is lost during downsampling in pooling layers [<xref rid="pone.0196302.ref030" ref-type="bibr">30</xref>], resulting in an output of lower resolution. A typical solution is to upsample the output using a learned deconvolution, as shown in <xref ref-type="fig" rid="pone.0196302.g008">Fig 8</xref>. Nevertheless, this is a sub-optimal approach since the start point is the sub-sample function of the input image [<xref rid="pone.0196302.ref015" ref-type="bibr">15</xref>]. To compensate the resolution loss, we apply a skip architecture that combines the coarse layer with the shallow layer to refine the spatial precision of the output, as introduced by Shelhamer et al.[<xref rid="pone.0196302.ref014" ref-type="bibr">14</xref>].</p><p><xref ref-type="fig" rid="pone.0196302.g009">Fig 9</xref> illustrates the basic architecture of FCN transfered from a pre-trained VGG 16-layer net. Like most of the applications of transfer learning, the last classifier layer of VGG-16 was discarded, and all the fully connected layers was transformed to convolutional layers. A 1<bold>&#x02573;</bold>1 convolutional layer <italic>conv8</italic> was appended and a feature map <italic>map8</italic> with dimension W/32 <bold>&#x02573;</bold> H/32 <bold>&#x02573;</bold> 3 was generated (3 channels of the feature map respond to 3 classes: <italic>Rice</italic>, <italic>Weeds</italic> and <italic>Others</italic> class, respectively). However, to generate a weed cover map, the class map should be the same size as the input image, so the last feature map <italic>map8</italic> was upsampled to full resolution using deconvolutional layer, as introduced by Shelhamer et al.[<xref rid="pone.0196302.ref014" ref-type="bibr">14</xref>]. The idea of skip architecture can be applied as followed: (1) if the last feature map <italic>map8</italic> is directly upsampled by a factor of 32, then a class map at full resolution is obtained, which is called FCN32s; (2) if the feature map <italic>map4</italic> is fused with 2 <bold>&#x02573;</bold> upsampling of feature map <italic>map8</italic> using the element-wise summation and the fusion result is upsampled by a factor of 16, the result is called FCN-16s; (3) if the feature map <italic>map3</italic> is fused with 2 <bold>&#x02573;</bold> upsampling of the prediction fused from <italic>map4</italic> and <italic>map8</italic> and the fusion result is upsampled by a factor of 8, then the output class map is called FCN-8s; (4) If this fashion is continued by combining even lower layers, a dense class map containing more detailed information will be obtained.</p><fig id="pone.0196302.g009" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0196302.g009</object-id><label>Fig 9</label><caption><title>FCN-VGG16 network with skip architecture.</title></caption><graphic xlink:href="pone.0196302.g009"/></fig></sec><sec id="sec009"><title>Evaluation metrics</title><p>In this work, four evaluation metrics were applied on classification accuracy, as introduced by Shelhamer et al.[<xref rid="pone.0196302.ref014" ref-type="bibr">14</xref>]. Let n<sub>ij</sub> denote the number of pixels of class i and predicted as class j, t<sub>i</sub> be the number of pixels belonging to class i, and c be the number of classes.</p><disp-formula id="pone.0196302.e005"><alternatives><graphic xlink:href="pone.0196302.e005.jpg" id="pone.0196302.e005g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M5"><mml:mtext>pixel</mml:mtext><mml:mspace width="4pt"/><mml:mtext>accuracy</mml:mtext><mml:mo>:</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:math></alternatives><label>(4)</label></disp-formula><disp-formula id="pone.0196302.e006"><alternatives><graphic xlink:href="pone.0196302.e006.jpg" id="pone.0196302.e006g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M6"><mml:mrow><mml:mtext>mean</mml:mtext><mml:mspace width="4pt"/><mml:mtext>accuracy:</mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mi>c</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives><label>(5)</label></disp-formula><disp-formula id="pone.0196302.e007"><alternatives><graphic xlink:href="pone.0196302.e007.jpg" id="pone.0196302.e007g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M7"><mml:mrow><mml:mtext>mean</mml:mtext><mml:mspace width="4pt"/><mml:mtext>IU</mml:mtext><mml:mo>:</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>c</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mstyle displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives><label>(6)</label></disp-formula><disp-formula id="pone.0196302.e008"><alternatives><graphic xlink:href="pone.0196302.e008.jpg" id="pone.0196302.e008g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M8"><mml:mrow><mml:mtext>frequency</mml:mtext><mml:mspace width="4pt"/><mml:mtext>weighted</mml:mtext><mml:mspace width="4pt"/><mml:mtext>IU</mml:mtext><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mstyle displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives><label>(7)</label></disp-formula><p>The proposed metrics covered both pixel accuracy and region intersection over union (IU), which will systematically evaluate the performance of the tested algorithm. According to Gabriela.et al. [<xref rid="pone.0196302.ref031" ref-type="bibr">31</xref>], IU was an object-based overlap measure typically used for imbalanced datasets. Since the pixels of <italic>Weeds</italic> class was in minority in our dataset, so the mean IU was the main metrics for our experiments.</p><p>In the following experiments, all the models were trained on the training dataset (892 imagery, as shown in the section &#x0201c;<xref ref-type="sec" rid="sec002">Materials and method</xref>&#x0201d;), and evaluated on the validation dataset which is independent from the training data (200 imagery in total). All the experimental results reported were based on the validation dataset.</p></sec></sec></sec><sec id="sec010"><title>Results and discussion</title><sec id="sec011"><title>Experiments with transfer learning</title><p>Since our training dataset is relatively small, the training images are highly correlated, which may cause overfitting during training process [<xref rid="pone.0196302.ref015" ref-type="bibr">15</xref>]. Instead of training from random initialization, the pre-trained CNNs was applied for fine-tuning to accelerate the training process and avoid the problem of overfitting. The AlexNet, VGGnet [<xref rid="pone.0196302.ref028" ref-type="bibr">28</xref>] and GoogLeNet [<xref rid="pone.0196302.ref029" ref-type="bibr">29</xref>] were cast into FCNs and the output was directly restored to full resolution with in-network upsampling, as shown in <xref ref-type="fig" rid="pone.0196302.g008">Fig 8</xref>.</p><p><xref ref-type="table" rid="pone.0196302.t001">Table 1</xref> compares the performance of FCNs based on different pre-trained CNN architectures, using different metrics covering the accuracy and efficiency. Though Shelhamer et al.[<xref rid="pone.0196302.ref014" ref-type="bibr">14</xref>] report an accuracy decrease of GoogLeNet in [<xref rid="pone.0196302.ref014" ref-type="bibr">14</xref>], our implementation of GoogLeNet used a pre-trained GoogLeNet for fine-tuning and achieved a performance close to VGG 16-layer net. In contrast with AlexNet and GoogLeNet, the FCN network based on the pre-trained VGG-16 achieved the highest score in terms of prediction accuracy, so the VGG 16-layer net was chosen as the basic architecture extended to FCN.</p><table-wrap id="pone.0196302.t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0196302.t001</object-id><label>Table 1</label><caption><title>Comparison of FCN with different pre-trained CNNs.</title><p>The inference time is the average number of 200 trials for 1000 *1000 images using a GTX 1060.</p></caption><alternatives><graphic id="pone.0196302.t001g" xlink:href="pone.0196302.t001"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Evaluation metrics</th><th align="center" rowspan="1" colspan="1">FCN-AlexNet</th><th align="center" rowspan="1" colspan="1">FCN-VGG16</th><th align="center" rowspan="1" colspan="1">FCN-GoogLeNet</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><bold><italic>pixel acc</italic>.</bold></td><td align="center" rowspan="1" colspan="1"><italic>0</italic>.<italic>862</italic></td><td align="center" rowspan="1" colspan="1"><italic>0</italic>.<italic>923</italic></td><td align="center" rowspan="1" colspan="1"><italic>0</italic>.<italic>919</italic></td></tr><tr><td align="center" rowspan="1" colspan="1"><bold>mean acc.</bold></td><td align="center" rowspan="1" colspan="1">0.724</td><td align="center" rowspan="1" colspan="1">0.795</td><td align="center" rowspan="1" colspan="1">0.776</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold>mean IU</bold></td><td align="center" rowspan="1" colspan="1">0.649</td><td align="center" rowspan="1" colspan="1">0.728</td><td align="center" rowspan="1" colspan="1">0.713</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold>f.w. IU</bold></td><td align="center" rowspan="1" colspan="1">0.807</td><td align="center" rowspan="1" colspan="1">0.876</td><td align="center" rowspan="1" colspan="1">0.869</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold>inference time</bold></td><td align="center" rowspan="1" colspan="1">0.106 s</td><td align="center" rowspan="1" colspan="1">0.318 s</td><td align="center" rowspan="1" colspan="1">0.169 s</td></tr></tbody></table></alternatives></table-wrap><p>The experimental results proved that all the transferred networks achieved a remarkable performance, even the worst model achieved approximately 89% of the state-of-art performance, as shown in <xref ref-type="table" rid="pone.0196302.t001">Table 1</xref>. The VGG-16 based network achieved the highest classification accuracy, with the mean IU up to 0.728. Nevertheless, the Alex-Net based network outperformed other models in terms of efficiency, with the average speed of 0.106 second per image, demonstrating that this model could be considered in the application asking for real-time processing.</p><p><xref ref-type="fig" rid="pone.0196302.g010">Fig 10</xref> listed the results of FCNs based on different pre-trained CNNs. From <xref ref-type="fig" rid="pone.0196302.g010">Fig 10</xref>, it can be observed that FCN-AlexNet misclassified the <italic>Rice</italic> as <italic>Weeds</italic> (with black dashed boundary), and FCN-GoogLeNet misclassified <italic>Others</italic> as weeds (with yellow dashed boundary), while FCN-VGG16 correctly distinguished different classes.</p><fig id="pone.0196302.g010" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0196302.g010</object-id><label>Fig 10</label><caption><title>Classification results of FCN with different pre-trained CNNs.</title><p>(a) UAV imagery; (b) corresponding GT labels; (c-e) Results obtained by FCN-AlexNet, FCN-VGG16 and FCN-GoogLeNet, respectively.</p></caption><graphic xlink:href="pone.0196302.g010"/></fig></sec><sec id="sec012"><title>Experiments with skip architectures</title><p>To make up the resolution loss caused by downsampling operations of the network, the skip architecture was applied to improve the prediction accuracy. In this work, FCN-32s, FCN-16s and FCN-8s were taken for the experiments of skip architectures, as <xref ref-type="fig" rid="pone.0196302.g009">Fig 9</xref>.</p><p><xref ref-type="table" rid="pone.0196302.t002">Table 2</xref> lists the performance of FCN network with different skip architectures. First, the coarse output of VGG-16 net was directly upsampled to full resolution (FCN-32s), resulting in a mean IU of 0.728. Following the idea of skip architecture, the lower layer <italic>pool4</italic> was fused with the coarse output before upsampling (FCN-16s). With this architecture, the mean IU was raised by 0.021 to 0.749, which means that the fusing strategy could actually improve the classification accuracy. This fashion was continued and even lower layer <italic>pool3</italic> was fused to the network (FCN-8s), achieving a minor improvement to 0.752 mean IU. At this time, the FCN network has met diminishing returns, both in terms of mean IU and other metrics, so we stop fusing even lower layers.</p><table-wrap id="pone.0196302.t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0196302.t002</object-id><label>Table 2</label><caption><title>Comparison of the of FCN network with different skip architectures, based on the pre-trained VGG 16-layer net.</title><p>Learning is end-to-end, pixel-to-pixel.</p></caption><alternatives><graphic id="pone.0196302.t002g" xlink:href="pone.0196302.t002"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Evaluation metrics</th><th align="center" rowspan="1" colspan="1">FCN-32s</th><th align="center" rowspan="1" colspan="1">FCN-16s</th><th align="center" rowspan="1" colspan="1">FCN-8s</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><bold><italic>pixel acc</italic>.</bold></td><td align="center" rowspan="1" colspan="1">0.923</td><td align="center" rowspan="1" colspan="1">0.928</td><td align="center" rowspan="1" colspan="1">0.935</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold>mean acc.</bold></td><td align="center" rowspan="1" colspan="1">0.795</td><td align="center" rowspan="1" colspan="1">0.811</td><td align="center" rowspan="1" colspan="1">0.807</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold>mean IU</bold></td><td align="center" rowspan="1" colspan="1">0.728</td><td align="center" rowspan="1" colspan="1">0.749</td><td align="center" rowspan="1" colspan="1">0.752</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold>f.w. IU</bold></td><td align="center" rowspan="1" colspan="1">0.876</td><td align="center" rowspan="1" colspan="1">0.886</td><td align="center" rowspan="1" colspan="1">0.892</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold>inference time</bold></td><td align="center" rowspan="1" colspan="1">0.318s</td><td align="center" rowspan="1" colspan="1">0.317s</td><td align="center" rowspan="1" colspan="1">0.323s</td></tr></tbody></table></alternatives></table-wrap><p><xref ref-type="fig" rid="pone.0196302.g011">Fig 11</xref> lists the classification results of FCN-32s, FCN-16s, and FCN-8s. From <xref ref-type="fig" rid="pone.0196302.g011">Fig 11</xref>, it can be observed that FCN-32s misclassifies <italic>Rice</italic> as <italic>Weeds</italic> (in black dotted boundary), and FCN-16s misclassifies the <italic>Weeds</italic> as <italic>Rice</italic> (in blue dotted boundary), while FCN-8s achieves the highest accuracy in rice recognition and weed detection. Also, the border of FCN-8s output is more closer to the GT labels, compared with other architectures.</p><fig id="pone.0196302.g011" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0196302.g011</object-id><label>Fig 11</label><caption><title>Classification results of FCN network with different skip architectures.</title><p>(a) UAV imagery; (b) corresponding GT labels; (c-e) Results obtained by FCN-32s, FCN-16s and FCN-8s, respectively.</p></caption><graphic xlink:href="pone.0196302.g011"/></fig></sec><sec id="sec013"><title>Comparison with other methods</title><sec id="sec014"><title>Patch-based CNN</title><p>The network architecture of Patch-based CNN is shown in <xref ref-type="fig" rid="pone.0196302.g012">Fig 12</xref>. Different from the architecture used in [<xref rid="pone.0196302.ref013" ref-type="bibr">13</xref>, <xref rid="pone.0196302.ref032" ref-type="bibr">32</xref>], the modified pre-trained VGG-16 network with 1/16 down-sampling factor was applied as the feature extractor. The stride of the last pooling layer was set to 1, because fusing the previous feature map <italic>map4</italic> with the coarse output could significantly improve the classification accuracy, as shown in <xref ref-type="table" rid="pone.0196302.t002">Table 2</xref>. Following the idea of Mnih V [<xref rid="pone.0196302.ref032" ref-type="bibr">32</xref>], a fully connected layer was appended to the last pooling layer of VGG-16 network, mapping the previous layer to 256 neurons, which is later rearranged into a 16&#x000d7;16 prediction areas. Finally, an upsampling operation was performed by a factor of 4 to restore the resolution.</p><fig id="pone.0196302.g012" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0196302.g012</object-id><label>Fig 12</label><caption><title>The network architecture of Patch-based CNN.</title></caption><graphic xlink:href="pone.0196302.g012"/></fig></sec><sec id="sec015"><title>Pixel-based CNN</title><p>Compared with FCNs and Patch-based CNN, the method of Pixel-based CNN is a classification method in an &#x0201c;image-label&#x0201d; mode, which will classify a specific patch into a single label. Following the idea of Sharma et al.[<xref rid="pone.0196302.ref033" ref-type="bibr">33</xref>], classification for the imagery was processed pixel by pixel, considering the neighboring patch centered at the processed pixel. Different from the architecture used in [<xref rid="pone.0196302.ref033" ref-type="bibr">33</xref>], the pre-trained AlexNet was applied for fine-tuning. The reason for this choice is that our data is high-resolution imagery, which is close to the images in ImageNet database, so directly applying the model pre-trained on ImageNet could significantly speed up the convergence and improve the performance.</p></sec><sec id="sec016"><title>Experiments and comparison</title><p>In the experiment on Patch-based CNN, the patch size was set to 200, so the 200&#x000d7;200 patches are imported to the classifier. In the experiment on Pixel-based CNN, the performance of 5&#x000d7;5, 25&#x000d7;25 and 51&#x000d7;51 neighboring was compared, and the optimal value 25&#x000d7;25 was selected as our experimental parameter. <xref ref-type="table" rid="pone.0196302.t003">Table 3</xref> lists the comparison of performance of Patch-based CNN, Pixel-based CNN and FCN-8s on our validation dataset. It could be seen from <xref ref-type="table" rid="pone.0196302.t003">Table 3</xref> that FCN approach outperformed other methods in terms of accuracy and efficiency.</p><table-wrap id="pone.0196302.t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0196302.t003</object-id><label>Table 3</label><caption><title>Performance comparison of Patch-based CNN, Pixel-based CNN and FCN-8s.</title></caption><alternatives><graphic id="pone.0196302.t003g" xlink:href="pone.0196302.t003"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Evaluation metrics</th><th align="center" rowspan="1" colspan="1">Patch-based CNN</th><th align="center" rowspan="1" colspan="1">Pixel-based CNN</th><th align="center" rowspan="1" colspan="1">FCN-8s</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><italic>pixel acc</italic>.</td><td align="center" rowspan="1" colspan="1">0.727</td><td align="center" rowspan="1" colspan="1">0.651</td><td align="center" rowspan="1" colspan="1">0.935</td></tr><tr><td align="center" rowspan="1" colspan="1">mean acc.</td><td align="center" rowspan="1" colspan="1">0.611</td><td align="center" rowspan="1" colspan="1">0.608</td><td align="center" rowspan="1" colspan="1">0.807</td></tr><tr><td align="center" rowspan="1" colspan="1">mean IU</td><td align="center" rowspan="1" colspan="1">0.528</td><td align="center" rowspan="1" colspan="1">0.446</td><td align="center" rowspan="1" colspan="1">0.752</td></tr><tr><td align="center" rowspan="1" colspan="1">f.w. IU</td><td align="center" rowspan="1" colspan="1">0.651</td><td align="center" rowspan="1" colspan="1">0.551</td><td align="center" rowspan="1" colspan="1">0.892</td></tr><tr><td align="center" rowspan="1" colspan="1">inference time</td><td align="center" rowspan="1" colspan="1">0.338s</td><td align="center" rowspan="1" colspan="1">721.776s</td><td align="center" rowspan="1" colspan="1">0.323s</td></tr></tbody></table></alternatives></table-wrap><p><xref ref-type="table" rid="pone.0196302.t004">Table 4</xref> lists the confusion matrix of three approaches. From <xref ref-type="table" rid="pone.0196302.t004">Table 4</xref>, it is obvious that FCN network achieved the highest classification accuracy for all classes. Especially for the classification of <italic>Weeds</italic>, the accuracy of FCN network was up to 0.883, which is significantly higher than other approaches (Patch-based CNN 0.005, and Pixel-based CNN 0.455).</p><table-wrap id="pone.0196302.t004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0196302.t004</object-id><label>Table 4</label><caption><title>Confusion matrix of three approaches for the validation dataset.</title></caption><alternatives><graphic id="pone.0196302.t004g" xlink:href="pone.0196302.t004"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Algorithm</th><th align="center" rowspan="1" colspan="1">GT/ Predicted class</th><th align="center" rowspan="1" colspan="1">Rice</th><th align="center" rowspan="1" colspan="1">Weed</th><th align="center" rowspan="1" colspan="1">Others</th></tr></thead><tbody><tr><td align="center" rowspan="3" colspan="1">Patch-based CNN</td><td align="center" rowspan="1" colspan="1"><bold>Rice</bold></td><td align="char" char="." rowspan="1" colspan="1"><bold>0.971</bold></td><td align="char" char="." rowspan="1" colspan="1">0.005</td><td align="char" char="." rowspan="1" colspan="1">0.024</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold>Weed</bold></td><td align="char" char="." rowspan="1" colspan="1">0.953</td><td align="char" char="." rowspan="1" colspan="1"><bold>0.005</bold></td><td align="char" char="." rowspan="1" colspan="1">0.042</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold>Others</bold></td><td align="char" char="." rowspan="1" colspan="1">0.154</td><td align="char" char="." rowspan="1" colspan="1">0.016</td><td align="char" char="." rowspan="1" colspan="1"><bold>0.830</bold></td></tr><tr><td align="center" rowspan="3" colspan="1">Pixel-based CNN</td><td align="center" rowspan="1" colspan="1"><bold>Rice</bold></td><td align="char" char="." rowspan="1" colspan="1"><bold>0.445</bold></td><td align="char" char="." rowspan="1" colspan="1">0.261</td><td align="char" char="." rowspan="1" colspan="1">0.295</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold>Weed</bold></td><td align="char" char="." rowspan="1" colspan="1">0.122</td><td align="char" char="." rowspan="1" colspan="1"><bold>0.455</bold></td><td align="char" char="." rowspan="1" colspan="1">0.423</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold>Others</bold></td><td align="char" char="." rowspan="1" colspan="1">0.006</td><td align="char" char="." rowspan="1" colspan="1">0.020</td><td align="char" char="." rowspan="1" colspan="1"><bold>0.974</bold></td></tr><tr><td align="center" rowspan="3" colspan="1">FCN-8s</td><td align="center" rowspan="1" colspan="1"><bold>Rice</bold></td><td align="char" char="." rowspan="1" colspan="1"><bold>0.956</bold></td><td align="char" char="." rowspan="1" colspan="1">0.017</td><td align="char" char="." rowspan="1" colspan="1">0.027</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold>Weed</bold></td><td align="char" char="." rowspan="1" colspan="1">0.054</td><td align="char" char="." rowspan="1" colspan="1"><bold>0.883</bold></td><td align="char" char="." rowspan="1" colspan="1">0.063</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold>Others</bold></td><td align="char" char="." rowspan="1" colspan="1">0.050</td><td align="char" char="." rowspan="1" colspan="1">0.010</td><td align="char" char="." rowspan="1" colspan="1"><bold>0.940</bold></td></tr></tbody></table></alternatives></table-wrap><p><xref ref-type="fig" rid="pone.0196302.g013">Fig 13</xref> lists the classification results of three algorithms in comparison. From <xref ref-type="fig" rid="pone.0196302.g013">Fig 13</xref>, it is obvious that the weed cover map generated by FCN network is more precise than other approaches.</p><fig id="pone.0196302.g013" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0196302.g013</object-id><label>Fig 13</label><caption><title>Classification results of three algorithms in comparison.</title><p>(a) UAV imagery; (b) corresponding GT labels; (c-e) Results obtained by Patch-based CNN, Pixel-based CNN and FCN-8s, respectively.</p></caption><graphic xlink:href="pone.0196302.g013"/></fig><p>In the Patch-based CNN algorithm, each patch is input to the network as an image, and the context within the patch is taken into consideration. However, the relationship between different patches is ignored, result in the discontinuities between patches. In comparison, the FCN method takes the whole image as input, which will consider the whole image overall and seamlessly. According to results of Patch-based CNN, most of the <italic>Weeds</italic> were misclassified as <italic>Rice</italic>, as <xref ref-type="fig" rid="pone.0196302.g013">Fig 13(c)</xref>. From <xref ref-type="table" rid="pone.0196302.t004">Table 4</xref>, the proportion of <italic>Weeds</italic> pixels classified as <italic>Rice</italic> was up to 0.953, which means that almost all the <italic>Weeds</italic> are recognized as <italic>Rice</italic>. One possible reason for this misclassification is that, the weeds are much less than rice in the training dataset, so the network was trained with rice data for much more iterations. Another possible reason is that weeds are similar with rice, both in color and texture features, so the network is more likely to classify the <italic>Weeds</italic> as <italic>Rice</italic>. Compared with Patch-based CNN, the FCN method uses patch-wise training strategy and can mitigate the problem of data imbalance, so the weeds in the imagery can be correctly classified, as shown in <xref ref-type="fig" rid="pone.0196302.g013">Fig 13(d)</xref>.</p><p>In the Pixel-based CNN algorithm, the whole image is processed pixel by pixel. For the classification of each pixel, its surrounding 25&#x000d7;25 neighborhood is input to the AlexNet. However, the classification accuracy of Pixel-based CNN is much lower than FCN (<xref ref-type="table" rid="pone.0196302.t003">Table 3</xref>), and change of the neighborhood size made no noticeable improvement. To speeding the computation, we set the batch size to 1000, which meant that the network will handle 1000 samples at one iteration, while the inference speed was still slower than FCN since too much redundant computation were preformed over the the neighborhoods. Compared with Pixel-based CNN approach, the FCN method took the whole image as input, resulting in better performance both in terms of accuracy and efficiency. As shown in <xref ref-type="fig" rid="pone.0196302.g013">Fig 13</xref>, a lot of <italic>Rice</italic> was misclassified as <italic>Weeds</italic> with Pixel-based CNN, while FCN generated a more accurate class map.</p><p>It can be seen from the comparison results that FCN method outperformed other approaches in terms of accuracy and efficiency. Also, the research on FCN and weed mapping could be expanded to operational situations of SSWM. Usually, the weed cover map produced by FCN method can be converted to a prescription herbicide treatment map [<xref rid="pone.0196302.ref003" ref-type="bibr">3</xref>] and then be transferred to machinery embedded with variable-rate spraying system. Variable-rate spraying system uses the prescription map to change flow rates according to weed densities of different sites, which may significantly promote the herbicide saving while enhancing the herbicide effectiveness.</p></sec></sec></sec><sec sec-type="conclusions" id="sec017"><title>Conclusions</title><p>In this paper, UAV imagery was collected over a rice field, and then the FCN method was proposed for semantic labeling and weed mapping of the imagery. Transfer learning was used to improve the generalization capability and skip architecture was applied to increase the prediction accuracy. After that, the FCN network was compared with another two deep learning approaches. Experimental results demonstrated that our FCN network outperformed others, both in terms of accuracy and efficiency. Especially for the recognition of weeds, our FCN approach achieved the highest accuracy compared with other methods.</p><p>However, FCN is a fully supervised algorithm, and the training and updating of the network rely on large amount of labeled images, which requires extensive manual labeling work. This requirement limits the application of this method. So in the future of this work, we plan to introduce the weak-supervised learning and unsupervised learning algorithm to reduce the manual labeling work and enhance the ease of application.</p></sec></body><back><ack><p>The authors thank Prof. Guohua Zhong for granting permission for imagery collection and field investigation of the studied site and Mr. Yuancheng Liang for providing professional instruction in Ground Truth labeling.</p></ack><ref-list><title>References</title><ref id="pone.0196302.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Pena</surname><given-names>JM</given-names></name>, <name><surname>Torres-Sanchez</surname><given-names>J</given-names></name>, <name><surname>de Castro</surname><given-names>AI</given-names></name>, <name><surname>Kelly</surname><given-names>M</given-names></name>, <name><surname>Lopez-Granados</surname><given-names>F</given-names></name>. <article-title>Weed mapping in early-season maize fields using object-based analysis of unmanned aerial vehicle (UAV) images</article-title>. <source>PLOS ONE. [Journal Article; Research Support, Non-U.S. Gov't]</source>. 2013 <year>2013</year>-<month>1</month>-<day>20</day>;<volume>8</volume>(<issue>10</issue>):<fpage>e77151</fpage>.</mixed-citation></ref><ref id="pone.0196302.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>L&#x000f3;pez-Granados</surname><given-names>F</given-names></name>, <name><surname>Torres-S&#x000e1;nchez</surname><given-names>J</given-names></name>, <name><surname>Serrano-P&#x000e9;rez</surname><given-names>A</given-names></name>, <name><surname>de Castro</surname><given-names>AI</given-names></name>, <name><surname>Mesas-Carrascosa</surname><given-names>FJ</given-names></name>, <name><surname>Pe&#x000f1;a</surname><given-names>J</given-names></name>. <article-title>Early season weed mapping in sunflower using UAV technology: variability of herbicide treatment maps against weed thresholds</article-title>. <source>PRECIS AGRIC</source>. <year>2016</year>;<volume>17</volume>(<issue>2</issue>):<fpage>183</fpage>&#x02013;<lpage>99</lpage>.</mixed-citation></ref><ref id="pone.0196302.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Lan</surname><given-names>Y</given-names></name>, <name><surname>Thomson</surname><given-names>SJ</given-names></name>, <name><surname>Huang</surname><given-names>Y</given-names></name>, <name><surname>Hoffmann</surname><given-names>WC</given-names></name>, <name><surname>Zhang</surname><given-names>H</given-names></name>. <article-title>Current status and future directions of precision aerial application for site-specific crop management in the USA</article-title>. <source>COMPUT ELECTRON AGR</source>. <year>2010</year>;<volume>74</volume>(<issue>1</issue>):<fpage>34</fpage>&#x02013;<lpage>8</lpage>.</mixed-citation></ref><ref id="pone.0196302.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>KOGER</surname><given-names>CH</given-names></name>, <name><surname>SHAW</surname><given-names>DR</given-names></name>, <name><surname>WATSON</surname><given-names>CE</given-names></name>, <name><surname>REDDY</surname><given-names>KN</given-names></name>. <article-title>Detecting Late-Season Weed Infestations in Soybean (Glycine max)1</article-title>. <source>WEED TECHNOL</source>. <year>2003</year>;<volume>17</volume>(<issue>4</issue>):<fpage>696</fpage>&#x02013;<lpage>704</lpage>.</mixed-citation></ref><ref id="pone.0196302.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Castro</surname><given-names>A</given-names></name>, <name><surname>Jurado-Exp&#x000f3;sito</surname><given-names>M</given-names></name>, <name><surname>A-Barrag&#x000e1;n</surname><given-names>JP</given-names></name>, <name><surname>L&#x000f3;pez-Granados</surname><given-names>F</given-names></name>. <article-title>Airborne multi-spectral imagery for mapping cruciferous weeds in cereal and legume crops</article-title>. <source>Precision Agric</source>. <year>2012</year>;<volume>13</volume>:<fpage>302</fpage>&#x02013;<lpage>21</lpage>.</mixed-citation></ref><ref id="pone.0196302.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>P&#x000e9;rez-Ortiz</surname><given-names>M</given-names></name>, <name><surname>Pe&#x000f1;a</surname><given-names>JM</given-names></name>, <name><surname>Guti&#x000e9;rrez</surname><given-names>PA</given-names></name>, <name><surname>Torres-S&#x000e1;nchez</surname><given-names>J</given-names></name>, <name><surname>Herv&#x000e1;s-Mart&#x000ed;nez</surname><given-names>C</given-names></name>, <name><surname>L&#x000f3;pez-Granados</surname><given-names>F</given-names></name>. <article-title>A semi-supervised system for weed mapping in sunflower crops using unmanned aerial vehicles and a crop row detection method</article-title>. <source>APPL SOFT COMPUT</source>. <year>2015</year>;<volume>37</volume>:<fpage>533</fpage>&#x02013;<lpage>44</lpage>.</mixed-citation></ref><ref id="pone.0196302.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Castaldi</surname><given-names>F</given-names></name>, <name><surname>Pelosi</surname><given-names>F</given-names></name>, <name><surname>Pascucci</surname><given-names>S</given-names></name>, <name><surname>Casa</surname><given-names>R</given-names></name>. <article-title>Assessing the potential of images from unmanned aerial vehicles (UAV) to support herbicide patch spraying in maize</article-title>. <source>PRECIS AGRIC</source>. <year>2017</year>;<volume>18</volume>(<issue>1</issue>):<fpage>76</fpage>&#x02013;<lpage>94</lpage>.</mixed-citation></ref><ref id="pone.0196302.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Alexandridis</surname><given-names>TK</given-names></name>, <name><surname>Tamouridou</surname><given-names>AA</given-names></name>, <name><surname>Pantazi</surname><given-names>XE</given-names></name>, <name><surname>Lagopodi</surname><given-names>AL</given-names></name>, <name><surname>Kashefi</surname><given-names>J</given-names></name>, <name><surname>Ovakoglou</surname><given-names>G</given-names></name>, <etal>et al</etal>
<article-title>Novelty Detection Classifiers in Weed Mapping: Silybum marianum Detection on UAV Multispectral Images</article-title>. <source>SENSORS-BASEL</source>. <year>2017</year>;<volume>17</volume>(<issue>9</issue>):<fpage>2007</fpage>.</mixed-citation></ref><ref id="pone.0196302.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Tamouridou</surname><given-names>A</given-names></name>, <name><surname>Alexandridis</surname><given-names>T</given-names></name>, <name><surname>Pantazi</surname><given-names>X</given-names></name>, <name><surname>Lagopodi</surname><given-names>A</given-names></name>, <name><surname>Kashefi</surname><given-names>J</given-names></name>, <name><surname>Kasampalis</surname><given-names>D</given-names></name>, <etal>et al</etal>
<article-title>Application of Multilayer Perceptron with Automatic Relevance Determination on Weed Mapping Using UAV Multispectral Imagery</article-title>. <source>SENSORS-BASEL</source>. <year>2017</year>;<volume>17</volume>(<issue>12</issue>):<fpage>2307</fpage>.</mixed-citation></ref><ref id="pone.0196302.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Bejiga</surname><given-names>MB</given-names></name>, <name><surname>Nouffidj</surname><given-names>AZA</given-names></name>, <name><surname>Melgani</surname><given-names>F</given-names></name>. <article-title>A Convolutional Neural Network Approach for Assisting Avalanche Search and Rescue Operations with UAV Imagery</article-title>. <source>REMOTE SENS-BASEL</source>. <year>2017</year>;<volume>100</volume>(<issue>9</issue>).</mixed-citation></ref><ref id="pone.0196302.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Hu</surname><given-names>F</given-names></name>, <name><surname>Xia</surname><given-names>GS</given-names></name>, <name><surname>Hu</surname><given-names>J</given-names></name>, <name><surname>Zhang</surname><given-names>L</given-names></name>. <article-title>Transferring Deep Convolutional Neural Networks for the Scene Classification of High-Resolution Remote Sensing Imagery</article-title>. <source>REMOTE SENS-BASEL</source>. <year>2015</year>(<issue>7</issue>):<fpage>14680</fpage>&#x02013;<lpage>707</lpage>.</mixed-citation></ref><ref id="pone.0196302.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>L&#x000e4;ngkvist</surname><given-names>M</given-names></name>, <name><surname>Kiselev</surname><given-names>A</given-names></name>, <name><surname>Alirezaie</surname><given-names>M</given-names></name>, <name><surname>Loutfi</surname><given-names>A</given-names></name>. <article-title>Classification and Segmentation of Satellite Orthoimagery Using Convolutional Neural Networks</article-title>. <source>REMOTE SENS-BASEL</source>. <year>2016</year>;<volume>329</volume>(<issue>8</issue>).</mixed-citation></ref><ref id="pone.0196302.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Fu</surname><given-names>G</given-names></name>, <name><surname>Liu</surname><given-names>C</given-names></name>, <name><surname>Zhou</surname><given-names>R</given-names></name>, <name><surname>Sun</surname><given-names>T</given-names></name>, <name><surname>Zhang</surname><given-names>Q</given-names></name>. <article-title>Classification for High Resolution Remote Sensing Imagery Using a Fully Convolutional Network</article-title>. <source>REMOTE SENS-BASEL</source>. <year>2017</year>;<volume>6</volume>(<issue>9</issue>):<fpage>498</fpage>.</mixed-citation></ref><ref id="pone.0196302.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Shelhamer</surname><given-names>E</given-names></name>, <name><surname>Long</surname><given-names>J</given-names></name>, <name><surname>Darrell</surname><given-names>T</given-names></name>. <article-title>Fully Convolutional Networks for Semantic Segmentation</article-title>. <source>IEEE Conference on Computer Vision &#x00026; Pattern Recognition. [Journal Article]</source>. 2015 <year>2017</year>-<month>4</month>-<day>01</day>;<volume>79</volume>(<issue>10</issue>):<fpage>3431</fpage>&#x02013;<lpage>40</lpage>.</mixed-citation></ref><ref id="pone.0196302.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Sherrah</surname><given-names>J</given-names></name>. <article-title>Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution Aerial Imagery</article-title>. <source>arXiv:1606.02585</source>. <year>2016</year>;<volume>9</volume>.</mixed-citation></ref><ref id="pone.0196302.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>Y</given-names></name>, <name><surname>Wang</surname><given-names>H</given-names></name>, <name><surname>Guo</surname><given-names>T</given-names></name>, <name><surname>Zhang</surname><given-names>J</given-names></name>, <name><surname>Tang</surname><given-names>X</given-names></name>, <name><surname>Chen</surname><given-names>Z</given-names></name>. <article-title>Breeding and application of high-quality and diseaseresistant rice variety, huahang No.31</article-title>. <source>Guangdong Agricultural Sciences</source>. <year>2013</year>(<issue>10</issue>):<fpage>8</fpage>&#x02013;<lpage>11</lpage>.</mixed-citation></ref><ref id="pone.0196302.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Yu</surname><given-names>J</given-names></name>, <name><surname>Gao</surname><given-names>H</given-names></name>, <name><surname>Pan</surname><given-names>L</given-names></name>, <name><surname>Yao</surname><given-names>Z</given-names></name>, <name><surname>Dong</surname><given-names>L</given-names></name>. <article-title>Mechanism of resistance to cyhalofop-butyl in Chinese sprangletop (Leptochloa chinensis (L.) Nees)</article-title>. <source>PESTIC BIOCHEM PHYS</source>. <year>2016</year>.</mixed-citation></ref><ref id="pone.0196302.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Schwartz</surname><given-names>AM</given-names></name>, <name><surname>Paskewitz</surname><given-names>SM</given-names></name>, <name><surname>Orth</surname><given-names>AP</given-names></name>, <name><surname>Tesch</surname><given-names>MJ</given-names></name>, <name><surname>Toong</surname><given-names>YC</given-names></name>. <article-title>The lethal effects of Cyperus iria on Aedes aegypti</article-title>. <source>J AM MOSQUITO CONTR</source>. <year>1998</year>;<volume>1</volume>(<issue>14</issue>):<fpage>78</fpage>&#x02013;<lpage>82</lpage>.</mixed-citation></ref><ref id="pone.0196302.ref019"><label>19</label><mixed-citation publication-type="other">Phantom_4_Pro_Pro_Plus_User_Manual_EN. <ext-link ext-link-type="uri" xlink:href="https://dl.djicdn.com/downloads/phantom_4_pro/20171017/Phantom_4_Pro_Pro_Plus_User_Manual_EN.pdf/">https://dl.djicdn.com/downloads/phantom_4_pro/20171017/Phantom_4_Pro_Pro_Plus_User_Manual_EN.pdf/</ext-link> (accessed on 14 November 2017).</mixed-citation></ref><ref id="pone.0196302.ref020"><label>20</label><mixed-citation publication-type="other">GS_Pro_User_Manual_EN_V1.8. <ext-link ext-link-type="uri" xlink:href="https://dl.djicdn.com/downloads/groundstation_pro/20170831/GS_Pro_User_Manual_EN_V1.8.pdf/">https://dl.djicdn.com/downloads/groundstation_pro/20170831/GS_Pro_User_Manual_EN_V1.8.pdf/</ext-link> (accessed on 14 November 2017).</mixed-citation></ref><ref id="pone.0196302.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Gao</surname><given-names>Z</given-names></name>, <name><surname>Wang</surname><given-names>L</given-names></name>, <name><surname>Zhou</surname><given-names>L</given-names></name>, <name><surname>Zhang</surname><given-names>J</given-names></name>. <article-title>HEp-2 Cell Image Classification with Deep Convolutional Neural Networks</article-title>. <source>IEEE Journal of Biomedical &#x00026; Health Informatics</source>. <year>2017</year>:<fpage>91</fpage>&#x02013;<lpage>9</lpage>.</mixed-citation></ref><ref id="pone.0196302.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>LeCun</surname><given-names>Y</given-names></name>, <name><surname>Bengio</surname><given-names>Y</given-names></name>, <name><surname>Hinton</surname><given-names>G</given-names></name>. <article-title>Deep learning</article-title>. <source>NATURE</source>. 2015 <year>2015</year>-<month>5</month>-<day>27</day>;<volume>521</volume>(<issue>7553</issue>):<fpage>436</fpage>&#x02013;<lpage>44</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature14539">10.1038/nature14539</ext-link></comment>
<?supplied-pmid 26017442?><pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation></ref><ref id="pone.0196302.ref023"><label>23</label><mixed-citation publication-type="other">Donahue J, Jia Y, Vinyals O, Hoffman J, Zhang N, Tzeng E, et al. DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition. 2013.</mixed-citation></ref><ref id="pone.0196302.ref024"><label>24</label><mixed-citation publication-type="book"><name><surname>Deng</surname><given-names>J</given-names></name>, <name><surname>Dong</surname><given-names>W</given-names></name>, <name><surname>Socher</surname><given-names>R</given-names></name>, <name><surname>Li</surname><given-names>L</given-names></name>, <name><surname>Li</surname><given-names>K</given-names></name>, <name><surname>Fei-Fei</surname><given-names>L</given-names></name>. <chapter-title>ImageNet: A large-scale hierarchical image database</chapter-title> In: Editor, editor; 2009. <source>Pub Place</source>: <publisher-name>IEEE</publisher-name>; <year>2009</year> p. <fpage>248</fpage>&#x02013;<lpage>55</lpage>.</mixed-citation></ref><ref id="pone.0196302.ref025"><label>25</label><mixed-citation publication-type="other">How to Retrain Inception's Final Layer for New Categories. <ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/tutorials/image_retraining/">https://www.tensorflow.org/tutorials/image_retraining/</ext-link> (accessed on 5 December 2017).</mixed-citation></ref><ref id="pone.0196302.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Maggiori</surname><given-names>E</given-names></name>, <name><surname>Tarabalka</surname><given-names>Y</given-names></name>, <name><surname>Charpiat</surname><given-names>G</given-names></name>, <name><surname>Alliez</surname><given-names>P</given-names></name>. <article-title>Convolutional Neural Networks for Large-Scale Remote-Sensing Image Classification</article-title>. <source>IEEE T GEOSCI REMOTE</source>. <year>2016</year>;<volume>55</volume>(<issue>2</issue>):<fpage>645</fpage>&#x02013;<lpage>57</lpage>.</mixed-citation></ref><ref id="pone.0196302.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Krizhevsky</surname><given-names>A</given-names></name>, <name><surname>Sutskever</surname><given-names>I</given-names></name>, <name><surname>Hinton</surname><given-names>GE</given-names></name>. <article-title>ImageNet classification with deep convolutional neural networks</article-title>. <source>International Conference on Neural Information Processing Systems</source>. <year>2012</year>;<volume>25</volume>(<issue>2</issue>):<fpage>1097</fpage>&#x02013;<lpage>105</lpage>.</mixed-citation></ref><ref id="pone.0196302.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Simonyan</surname><given-names>K</given-names></name>, <name><surname>Zisserman</surname><given-names>A</given-names></name>. <article-title>VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION</article-title>. <source>arXiv:1409.1556v6 [cs.CV]</source>. <year>2015</year>(<issue>4</issue>).</mixed-citation></ref><ref id="pone.0196302.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>Szegedy</surname><given-names>C</given-names></name>, <name><surname>Liu</surname><given-names>W</given-names></name>, <name><surname>Jia</surname><given-names>Y</given-names></name>, <name><surname>Sermanet</surname><given-names>P</given-names></name>, <name><surname>Reed</surname><given-names>S</given-names></name>. <article-title>Going deeper with convolutions</article-title>. <source>arXiv:1409.4842v1</source>. <year>2014</year>;<volume>9</volume>.</mixed-citation></ref><ref id="pone.0196302.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Maggiori</surname><given-names>E</given-names></name>, <name><surname>Tarabalka</surname><given-names>Y</given-names></name>, <name><surname>Charpiat</surname><given-names>G</given-names></name>, <name><surname>Alliez</surname><given-names>P</given-names></name>. <article-title>High-Resolution Semantic Labeling with Convolutional Neural Networks</article-title>. <source>arXiv:1611.01962</source>. <year>2016</year>;<month>11</month>.</mixed-citation></ref><ref id="pone.0196302.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Csurka</surname><given-names>G</given-names></name>, <name><surname>Larlus</surname><given-names>D</given-names></name>, <name><surname>Perronnin</surname><given-names>F</given-names></name>. <article-title>What is a good evaluation measure for semantic segmentation?</article-title>
<source>BMVC</source>. <year>2013</year>;<volume>27</volume>:<fpage>2013</fpage>.</mixed-citation></ref><ref id="pone.0196302.ref032"><label>32</label><mixed-citation publication-type="book"><name><surname>Mnih</surname><given-names>V</given-names></name>. <source>Machine Learning for Aerial Image Labeling</source>. <publisher-loc>Toronto, Canada</publisher-loc>: <publisher-name>University of Toronto</publisher-name>; <year>2013</year>.</mixed-citation></ref><ref id="pone.0196302.ref033"><label>33</label><mixed-citation publication-type="journal"><name><surname>Sharma</surname><given-names>A</given-names></name>, <name><surname>Liu</surname><given-names>X</given-names></name>, <name><surname>Yang</surname><given-names>X</given-names></name>, <name><surname>Shi</surname><given-names>D</given-names></name>. <article-title>A patch-based convolutional neural network for remote sensing image classification</article-title>. <source>Neural Networks</source>. <year>2017</year>;<volume>95</volume>:<fpage>19</fpage>&#x02013;<lpage>28</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neunet.2017.07.017">10.1016/j.neunet.2017.07.017</ext-link></comment>
<?supplied-pmid 28843092?><pub-id pub-id-type="pmid">28843092</pub-id></mixed-citation></ref></ref-list></back></article>