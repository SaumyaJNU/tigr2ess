<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.0 20120330//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.0?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Int J Anal Chem</journal-id><journal-id journal-id-type="iso-abbrev">Int J Anal Chem</journal-id><journal-id journal-id-type="publisher-id">IJAC</journal-id><journal-title-group><journal-title>International Journal of Analytical Chemistry</journal-title></journal-title-group><issn pub-type="ppub">1687-8760</issn><issn pub-type="epub">1687-8779</issn><publisher><publisher-name>Hindawi</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6076898</article-id><article-id pub-id-type="doi">10.1155/2018/8032831</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Authenticity Detection of Black Rice by Near-Infrared Spectroscopy and Support Vector Data Description</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Hui</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref><xref ref-type="aff" rid="I2">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-4652-2053</contrib-id><name><surname>Tan</surname><given-names>Chao</given-names></name><email>chaotan1112@163.com</email><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Lin</surname><given-names>Zan</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref><xref ref-type="aff" rid="I3">
<sup>3</sup>
</xref></contrib></contrib-group><aff id="I1">
<sup>1</sup>Key Lab of Process Analysis and Control of Sichuan Universities, Yibin University, Yibin, Sichuan 644000, China</aff><aff id="I2">
<sup>2</sup>Hospital, Yibin University, Yibin, Sichuan 644000, China</aff><aff id="I3">
<sup>3</sup>The First Affiliated Hospital, Chongqing Medical University, Chongqing 400016, China</aff><author-notes><fn fn-type="other"><p>Academic Editor: Richard G. Brereton</p></fn></author-notes><pub-date pub-type="collection"><year>2018</year></pub-date><pub-date pub-type="epub"><day>9</day><month>7</month><year>2018</year></pub-date><volume>2018</volume><elocation-id>8032831</elocation-id><history><date date-type="received"><day>26</day><month>3</month><year>2018</year></date><date date-type="rev-recd"><day>3</day><month>6</month><year>2018</year></date><date date-type="accepted"><day>11</day><month>6</month><year>2018</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2018 Hui Chen et al.</copyright-statement><copyright-year>2018</copyright-year><license xlink:href="https://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><abstract><p>Black rice is an important rice species in Southeast Asia. It is a common phenomenon to pass low-priced black rice off as high-priced ones for economic benefit, especially in some remote towns. There is increasing need for the development of fast, easy-to-use, and low-cost analytical methods for authenticity detection. The feasibility to utilize near-infrared (NIR) spectroscopy and support vector data description (SVDD) for such a goal is explored. Principal component analysis (PCA) is used for exploratory analysis and feature extraction. Another two data description methods, i.e., k-nearest neighbor data description (KNNDD) and GAUSS method, are used as the reference. A total of 142 samples from three brands were collected for spectral analysis. Each time, the samples of a brand serve as the target class whereas other samples serve as the outlier class. Based on both the first two principal components (PCs) and original variables, three types of data descriptions were constructed. On average, the optimized SVDD model achieves acceptable performance, i.e., a specificity of 100% and a sensitivity of 94.2% on the independent test set with tight boundary. It indicates that SVDD combined with NIR is feasible and effective for authenticity detection of black rice.</p></abstract><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>21375118</award-id><award-id>J1310041</award-id></award-group><award-group><funding-source>Scientific Research Foundation of Sichuan Provincial Education Department of China</funding-source><award-id>17TD0048</award-id></award-group><award-group><funding-source>Yibin University</funding-source><award-id>2017ZD05</award-id></award-group><award-group><funding-source>Department of Science and Technology of Sichuan Province</funding-source><award-id>2018JY0504</award-id></award-group><award-group><funding-source>Sichuan University</funding-source><award-id>2015006</award-id><award-id>2016002</award-id></award-group></funding-group></article-meta></front><body><sec id="sec1"><title>1. Introduction</title><p>Black rice is an economically important special rice species and has been consumed for a long time in Southeast Asia including China [<xref rid="B1" ref-type="bibr">1</xref>&#x02013;<xref rid="B3" ref-type="bibr">3</xref>]. Many researches have showed that black rice has considerably strong free-radical scavenging and antioxidation effects, as well as other biological effects of its extracts such as antimutagenic and anticarcinogenic [<xref rid="B4" ref-type="bibr">4</xref>, <xref rid="B5" ref-type="bibr">5</xref>]. Black rice quality in terms of nutrition is also valuable for its protein content and the balance of essential amino acids. In fact, black rice is also a mixture of various carbohydrates. There exist varying amounts of nutrient in different kinds of black rice because of genetic and environmental factors. In market, there exist many brands of black rice. The quality and price of them vary greatly and renowned brands have higher price. However, illegal tradesman often passes low-priced black rice off as high-priced ones for economic benefit, especially in some remote towns.</p><p>How to discriminate different types of black rice is interesting. Up to now, it is mainly dependent on human senses. More objective and novel methods are maybe based on complex instruments such as high performance liquid chromatography or mass spectroscopy (MS) [<xref rid="B9" ref-type="bibr">6</xref>]. In recent years, molecular spectroscopy has drawn more attention and proved to be a powerful tool for authenticity detection [<xref rid="B10" ref-type="bibr">7</xref>&#x02013;<xref rid="B12" ref-type="bibr">9</xref>]. In particular, near-infrared (NIR) spectroscopy becomes the most widely used technique in various fields including cigarettes [<xref rid="B13" ref-type="bibr">10</xref>], food [<xref rid="B14" ref-type="bibr">11</xref>], textile [<xref rid="B15" ref-type="bibr">12</xref>], medicine [<xref rid="B16" ref-type="bibr">13</xref>], and drug [<xref rid="B17" ref-type="bibr">14</xref>]. It is capable of rapidly obtaining a vector/matrix signal of a complex sample and therefore provides the chance of executing a in-depth qualitative or quantitative analysis. Detection of food authenticity is a important task in food analysis and aims to answer the question on which class a particular sample belongs to by its spectral signal. Often, it can be realized by comparing spectra of a specimen to be identified with spectra of &#x0201c;known&#x0201d; or &#x0201c;standard.&#x0201d; As for NIR spectroscopy, however, spectral signals for complex food systems are characterized by peak overlapping and poor resolution. So, an appropriate chemometric model is indispensable for a NIR-based application.</p><p>For the perspective of modeling, chemometrics involving qualitative tasks can be divided into two categories: classification and one-class classification, i.e., data description [<xref rid="B18" ref-type="bibr">15</xref>]. Classification is vey often considered as a synonym of discriminant analysis methods since they assign a new sample to one of a set of predefined classes. The corresponding classifier is trained on a training set. Data description differs in one essential aspect from the conventional classification since it is assumed that only information on a single class is available. Data description problems are common in the real world where positive objects are widely available but negative ones are maybe hard, expensive, or even impossible to gather [<xref rid="B19" ref-type="bibr">16</xref>]. In the literature, three main approaches can be distinguished: the density estimation, the boundary methods, and the reconstruction methods [<xref rid="B20" ref-type="bibr">17</xref>]. General demand of any authentication problem is that a genuine class, i.e., a target class, must be known [<xref rid="B21" ref-type="bibr">18</xref>, <xref rid="B22" ref-type="bibr">19</xref>]. The target class is always unique for a specific authentication problem. Any other objects, or classes of objects, that are not members of the target class are considered as outliers. This also means that just samples of the target class can be utilized and that no information on the other classes is present. For data description, the boundary surrounding the target class has to be estimated from available data, such that it accepts as much of the target samples as possible and minimizes the error of accepting outlier. Up to now, much effort has been expended to develop classification algorithms, and the concept of data description is also of interest and noticeable [<xref rid="B23" ref-type="bibr">20</xref>&#x02013;<xref rid="B26" ref-type="bibr">23</xref>], especially in the cases where it is impossible to meaningfully define all of the classes and obtain fully representative samples. In food authenticity, the interest is focused on a single target class so as to verify compliance of samples with the features of that class, and a data description approach should be adopted to build an enclosed boundary around the target class.</p><p>The present work focuses on exploring the feasibility to utilize near-infrared (NIR) spectroscopy and support vector data description (SVDD) for authenticity diction of black rice. Principal component analysis (PCA) is used for exploratory analysis and feature extraction. Another two data description methods, i.e., k-nearest neighbor data description (KNNDD) and GAUSS method, are used as the reference. A total of 142 samples from three brands were collected for spectral analysis. All spectra were preprocessed beforehand by standard normal transformation (SNV). Each time, the samples of a brand serve as the target class whereas other samples serve as the outlier class. Based on both the first two principal components (PCs) and original variables, three types of data descriptions were constructed. On average, the optimized SVDD model achieves acceptable performance, i.e., a specificity of 100% and a sensitivity of 94.2% on the independent test set with tight boundary. The effect of training set size and the parameter of kernel width have also been discussed. It indicates that SVDD combined with NIR is feasible and effective for authenticity detection of black rice.</p></sec><sec id="sec2"><title>2. Theory and Methods</title><p>Many methods have been developed to solve the one-class or data description problem and they can be divided into three main categories: density, boundary, and reconstruction methods. Here, three algorithms, i.e., support vector data description, Nearest Neighbor Method, and Gaussian Method, are introduced and used for experiments, among which the first two are boundary methods and the last one belongs to density method.</p><sec id="sec2.1"><title>2.1. Support Vector Data Description (SVDD)</title><p>SVDD is a novel algorithm for one-class classification problems, which has been proposed by Tax [<xref rid="B18" ref-type="bibr">15</xref>], inspired by the idea of the support vector machines. It focuses on finding a minimum hypersphere around the target class. The hypersphere can be used to decide whether new objects are targets or outliers. Such a sphere is characterized only by center <bold>a</bold> and radius <italic>R</italic>. When seeking sphere, it needs to minimize the volume of the sphere by minimizing <italic>R</italic><sup>2</sup> and demand that the sphere covers as many training samples as possible. Given the training set {<bold>x</bold><sub><italic>i</italic></sub>, <italic>i</italic> = 1,2,&#x02026;, <italic>N</italic>}, the task in SVDD is to minimize error function:<disp-formula id="EEq1"><label>(1)</label><mml:math id="M1"><mml:mtable style="T36"><mml:mtr><mml:mtd><mml:maligngroup/><mml:mi mathvariant="normal">min</mml:mi><mml:mo>&#x02061;</mml:mo><mml:malignmark/><mml:mo id="LSAYGWD0"/><mml:mi>&#x02003;</mml:mi><mml:mi>L</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.4pt" depth="2.484pt"/><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">a</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003b6;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.4pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mrow><mml:msub><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b6;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="EEq2"><label>(2)</label><mml:math id="EEq2EAAAMBCDCA"><mml:mtable><mml:mtr><mml:mtd><mml:maligngroup/><mml:mtext>s.t.</mml:mtext><mml:malignmark/><mml:mo id="LSAYGWD1"/><mml:mi>&#x02003;</mml:mi><mml:mfenced open="&#x02016;" close="&#x02016;" separators="|"><mml:mrow><mml:mspace height="5.32999pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold">a</mml:mi><mml:mspace height="5.32999pt" depth="2.484pt"/></mml:mrow></mml:mfenced><mml:mo>&#x02264;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003b6;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="10pt"/><mml:msub><mml:mrow><mml:mi>&#x003b6;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02265;</mml:mo><mml:mn mathvariant="normal">0</mml:mn><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02200;</mml:mo><mml:mi mathvariant="normal">i</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <bold>a</bold> and <italic>R</italic> are the center and the radius of the hypersphere, respectively; <italic>C</italic> is the penalty factor which regulates the hyperspherical volume and error, i.e., the number of target objects rejected; <italic>&#x003b6;</italic><sub><italic>i</italic></sub> is a slack variable for allowable error limitation. Almost all objects are within the sphere. This optimization problem can be solved by Lagrange multiplier method [<xref rid="B27" ref-type="bibr">24</xref>].</p><p>Because the target class is not spherically distributed in most cases, some traditional decision rules may not work well. To make a more effective and flexible decision, the original data can be implicitly transformed to a higher dimension by the so-called kernel function <italic>K</italic>(<bold>x</bold><sub><italic>i</italic></sub>, <bold>x</bold><sub><italic>j</italic></sub>). Several kernel functions including linear, polynomial, Gaussian, radial basis function (RBF) are available [<xref rid="B28" ref-type="bibr">25</xref>, <xref rid="B29" ref-type="bibr">26</xref>]. In this work, the RBF kernel, the most commonly used kernel in machine learning, was used. The form of RBF kernel is<disp-formula id="EEq3"><label>(3)</label><mml:math id="M2"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mi>K</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="4.29pt" depth="4.19899pt"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace height="4.29pt" depth="4.19899pt"/></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="20.05989pt" depth="7.18999pt"/><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mfenced open="&#x02016;" close="&#x02016;" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mspace height="20.05989pt" depth="7.18999pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>&#x003c3;</italic> is a key parameter for controlling the boundary tightness.</p></sec><sec id="sec2.2"><title>2.2. Nearest Neighbor Method</title><p>The most straightforward and simplest method to obtain a one-class model is to estimate the density of the training set. Unfortunately, it often requires a large number of samples to avoid the curse of dimensionality. Instead of estimating whole probability densities, an indication of the resemblance can also be acquired by comparing distances. Nearest neighbor method can be derived from a local density estimation [<xref rid="B30" ref-type="bibr">27</xref>]. It avoids the explicit density estimation by only using distances to the first nearest neighbor. In the process of density estimation, a cell, often an hypersphere in d-dimension space, is centered around the test object <bold>z</bold>. The cell volume is grown until it contains <italic>k</italic> objects from the training set. The local density can be estimated by<disp-formula id="EEq4"><label>(4)</label><mml:math id="M3"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mspace height="4.59pt" depth="0.0pt"/><mml:mi mathvariant="bold">z</mml:mi><mml:mspace height="4.59pt" depth="0.0pt"/></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>k</mml:mi><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mfenced open="&#x02016;" close="&#x02016;" separators="|"><mml:mrow><mml:mi mathvariant="bold">z</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>N</mml:mi><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>NN</italic><sub><italic>k</italic></sub><sup><italic>tr</italic></sup>(<bold>z</bold>) and <italic>V</italic><sub><italic>k</italic></sub> are the <italic>k</italic> nearest neighbors of <bold>z</bold> in the training set and the volume of the cell containing this object. Later, we will use KNNDD to denote this method.</p><p>For an unknown test object <bold>z</bold>, the distance from it to its nearest neighbor in the training set NN<sup><italic>tr</italic></sup>(<bold>z</bold>) is compared with the distance from NN<sup><italic>tr</italic></sup>(<bold>z</bold>) to its nearest neighbor. The test object <bold>z</bold> can be accepted when its local density is larger or equal to the density of the nearest neighbor. It seems to be very useful for distributions characterized by fast decaying probabilities. Obviously, the method can easily be generalized to a larger number of neighbors k. That is, instead of taking the first nearest neighbor into account, the <italic>k</italic>th neighbor should be considered.</p></sec><sec id="sec2.3"><title>2.3. Gaussian Method</title><p>When a proper probability model is assumed and the sample size is sufficient, density method is advantageous for one-class problem. With the optimization of the threshold, a minimum volume can be automatically found for the given probability density. When only a little amount of samples is available, the simplest model is the unimodal Gaussian/Normal distribution. It fits a probability density model as follows:<disp-formula id="EEq5"><label>(5)</label><mml:math id="M4"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="4.29pt" depth="0.0pt"/><mml:mi mathvariant="bold">x</mml:mi><mml:mspace height="4.29pt" depth="0.0pt"/></mml:mrow></mml:mfenced><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn mathvariant="normal">2</mml:mn><mml:mi>&#x003c0;</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi mathvariant="normal">&#x003a3;</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn mathvariant="normal">0.5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mspace height="12.87999pt" depth="7.06999pt"/><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="5.32999pt" depth="2.44pt"/><mml:mi mathvariant="bold">x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mspace height="5.32999pt" depth="2.44pt"/></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="normal">&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mspace height="5.32999pt" depth="2.44pt"/><mml:mi mathvariant="bold">x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mspace height="5.32999pt" depth="2.44pt"/></mml:mrow></mml:mfenced><mml:mspace height="12.87999pt" depth="7.06999pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <bold><italic>&#x003bc;</italic></bold> is the mean and &#x003a3; is the covariance matrix. Both should be estimated from the training set. For <italic>d</italic> dimensional data, the number of the parameters is<disp-formula id="EEq6"><label>(6)</label><mml:math id="M5"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:mfrac><mml:mi>d</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mspace height="7.08pt" depth="0.23pt"/><mml:mi>d</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn><mml:mspace height="7.08pt" depth="0.23pt"/></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>The method imposes a strict unimodal and convex density model on the data. The main computational effort is maybe the inversion of the covariance matrix. In case of badly scaled data or data with singular directions, it is difficult to calculate the inverse of &#x003a3; and it can be approximated by the pseudoinverse &#x003a3;<sup>+</sup> = &#x003a3;<sup><italic>T</italic></sup>(&#x003a3;&#x003a3;<sup><italic>T</italic></sup>)<sup>&#x02212;1</sup>or by introducing regularization (adding a small constant <italic>&#x003bb;</italic> to the diagonal, i.e., &#x003a3;&#x02032; = &#x003a3; + <italic>&#x003bb; </italic><bold>I</bold>). In the last case, the user needs to supply a parameter <italic>&#x003bb;</italic>. This is also the only magic parameter that requires a user to provide.</p><p>Finally, a threshold on the probability density needs to be set for distinguishing between target and outlier data. Accepting 95% of the objects requires a threshold on the Mahanalobis distance<disp-formula id="EEq7"><label>(7)</label><mml:math id="M6"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="5.32999pt" depth="2.44pt"/><mml:mi mathvariant="bold">x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mspace height="5.32999pt" depth="2.44pt"/></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="normal">&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mspace height="5.32999pt" depth="2.44pt"/><mml:mi mathvariant="bold">x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold-italic">&#x003bc;</mml:mi><mml:mspace height="5.32999pt" depth="2.44pt"/></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>of<disp-formula id="EEq8"><label>(8)</label><mml:math id="M7"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.24498pt" depth="2.984pt"/><mml:msubsup><mml:mrow><mml:mi>&#x003c7;</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msubsup><mml:mspace height="9.24498pt" depth="2.984pt"/></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mspace height="6.35999pt" depth="0.12pt"/><mml:mn mathvariant="normal">0.95</mml:mn><mml:mspace height="6.35999pt" depth="0.12pt"/></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where (<italic>&#x003c7;</italic><sub><italic>d</italic></sub><sup>2</sup>)<sup>&#x02212;1</sup>is the inverse <italic>&#x003c7;</italic><sub><italic>d</italic></sub><sup>2</sup> with <italic>d</italic> degrees of freedom. This method is expected to work effectively only if the data is unimodal and convex. To obtain a more flexible density method, it can be extended to a mixture of Gaussians. Later, we will use GAUSS to denote this method.</p></sec></sec><sec id="sec3"><title>3. Experimental</title><sec id="sec3.1"><title>3.1. Sample Preparation</title><p>A total of 142 samples/bag of black rice of three brands were purchased from local supermarkets in China. They were from different supplier and let us mark them as A, B, and C brands. These samples were collected from three batches of A, two batches of B, and three batches of C but different packages. For A or C, forty-eight bags of rice were sampled, sixteen bags for each batch; For B, forty-six bags of rice were sampled, twenty-three bags for each batch. In total, the number of samples belonging to A, B, and C are 48, 46, and 48, respectively. The time it takes to collect the sample is about six months. The samples of each brand could serve as the target class whereas other samples acted as the outlier class. All samples were stored in the laboratory kept at 25&#x000b0;C for more than 7 days in order to achieve a temperature balance. To reduce the effect of environment, the NIR spectra of all samples were recorded on the same day.</p></sec><sec id="sec3.2"><title>3.2. Spectral Measurement and Preprocessing</title><p>Spectra of different samples collected on an Antaris II FT-NIR spectrometer (Thermo Scientific Co./USA) were equipped with an integrating sphere module, a rotating sample cup, and a InGaAs detector, as well as a tungsten lamp as the light source. The sample was poured into a standard sample cup with a 50 mm diameter and the height was controlled on about<bold> 30 mm</bold> for preventing light leak. An internal gold reference was used for automatic background collection. A specific sample cup spinner accessory for the integrating sphere sampling module that allows multipoint reflection measurements of heterogeneous solids such as powders, granules, and pellets, was used for obtaining NIR spectra of high quality. In this way, the final spectrum is the average of the spectra collected at different locations, which can reduce the effect of heterogeneity of solids to some extent.</p><p>The NIR spectrum was measured in the region of 10,000&#x02013;4000 cm<sup>&#x02212;1</sup> with 32 scans at a resolution of 3.856 cm<sup>&#x02212;1</sup>. Each spectrum contains 1557 data points. The experimental temperature and the related humidity were controlled around 25&#x000b0;C and 60%, respectively. Preprocessing of spectra is often of great importance if reasonable results need to be obtained whether it is concerned with qualitative or quantitative tasks. Several methods of preprocessing were attempted. In comparison with other preprocessing methods, standard normal transformation (SNV) achieved a satisfactory performance without the need of a reference spectrum and user decision for the computation. So, all spectra were preprocessed by SNV. The spectral measurement was controlled by the Result software [<xref rid="B31" ref-type="bibr">28</xref>]. DD toolbox was used for one-class classifier modeling [<xref rid="B18" ref-type="bibr">15</xref>]. All calculation was made on MATLAB 2015b for Windows.</p></sec></sec><sec id="sec4"><title>4. Results and Discussions</title><sec id="sec4.1"><title>4.1. NIR Spectral Analysis</title><p>
<xref ref-type="fig" rid="fig1">Figure 1</xref> shows the NIR spectra and all the preprocessed spectra of black rice samples by SNV. Seen from <xref ref-type="fig" rid="fig1">Figure 1</xref>, the spectra of three types of black rice share very similar absorbance patterns in the range of 4000-11000 cm<sup>&#x02212;1</sup>. They can hardly be distinguished just by naked eyes. General features of a NIR spectrum of solid samples include a multiplicative response to changes in particle size. SNV treatment autoscales each spectrum based on calculating the mean and standard deviation between the densities. It is also clear in <xref ref-type="fig" rid="fig1">Figure 1</xref> that, by preprocessing, some additive and multiplicative effects have been removed.</p><p>It is well known that major components of black rice are complex molecules from the polymerization of monomers such as amino acids or carbohydrates. Each monomer exhibits specific chemical groups such as carboxylic and amine functions in amino acids. As each chemical group may absorb the infrared region light, it appears useful to clearly identify the characteristic NIR bands of these groups. Because NIR spectrum corresponds to molecular responses of the overtone and combination bands, for each fundamental absorption band, there exists several overtones with decreasing intensity corresponding to the increasing multiple or transition number. All the bands can form a myriad of combination bands with intensities increasing as frequency decreases. NIR band intensities are much weaker than their corresponding mid-infrared fundamentals by a factor of 10-100. In <xref ref-type="fig" rid="fig1">Figure 1</xref>, two strong bands at 5175 cm<sup>&#x02212;1</sup> and 6930 cm<sup>&#x02212;1</sup> result from the absorbance of water, among which the peaks around 5175 cm<sup>&#x02212;1</sup> are the combination of asymmetric stretching and bending vibration of H<sub>2</sub>O. The band of 8200-8600 cm<sup>&#x02212;1</sup> can be attributed to the second overtones of C-H stretching in various groups. The wider bands in 6100&#x02013;7000 cm<sup>&#x02212;1</sup> are mainly caused by the overlapping of the first overtones of O-H and N-H stretching. The two peaks at 4266cm<sup>&#x02212;1</sup> and 4335cm<sup>&#x02212;1</sup>, which can be attributed to C-H stretching and C-H deformation, are very stable and carry much useful information. However, accurate assignments of each peak were maybe difficult due to low resolution and baseline shift; therefore, it is necessary to resort to chemometric methods to extract the useful information from spectra for identification purposes.</p><p>Furthermore, one of the most interesting applications of NIR technique in the food analysis is total quality evaluation as it can provide fingerprint information of a sample. Different brands of black rice mean different balances/ratios of diverse chemical constituents and physicochemical properties, rather than simple amount of each constituent. NIR spectra contain rich information on chemical constituents and physicochemical properties. Although the quality of black rice is generally assessed by sensory evaluation, its taste is actually a function of chemical constituents such as protein, moisture, amylose, fatty acid, and minerals. Therefore, an overall evaluation is preferred based on NIR spectroscopy.</p><p>Principal component analysis (PCA), the most widespread multivariate tool, was used for an exploratory analysis and dimensional reduction. Unlike other applications, the main goal of the present work using PCA was to map the original data into its principal component score space (i.e., the first two), based on which the subsequent modeling was carried out. So, all samples were considered as a whole for PCA and mean-centering pretreatment. By computation, the first two PCs explain 79.4% and 18.4% of the total variances, respectively, and they may contain most of the useful information in the original spectra. Because of this, we decided to use the first two components as the input of subsequent data description methods.</p></sec><sec id="sec4.2"><title>4.2. Authenticity Detection by Data Description</title><p>Given a dataset, in general, the selection of a representative training set upon which training the prediction model is performed is very important. For this purpose, in our work, the Kennard and Stone (KS) algorithm [<xref rid="B32" ref-type="bibr">29</xref>] was first used to rank all samples of each class in the dataset under consideration, thereby producing three sequences (A, B, and C). The KS algorithm consists of two main steps: taking the pair of samples between which the Euclidean distance of x-vectors (predictor) is largest, and then sequentially selecting a sample to maximize the Euclidean distances between x-vectors of already selected samples and the remaining samples. This process is repeated until all samples are picked out. The former samples are more representative than the latter one. When A class served as the target class, only the first thirty samples in A sequence were used as the training set for constructing data description. The remaining samples in A sequence and all samples in B and C sequences were used as the test set (the same partition of the sample set for the cases using B or C as the target class). Based on the first two PCs of the training set, three types of data descriptions mentioned above, i.e., SVDD, KNNDD, and GAUSS, were constructed. SVDD used the Gaussian kernel. <xref ref-type="fig" rid="fig2"> Figure 2</xref> gives the optimized data description boundary of class A based on the training set. It seems that the boundary of SVDD is tightest. All the descriptions differ from conventional classification because they always obtain a closed boundary around one of the target classes. Unlike density methods such as GAUSS, SVDD does not require a strict representative sampling of the target class; a sampling containing extreme objects is also acceptable. This can be found explicitly in the error definition of SVDD, which minimizes the volume of the description plus the sum of slack variables for objects outside the description. A conventional classifier, on the contrary, distinguishes between two/multiple classes without focusing on any of the classes and aims to minimize the probability of overall error. It is expected to perform very poorly when just the target class is available or the dataset is relatively small. Food validation or authenticity detection is often the case.</p><p>
<xref ref-type="fig" rid="fig3">Figure 3</xref> shows the application of the data description models of class A on the test set. Only one target sample was identified as outlier by SVDD. Even if the KNNDD and GAUSS correctly identified all samples, the false positive would increase when more test samples were used in the future. Similarly, Classes B and C were considered as the target class, and three corresponding data descriptions were constructed. <xref ref-type="fig" rid="fig4"> Figure 4</xref> shows the application of the data description models of class B on the test sets. Now, all the models correctly identified the target samples and the corresponding outliers but the SVDD use the tightest boundary, maybe implying better generalization ability. Both KNNDD and GAUSS produce looser borders. It should be noted that each time the so-called &#x0201c;fake&#x0201d; black rice is actually simulated by the samples from nontarget class.</p><p>The character of the SVDD heavily depends on the width parameter of the Gaussian kernel, which is very crucial as it can provide different prediction performance and leads to overfitting problem. Several previous studies have reported how to optimize SVDD [<xref rid="B33" ref-type="bibr">30</xref>]. The penalty term is sample rejection rate, i.e., the approximate proportion of samples misclassified in a training set. The other tunable parameter is kernel width. A large width can lead to a less complicated boundary and a relatively large width (compared to the maximum distance between samples in training set) could lead to a rigid hypersphere. In this work, based on the average nearest neighbor distance in the dataset, one can distinguish three types of cases: very small, very large, and intermediate values. By changing the value, the description ranges from Parzen density estimation, via a mixture of Gaussian to the rigid hypersphere, can be observed in <xref ref-type="fig" rid="fig5">Figure 5</xref>, which shows the influence of the kernel parameter on the boundary of SVDD when using class A as the target class. The boundary of SVDD seems to be sensitive to the kernel parameter. With the increase of the width, the boundary undergoes a complex change; it gradually achieves the optimum and then gets worse. For different cases, the number of support vectors is also different. In order to facilitate the comparison, <xref ref-type="fig" rid="fig6">Figure 6</xref> gives a similar ensemble plot of the influence of the kernel parameter on the boundary. Based on the shape and the compactness of the edge of the description, the optimal width parameter is 0.85 for this case A. Such a boundary contains all the target samples, among which six samples are just on the edge, and the shape is also simple. Also, one important advantage of SVDD over some traditional methods is that the classifier does not require that the data follow a normal distribution. However, there exist some alternative procedures for optimizing kernel width such as cross-validation, bootstrapping, and the consistency evaluation of the classifier using only the error of the nontarget class [<xref rid="B18" ref-type="bibr">15</xref>, <xref rid="B33" ref-type="bibr">30</xref>, <xref rid="B34" ref-type="bibr">31</xref>].</p><p>Taking the first case as an example (A as the target class), instead of the PCs, the original spectral variables were used as the independent variables for constructing data descriptions. On the independent test set, all these models including SVDD, KNNDD, and GAUSS achieved a specificity of 100% (the ration of outliers that were rejected), while the corresponding sensitivity, i.e., the ratio of the target class that was accepted, is 100% for GUASS and 94.4% for both SVDD and KNNDD, despite whether PCs or original variables are used. It indicates that using PCs or original variables does not make substantial difference. However, using all features is likely to result in overfitting, while using PCs will likely reduce overfitting. Also, using PCs makes the computation to be faster and to be more convenient for visual purposes. When B or C is the target class, the corresponding specificity and sensitivity have also been summarized in <xref rid="tab1" ref-type="table">Table 1</xref>. On average, the SVDD achieves best prediction, with the specificity of 100% and the sensitivity of 94.2%.</p><p>On the whole, the data description, especially SVDD, achieved an acceptable sensitivity and specificity for the so-called small-sample problem. Such a procedure is maybe potential tool for authenticity detection of various foods including black rice.</p></sec></sec><sec id="sec5"><title>5. Conclusions</title><p>The work reveals that NIR spectroscopy combined with support vector data description is feasible and advantageous to implement authenticity detection of black rice. It can serve as an alternative to laborious, time-consuming, wet chemical methods and sensory analysis of human. However for obtaining more reliable results, more samples need to be collected, which remains our next work.</p></sec></body><back><ack><title>Acknowledgments</title><p>This work was supported by the National Natural Science Foundation of China (21375118, J1310041), Scientific Research Foundation of Sichuan Provincial Education Department of China (17TD0048), Scientific Research Foundation of Yibin University (2017ZD05), the Applied Basic Research Programs of Science and Technology Department of Sichuan Province of China (2018JY0504), and Opening Fund of Key Lab of Process Analysis and Control of Sichuan Universities of China (2015006, 2016002).</p></ack><sec sec-type="data-availability"><title>Data Availability</title><p>The spectra data used to support the findings of this study are available from the corresponding author upon request.</p></sec><sec><title>Conflicts of Interest</title><p>The authors declare that there are no conflicts of interest regarding the publication of this paper.</p></sec><ref-list><ref id="B1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tananuwong</surname><given-names>K.</given-names></name><name><surname>Tewaruth</surname><given-names>W.</given-names></name></person-group><article-title>Extraction and application of antioxidants from black glutinous rice</article-title><source><italic toggle="yes">LWT- Food Science and Technology</italic></source><year>2010</year><volume>43</volume><issue>3</issue><fpage>476</fpage><lpage>481</lpage><pub-id pub-id-type="other">2-s2.0-72649092065</pub-id><pub-id pub-id-type="doi">10.1016/j.lwt.2009.09.014</pub-id></element-citation></ref><ref id="B2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>C.</given-names></name><name><surname>Zawistowski</surname><given-names>J.</given-names></name><name><surname>Ling</surname><given-names>W.</given-names></name><name><surname>Kitts</surname><given-names>D. D.</given-names></name></person-group><article-title>Black rice (Oryza sativa L. indica) pigmented fraction suppresses both reactive oxygen species and nitric oxide in chemical and biological model systems</article-title><source><italic toggle="yes">Journal of Agricultural and Food Chemistry</italic></source><year>2003</year><volume>51</volume><issue>18</issue><fpage>5271</fpage><lpage>5277</lpage><pub-id pub-id-type="other">2-s2.0-0041512056</pub-id><pub-id pub-id-type="doi">10.1021/jf034466n</pub-id><?supplied-pmid 12926869?><pub-id pub-id-type="pmid">12926869</pub-id></element-citation></ref><ref id="B3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>C. J.</given-names></name><name><surname>Wang</surname><given-names>H. M.</given-names></name><name><surname>Liu</surname><given-names>C. L.</given-names></name><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Meng</surname><given-names>X. M.</given-names></name></person-group><article-title>uncertainty evaluation Of toatal phenol determination on black rice by spectrophotometry</article-title><source><italic toggle="yes">China Food Additives</italic></source><year>2017</year><volume>2</volume><fpage>172</fpage><lpage>175</lpage></element-citation></ref><ref id="B4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hyun</surname><given-names>J. W.</given-names></name><name><surname>Chung</surname><given-names>H. S.</given-names></name></person-group><article-title>Cyanidin and malvidin from <italic>Oryza sativa</italic> cv. heugjinjubyeo mediate cytotoxicity against human monocytic leukemia cells by arrest of G(2)/M phase and induction of apoptosis</article-title><source><italic toggle="yes">Journal of Agricultural and Food Chemistry</italic></source><year>2004</year><volume>52</volume><issue>8</issue><fpage>2213</fpage><lpage>2217</lpage><pub-id pub-id-type="doi">10.1021/jf030370h</pub-id><pub-id pub-id-type="other">2-s2.0-1842682909</pub-id><pub-id pub-id-type="pmid">15080622</pub-id></element-citation></ref><ref id="B5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>Z. F.</given-names></name><name><surname>Wang</surname><given-names>W. J.</given-names></name><name><surname>Wang</surname><given-names>X. Y.</given-names></name><name><surname>Fang</surname><given-names>Y.</given-names></name></person-group><article-title>Determination of trace elements content of the same ogill of black rice and ordinary rice</article-title><source><italic toggle="yes">Chemical Engineering</italic></source><year>2015</year><volume>11</volume><fpage>27</fpage><lpage>31</lpage></element-citation></ref><ref id="B9"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>B.</given-names></name><name><surname>Rong</surname><given-names>Z. Q.</given-names></name><name><surname>Shi</surname><given-names>Y.</given-names></name><name><surname>Wu</surname><given-names>J. G.</given-names></name><name><surname>Shi</surname><given-names>C. H.</given-names></name></person-group><article-title>Prediction of the amino acid composition in brown rice using different sample status by near-infrared reflectance spectroscopy</article-title><source><italic toggle="yes">Food Chemistry</italic></source><year>2011</year><volume>127</volume><issue>1</issue><fpage>275</fpage><lpage>281</lpage><pub-id pub-id-type="other">2-s2.0-79951513277</pub-id><pub-id pub-id-type="doi">10.1016/j.foodchem.2010.12.110</pub-id></element-citation></ref><ref id="B10"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cozzolino</surname><given-names>D.</given-names></name></person-group><article-title>Near Infrared Spectroscopy and Food Authenticity</article-title><source><italic toggle="yes">Advances in Food Traceability Techniques and Technologies: Improving Quality Throughout the Food Chain</italic></source><year>2016</year><fpage>119</fpage><lpage>136</lpage><pub-id pub-id-type="other">2-s2.0-84988024360</pub-id></element-citation></ref><ref id="B11"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Domingo</surname><given-names>E.</given-names></name><name><surname>Tirelli</surname><given-names>A. A.</given-names></name><name><surname>Nunes</surname><given-names>C. A.</given-names></name><name><surname>Guerreiro</surname><given-names>M. C.</given-names></name><name><surname>Pinto</surname><given-names>S. M.</given-names></name></person-group><article-title>Melamine detection in milk using vibrational spectroscopy and chemometrics analysis: a review</article-title><source><italic toggle="yes">Food Research International</italic></source><year>2014</year><volume>60</volume><fpage>131</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1016/j.foodres.2013.11.006</pub-id><pub-id pub-id-type="other">2-s2.0-84900005683</pub-id></element-citation></ref><ref id="B12"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>H.</given-names></name><name><surname>Tan</surname><given-names>C.</given-names></name><name><surname>Lin</surname><given-names>Z.</given-names></name><name><surname>Wu</surname><given-names>T.</given-names></name></person-group><article-title>Detection of melamine adulteration in milk by near-infrared spectroscopy and one-class partial least squares</article-title><source><italic toggle="yes">Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy</italic></source><year>2017</year><volume>173</volume><fpage>832</fpage><lpage>836</lpage><pub-id pub-id-type="other">2-s2.0-84994824039</pub-id><pub-id pub-id-type="doi">10.1016/j.saa.2016.10.051</pub-id></element-citation></ref><ref id="B13"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>C.</given-names></name><name><surname>Li</surname><given-names>M.</given-names></name><name><surname>Qin</surname><given-names>X.</given-names></name></person-group><article-title>Study of the feasibility of distinguishing cigarettes of different brands using an Adaboost algorithm and near-infrared spectroscopy</article-title><source><italic toggle="yes">Analytical and Bioanalytical Chemistry</italic></source><year>2007</year><volume>389</volume><issue>2</issue><fpage>667</fpage><lpage>674</lpage><pub-id pub-id-type="other">2-s2.0-34548249602</pub-id><pub-id pub-id-type="doi">10.1007/s00216-007-1461-2</pub-id><pub-id pub-id-type="pmid">17641880</pub-id></element-citation></ref><ref id="B14"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>J.</given-names></name><name><surname>Lin</surname><given-names>H.</given-names></name><name><surname>Chen</surname><given-names>Q.</given-names></name><name><surname>Huang</surname><given-names>X.</given-names></name><name><surname>Sun</surname><given-names>Z.</given-names></name><name><surname>Zhou</surname><given-names>F.</given-names></name></person-group><article-title>Identification of egg&#x02019;s freshness using NIR and support vector data description</article-title><source><italic toggle="yes">Journal of Food Engineering</italic></source><year>2010</year><volume>98</volume><issue>4</issue><fpage>408</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1016/j.jfoodeng.2010.01.018</pub-id></element-citation></ref><ref id="B15"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruckebusch</surname><given-names>C.</given-names></name><name><surname>Orhan</surname><given-names>F.</given-names></name><name><surname>Durand</surname><given-names>A.</given-names></name><name><surname>Boubellouta</surname><given-names>T.</given-names></name><name><surname>Huvenne</surname><given-names>J. P.</given-names></name></person-group><article-title>Quantitative analysis of cotton-polyester textile blends from near-infrared spectra</article-title><source><italic toggle="yes">Applied Spectroscopy</italic></source><year>2006</year><volume>60</volume><issue>5</issue><fpage>539</fpage><lpage>544</lpage><pub-id pub-id-type="other">2-s2.0-33646874181</pub-id><pub-id pub-id-type="doi">10.1366/000370206777412194</pub-id><pub-id pub-id-type="pmid">16756705</pub-id></element-citation></ref><ref id="B16"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>H.</given-names></name><name><surname>Lin</surname><given-names>Z.</given-names></name><name><surname>Wu</surname><given-names>H.</given-names></name><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Wu</surname><given-names>T.</given-names></name><name><surname>Tan</surname><given-names>C.</given-names></name></person-group><article-title>Diagnosis of colorectal cancer by near-infrared optical fiber spectroscopy and random forest</article-title><source><italic toggle="yes">Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy</italic></source><year>2015</year><volume>135</volume><fpage>185</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1016/j.saa.2014.07.005</pub-id></element-citation></ref><ref id="B17"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D&#x000e9;gardin</surname><given-names>K.</given-names></name><name><surname>Guillemain</surname><given-names>A.</given-names></name><name><surname>Guerreiro</surname><given-names>N. V.</given-names></name><name><surname>Roggo</surname><given-names>Y.</given-names></name></person-group><article-title>Near infrared spectroscopy for counterfeit detection using a large database of pharmaceutical tablets</article-title><source><italic toggle="yes">Journal of Pharmaceutical and Biomedical Analysis</italic></source><year>2016</year><volume>128</volume><fpage>89</fpage><lpage>97</lpage><pub-id pub-id-type="other">2-s2.0-84969835320</pub-id><pub-id pub-id-type="doi">10.1016/j.jpba.2016.05.004</pub-id><?supplied-pmid 27236101?><pub-id pub-id-type="pmid">27236101</pub-id></element-citation></ref><ref id="B18"><label>15</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tax</surname><given-names>D. M.</given-names></name></person-group><source><italic toggle="yes">One-class classification</italic></source><year>2001</year><publisher-loc>The Netherlands</publisher-loc><publisher-name>Delft University of Technology, Delft</publisher-name><pub-id pub-id-type="other">MR2715427</pub-id></element-citation></ref><ref id="B19"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krawczyk</surname><given-names>B.</given-names></name><name><surname>Wo&#x0017a;niak</surname><given-names>M.</given-names></name></person-group><article-title>Diversity measures for one-class classifier ensembles</article-title><source><italic toggle="yes">Neurocomputing</italic></source><year>2014</year><volume>126</volume><fpage>36</fpage><lpage>44</lpage><pub-id pub-id-type="other">2-s2.0-84887613816</pub-id><pub-id pub-id-type="doi">10.1016/j.neucom.2013.01.053</pub-id></element-citation></ref><ref id="B20"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazhelis</surname><given-names>O.</given-names></name></person-group><article-title>One-Class Classifiers: A Review and Analysis of Suitability in the Context of Mobile-Masquerader Detection</article-title><source><italic toggle="yes">Advances in end-user data-mining techniques</italic></source><year>2006</year><volume>30</volume><fpage>39</fpage><lpage>47</lpage></element-citation></ref><ref id="B21"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oliveri</surname><given-names>P.</given-names></name></person-group><article-title>Class-modelling in food analytical chemistry: Development, sampling, optimisation and validation issues - A tutorial</article-title><source><italic toggle="yes">Analytica Chimica Acta</italic></source><year>2017</year><volume>982</volume><fpage>9</fpage><lpage>19</lpage><pub-id pub-id-type="other">2-s2.0-85019758789</pub-id><pub-id pub-id-type="pmid">28734370</pub-id></element-citation></ref><ref id="B22"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oliveri</surname><given-names>P.</given-names></name><name><surname>Downey</surname><given-names>G.</given-names></name></person-group><article-title>Multivariate class modeling for the verification of food-authenticity claims</article-title><source><italic toggle="yes">TrAC - Trends in Analytical Chemistry</italic></source><year>2012</year><volume>35</volume><fpage>74</fpage><lpage>86</lpage><pub-id pub-id-type="other">2-s2.0-84859731462</pub-id><pub-id pub-id-type="doi">10.1016/j.trac.2012.02.005</pub-id></element-citation></ref><ref id="B23"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Forina</surname><given-names>M.</given-names></name><name><surname>Armanino</surname><given-names>C.</given-names></name><name><surname>Leardi</surname><given-names>R.</given-names></name><name><surname>Drava</surname><given-names>G.</given-names></name></person-group><article-title>A class-modelling technique based on potential functions</article-title><source><italic toggle="yes">Journal of Chemometrics</italic></source><year>1991</year><volume>5</volume><issue>5</issue><fpage>435</fpage><lpage>453</lpage><pub-id pub-id-type="doi">10.1002/cem.1180050504</pub-id></element-citation></ref><ref id="B24"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodionova</surname><given-names>O. Y.</given-names></name><name><surname>Oliveri</surname><given-names>P.</given-names></name><name><surname>Pomerantsev</surname><given-names>A. L.</given-names></name></person-group><article-title>Rigorous and compliant approaches to one-class classification</article-title><source><italic toggle="yes">Chemometrics and Intelligent Laboratory Systems</italic></source><year>2016</year><volume>159</volume><fpage>89</fpage><lpage>96</lpage><pub-id pub-id-type="other">2-s2.0-84993970494</pub-id><pub-id pub-id-type="doi">10.1016/j.chemolab.2016.10.002</pub-id></element-citation></ref><ref id="B25"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>L.</given-names></name><name><surname>Yan</surname><given-names>S.</given-names></name><name><surname>Cai</surname><given-names>C.</given-names></name><name><surname>Yu</surname><given-names>X.</given-names></name></person-group><article-title>One-class partial least squares (OCPLS) classifier</article-title><source><italic toggle="yes">Chemometrics and Intelligent Laboratory Systems</italic></source><year>2013</year><volume>126</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="other">2-s2.0-84877668594</pub-id><pub-id pub-id-type="doi">10.1016/j.chemolab.2013.04.008</pub-id></element-citation></ref><ref id="B26"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brereton</surname><given-names>R. G.</given-names></name></person-group><article-title>One-class classifiers</article-title><source><italic toggle="yes">Journal of Chemometrics</italic></source><year>2011</year><volume>25</volume><issue>5</issue><fpage>225</fpage><lpage>246</lpage><pub-id pub-id-type="other">2-s2.0-79956325552</pub-id><pub-id pub-id-type="doi">10.1002/cem.1397</pub-id></element-citation></ref><ref id="B27"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>Z. S.</given-names></name><name><surname>Chen</surname><given-names>B.</given-names></name><name><surname>Miao</surname><given-names>Z. M.</given-names></name><name><surname>Ni</surname><given-names>G. Q.</given-names></name></person-group><article-title>Overview of study on one-class classifier</article-title><source><italic toggle="yes">Acta Electronica Sinica</italic></source><year>2009</year><volume>37</volume><fpage>2496</fpage><lpage>2503</lpage></element-citation></ref><ref id="B28"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uslu</surname><given-names>F. S.</given-names></name><name><surname>Binol</surname><given-names>H.</given-names></name><name><surname>Ilarslan</surname><given-names>M.</given-names></name><name><surname>Bal</surname><given-names>A.</given-names></name></person-group><article-title>Improving SVDD classification performance on hyperspectral images via correlation based ensemble technique</article-title><source><italic toggle="yes">Optics and Lasers in Engineering</italic></source><year>2016</year><volume>89</volume><fpage>169</fpage><lpage>177</lpage><pub-id pub-id-type="other">2-s2.0-84961820331</pub-id><pub-id pub-id-type="doi">10.1016/j.optlaseng.2016.03.006</pub-id></element-citation></ref><ref id="B29"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duan</surname><given-names>L.</given-names></name><name><surname>Xie</surname><given-names>M.</given-names></name><name><surname>Bai</surname><given-names>T.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name></person-group><article-title>A new support vector data description method for machinery fault diagnosis with unbalanced datasets</article-title><source><italic toggle="yes">Expert Systems with Applications</italic></source><year>2016</year><volume>64</volume><fpage>239</fpage><lpage>246</lpage><pub-id pub-id-type="other">2-s2.0-84980392384</pub-id><pub-id pub-id-type="doi">10.1016/j.eswa.2016.07.039</pub-id></element-citation></ref><ref id="B30"><label>27</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tax</surname><given-names>D.</given-names></name><name><surname>Duin</surname><given-names>R.</given-names></name></person-group><article-title>Data description in subspaces</article-title><conf-name>Proceedings of the 15th International Conference on Pattern Recognition</conf-name><conf-date>2002</conf-date><conf-loc>Barcelona, Spain</conf-loc><fpage>672</fpage><lpage>675</lpage><pub-id pub-id-type="doi">10.1109/ICPR.2000.906164</pub-id></element-citation></ref><ref id="B31"><label>28</label><mixed-citation publication-type="other"><comment>Thermo Scientific, Result Integration Software user Guide</comment></mixed-citation></ref><ref id="B32"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kennard</surname><given-names>R. W.</given-names></name><name><surname>Stone</surname><given-names>L. A.</given-names></name></person-group><article-title>Computer aided design of experiments</article-title><source><italic toggle="yes">Technometrics</italic></source><year>1969</year><volume>11</volume><issue>1</issue><fpage>137</fpage><lpage>148</lpage><pub-id pub-id-type="doi">10.1080/00401706.1969.10490666</pub-id><pub-id pub-id-type="other">Zbl0165.53102</pub-id><pub-id pub-id-type="other">2-s2.0-84894887900</pub-id></element-citation></ref><ref id="B33"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kittiwachana</surname><given-names>S.</given-names></name><name><surname>Ferreira</surname><given-names>D. L. S.</given-names></name><name><surname>Lloyd</surname><given-names>G. R.</given-names></name><etal/></person-group><article-title>One class classifiers for process monitoring illustrated by the application to online HPLC of a continuous process</article-title><source><italic toggle="yes">Journal of Chemometrics</italic></source><year>2010</year><volume>24</volume><issue>3-4</issue><fpage>96</fpage><lpage>110</lpage><pub-id pub-id-type="other">2-s2.0-77951066031</pub-id><pub-id pub-id-type="doi">10.1002/cem.1281</pub-id></element-citation></ref><ref id="B34"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhong</surname><given-names>L. S.</given-names></name><name><surname>Hou</surname><given-names>C. R.</given-names></name></person-group><article-title>Fault monitoring of industrial process based on independent component and support vector description (IC-SVDD</article-title><source><italic toggle="yes">Computers and Applied Chemistry</italic></source><year>2017</year><volume>34</volume><fpage>285</fpage><lpage>290</lpage></element-citation></ref></ref-list></back><floats-group><fig id="fig1" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Original near-infrared (NIR) spectra (a) and all the preprocessed spectra (b) by standard normal transformation (SNV).</p></caption><graphic xlink:href="IJAC2018-8032831.001"/></fig><fig id="fig2" orientation="portrait" position="float"><label>Figure 2</label><caption><p>Data description boundary of class A on the first two-principal-component space based on the training set.</p></caption><graphic xlink:href="IJAC2018-8032831.002"/></fig><fig id="fig3" orientation="portrait" position="float"><label>Figure 3</label><caption><p>Application of the data description models of class A on the test set.</p></caption><graphic xlink:href="IJAC2018-8032831.003"/></fig><fig id="fig4" orientation="portrait" position="float"><label>Figure 4</label><caption><p>Application of the data description models of class B on the test set.</p></caption><graphic xlink:href="IJAC2018-8032831.004"/></fig><fig id="fig5" orientation="portrait" position="float"><label>Figure 5</label><caption><p>The influence of the kernel parameter on the boundary of support vector data description (SVDD) using class A as the target class (classification error on the target class is set as 0.1).</p></caption><graphic xlink:href="IJAC2018-8032831.005"/></fig><fig id="fig6" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Ensemble of the influence of the kernel parameter on the boundary of support vector data description (SVDD) on the same plot.</p></caption><graphic xlink:href="IJAC2018-8032831.006"/></fig><table-wrap id="tab1" orientation="portrait" position="float"><label>Table 1</label><caption><p>Summary of the performance of different models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Target class</th><th colspan="2" align="center" rowspan="1">GAUSS</th><th colspan="2" align="center" rowspan="1">KNNDD</th><th colspan="2" align="center" rowspan="1">SVDD</th></tr><tr><th align="left" rowspan="1" colspan="1">&#x02009;</th><th align="center" rowspan="1" colspan="1">SPE</th><th align="center" rowspan="1" colspan="1">SEN</th><th align="center" rowspan="1" colspan="1">SPE</th><th align="center" rowspan="1" colspan="1">SEN</th><th align="center" rowspan="1" colspan="1">SPE</th><th align="center" rowspan="1" colspan="1">SEN</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">A</td><td align="center" rowspan="1" colspan="1">100%</td><td align="center" rowspan="1" colspan="1">100%</td><td align="center" rowspan="1" colspan="1">100%</td><td align="center" rowspan="1" colspan="1">94.4%</td><td align="center" rowspan="1" colspan="1">100%</td><td align="center" rowspan="1" colspan="1">94.4%</td></tr><tr><td align="left" rowspan="1" colspan="1">B</td><td align="center" rowspan="1" colspan="1">96.8%</td><td align="center" rowspan="1" colspan="1">87.5%</td><td align="center" rowspan="1" colspan="1">96.8%%</td><td align="center" rowspan="1" colspan="1">93.8%</td><td align="center" rowspan="1" colspan="1">100%</td><td align="center" rowspan="1" colspan="1">93.8%</td></tr><tr><td align="left" rowspan="1" colspan="1">C</td><td align="center" rowspan="1" colspan="1">97.8%</td><td align="center" rowspan="1" colspan="1">88.9%</td><td align="center" rowspan="1" colspan="1">98.9%</td><td align="center" rowspan="1" colspan="1">88.9%</td><td align="center" rowspan="1" colspan="1">100%</td><td align="center" rowspan="1" colspan="1">94.4%</td></tr><tr><td align="left" rowspan="1" colspan="1">Average</td><td align="center" rowspan="1" colspan="1">98.2%</td><td align="center" rowspan="1" colspan="1">92.1%</td><td align="center" rowspan="1" colspan="1">98.5%</td><td align="center" rowspan="1" colspan="1">92.3%</td><td align="center" rowspan="1" colspan="1">100%</td><td align="center" rowspan="1" colspan="1">94.2%</td></tr></tbody></table><table-wrap-foot><fn><p>
<italic>Note</italic>. SPE and SEN denote the specificity and sensitivity, respectively.</p></fn></table-wrap-foot></table-wrap></floats-group></article>