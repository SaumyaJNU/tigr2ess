<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 39.96?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6301652</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0208501</article-id><article-id pub-id-type="publisher-id">PONE-D-18-15391</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Agriculture</subject><subj-group><subject>Pests</subject><subj-group><subject>Insect Pests</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Digital Imaging</subject><subj-group><subject>Grayscale</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Plants</subject><subj-group><subject>Grasses</subject><subj-group><subject>Rice</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Animal Studies</subject><subj-group><subject>Experimental Organism Systems</subject><subj-group><subject>Plant and Algal Models</subject><subj-group><subject>Rice</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject><subj-group><subject>Image Analysis</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Agriculture</subject><subj-group><subject>Pest Control</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Invertebrates</subject><subj-group><subject>Arthropoda</subject><subj-group><subject>Insects</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Entomology</subject><subj-group><subject>Insect Pheromones</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Biochemistry</subject><subj-group><subject>Pheromones</subject><subj-group><subject>Insect Pheromones</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>PENYEK: Automated brown planthopper detection from imperfect sticky pad images using deep convolutional neural network</article-title><alt-title alt-title-type="running-head">Automated brown planthopper detection from imperfect sticky pad images using deep convolutional neural network</alt-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6362-5006</contrib-id><name><surname>Nazri</surname><given-names>Azree</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Software</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Visualization</role><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="aff" rid="aff002"><sup>2</sup></xref><xref ref-type="corresp" rid="cor001">*</xref></contrib><contrib contrib-type="author"><name><surname>Mazlan</surname><given-names>Norida</given-names></name><role content-type="http://credit.casrai.org/">Resources</role><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib><contrib contrib-type="author"><name><surname>Muharam</surname><given-names>Farrah</given-names></name><role content-type="http://credit.casrai.org/">Funding acquisition</role><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>Faculty of Computer Science &#x00026; Information Technology, UPM, Serdang, Malaysia</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Institute of BioScience, UPM, Serdang, Malaysia</addr-line></aff><aff id="aff003"><label>3</label>
<addr-line>Faculty of Agriculture, UPM, Serdang, Malaysia</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Mankin</surname><given-names>Richard</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1"><addr-line>US Department of Agriculture, UNITED STATES</addr-line></aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>azree@upm.edu.my</email></corresp></author-notes><pub-date pub-type="epub"><day>20</day><month>12</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>13</volume><issue>12</issue><elocation-id>e0208501</elocation-id><history><date date-type="received"><day>11</day><month>6</month><year>2018</year></date><date date-type="accepted"><day>18</day><month>11</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; 2018 Nazri et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Nazri et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0208501.pdf"/><abstract><p>Rice is a staple food in Asia and it contributes significantly to the Gross Domestic Product (GDP) of Malaysia and other developing countries. Brown Planthopper (BPH) causes high levels of economic loss in Malaysia. Identification of BPH presence and monitoring of its abundance has been conducted manually by experts and is time-consuming, fatiguing and tedious. Automated detection of BPH has been proposed by many studies to overcome human fallibility. However, all studies regarding automated recognition of BPH are investigated based on intact specimen although most of the specimens are imperfect, with missing parts have distorted shapes. The automated recognition of an imperfect insect image is more difficult than recognition of the intact specimen. This study proposes an automated, deep-learning-based detection pipeline, PENYEK, to identify BPH pest in images taken from a readily available sticky pad, constructed by clipping plastic sheets onto steel plates and spraying with glue. This study explores the effectiveness of a convolutional neural network (CNN) architecture, VGG16, in classifying insects as BPH or benign based on grayscale images constructed from Euclidean Distance Maps (EDM). The pipeline identified imperfect images of BPH with an accuracy of 95% using deep-learning&#x02019;s hyperparameters: softmax, a mini-batch of 30 and an initial learning rate of 0.0001.</p></abstract><funding-group><award-group id="award001"><funding-source><institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100003200</institution-id><institution>Kementerian Sains, Teknologi dan Inovasi</institution></institution-wrap></funding-source><award-id>HICoE &#x02013; ITAFoS/2017/FC3</award-id></award-group><award-group id="award002"><funding-source><institution>Kementerian Pengajian Tinggi Malaysia</institution></funding-source><award-id>5524959</award-id><principal-award-recipient><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6362-5006</contrib-id><name><surname>Nazri</surname><given-names>Azree</given-names></name></principal-award-recipient></award-group><award-group id="award003"><funding-source><institution>Institute BioScience, Universiti Putra Malaysia (MY)</institution></funding-source><award-id>9538100</award-id><principal-award-recipient><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6362-5006</contrib-id><name><surname>Nazri</surname><given-names>Azree</given-names></name></principal-award-recipient></award-group><funding-statement>The authors gratefully acknowledge the support of the HICoE ITAFoS (HICoE &#x02013; ITAFoS/2017/FC3), GPIPM (vote No: 9538100) and Fundamental Research Grant Scheme (FRGS) (Vote No: 5524959).</funding-statement></funding-group><counts><fig-count count="6"/><table-count count="7"/><page-count count="13"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>Data are available on the Figshare repository at <ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/DATASET_BPH_and_BENIGN/7015658">https://figshare.com/articles/DATASET_BPH_and_BENIGN/7015658</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>Data are available on the Figshare repository at <ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/DATASET_BPH_and_BENIGN/7015658">https://figshare.com/articles/DATASET_BPH_and_BENIGN/7015658</ext-link>.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>Rice, a staple food in Malaysia and the most important crop in South East Asia, is being damaged by a rice planthopper complex which has now become a challenge to farmers at the national level. The rice planthopper complex has three species: the brown planthopper (BPH), <italic>Nilaparvata lugens (Stal</italic>), the whiteback planthopper, <italic>Sogatella furcifera (Harvath</italic>), and the leafhopper, <italic>Nephotettix virescens</italic>. The most destructive hopper among them is the BPH, which is considered as the most serious pest of rice in both temperate and tropical region of east and south Asia[<xref rid="pone.0208501.ref001" ref-type="bibr">1</xref>]. Pesticides are used to reduce the rice pest outbreaks caused by rice pests. However, the uncontrolled use of pesticide affects soil in long term. A pest management system needs to be devised in order to eliminate pests and reduce environmental contamination. Such a program requires that pest insects be counted and precisely targeted, which is a time-consuming task.</p><p>Monitoring is an important aspect of pheromone-based pest control [<xref rid="pone.0208501.ref002" ref-type="bibr">2</xref>], [<xref rid="pone.0208501.ref003" ref-type="bibr">3</xref>]. Traditional pest identification and counting has been done by the experts through visual recognition. Traditional methods include (1) tapping on stem, (2) counting on stem and (3) cage usage. Tapping on stem involves identification and counting pests on enamel plate after stooping and flapping the stems. The enamel plate is coated with glue to avoid the escape of planthoppers. The second technique is by using a sweep net to sweep the rice pests. The human expert then visually counts the number of rice pests in the cage or net. Counting directly on stem involves visually identifying the planthoppers and then counting the rice pests on the rice stems. Other pest monitoring systems have been developed that trap insect pests and capture the trap surface images digitally for subsequent analysis by human experts. Unarguably, these manual methods are time consuming and tedious. The low accurate rate typically occurs when the number of rice pests on the enamel plate is over fifty. At this stage, the experts estimate the number of the rice pests based on their personal experience.</p><p>Advances in machine learning have revolutionized computer vision and object recognition in general and in pest identification in particular [<xref rid="pone.0208501.ref004" ref-type="bibr">4</xref>]. Machine learning helps traditional methods of pest identification and counting by automating the process. Previous works have considered insect classification from image processing in overcoming humanly fallible including image acquisition settings and features which typically use insect specimens as image sources [<xref rid="pone.0208501.ref005" ref-type="bibr">5</xref>]&#x02013;[<xref rid="pone.0208501.ref010" ref-type="bibr">10</xref>]. However, insect species have their own drawback in which they are well-preserved and imaged in an ideal laboratory environment. In addition, specimen images are consistent and captured at high resolution. Many experts have captured specimens in the wild to classify the images, yet still image them under laboratory conditions [<xref rid="pone.0208501.ref004" ref-type="bibr">4</xref>], [<xref rid="pone.0208501.ref011" ref-type="bibr">11</xref>&#x02013;<xref rid="pone.0208501.ref015" ref-type="bibr">15</xref>]. The drawback in using this data acquisition is that image quality is typically worse than the specimen in laboratory case, although it can be controlled by imaging all the insects under a standard orientation or lighting.</p><p>Several pest identifications and classification systems have been proposed that based on machine learning. Image analysis with scene interpretation was proposed by Kumar et al. [<xref rid="pone.0208501.ref016" ref-type="bibr">16</xref>] by developing automatic detection of harmful insects in the greenhouse. The features extracted from the image are generated from three feature extraction methods: Gabor Filter, Pyramidal Histogram of Gradient and Colour data. The empirical results showed that the proposed system was able to detect 98.5% whiteflies (total was 1,283 whiteflies) and 91.8% greenfly (total was 49 greenfly). Support Vector Machine (SVM) was used as the classifier of choice. Early pest detection system using images captured by pan tile camera to detect and classify pests was proposed [<xref rid="pone.0208501.ref017" ref-type="bibr">17</xref>]. The images were recorded and delivered to a central server where the processing and the analysis were done. In the central server, the images were extracted and classified from video frames. Again, SVM was used to classify the images frame by frame. The aim of this system is to estimate the density of pests inside the greenhouse.</p><p>Another method used a network of cameras for the continuous survey of a greenhouse in estimating the population of pests. In this system, one camera was observing the sticky trap while the second was observing the plant and none flying insect pests [<xref rid="pone.0208501.ref016" ref-type="bibr">16</xref>]. The streaming images were analysed with a priori knowledge about the visual appearance of the detected insects. The classification and interpretation of the extracted features from images was done using neural learning and knowledge-based techniques. Image analysis techniques were used by Cho et al [<xref rid="pone.0208501.ref018" ref-type="bibr">18</xref>], who developed an automated identification of whiteflies, aphids and thrips in the greenhouse by using image analysis technique. The goal of the proposed system is quite comprehensive in which they aimed to accurately estimate the density of pests for pest management strategy and minimize the use of pesticides. Data acquisition was done by installing wireless cameras that continuously observe the sticky trap. The captured images are then sent to the cloud server to process and identify the extracted insect pests.</p><p>The bag-of-words approach and gradient-based features were proposed for developing a framework that can classify insect pests from the paddy field [<xref rid="pone.0208501.ref019" ref-type="bibr">19</xref>]. The data acquisition was done by collecting insect pest images from Google Images together with images taken by their faculty in the paddy fields. The images were regionalized using Scale-Invariant Feature Transform (SIFT) and Speed-Up Robust Features (SURF) descriptors. Codebooks were developed to map the descriptors into a fixed-length vector in histogram space and classify the feature histograms based on the Histogram of Oriented Gradient (HOG) descriptors and using SVM as the classifier. The accuracy of the classification was 90%. One study developed an automated identification and counting system for different insect pests captured with light-traps and proposed a novel segmentation method for middle-sized touching insects from an image [<xref rid="pone.0208501.ref020" ref-type="bibr">20</xref>]. Normalized cuts (NCuts) together with the optical flow angle was used to separate the touching insects according to the number of insects in each connected region. The proposed segmentation method was compared to k-means and watershed methods and achieved a better segmentation result.</p><p>Algorithmically, insect classification needs features to be recognized by classifiers that include wing structures [<xref rid="pone.0208501.ref005" ref-type="bibr">5</xref>]&#x02013;[<xref rid="pone.0208501.ref009" ref-type="bibr">9</xref>], colour histogram features [<xref rid="pone.0208501.ref021" ref-type="bibr">21</xref>], [<xref rid="pone.0208501.ref022" ref-type="bibr">22</xref>], morphometric measurements [<xref rid="pone.0208501.ref009" ref-type="bibr">9</xref>], [<xref rid="pone.0208501.ref010" ref-type="bibr">10</xref>], [<xref rid="pone.0208501.ref023" ref-type="bibr">23</xref>], [<xref rid="pone.0208501.ref024" ref-type="bibr">24</xref>], local image features [<xref rid="pone.0208501.ref021" ref-type="bibr">21</xref>], [<xref rid="pone.0208501.ref022" ref-type="bibr">22</xref>], [<xref rid="pone.0208501.ref025" ref-type="bibr">25</xref>]&#x02013;[<xref rid="pone.0208501.ref028" ref-type="bibr">28</xref>], and global image features [<xref rid="pone.0208501.ref029" ref-type="bibr">29</xref>]. The features extracted from insect images are classified by various machine learning algorithms include SVM [<xref rid="pone.0208501.ref010" ref-type="bibr">10</xref>]&#x02013;[<xref rid="pone.0208501.ref012" ref-type="bibr">12</xref>][<xref rid="pone.0208501.ref010" ref-type="bibr">10</xref>], [<xref rid="pone.0208501.ref012" ref-type="bibr">12</xref>], [<xref rid="pone.0208501.ref025" ref-type="bibr">25</xref>], Artificial Neural Network (ANN) [<xref rid="pone.0208501.ref026" ref-type="bibr">26</xref>], [<xref rid="pone.0208501.ref029" ref-type="bibr">29</xref>], K-Nearest Neighbours (KNN) [<xref rid="pone.0208501.ref026" ref-type="bibr">26</xref>], [<xref rid="pone.0208501.ref029" ref-type="bibr">29</xref>]and ensemble methods [<xref rid="pone.0208501.ref025" ref-type="bibr">25</xref>][<xref rid="pone.0208501.ref011" ref-type="bibr">11</xref>], [<xref rid="pone.0208501.ref026" ref-type="bibr">26</xref>]. The identification of insect pests based on deep learning is still largely unexplored.</p><p>In trap-based pest monitoring especially with sticky pads, there are many challenges such as low image quality, inconsistencies derived from illumination, movement of the trap, movement of the pests, camera out of focus, the appearance of other objects, decay or damage to the insect, the presence of benign insects and many more. Among these challenges, the effects of decay or damage on insect images are still not yet overcome. Decayed or damaged insects have different outlines and morphology compared to intact insects. Sticky pads usually trap while they are flying towards the pad surface. The impact of the collision between the pad and the insects distort the body and morphology of insects. It is difficult to design an automated system that can identify distorted insect pests. Thus, a method for such a system is in urgent need. Various datasets have been utilized to push this area forward [<xref rid="pone.0208501.ref030" ref-type="bibr">30</xref>], [<xref rid="pone.0208501.ref031" ref-type="bibr">31</xref>]; yet decayed and damageed pest datasets are largely missing. Concerned by this gap in research, this study proposes an identification and classification of imperfect pest&#x02019;s pipeline with CNN as the image classifier.</p></sec><sec id="sec002"><title>Materials and method</title><p>This section describes the collection, curation and pre-processing of sticky pad images. Samples of insects were collected from the granary area Sawah Sempadan (3<sup>o</sup>27&#x02019;55.94&#x0201d;N, 101<sup>o</sup>11&#x02019;33.33&#x0201d;E), Tanjung Karang, Selangor, Malaysia (authority: Norida, M. 1&#x02014;Institute Tropical agriculture and food security, Universiti Putra Malaysia. 2. Department of Agriculture Technology, Faculty of Agriculture, Unversiti Putra Malaysa). The paddy was about 70 days after planting and infested with BPH. Sticky glue was sprayed on plastic sheet which stick on the steel board plate. The plate was put on the base of paddy stem, where the BPH are usually located. Then, the stems were tapped for few times to ensure BPH were adhered on the glued. These methods were repeated few times at different paddy plant to get enough samples.</p><p>The plates, the sticky pad, consisting of trapped BPH were brought to a laboratory. The BPH, either short or long winged, were then manually identified and labelled with bounding circles by an entomologist. The digital images of the sticky pads were then captured by a Huawei P9 Plus smartphone camera under the laboratory room illumination and stored in JPEG format. The smartphone camera is dual 12 megapixels monochrome and colour camera, with focal length of 27mm and F2.2 aperture. All images do not have a temporal correlation with each other, which makes all the labelled BPH unique. <xref ref-type="fig" rid="pone.0208501.g001">Fig 1A</xref> shows a sticky pad image with all BPHs labelled with red bounding blue but cluttered with other types of insects while <xref ref-type="fig" rid="pone.0208501.g001">Fig 1B</xref> demonstrates an image containing benign insects (<italic>Horvath</italic>, <italic>Nephotettix virescens</italic> and other unidentified insects).</p><fig id="pone.0208501.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0208501.g001</object-id><label>Fig 1</label><caption><title>Examples of images captured from sticky trap.</title><p>(a) trap with BPH and (b) trap with no BPH.</p></caption><graphic xlink:href="pone.0208501.g001"/></fig><sec id="sec003"><title>Dataset construction</title><p>Every single BPH and benign insect image was cropped from the sticky pad image to become an independent image patch. The positions and locations of these insects were random and unstructured. <xref ref-type="fig" rid="pone.0208501.g002">Fig 2A and 2B</xref> show BPH and benign images in their original position from the wild.</p><fig id="pone.0208501.g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0208501.g002</object-id><label>Fig 2</label><caption><title>Examples of cropped images from sticky pad.</title><p>(a) Positive patched containing a brown planthopper and (b) negative patches (benign).</p></caption><graphic xlink:href="pone.0208501.g002"/></fig><p>The dataset of both BPH and benign insects was split randomly into 3 sets: the training, validation and test sets. The statistics of each set is the same as the entire dataset as shown in <xref rid="pone.0208501.t001" ref-type="table">Table 1</xref>.</p><table-wrap id="pone.0208501.t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0208501.t001</object-id><label>Table 1</label><caption><title>Statistics of constructed datasets.</title></caption><alternatives><graphic id="pone.0208501.t001g" xlink:href="pone.0208501.t001"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="justify" rowspan="1" colspan="1">Dataset</th><th align="justify" rowspan="1" colspan="1"># images with BPH</th><th align="justify" rowspan="1" colspan="1"># images with benign</th></tr></thead><tbody><tr><td align="justify" rowspan="1" colspan="1">Total</td><td align="justify" rowspan="1" colspan="1">337</td><td align="justify" rowspan="1" colspan="1">350</td></tr><tr><td align="justify" rowspan="1" colspan="1">Training</td><td align="justify" rowspan="1" colspan="1">236</td><td align="justify" rowspan="1" colspan="1">245</td></tr><tr><td align="justify" rowspan="1" colspan="1">Validation</td><td align="justify" rowspan="1" colspan="1">34</td><td align="justify" rowspan="1" colspan="1">35</td></tr><tr><td align="justify" rowspan="1" colspan="1">Test</td><td align="justify" rowspan="1" colspan="1">67</td><td align="justify" rowspan="1" colspan="1">70</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec004"><title>Pre-processing</title><p>The sticky pad images were collected in real production environments which resulted in different imaging conditions at different times. As shown in <xref ref-type="fig" rid="pone.0208501.g002">Fig 2A and 2B</xref>, the most apparent challenge is illumination. To eliminate the potential negative effects of illumination variability on detection performance, the RGB image was transformed into a grayscale image [<xref rid="pone.0208501.ref032" ref-type="bibr">32</xref>].</p></sec><sec id="sec005"><title>PENYEK</title><p>A schematic diagram of PENYEK is shown in <xref ref-type="fig" rid="pone.0208501.g003">Fig 3</xref>. The proposed pipeline is called PENYEK because the word PENYEK means flatten object originated from the Malay language. The first step in the pipeline process was to crop the digital BPH images obtained from a sticky pad image. Median filtering was applied as a binarization step to reduce background noise and further to preserve the edges of the insect regions. The median filtering process replaces the centre value of the patch with the median value of all neighbouring pixel values. The second step in binarization process was the application of an iterative multiple thresholding algorithm to separate the image pixels into the foreground and background. The threshold estimation depends on the maximization of the between-class variances of the pixel values [<xref rid="pone.0208501.ref033" ref-type="bibr">33</xref>]. This estimates the threshold iteratively and returns two optimal thresholds. The iteration continues until the errors become small or the thresholds no longer change. This iterative multiple threshold process changed RGB regions into binary images which area then processed using morphological closing and opening operations. The unwanted insect pests around the regions, which are smaller than the user-specific threshold, were removed using a size filtering method.</p><fig id="pone.0208501.g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0208501.g003</object-id><label>Fig 3</label><caption><title>Illustration of the PENYEK classification pipeline.</title></caption><graphic xlink:href="pone.0208501.g003"/></fig><p>Next, the binary images were transformed by one of the six selected binary operations as shown in <xref ref-type="fig" rid="pone.0208501.g004">Fig 4</xref>: outline, fill hole, Skeletonize, Distance Map, Watershed and Voronoi. These filtered images were then input into convolutional neural network (CNN) VGG16 to extract features from the filtered images. The filtered images were then classified as: BPH and benign.</p><fig id="pone.0208501.g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0208501.g004</object-id><label>Fig 4</label><caption><title>Different types of filtering are applied on captured BP images.</title><p>These include outline, fill hole, skeletonize, distance map, watershed and Voronoi.</p></caption><graphic xlink:href="pone.0208501.g004"/></fig><p><xref rid="pone.0208501.t002" ref-type="table">Table 2</xref> presents several proposed pre-processing and CNN architecture pipelines for PENYEK. The selection of colour and filter affects the feature produced by CNN VGG16 architecture. For example, image model A uses grayscale images and these images are filtered by Outline operation. The outline grayscale image is shown in <xref ref-type="fig" rid="pone.0208501.g004">Fig 4</xref>. Model B, C, D, E and F use Fill holes, Skeletonize, Watershed, Voronoi and Euclidean distance map, respectively, as shown in <xref ref-type="fig" rid="pone.0208501.g004">Fig 4</xref>. An intact RGB image is used without any filter is operated by model G.</p><table-wrap id="pone.0208501.t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0208501.t002</object-id><label>Table 2</label><caption><title>Details of a combination of system components.</title></caption><alternatives><graphic id="pone.0208501.t002g" xlink:href="pone.0208501.t002"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="justify" rowspan="1" colspan="1">Image model</th><th align="justify" rowspan="1" colspan="1">Colour</th><th align="justify" rowspan="1" colspan="1">Filter</th></tr></thead><tbody><tr><td align="justify" rowspan="1" colspan="1"><italic>A</italic></td><td align="justify" rowspan="1" colspan="1"><italic>Grayscale</italic></td><td align="justify" rowspan="1" colspan="1"><italic>Outline</italic></td></tr><tr><td align="justify" rowspan="1" colspan="1"><italic>B</italic></td><td align="justify" rowspan="1" colspan="1"><italic>Grayscale</italic></td><td align="justify" rowspan="1" colspan="1"><italic>Fill holes</italic></td></tr><tr><td align="justify" rowspan="1" colspan="1"><italic>C</italic></td><td align="justify" rowspan="1" colspan="1"><italic>Grayscale</italic></td><td align="justify" rowspan="1" colspan="1"><italic>Skeletonize</italic></td></tr><tr><td align="justify" rowspan="1" colspan="1"><italic>D</italic></td><td align="justify" rowspan="1" colspan="1"><italic>Grayscale</italic></td><td align="justify" rowspan="1" colspan="1"><italic>Watershed</italic></td></tr><tr><td align="justify" rowspan="1" colspan="1"><italic>E</italic></td><td align="justify" rowspan="1" colspan="1"><italic>Grayscale</italic></td><td align="justify" rowspan="1" colspan="1"><italic>Voronoi</italic></td></tr><tr><td align="justify" rowspan="1" colspan="1"><italic>F</italic></td><td align="justify" rowspan="1" colspan="1"><italic>Grayscale</italic></td><td align="justify" rowspan="1" colspan="1"><italic>Euclidean Distance Map</italic></td></tr><tr><td align="justify" rowspan="1" colspan="1"><italic>G</italic></td><td align="justify" rowspan="1" colspan="1"><italic>RGB</italic></td><td align="justify" rowspan="1" colspan="1"><italic>-</italic></td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec006"><title>CNN architecture</title><p>The CNN architecture typically consists of three different layers: convolutional layer, pooling layer and a fully connecter layer.</p><sec id="sec007"><title>Convolutional layers</title><p>This layer consists of kernels (filters) which slide across the insect image. A kernel is the matrix to be convolved with the input image and stride length controls how much the filter convolves across the input image. This layer performs the convolution on the input image with the kernel using Eq (<xref ref-type="disp-formula" rid="pone.0208501.e001">1</xref>).
<disp-formula id="pone.0208501.e001"><alternatives><graphic xlink:href="pone.0208501.e001.jpg" id="pone.0208501.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives><label>(1)</label></disp-formula>
where <italic>x</italic> is pixel, <italic>h</italic> is filter, and <italic>N</italic> is the number of elements in <italic>x</italic>. The output vector is <italic>y</italic>. The subscripts denote the nth element of the vector and <italic>k</italic> is the current kernel.</p></sec><sec id="sec008"><title>Pooling layers</title><p>This down-sampling layer reduces the dimension of output neurons from the convolutional layer to lessen the computational intensity and prevent the overfitting. The max-pooling operation is used in this study. Max-pooling operation selects only the maximum value in each feature map and consequently reducing the number of output neurons.</p></sec><sec id="sec009"><title>Fully connected layers</title><p>This layer has full connection to all the activations in the previous layer.</p><p><xref rid="pone.0208501.t003" ref-type="table">Table 3</xref> is the summary that details 7 different CNN structures in the observation of the behaviour of variation in model characteristics by training them from scratch (i.e. randomly initializing the layers) on representative filter operation subsets of the whole insect pest image data for BPH classification. This empirical analysis acts as a pilot study, as it allows to establish the feasibility of deep learning methods for the described image analysis problems. Considering a high visual complexity of imperfect BPH images. For example, CNN model 1 has 6 number of layers, 2 number of convolutional layers, (3,3) kernel sizes, (16,16) numbers of feature maps, (3,3) kernel sizes for pooling layer and (128,3) number of fully connected layer outputs. This step is to investigate which filter (<xref ref-type="fig" rid="pone.0208501.g004">Fig 4</xref>) performs better in BPH classification to be included in PENYEK pipeline.</p><table-wrap id="pone.0208501.t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0208501.t003</object-id><label>Table 3</label><caption><title>Details of CNN architectures for BN classification on sticky pad datasets.</title></caption><alternatives><graphic id="pone.0208501.t003g" xlink:href="pone.0208501.t003"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="justify" rowspan="1" colspan="1"><italic>CNN structure number</italic></th><th align="justify" rowspan="1" colspan="1"><italic>Total number of layers</italic></th><th align="justify" rowspan="1" colspan="1"><italic>Number of convolutional layers</italic></th><th align="justify" rowspan="1" colspan="1"><italic>Kernel sizes (convolutional layers)</italic></th><th align="justify" rowspan="1" colspan="1"><italic>Number of feature maps</italic></th><th align="justify" rowspan="1" colspan="1"><italic>Kernel sizes (pooling layers)</italic></th><th align="justify" rowspan="1" colspan="1"><italic>Number of fully connected layer outputs</italic></th></tr></thead><tbody><tr><td align="justify" rowspan="1" colspan="1"><italic>1</italic></td><td align="justify" rowspan="1" colspan="1"><italic>6</italic></td><td align="justify" rowspan="1" colspan="1"><italic>2</italic></td><td align="justify" rowspan="1" colspan="1"><italic>3</italic>,<italic>3</italic></td><td align="justify" rowspan="1" colspan="1"><italic>16</italic>,<italic>16</italic></td><td align="justify" rowspan="1" colspan="1"><italic>3</italic>,<italic>3</italic></td><td align="justify" rowspan="1" colspan="1"><italic>128</italic>,<italic>3</italic></td></tr><tr><td align="justify" rowspan="1" colspan="1"><italic>2</italic></td><td align="justify" rowspan="1" colspan="1"><italic>6</italic></td><td align="justify" rowspan="1" colspan="1"><italic>2</italic></td><td align="justify" rowspan="1" colspan="1"><italic>7</italic>,<italic>3</italic></td><td align="justify" rowspan="1" colspan="1"><italic>16</italic>,<italic>16</italic></td><td align="justify" rowspan="1" colspan="1"><italic>2</italic>,<italic>2</italic></td><td align="justify" rowspan="1" colspan="1"><italic>156</italic>,<italic>4</italic></td></tr><tr><td align="justify" rowspan="1" colspan="1"><italic>3</italic></td><td align="justify" rowspan="1" colspan="1"><italic>7</italic></td><td align="justify" rowspan="1" colspan="1"><italic>2</italic></td><td align="justify" rowspan="1" colspan="1"><italic>9</italic>,<italic>9</italic></td><td align="justify" rowspan="1" colspan="1"><italic>16</italic>,<italic>16</italic></td><td align="justify" rowspan="1" colspan="1"><italic>3</italic>,<italic>3</italic></td><td align="justify" rowspan="1" colspan="1"><italic>128</italic>,<italic>128</italic>,<italic>3</italic></td></tr><tr><td align="justify" rowspan="1" colspan="1"><italic>4</italic></td><td align="justify" rowspan="1" colspan="1"><italic>7</italic></td><td align="justify" rowspan="1" colspan="1"><italic>2</italic></td><td align="justify" rowspan="1" colspan="1"><italic>7</italic>,<italic>5</italic></td><td align="justify" rowspan="1" colspan="1"><italic>16</italic>,<italic>16</italic></td><td align="justify" rowspan="1" colspan="1"><italic>2</italic>,<italic>2</italic></td><td align="justify" rowspan="1" colspan="1"><italic>256</italic>,<italic>128</italic>,<italic>3</italic></td></tr><tr><td align="justify" rowspan="1" colspan="1"><italic>5</italic></td><td align="justify" rowspan="1" colspan="1"><italic>9</italic></td><td align="justify" rowspan="1" colspan="1"><italic>3</italic></td><td align="justify" rowspan="1" colspan="1"><italic>7</italic>,<italic>5</italic>,<italic>3</italic></td><td align="justify" rowspan="1" colspan="1"><italic>24</italic>,<italic>16</italic>,<italic>16</italic></td><td align="justify" rowspan="1" colspan="1"><italic>2</italic>,<italic>2</italic>,<italic>2</italic></td><td align="justify" rowspan="1" colspan="1"><italic>256</italic>,<italic>128</italic>,<italic>3</italic></td></tr><tr><td align="justify" rowspan="1" colspan="1"><italic>6</italic></td><td align="justify" rowspan="1" colspan="1"><italic>10</italic></td><td align="justify" rowspan="1" colspan="1"><italic>4</italic></td><td align="justify" rowspan="1" colspan="1"><italic>9</italic>,<italic>7</italic>,<italic>5</italic>,<italic>3</italic></td><td align="justify" rowspan="1" colspan="1"><italic>32</italic>,<italic>128</italic>,<italic>128</italic>,<italic>128</italic></td><td align="justify" rowspan="1" colspan="1"><italic>3</italic>,<italic>3</italic>,<italic>3</italic></td><td align="justify" rowspan="1" colspan="1"><italic>2048</italic>,<italic>2048</italic>,<italic>3</italic></td></tr><tr><td align="justify" rowspan="1" colspan="1"><italic>7</italic></td><td align="justify" rowspan="1" colspan="1"><italic>11</italic></td><td align="justify" rowspan="1" colspan="1"><italic>5</italic></td><td align="justify" rowspan="1" colspan="1"><italic>11</italic>,<italic>5</italic>,<italic>3</italic>,<italic>3</italic>,<italic>3</italic></td><td align="justify" rowspan="1" colspan="1"><italic>96</italic>,<italic>256</italic>,<italic>384</italic>,<italic>384</italic>,<italic>256</italic></td><td align="justify" rowspan="1" colspan="1"><italic>3</italic>,<italic>3</italic>,<italic>3</italic></td><td align="justify" rowspan="1" colspan="1"><italic>4096</italic>,<italic>4096</italic>,<italic>3</italic></td></tr></tbody></table></alternatives></table-wrap><p>The chosen binary operator is combined with CNN VGG16 architecture to form a complete PENYEK pipeline. In this last step, the binary operator + CNN VGG16 trained on a large volume of annotated data is used. In other words, this study uses VGG16 architecture instead of training a custom CNN from scratch for the task of BPH classification. VGG16 architecture is one of the transfer learning approaches in deep learning that has several advantages such as the architecture&#x02019;s weights and biases have been trained over millions of images to classify thousands of object and classes. This makes the weights and biases connecting its neurons have been optimally calculated. As the used dataset is small in nature, training VGG16 on a small dataset greatly affects the VGG16&#x02019;s ability to generalize, often result in overfitting.</p><p>To overcome aforementioned problems in training CNN, this research uses fine-tuning technique using the following strategies:</p><list list-type="bullet"><list-item><p>Use a smaller learning rate,</p></list-item><list-item><p>Truncate the last layer from 1000 classes to 2 classes, and</p></list-item><list-item><p>Freeze the weights of the first few layers.</p></list-item></list><p>The VGG16 architecture that performs these strategies is shown in <xref ref-type="fig" rid="pone.0208501.g005">Fig 5</xref>. In general, VGG16 architecture is a 16-layer network. A new feature with the main feature of this architecture was the increased depth of the network. The input images have the size of 224x224 are passed through 5 blocks of convolutional layers where each block is composed of increasing numbers of 3x3 kernels. The stride is fixed to 1 while the convolutional layer inputs are padded such that the spatial resolution is preserved after convolution. The blocks are separated by max-pooling layers. Max-pooling is performed over 22 windows with stride 2. The 5 blocks of convolutional layers are followed by three fully-connected layers. The final layer is a soft-max layer that outputs class probabilities.</p><fig id="pone.0208501.g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0208501.g005</object-id><label>Fig 5</label><caption><title>VGG architecture.</title></caption><graphic xlink:href="pone.0208501.g005"/></fig></sec></sec><sec id="sec010"><title>Experiments</title><p>The CNN structures and VGG16 architecture are evaluated based on the intact/imperfect insect pest image patches. These image patches contain 2 distinct categories: positive and negative.</p><p>Positive patches are derived from manually labelled bounding circle, where each one represents a BPH. <xref ref-type="fig" rid="pone.0208501.g002">Fig 2A</xref> shows the positive patches. Negative patches contain most of the uninteresting &#x0201c;negative&#x0201d; areas and benign insects as shown in <xref ref-type="fig" rid="pone.0208501.g002">Fig 2B</xref>. To get the uninteresting &#x0201c;negative&#x0201d; areas, this study applies the Canny edge detector to find patches, those that do not contain any BPH. To make the classifier more discriminative, a bootstrapping approach was performed to find useful training patches.</p><p>In machine learning, the larger the dataset, the better the generalization performance. In this study, the amount of training data, which is represented by the number of training patches, is much smaller than standard small-scale image classification datasets frequently used by the deep learning field. Therefore, data augmentation was performed to increase the number of images for training, and incorporated invariance to basic geometric transformations into the classifier.</p></sec></sec><sec id="sec011"><title>Results and discussion</title><p>This section quantitively describes the results of the PENYEK performance and then discusses qualitative visual results.</p><sec id="sec012"><title>Quantitative</title><p>The first experiment is a pilot study to evaluate binary filtering operations: Outline, Fill holes, Skeletonize, Watershed, Voronoi and Euclidean distance map. Seven different CNN structures were devised to create a novel pipeline for identification and classification of heterogenous BPH to test the filters. <xref rid="pone.0208501.t004" ref-type="table">Table 4</xref> shows the empirical results of the pilot study. On observing the classification performance using the test accuracies, the red highlighted CNN architecture with EDM achieves favourable results on the small representative dataset. Based on empirical analysis and insight to accurately model the characteristics of BPH images, EDM is selected for further evaluation with CNN VGG16 for BPH classification.</p><table-wrap id="pone.0208501.t004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0208501.t004</object-id><label>Table 4</label><caption><title>Details of empirically evaluated CNN structures for BPH classification on sticky pad images.</title><p>5 cross-validation (CV).</p></caption><alternatives><graphic id="pone.0208501.t004g" xlink:href="pone.0208501.t004"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="justify" rowspan="1" colspan="1">Image Model and CNN structures</th><th align="justify" rowspan="1" colspan="1">Validation average accuracy (BPH classification)</th><th align="justify" rowspan="1" colspan="1">Test average accuracy<break/>(BPH classification)</th></tr></thead><tbody><tr><td align="justify" rowspan="1" colspan="1">A (1,2,3,4,5,6,7)</td><td align="justify" rowspan="1" colspan="1">0.4235</td><td align="justify" rowspan="1" colspan="1">0.4333</td></tr><tr><td align="justify" rowspan="1" colspan="1">B (1,2,3,4,5,6,7)</td><td align="justify" rowspan="1" colspan="1">0.6208</td><td align="justify" rowspan="1" colspan="1">0.6140</td></tr><tr><td align="justify" rowspan="1" colspan="1">C (1,2,3,4,5,6,7)</td><td align="justify" rowspan="1" colspan="1">0.7025</td><td align="justify" rowspan="1" colspan="1">0.6417</td></tr><tr><td align="justify" rowspan="1" colspan="1">D (1,2,3,4,5,6,7)</td><td align="justify" rowspan="1" colspan="1">0.7</td><td align="justify" rowspan="1" colspan="1">0.6432</td></tr><tr><td align="justify" rowspan="1" colspan="1">E (1,2,3,4,5,5,6,7)</td><td align="justify" rowspan="1" colspan="1">0.6951</td><td align="justify" rowspan="1" colspan="1">0.5376</td></tr><tr><td align="justify" rowspan="1" colspan="1">F (1,2,3,4,5,6,7)</td><td align="justify" rowspan="1" colspan="1">0.7056</td><td align="justify" rowspan="1" colspan="1">0.7571</td></tr><tr><td align="justify" rowspan="1" colspan="1">G (1,2,3,4,5,6,7)</td><td align="justify" rowspan="1" colspan="1">0.4948</td><td align="justify" rowspan="1" colspan="1">0.4948</td></tr></tbody></table></alternatives></table-wrap><p>PENYEK pipeline is shown in <xref ref-type="fig" rid="pone.0208501.g003">Fig 3</xref> in which the pipeline combines EDM and VGG16. VGG16 architecture accepts 224x224 RGB images as input, but in this study, the grayscale image with the same size is fed to the architecture. <xref rid="pone.0208501.t005" ref-type="table">Table 5</xref> shows the performance of PENYEK by varying the parameters. PENYEK shows promising performance with 95% accuracy, 94% sensitivity, 92% specificity and 93% AUC. This is true when VGG15 is fine-tuned and accepts 224x224 image size as input. However, the performance of VGG16 deteriorates when the architecture is learnt from scratch with 90% accuracy, 90% sensitivity, 83% specificity and 88% AUC. Reducing the size of images further decreases the performance of VGG16 as shown in <xref rid="pone.0208501.t005" ref-type="table">Table 5</xref>. Overall, PENYEK achieves better performance than random performance.</p><table-wrap id="pone.0208501.t005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0208501.t005</object-id><label>Table 5</label><caption><title>Performance of VGG16.</title><p>5 CV.</p></caption><alternatives><graphic id="pone.0208501.t005g" xlink:href="pone.0208501.t005"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="justify" rowspan="1" colspan="1">Method</th><th align="justify" rowspan="1" colspan="1">Training</th><th align="justify" rowspan="1" colspan="1">Accuracy (%)</th><th align="justify" rowspan="1" colspan="1">Sensitivity (%)</th><th align="justify" rowspan="1" colspan="1">Specificity (%)</th><th align="justify" rowspan="1" colspan="1">AUC (%)</th></tr></thead><tbody><tr><td align="justify" rowspan="1" colspan="1">224x224 + EDM + VGG16</td><td align="justify" rowspan="1" colspan="1">Transfer learning</td><td align="justify" rowspan="1" colspan="1">95</td><td align="justify" rowspan="1" colspan="1">94</td><td align="justify" rowspan="1" colspan="1">92</td><td align="justify" rowspan="1" colspan="1">93</td></tr><tr><td align="justify" rowspan="1" colspan="1">224x224 + EDM + VGG16</td><td align="justify" rowspan="1" colspan="1">From scratch</td><td align="justify" rowspan="1" colspan="1">90</td><td align="justify" rowspan="1" colspan="1">90</td><td align="justify" rowspan="1" colspan="1">83</td><td align="justify" rowspan="1" colspan="1">88</td></tr><tr><td align="justify" rowspan="1" colspan="1">21X21 + EDM + VGG16</td><td align="justify" rowspan="1" colspan="1">Transfer learning</td><td align="justify" rowspan="1" colspan="1">78</td><td align="justify" rowspan="1" colspan="1">73</td><td align="justify" rowspan="1" colspan="1">81</td><td align="justify" rowspan="1" colspan="1">82</td></tr><tr><td align="justify" rowspan="1" colspan="1">49x49 + EDM+ VGG16</td><td align="justify" rowspan="1" colspan="1">Transfer learning</td><td align="justify" rowspan="1" colspan="1">79</td><td align="justify" rowspan="1" colspan="1">75</td><td align="justify" rowspan="1" colspan="1">80</td><td align="justify" rowspan="1" colspan="1">85</td></tr></tbody></table></alternatives></table-wrap><p><xref rid="pone.0208501.t006" ref-type="table">Table 6</xref> shows the effect of data augmentation on the proposed pipeline performance, this study performs experiments on the VGG16 with input size 224x224 by either using (1) both rotational and translational augmentation; (2) only rotational augmentation; (3) only translational augmentation; and (4) no augmentation. The empirical results show that both translational and rotational augmentations improved the performance compared to no augmentation at all. The accuracy of the data augmentation is 95% while the AUC is 93% and the accuracy of 71% and AUC of 78% for no augmentation.</p><table-wrap id="pone.0208501.t006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0208501.t006</object-id><label>Table 6</label><caption><title>Effectiveness of data augmentation.</title><p>5 CV.</p></caption><alternatives><graphic id="pone.0208501.t006g" xlink:href="pone.0208501.t006"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="justify" rowspan="1" colspan="1">Method</th><th align="justify" rowspan="1" colspan="1">Accuracy (%)</th><th align="justify" rowspan="1" colspan="1">AUC (%)</th></tr></thead><tbody><tr><td align="justify" rowspan="1" colspan="1">VGG16 + Trans &#x00026; Rot Aug</td><td align="justify" rowspan="1" colspan="1">95</td><td align="justify" rowspan="1" colspan="1">93</td></tr><tr><td align="justify" rowspan="1" colspan="1">VGG16 + Rot Aug</td><td align="justify" rowspan="1" colspan="1">92.6</td><td align="justify" rowspan="1" colspan="1">91</td></tr><tr><td align="justify" rowspan="1" colspan="1">VGG16 + Trans Aug</td><td align="justify" rowspan="1" colspan="1">88.9</td><td align="justify" rowspan="1" colspan="1">91</td></tr><tr><td align="justify" rowspan="1" colspan="1">VGG16 + No Aug</td><td align="justify" rowspan="1" colspan="1">71.3</td><td align="justify" rowspan="1" colspan="1">78.2</td></tr></tbody></table></alternatives></table-wrap><p>Eventually, the performance of PENYEK pipeline is compared with state-of-the-art approaches for image analysis in digital pest identification and classification. These include handcrafted texture and colour descriptors such as the GLCM features, Gabor filter-bank features, LBP histograms, gray histograms, HSV histograms and RGB histograms followed by random forest machine learning. The average classification accuracy and AUC using the state-of-the-art methods is shown in <xref rid="pone.0208501.t007" ref-type="table">Table 7</xref> for BPH classification. Overall, the accuracy and AUC of PENYEK outperform the state-of-the-art methods.</p><table-wrap id="pone.0208501.t007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0208501.t007</object-id><label>Table 7</label><caption><title>Average classification accuracy and AUC of applied methods for BPH classification.</title></caption><alternatives><graphic id="pone.0208501.t007g" xlink:href="pone.0208501.t007"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="justify" rowspan="1" colspan="1">Method</th><th align="justify" rowspan="1" colspan="1">Training</th><th align="justify" rowspan="1" colspan="1">Accuracy (%)</th><th align="justify" rowspan="1" colspan="1">AUC (%)</th></tr></thead><tbody><tr><td align="justify" rowspan="1" colspan="1">GLCM + random forest</td><td align="justify" rowspan="1" colspan="1">From scratch</td><td align="justify" rowspan="1" colspan="1">69.93</td><td align="justify" rowspan="1" colspan="1">69.42</td></tr><tr><td align="justify" rowspan="1" colspan="1">Gabor filter + random forest</td><td align="justify" rowspan="1" colspan="1">From scratch</td><td align="justify" rowspan="1" colspan="1">66.27</td><td align="justify" rowspan="1" colspan="1">68.56</td></tr><tr><td align="justify" rowspan="1" colspan="1">LBP histograms + random forest</td><td align="justify" rowspan="1" colspan="1">From scratch</td><td align="justify" rowspan="1" colspan="1">65.79</td><td align="justify" rowspan="1" colspan="1">63.48</td></tr><tr><td align="justify" rowspan="1" colspan="1">Gray histograms + random forest</td><td align="justify" rowspan="1" colspan="1">From scratch</td><td align="justify" rowspan="1" colspan="1">71.55</td><td align="justify" rowspan="1" colspan="1">75.04</td></tr><tr><td align="justify" rowspan="1" colspan="1">HSV histograms + random forest</td><td align="justify" rowspan="1" colspan="1">From scratch</td><td align="justify" rowspan="1" colspan="1">71.62</td><td align="justify" rowspan="1" colspan="1">75.46</td></tr><tr><td align="justify" rowspan="1" colspan="1">RGB histograms + random forest</td><td align="justify" rowspan="1" colspan="1">From scratch</td><td align="justify" rowspan="1" colspan="1">75.67</td><td align="justify" rowspan="1" colspan="1">77.41</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec013"><title>Qualitative</title><p><xref ref-type="fig" rid="pone.0208501.g006">Fig 6</xref> shows an example of the proposed pipeline in operation. In <xref ref-type="fig" rid="pone.0208501.g006">Fig 6</xref>, all images in the figure are correctly classified.</p><fig id="pone.0208501.g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0208501.g006</object-id><label>Fig 6</label><caption><title>Classification results of 12 test images fed into VGG16.</title><p>The image size is 244x244 in RGB image. BPH and benign insect pests are classified using the PENYEK pipeline at the specified confidence percentage.</p></caption><graphic xlink:href="pone.0208501.g006"/></fig></sec></sec><sec sec-type="conclusions" id="sec014"><title>Conclusion</title><p>Accurate insect pest detection is very important in agriculture for the estimation of pest population density and dynamics in fields which allows for precision pesticide application. Due to the complex environment background of living pests, it is a big challenge to automatically identify them by image processing. The major challenge in the state-of-the-art automated system is to identify imperfect images. To replace human expertise and to overcome the aforementioned major challenges in the automated system, this study proposed an automated detection pipeline for Brown Planthopper in paddy fields called PENYEK. The PENYEK pipeline leveraged the architecture of VGG16 and Euclidean Distance Map (EDM) by applying the pre-trained weights and biases for classifying imperfect images. VGG16 network pre-trained on the large ImageNet dataset is fine-tuned to learn features of the BPH image dataset. The VGG16 architecture learned to identify BPH based only on positive and negative training samples. The insect pest images are in grayscale and achieves lower accuracy in RGB.</p><p>The first component of the proposed pipeline is image processing by applying binary filtering operations and other pre-processing techniques on image patches. From the performance of several CNN structures, EDM shows the best performance of all in term of accuracy. The second component is the VGG16 pre-trained architecture that has been fine-tuned to be trained in a small dataset. The performance of VGG16 architecture increases when fed with EDM images in term of accuracy, sensitivity, specificity and AUC. Moreover, the VGG16 architecture outperforms the state-of-the-art methods in image analysis in term of accuracy and AUC. All in all, qualitative and quantitative empirical results demonstrate the effectiveness of PENYEK pipeline on an insect pest image dataset.</p></sec></body><back><ack><p>The authors gratefully acknowledge the support of the HICoE ITAFoS (HICoE&#x02013;ITAFoS/2017/FC3), Fundamental Research Grant Scheme (FRGS) (Vote No: 5524959) and GP-IPM (vote No: 9538100).</p></ack><ref-list><title>References</title><ref id="pone.0208501.ref001"><label>1</label><mixed-citation publication-type="journal">M. D. P. and <name><surname>Khan</surname><given-names>Z. R.</given-names></name>, <source>Insect Pests of Rice</source>, vol. <volume>27</volume>, no. <issue>02</issue>
<year>1994</year>.</mixed-citation></ref><ref id="pone.0208501.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Witzgall</surname><given-names>P.</given-names></name>, <name><surname>Kirsch</surname><given-names>P.</given-names></name>, and <name><surname>Cork</surname><given-names>A.</given-names></name>, &#x0201c;<article-title>Sex pheromones and their impact on pest management</article-title>,&#x0201d; <source>J. Chem. Ecol.</source>, vol. <volume>36</volume>, no. <issue>1</issue>, pp. <fpage>80</fpage>&#x02013;<lpage>100</lpage>, <year>2010</year>
<pub-id pub-id-type="doi">10.1007/s10886-009-9737-y</pub-id>
<?supplied-pmid 20108027?><pub-id pub-id-type="pmid">20108027</pub-id></mixed-citation></ref><ref id="pone.0208501.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Carde</surname><given-names>R. T.</given-names></name> and <name><surname>Minks</surname><given-names>A. K.</given-names></name>, &#x0201c;<article-title>Control of Moth Pests by Mating Disruption: Successes and Constraints</article-title>,&#x0201d; <source>Annu. Rev. Entomol.</source>, vol. <volume>40</volume>, no. <issue>1</issue>, pp. <fpage>559</fpage>&#x02013;<lpage>585</lpage>, <year>1995</year>.</mixed-citation></ref><ref id="pone.0208501.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Cho</surname><given-names>J.</given-names></name>
<etal>et al.</etal>, &#x0201c;<article-title>Automatic identification of whiteflies, aphids and thrips in greenhouse based on image analysis</article-title>,&#x0201d; <source>Iinternational J. Math. Comput. Simul.</source>, vol. <volume>1</volume>, no. <issue>1</issue>, pp. <fpage>46</fpage>&#x02013;<lpage>53</lpage>, <year>2007</year>.</mixed-citation></ref><ref id="pone.0208501.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Kang</surname><given-names>S. H.</given-names></name>, <name><surname>Song</surname><given-names>S. H.</given-names></name>, and <name><surname>Lee</surname><given-names>S. H.</given-names></name>, &#x0201c;<article-title>Identification of butterfly species with a single neural network system</article-title>,&#x0201d; <source>J. Asia. Pac. Entomol.</source>, vol. <volume>15</volume>, no. <issue>3</issue>, pp. <fpage>431</fpage>&#x02013;<lpage>435</lpage>, <year>2012</year>.</mixed-citation></ref><ref id="pone.0208501.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Kang</surname><given-names>S. H.</given-names></name>, <name><surname>Cho</surname><given-names>J. H.</given-names></name>, and <name><surname>Lee</surname><given-names>S. H.</given-names></name>, &#x0201c;<article-title>Identification of butterfly based on their shapes when viewed from different angles using an artificial neural network</article-title>,&#x0201d; <source>J. Asia. Pac. Entomol.</source>, vol. <volume>17</volume>, no. <issue>2</issue>, pp. <fpage>143</fpage>&#x02013;<lpage>149</lpage>, <year>2014</year>.</mixed-citation></ref><ref id="pone.0208501.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Arbuckle</surname><given-names>T.</given-names></name>, <name><surname>Schr&#x000f6;der</surname><given-names>S.</given-names></name>, <name><surname>Steinhage</surname><given-names>V.</given-names></name>, and <name><surname>Wittmann</surname><given-names>D.</given-names></name>, &#x0201c;<article-title>Biodiversity informatics in action: identification and monitoring of bee species using ABIS</article-title>,&#x0201d; <source>EnviroInfo</source>, vol. <volume>1</volume>, pp. <fpage>425</fpage>&#x02013;<lpage>430</lpage>, <year>2001</year>.</mixed-citation></ref><ref id="pone.0208501.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Weeks</surname><given-names>P. J. D.</given-names></name>, <name><surname>O&#x02019;Neill</surname><given-names>M. A.</given-names></name>, <name><surname>Gaston</surname><given-names>K. J.</given-names></name>, and <name><surname>Gauld</surname><given-names>I. D.</given-names></name>, &#x0201c;<article-title>Automating insect identification: exploring the limitations of a prototype system</article-title>,&#x0201d; <source>J. Appl. Entomol.</source>, vol. <volume>123</volume>, no. <issue>1</issue>, pp. <fpage>1</fpage>&#x02013;<lpage>8</lpage>, <year>1999</year>.</mixed-citation></ref><ref id="pone.0208501.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Tofilski</surname><given-names>A.</given-names></name>, &#x0201c;<article-title>DrawWing, a program for numerical description of insect wings</article-title>,&#x0201d; <source>J. Insect Sci.</source>, vol. <volume>4</volume>, no. <issue>17</issue>, pp. <fpage>1</fpage>&#x02013;<lpage>5</lpage>, <year>2004</year>.<pub-id pub-id-type="pmid">15861217</pub-id></mixed-citation></ref><ref id="pone.0208501.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>J.</given-names></name>, <name><surname>Lin</surname><given-names>C.</given-names></name>, <name><surname>Ji</surname><given-names>L.</given-names></name>, and <name><surname>Liang</surname><given-names>A.</given-names></name>, &#x0201c;<article-title>A new automatic identification system of insect images at the order level</article-title>,&#x0201d; <source>Knowledge-Based Syst.</source>, vol. <volume>33</volume>, pp. <fpage>102</fpage>&#x02013;<lpage>110</lpage>, <year>2012</year>.</mixed-citation></ref><ref id="pone.0208501.ref011"><label>11</label><mixed-citation publication-type="other">G. Mart&#x000ed;nez-Mu&#x000f1;oz et al., &#x0201c;Dictionary-free categorization of very similar objects via stacked evidence trees,&#x0201d; in 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009, 2009, pp. 549&#x02013;556.</mixed-citation></ref><ref id="pone.0208501.ref012"><label>12</label><mixed-citation publication-type="other">N. Larios, B. Soran, L. G. Shapiro, G. Mart&#x000ed;nez-Mu&#x000f1;oz, J. Lin, and T. G. Dietterich, &#x0201c;Haar random forest features and SVM spatial matching kernel for stonefly species identification,&#x0201d; in Proceedings&#x02014;International Conference on Pattern Recognition, 2010, pp. 2624&#x02013;2627.</mixed-citation></ref><ref id="pone.0208501.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Lytle</surname><given-names>D. A.</given-names></name>
<etal>et al.</etal>, &#x0201c;<article-title>Automated processing and identification of benthic invertebrate samples</article-title>,&#x0201d; <source>J. North Am. Benthol. Soc.</source>, vol. <volume>29</volume>, no. <issue>3</issue>, pp. <fpage>867</fpage>&#x02013;<lpage>874</lpage>, <year>2010</year>.</mixed-citation></ref><ref id="pone.0208501.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Al-Saqer</surname><given-names>S. M.</given-names></name>, <name><surname>Weckler</surname><given-names>P.</given-names></name>, <name><surname>Solie</surname><given-names>J.</given-names></name>, <name><surname>Stone</surname><given-names>M.</given-names></name>, and <name><surname>Wayadande</surname><given-names>A.</given-names></name>, &#x0201c;<article-title>Identification of pecan weevils through image processing</article-title>,&#x0201d; <source>Am. J. Agric. Biol. Sci.</source>, vol. <volume>6</volume>, no. <issue>1</issue>, pp. <fpage>69</fpage>&#x02013;<lpage>79</lpage>, <year>2011</year>.</mixed-citation></ref><ref id="pone.0208501.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Mayo</surname><given-names>M.</given-names></name> and <name><surname>Watson</surname><given-names>A. T.</given-names></name>, &#x0201c;<article-title>Automatic species identification of live moths</article-title>,&#x0201d; <source>Knowledge-Based Syst.</source>, vol. <volume>20</volume>, no. <issue>2</issue>, pp. <fpage>195</fpage>&#x02013;<lpage>202</lpage>, <year>2007</year>.</mixed-citation></ref><ref id="pone.0208501.ref016"><label>16</label><mixed-citation publication-type="other">R. Kumar, V. Martin, S. M.-P. of the 20th, and undefined 2010, &#x0201c;Robust insect classification applied to real time greenhouse infestation monitoring,&#x0201d; homepages.inf.ed.ac.uk.</mixed-citation></ref><ref id="pone.0208501.ref017"><label>17</label><mixed-citation publication-type="other">R. Mundada, V. G.-I. J. of Electronics, and undefined 2013, &#x0201c;Detection and classification of pests in greenhouse using image processing,&#x0201d; pdfs.semanticscholar.org.</mixed-citation></ref><ref id="pone.0208501.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Qiao</surname><given-names>M.</given-names></name>
<etal>et al.</etal>, &#x0201c;<article-title>Density estimation of Bemisia tabaci (Hemiptera: Aleyrodidae) in a greenhouse using sticky traps in conjunction with an image processing system</article-title>,&#x0201d; <source>J. Asia. Pac. Entomol.</source>, vol. <volume>11</volume>, no. <issue>1</issue>, pp. <fpage>25</fpage>&#x02013;<lpage>29</lpage>, <year>2008</year>.</mixed-citation></ref><ref id="pone.0208501.ref019"><label>19</label><mixed-citation publication-type="other">K. Venugoban, A. R.-I. J. of Machine, and undefined 2014, &#x0201c;Image classification of paddy field insect pests using gradient-based features,&#x0201d; search.proquest.com.</mixed-citation></ref><ref id="pone.0208501.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Yao</surname><given-names>Q.</given-names></name>
<etal>et al.</etal>, &#x0201c;<article-title>Segmentation of touching insects based on optical flow and NCuts</article-title>,&#x0201d; <source>Biosyst. Eng.</source>, vol. <volume>114</volume>, no. <issue>2</issue>, pp. <fpage>67</fpage>&#x02013;<lpage>77</lpage>, <year>2013</year>.</mixed-citation></ref><ref id="pone.0208501.ref021"><label>21</label><mixed-citation publication-type="other">L. Q. Zhu and Z. Zhang, &#x0201c;Auto-classification of insect images based on color histogram and GLCM,&#x0201d; in Proceedings&#x02014;2010 7th International Conference on Fuzzy Systems and Knowledge Discovery, FSKD 2010, 2010, vol. 6, pp. 2589&#x02013;2593.</mixed-citation></ref><ref id="pone.0208501.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Kaya</surname><given-names>Y.</given-names></name> and <name><surname>Kayci</surname><given-names>L.</given-names></name>, &#x0201c;<article-title>Application of artificial neural network for automatic detection of butterfly species using color and texture features</article-title>,&#x0201d; <source>Vis. Comput.</source>, vol. <volume>30</volume>, no. <issue>1</issue>, pp. <fpage>71</fpage>&#x02013;<lpage>79</lpage>, <year>2014</year>.</mixed-citation></ref><ref id="pone.0208501.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Fedor</surname><given-names>P.</given-names></name>, <name><surname>Malenovsk&#x000fd;</surname><given-names>I.</given-names></name>, <name><surname>Vanhara</surname><given-names>J.</given-names></name>, <name><surname>Sierka</surname><given-names>W.</given-names></name>, and <name><surname>Havel</surname><given-names>J.</given-names></name>, &#x0201c;<article-title>Thrips (Thysanoptera) identification using artificial neural networks.,</article-title>&#x0201d; <source>Bull. Entomol. Res.</source>, vol. <volume>98</volume>, no. <issue>5</issue>, pp. <fpage>437</fpage>&#x02013;<lpage>447</lpage>, <year>2008</year>
<pub-id pub-id-type="doi">10.1017/S0007485308005750</pub-id>
<?supplied-pmid 18423077?><pub-id pub-id-type="pmid">18423077</pub-id></mixed-citation></ref><ref id="pone.0208501.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Yaakob</surname><given-names>S. N.</given-names></name> and <name><surname>Jain</surname><given-names>L.</given-names></name>, &#x0201c;<article-title>An insect classification analysis based on shape features using quality threshold ARTMAP and moment invariant</article-title>,&#x0201d; <source>Appl. Intell.</source>, vol. <volume>37</volume>, no. <issue>1</issue>, pp. <fpage>12</fpage>&#x02013;<lpage>30</lpage>, <month>7</month>
<year>2012</year>.</mixed-citation></ref><ref id="pone.0208501.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Wen</surname><given-names>C.</given-names></name>, <name><surname>Guyer</surname><given-names>D. E.</given-names></name>, and <name><surname>Li</surname><given-names>W.</given-names></name>, &#x0201c;<article-title>Local feature-based identification and classification for orchard insects</article-title>,&#x0201d; <source>Biosyst. Eng.</source>, vol. <volume>104</volume>, no. <issue>3</issue>, pp. <fpage>299</fpage>&#x02013;<lpage>307</lpage>, <year>2009</year>.</mixed-citation></ref><ref id="pone.0208501.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Wen</surname><given-names>C.</given-names></name> and <name><surname>Guyer</surname><given-names>D.</given-names></name>, &#x0201c;<article-title>Image-based orchard insect automated identification and classification method</article-title>,&#x0201d; <source>Comput. Electron. Agric.</source>, vol. <volume>89</volume>, pp. <fpage>110</fpage>&#x02013;<lpage>115</lpage>, <year>2012</year>.</mixed-citation></ref><ref id="pone.0208501.ref027"><label>27</label><mixed-citation publication-type="other">A. Lu, X. Hou, C.-L. Liu, and X. Chen, &#x0201c;Insect species recognition using discriminative local soft coding,&#x0201d; in Proceedings&#x02014;International Conference on Pattern Recognition, 2012.</mixed-citation></ref><ref id="pone.0208501.ref028"><label>28</label><mixed-citation publication-type="other">N. Larios et al., &#x0201c;Stacked spatial-pyramid kernel: An object-class recognition method to combine scores from random trees,&#x0201d; in 2011 IEEE Workshop on Applications of Computer Vision, WACV 2011, 2011, pp. 329&#x02013;335.</mixed-citation></ref><ref id="pone.0208501.ref029"><label>29</label><mixed-citation publication-type="other">X. L. Li, S. G. Huang, M. Q. Zhou, and G. H. Geng, &#x0201c;KNN-spectral regression LDA for insect recognition,&#x0201d; in 2009 1st International Conference on Information Science and Engineering, ICISE 2009, 2009, pp. 1315&#x02013;1318.</mixed-citation></ref><ref id="pone.0208501.ref030"><label>30</label><mixed-citation publication-type="other">X. Zhang, Y. Yang, Z. Han, H. Wang, C. G.-A. C. Surveys, and undefined 2013, &#x0201c;Object class detection: A survey,&#x0201d; dl.acm.org.</mixed-citation></ref><ref id="pone.0208501.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Andreopoulos</surname><given-names>A.</given-names></name> and <name><surname>Tsotsos</surname><given-names>J. K.</given-names></name>, &#x0201c;<article-title>50 Years of object recognition: Directions forward</article-title>,&#x0201d; <source>Comput. Vis. Image Underst.</source>, vol. <volume>117</volume>, no. <issue>8</issue>, pp. <fpage>827</fpage>&#x02013;<lpage>891</lpage>, <year>2013</year>.</mixed-citation></ref><ref id="pone.0208501.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Nikitenko</surname><given-names>D.</given-names></name>, <name><surname>Wirth</surname><given-names>M.</given-names></name>, and <name><surname>Trudel</surname><given-names>K.</given-names></name>, &#x0201c;<article-title>Applicability of white-balancing algorithms to restoring faded colour slides: An empirical evaluation</article-title>,&#x0201d; <source>J. Multimed.</source>, vol. <volume>3</volume>, no. <issue>5</issue>, pp. <fpage>9</fpage>&#x02013;<lpage>18</lpage>, <year>2008</year>.</mixed-citation></ref><ref id="pone.0208501.ref033"><label>33</label><mixed-citation publication-type="other">S. Reddi, &#x02026; S. R.-I. T. on, and undefined 1984, &#x0201c;An optimal multiple threshold scheme for image segmentation,&#x0201d; ieeexplore.ieee.org.</mixed-citation></ref></ref-list></back></article>