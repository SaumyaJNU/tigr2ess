<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6209949</article-id><article-id pub-id-type="doi">10.3390/s18103299</article-id><article-id pub-id-type="publisher-id">sensors-18-03299</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Accurate Weed Mapping and Prescription Map Generation Based on Fully Convolutional Networks Using UAV Imagery</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Huang</surname><given-names>Huasheng</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03299">1</xref><xref ref-type="aff" rid="af2-sensors-18-03299">2</xref></contrib><contrib contrib-type="author"><name><surname>Deng</surname><given-names>Jizhong</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03299">1</xref><xref ref-type="aff" rid="af2-sensors-18-03299">2</xref><xref rid="c1-sensors-18-03299" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Lan</surname><given-names>Yubin</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03299">1</xref><xref ref-type="aff" rid="af2-sensors-18-03299">2</xref><xref rid="c1-sensors-18-03299" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Aqing</given-names></name><xref ref-type="aff" rid="af3-sensors-18-03299">3</xref></contrib><contrib contrib-type="author"><name><surname>Deng</surname><given-names>Xiaoling</given-names></name><xref ref-type="aff" rid="af2-sensors-18-03299">2</xref><xref ref-type="aff" rid="af3-sensors-18-03299">3</xref></contrib><contrib contrib-type="author"><name><surname>Wen</surname><given-names>Sheng</given-names></name><xref ref-type="aff" rid="af2-sensors-18-03299">2</xref><xref ref-type="aff" rid="af4-sensors-18-03299">4</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9781-6086</contrib-id><name><surname>Zhang</surname><given-names>Huihui</given-names></name><xref ref-type="aff" rid="af5-sensors-18-03299">5</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Yali</given-names></name><xref ref-type="aff" rid="af1-sensors-18-03299">1</xref><xref ref-type="aff" rid="af2-sensors-18-03299">2</xref></contrib></contrib-group><aff id="af1-sensors-18-03299"><label>1</label>College of Engineering, South China Agricultural University, Wushan Road, Guangzhou 510642, China; <email>huanghsheng@stu.scau.edu.cn</email> (H.H.); <email>ylzhang@scau.edu.cn</email> (Y.Z.)</aff><aff id="af2-sensors-18-03299"><label>2</label>National Center for International Collaboration Research on Precision Agricultural Aviation Pesticide Spraying Technology, Wushan Road, Guangzhou 510624, China; <email>dengxl@scau.edu.cn</email> (X.D.); <email>vincen@scau.edu.cn</email> (S.W.)</aff><aff id="af3-sensors-18-03299"><label>3</label>College of Electronic Engineering, South China Agricultural University, Wushan Road, Guangzhou 510624, China; <email>yangaqing@stu.scau.edu.cn</email></aff><aff id="af4-sensors-18-03299"><label>4</label>Engineering Fundamental Teaching and Training Center, South China Agricultural University, Wushan Road, Guangzhou 510624, China</aff><aff id="af5-sensors-18-03299"><label>5</label>USDA, Agricultural Research Service, Water Management Research Unit, 2150 Centre Ave., Building D, Suite 320, Fort Collins, CO 80526-8119, USA; <email>huihui.zhang@ars.usda.gov</email></aff><author-notes><corresp id="c1-sensors-18-03299"><label>*</label>Correspondence: <email>jz-deng@scau.edu.cn</email> (J.D.); <email>ylan@scau.edu.cn</email> (Y.L.)</corresp></author-notes><pub-date pub-type="epub"><day>01</day><month>10</month><year>2018</year></pub-date><pub-date pub-type="collection"><month>10</month><year>2018</year></pub-date><volume>18</volume><issue>10</issue><elocation-id>3299</elocation-id><history><date date-type="received"><day>25</day><month>8</month><year>2018</year></date><date date-type="accepted"><day>18</day><month>9</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; 2018 by the authors.</copyright-statement><copyright-year>2018</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Chemical control is necessary in order to control weed infestation and to ensure a rice yield. However, excessive use of herbicides has caused serious agronomic and environmental problems. Site specific weed management (SSWM) recommends an appropriate dose of herbicides according to the weed coverage, which may reduce the use of herbicides while enhancing their chemical effects. In the context of SSWM, the weed cover map and prescription map must be generated in order to carry out the accurate spraying. In this paper, high resolution unmanned aerial vehicle (UAV) imagery were captured over a rice field. Different workflows were evaluated to generate the weed cover map for the whole field. Fully convolutional networks (FCN) was applied for a pixel-level classification. Theoretical analysis and practical evaluation were carried out to seek for an architecture improvement and performance boost. A chessboard segmentation process was used to build the grid framework of the prescription map. The experimental results showed that the overall accuracy and mean intersection over union (mean IU) for weed mapping using FCN-4s were 0.9196 and 0.8473, and the total time (including the data collection and data processing) required to generate the weed cover map for the entire field (50 &#x000d7; 60 m) was less than half an hour. Different weed thresholds (0.00&#x02013;0.25, with an interval of 0.05) were used for the prescription map generation. High accuracies (above 0.94) were observed for all of the threshold values, and the relevant herbicide saving ranged from 58.3% to 70.8%. All of the experimental results demonstrated that the method used in this work has the potential to produce an accurate weed cover map and prescription map in SSWM applications.</p></abstract><kwd-group><kwd>UAV</kwd><kwd>semantic labeling</kwd><kwd>FCN</kwd><kwd>weed mapping</kwd><kwd>prescription map</kwd></kwd-group></article-meta></front><body><sec id="sec1-sensors-18-03299"><title>1. Introduction</title><p>Chemical control is necessary to control weed infestation and to ensure rice production [<xref rid="B1-sensors-18-03299" ref-type="bibr">1</xref>]. Traditionally, the chemical control strategy has three steps: a single pre-emergence herbicide application, a post-emergence herbicide treatment, and an optional late post-emergence chemical spray [<xref rid="B2-sensors-18-03299" ref-type="bibr">2</xref>]. This strategy has been proven to be effective for weed control, through years of rice cultivation applications [<xref rid="B3-sensors-18-03299" ref-type="bibr">3</xref>]. However, the consistently increased use of herbicides has caused a negative impact on rice production and the environment [<xref rid="B4-sensors-18-03299" ref-type="bibr">4</xref>]. Usually farmers carry out uniform herbicide spraying over the entire field and do not consider the distribution of weed infestations [<xref rid="B5-sensors-18-03299" ref-type="bibr">5</xref>]. Excessive herbicides are applied over the areas with an absence of weed infestations, resulting in environmental pollution and chemical residues [<xref rid="B1-sensors-18-03299" ref-type="bibr">1</xref>]. Site specific weed management (SSWM) recommends a chemical reduction in the application and utilization of adequate herbicides based on the weed coverage [<xref rid="B6-sensors-18-03299" ref-type="bibr">6</xref>]. In the context of SSWM, the prescription map can provide decision making information for a variable-rate spraying machine (i.e., tractors or UAVs), which may reduce the use of herbicides while enhancing their chemical effects [<xref rid="B7-sensors-18-03299" ref-type="bibr">7</xref>]. </p><p>However, in order to obtain a prescription map, it is necessary to produce a weed cover map [<xref rid="B8-sensors-18-03299" ref-type="bibr">8</xref>]. UAV remote sensing provides a non-destructive and cost-effective platform for rapid monitoring of weed infestations [<xref rid="B9-sensors-18-03299" ref-type="bibr">9</xref>]. Compared with other remote sensing platforms (i.e., satellite and piloted aircraft remote sensing), UAV is able to fly at a low altitude and can capture high resolution imagery, which may monitor small weed patches in detail [<xref rid="B10-sensors-18-03299" ref-type="bibr">10</xref>]. Several works on weed mapping using UAV remote sensing have been conducted [<xref rid="B4-sensors-18-03299" ref-type="bibr">4</xref>,<xref rid="B6-sensors-18-03299" ref-type="bibr">6</xref>,<xref rid="B8-sensors-18-03299" ref-type="bibr">8</xref>]. Pe&#x000f1;a et al. [<xref rid="B6-sensors-18-03299" ref-type="bibr">6</xref>] used UAV multispectral imagery for weed mapping in maize fields. An automatic object-based image analysis (OBIA) procedure was developed, and a weed cover map was produced with 86% overall accuracy. L&#x000f3;pez-Granados et al. [<xref rid="B4-sensors-18-03299" ref-type="bibr">4</xref>] focused on the evaluation of different sensors (red, green, and blue (RGB) and multispectral cameras) and altitudes (30 and 60 m) for weed mapping. A robust image analysis method was developed for weed mapping, and high accuracies were observed using the multispectral camera at any flight altitude. Most of these studies were based on the OBIA framework. The use of the OBIA method requires a process of feature selection, which must be performed by manual designs [<xref rid="B6-sensors-18-03299" ref-type="bibr">6</xref>]. Although hand-designed features (i.e., texture features and vegetation indices) [<xref rid="B11-sensors-18-03299" ref-type="bibr">11</xref>,<xref rid="B12-sensors-18-03299" ref-type="bibr">12</xref>,<xref rid="B13-sensors-18-03299" ref-type="bibr">13</xref>] are a proven approach, they are application dependent and hard to generalize [<xref rid="B14-sensors-18-03299" ref-type="bibr">14</xref>]. </p><p>Fully convolutional networks (FCN) is an automatic feature learning algorithm that can address the disadvantages of OBIA approaches [<xref rid="B15-sensors-18-03299" ref-type="bibr">15</xref>]. FCN implements the forward and backward process in an end-to-end mode, which performs the feature learning automatically [<xref rid="B16-sensors-18-03299" ref-type="bibr">16</xref>]. In recent years, FCN has achieved great success in computer vision [<xref rid="B16-sensors-18-03299" ref-type="bibr">16</xref>,<xref rid="B17-sensors-18-03299" ref-type="bibr">17</xref>] and remote sensing applications [<xref rid="B18-sensors-18-03299" ref-type="bibr">18</xref>,<xref rid="B19-sensors-18-03299" ref-type="bibr">19</xref>,<xref rid="B20-sensors-18-03299" ref-type="bibr">20</xref>]. FCN introduces the pixel-to-pixel translation in an end-to-end mode, which shows great potential for the weed mapping of UAV imagery. However, no related study on weed mapping using FCN can be accessed, except for the work of [<xref rid="B21-sensors-18-03299" ref-type="bibr">21</xref>,<xref rid="B22-sensors-18-03299" ref-type="bibr">22</xref>]. In these works, semantic labeling approaches were directly applied on the collected UAV imagery. The experimental results showed that the semantic labeling approaches outperformed others in terms of accuracy and efficiency. However, both of these works did not generate a weed cover map or a prescription map for the whole field, which may not satisfy the requirement of practical SSWM applications. The objective of this work is as follows: to (1) compare the performance of different workflows so as to generate a weed cover map for the whole field; (2) carry out the theoretical analysis and practical evaluation to seek for an architecture improvement of FCN, which may bring a performance boost in accuracy and efficiency; and (3) generate a prescription map, which may provide decision-making information for SSWM applications.</p></sec><sec id="sec2-sensors-18-03299"><title>2. Data Collection</title><sec id="sec2dot1-sensors-18-03299"><title>2.1. Study Field and Data Collection</title><p>Experiments were conducted in a rice field located in Guangdong Province, China (23&#x000b0;14&#x02032;25&#x02033; N, 113&#x000b0;38&#x02032;12&#x02033; E, in reference system datum WGS84). The field was a rectangle area of 90 &#x000d7; 60 m with flat ground. The seeds were sown on 21 August 2017, and the rice started to emerge 15 days after sowing. The study plot was naturally infested with <italic>Cyperus iric</italic> [<xref rid="B23-sensors-18-03299" ref-type="bibr">23</xref>] and <italic>L. chinensis</italic> [<xref rid="B24-sensors-18-03299" ref-type="bibr">24</xref>]. The photograph of the studied field is illustrated in <xref ref-type="fig" rid="sensors-18-03299-f001">Figure 1</xref>. </p><p>Data collection was carried out on 2nd and 10th October 2017, when the rice and weeds were both in their early growth stages, and when the herbicide treatment was recommended. Phantom 4 (SZ DJI Technology Co., Ltd., Shenzhen, China) was used for data collection, and a 50 &#x000d7; 60 m plot was delimited in order to perform the flights. During the experiments, the flight altitude was set to 10 m, with a resolution of 0.5 cm per pixel. Sequences with a 70% forward-lap and 60% side-lap imagery were collected to cover the entire experimental plot. On 2nd and 10th October, 54 and 50 imagery (3000 &#x000d7; 4000 pixels) were collected in the experiments, respectively.</p></sec><sec id="sec2dot2-sensors-18-03299"><title>2.2. Dataset Preparation</title><p>Image mosaicking is an important step prior to image analysis [<xref rid="B4-sensors-18-03299" ref-type="bibr">4</xref>]. In this work, the collected imagery were stitched together to form the ortho-mosaicked imagery using the software of Photoscan [<xref rid="B25-sensors-18-03299" ref-type="bibr">25</xref>]. However, the ortho-mosaicked imagery is usually quite large (14,000 &#x000d7; 13,000 pixels in our work), making it a difficult task to carry out the data processing with limited CPU and GPU memory. In order to address this problem and to retain the original spatial resolution, we split the ortho-mosaicked imagery into small patches (1000 &#x000d7; 1000 pixels), similar with the work of Zhang et al. [<xref rid="B26-sensors-18-03299" ref-type="bibr">26</xref>]. Following this strategy, the datasets of D02-1 and D10-1 were generated from the ortho-mosaicked imagery obtained on 2nd and 10th October 2017. Besides that, we also directly split the collected imagery into small patches (1000 &#x000d7; 1000 pixels), which generated the dataset of D02-2 and D10-2, as shown in <xref rid="sensors-18-03299-t001" ref-type="table">Table 1</xref>.</p><p>For each imagery in the dataset, its corresponding ground truth (GT) label data was produced by careful manual labeling. With the high spatial resolution of UAV imagery, the weed-crop discrimination can be visually accessed, making it feasible to manually label the imagery at a pixel level. Thus, each sample in the dataset represented one image-GT label pair, and the GT label is used as the standard when evaluating the performance of the classifiers. Three image-GT label pairs are illustrated in <xref ref-type="fig" rid="sensors-18-03299-f002">Figure 2</xref>.</p></sec></sec><sec id="sec3-sensors-18-03299"><title>3. Methodology</title><p>In this work, two different workflows were applied to produce the weed cover map for the whole field. The performance of both workflows were evaluated and compared. Fully convolutional networks (FCN) was employed for the pixel level classification. Finally, the chessboard segmentation method was used to produce the prescription map based on the weed cover map.</p><sec id="sec3dot1-sensors-18-03299"><title>3.1. Workflow</title><p>Two different workflows were adopted as candidates, as shown in <xref ref-type="fig" rid="sensors-18-03299-f003">Figure 3</xref>. The first workflow conducted the mosaicking operation to generate the ortho-mosaicked imagery for the whole field, and then it performed a per-pixel classification to create the weed cover map. Inspired by the fact that some sections in the ortho-mosaicked imagery were blurring, which made it difficult to distinguish and may cause misclassification during the classification stage, we directly applied the labeling process on the collected imagery in the second workflow, which may avoid the ambiguous pixels in the classification stage. After that, the mosaicking process was conducted on the classification results, using the geo-information in the collected imagery. All of the mosaicking operations were performed using the software of Photoscan. </p><p>The evaluation of the workflows was measured for accuracy and efficiency. The accuracy was evaluated by the overall accuracy and the mean intersection over union (mean IU) [<xref rid="B16-sensors-18-03299" ref-type="bibr">16</xref>], and the time efficiency was measured using the total time required to generate the weed cover map, including data collection and data processing.</p></sec><sec id="sec3dot2-sensors-18-03299"><title>3.2. Semantic Labeling</title><p>Classical FCN-8s was proven to be effective on weed mapping of UAV imagery [<xref rid="B21-sensors-18-03299" ref-type="bibr">21</xref>], which outperformed the traditional methods in terms of accuracy and efficiency. In this work, we sought for an optimal network architecture that will bring about a performance improvement. </p><p>Similar with the network architecture of classical FCN-8s [<xref rid="B16-sensors-18-03299" ref-type="bibr">16</xref>], an ImageNet pre-trained Convolutional Neural Network (CNN) [<xref rid="B27-sensors-18-03299" ref-type="bibr">27</xref>] was adapted to fully convolutional networks and was transferred to our study using a fine-tuning technique. Besides that, two modifications were conducted on the baseline architecture of FCN-8s. (1) In the previous experiments on the skip architecture [<xref rid="B21-sensors-18-03299" ref-type="bibr">21</xref>], it was proven that the fusion of the prediction results (fc8) and the shallow layer of pool4 can effectively increase the prediction accuracy, as shown in <xref ref-type="fig" rid="sensors-18-03299-f004">Figure 4</xref>a. However, the fusion with other shallow layers brings no performance boost. This result indicated that the information from pool4 is crucial for the classification task, so that the fusion with this layer can make up the information loss caused by the downsampling operation. However, this strategy cannot properly address this problem, which resulted in low precision and blurred edges in the classification result [<xref rid="B26-sensors-18-03299" ref-type="bibr">26</xref>]. Based on this result, the skip architecture and the last pooling operation (pool5) were removed so as to avoid the information loss of the layer of pool4. (2) The original network was designed for the dataset of PASCAL VOC 2011 segmentation challenge [<xref rid="B28-sensors-18-03299" ref-type="bibr">28</xref>], which has 1000 different classes. However, our dataset only has three categories (rice, weeds, and others). According to the work of Stathakis et al. [<xref rid="B29-sensors-18-03299" ref-type="bibr">29</xref>], there should be a positive correlation between the number of output classes and the number of neurons in the intermediate fully connected layers. Based on this theory, the number of feature maps of intermediate layers (fc6 and fc7, which were transformed from the fully connected layers) was reduced. The number of feature maps of the intermediate layers (fc6 and fc7) was set to 2048 through several experiments and an evaluation on the validation set (refer to <xref ref-type="sec" rid="sec4-sensors-18-03299">Section 4</xref>). The network architecture of classical FCN-8s and the modified FCN-4s can be seen from <xref ref-type="fig" rid="sensors-18-03299-f004">Figure 4</xref>. </p><p>Besides the modified FCN-4s, the classical FCN-8s [<xref rid="B21-sensors-18-03299" ref-type="bibr">21</xref>] and Deeplab [<xref rid="B22-sensors-18-03299" ref-type="bibr">22</xref>] were also applied and evaluated as comparison. For the FCN-8s, an ImageNet pre-trained CNN [<xref rid="B27-sensors-18-03299" ref-type="bibr">27</xref>] was applied as a baseline architecture. The final classification layer was removed, and all of the fully connected layers were converted to convolutions. Skip architecture was built to improve the prediction precision. The lower layers (pool4 and pool5) were fused with the higher layer (fc8), as shown in <xref ref-type="fig" rid="sensors-18-03299-f004">Figure 4</xref>a. For the Deeplab approach, a 101-layer ResNet [<xref rid="B30-sensors-18-03299" ref-type="bibr">30</xref>] was adapted in fully convolutional forms, similar with the approach of FCN-8s. The weights pre-trained on ImageNet [<xref rid="B30-sensors-18-03299" ref-type="bibr">30</xref>] were transferred to our dataset using fine-tuning. Atrous convolution [<xref rid="B17-sensors-18-03299" ref-type="bibr">17</xref>] was applied to extend the field of view (FOW) of the convolutional filters, and the fully connected random filed (CRF) [<xref rid="B31-sensors-18-03299" ref-type="bibr">31</xref>] was used to further improve the prediction accuracy.</p><p>In this section, the accuracy was evaluated by the overall accuracy and mean intersection over union (mean IU) [<xref rid="B16-sensors-18-03299" ref-type="bibr">16</xref>], similar to <xref ref-type="sec" rid="sec3dot1-sensors-18-03299">Section 3.1</xref>. However, the time efficiency was also measured using the processing time for one single image, which is the normal way for the evaluation adopted by most semantic labeling approaches [<xref rid="B16-sensors-18-03299" ref-type="bibr">16</xref>,<xref rid="B17-sensors-18-03299" ref-type="bibr">17</xref>].</p></sec><sec id="sec3dot3-sensors-18-03299"><title>3.3. Prescription Map Generation</title><p>The prescription map can be generated from the weed cover map. According to the work of L&#x000f3;pez-Granados [<xref rid="B4-sensors-18-03299" ref-type="bibr">4</xref>], a chessboard segmentation process was applied to build a grid framework of the prescription map. The weed cover map was split into small grids, and the comparison between the weed coverage of each grid and a given threshold was conducted: if the weed coverage of the grid is larger than the threshold value, it will be marked as a treatment area, otherwise it will be marked as a non-treatment area. The grid size is adjustable according to the different spraying machines, and it was set to 0.5 &#x000d7; 0.5 m in this work, in accordance with the site-specific sprayer [<xref rid="B32-sensors-18-03299" ref-type="bibr">32</xref>].</p><p>For this section, the accuracy was calculated from two prescription maps (one generated from the weed cover map output by our algorithm, and the other from the GT label), which can be given by the following:<disp-formula id="FD1-sensors-18-03299"><label>(1)</label><mml:math id="mm1"><mml:mrow><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mi>accuracy</mml:mi><mml:mo>=</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>y</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:mtext>&#x000a0;</mml:mtext></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For each prescription map, its relevant herbicide saving was calculated. According to the work of de Castro [<xref rid="B8-sensors-18-03299" ref-type="bibr">8</xref>], herbicide saving is calculated in terms of the non-treatment area, which can be given by the following:<disp-formula id="FD2-sensors-18-03299"><label>(2)</label><mml:math id="mm2"><mml:mrow><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mi>herbicide</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mi>saving</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mtext>&#x000a0;</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mtext>&#x000a0;</mml:mtext></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="mm3"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm4"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent the proportion of the non-treatment and treatment areas.</p></sec></sec><sec id="sec4-sensors-18-03299"><title>4. Results and Discussions </title><p>In this section, the experiments on workflows, semantic labeling, and prescription map generation will be conducted. In the experiments on workflows and semantic labeling approaches, the dataset was divided into training, validation, and testing set. The three datasets were used for parameter updating, hyper parameter tuning, and performance evaluation, respectively. All of the experiments were conducted on a computer with an Intel i7-7700 CPU and a NVIDIA GTX 1080 Ti GPU. During the process of weed mapping, the mosaicking operation was carried out in the CPU, while the semantic labeling approaches were performed using the GPU.</p><sec id="sec4dot1-sensors-18-03299"><title>4.1. Workflow</title><p>In this section, two workflows (mosaicking-labeling and labeling-mosaicking) were applied in order to generate the weed cover map for the whole field. For the workflow of mosaicking-labeling, the dataset D02-1 (182 samples) was adopted as a training set. From dataset D10-1, 30% was randomly selected as validation set (54 samples), and the rest samples in the dataset D10-1 (128 samples) were used as the testing dataset. There were two reasons for this choice, namely: (1) the training set and testing set were chosen from different dates, which may evaluate the generalization capability of the algorithm, and (2) the validation set and testing set were selected from the same date, which may ensure that the two datasets belonged to the same distribution. For the workflow of labeling-mosaicking, the dataset D02-2 (648 samples) was used as training set, and 30% of dataset D10-2 (180 samples) was randomly selected as the validation set. However, we still used the testing set of the previous workflow (mosaicking-labeling) as the testing set of this workflow (labeling-mosaicking), since the classification on the ortho-mosaicked imagery is the ultimate objective of our algorithm.</p><p>In this section, the classical FCN-8s was used for the semantic labeling tasks. The quantitative results are listed in <xref rid="sensors-18-03299-t002" ref-type="table">Table 2</xref>. From <xref rid="sensors-18-03299-t002" ref-type="table">Table 2</xref>, it can be seen that both workflows obtained an approximate accuracy. However, because of the high overlapping in the collected imagery, directly processing on the collected imagery introduced too much redundant computation, which significantly lowered the inference speed. From this perspective, the workflow of mosaicking-labeling is the optimal solution, and will be considered as the default framework for the following experiments. </p></sec><sec id="sec4dot2-sensors-18-03299"><title>4.2. Semantic Labeling</title><p>In this section, the dataset (training, validation and testing set) was the same as the workflow of mosaicking-labeling (<xref ref-type="sec" rid="sec4dot1-sensors-18-03299">Section 4.1</xref>). FCN-8s, Deeplab, and our modified FCN-4s were applied for our dataset, respectively. The quantitative results and confusion matrix by different approaches are shown in <xref rid="sensors-18-03299-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-18-03299-t004" ref-type="table">Table 4</xref>. From <xref rid="sensors-18-03299-t003" ref-type="table">Table 3</xref>, it is obvious that Deeplab and FCN-4s outperformed FCN-8s in accuracy. From <xref rid="sensors-18-03299-t004" ref-type="table">Table 4</xref>, it can be seen that the weed recognition rate of Deeplab and FCN-4s is above 0.90, which is higher than that of FCN-8s. There were two reasons possible for this result, namely: (1) the Deeplab used CRF to refine the spatial details, which increased the prediction accuracy, and (2) the FCN-4s removed the last pooling layer, which reduced the information loss and obtained performance boost. </p><p>Although the Deeplab method achieved a satisfactory result for accuracy, the CRF introduced too much computation, which significantly slowed down the inference speed (<xref rid="sensors-18-03299-t003" ref-type="table">Table 3</xref>). Therefore, it can be concluded that the FCN-4s strikes the best tradeoff between accuracy and efficiency. From <xref rid="sensors-18-03299-t003" ref-type="table">Table 3</xref>, it can be found that the total time (including data collection and data processing) needed to generate the weed cover map for the entire field (50 &#x000d7; 60m) using FCN-4s is less than half an hour, demonstrating its rapid response capability on weed infestation monitoring.</p><p>The weed cover maps generated by the different approaches are shown in <xref ref-type="fig" rid="sensors-18-03299-f005">Figure 5</xref>. From <xref ref-type="fig" rid="sensors-18-03299-f005">Figure 5</xref>, it can be seen that (1) the weeds (in yellow dashed lines) were misclassified as others by FCN-8s, while they were properly recognized by Deeplab and FCN-4s; (2) the rice (in blue dashed lines) was misclassified as weeds by FCN-8s, while they were well classified by Deeplab and FCN-4s. From the qualitative results of <xref ref-type="fig" rid="sensors-18-03299-f005">Figure 5</xref>, it can be concluded that the FCN-4s obtained a satisfactory result with a simplified architecture in an end-to-end mode, which required no post-processing.</p></sec><sec id="sec4dot3-sensors-18-03299"><title>4.3. Prescription Map Generation</title><p>The prescription map can be generated from a weed cover map with a given weed threshold. According to the experimental results in <xref rid="sensors-18-03299-t003" ref-type="table">Table 3</xref>, the weed cover map obtained by FCN-4s was used to generate the prescription map. For a given weed threshold, the grid with a higher weed coverage will be marked as the treatment area. In this section, six thresholds (0.00&#x02013;0.25, with an interval of 0.05) were evaluated. The accuracy using different weed thresholds is shown in <xref ref-type="fig" rid="sensors-18-03299-f006">Figure 6</xref>. From <xref ref-type="fig" rid="sensors-18-03299-f006">Figure 6</xref>, it can be seen that, with increasing threshold values, the accuracy consistently increases. The reason for this result is that large weed patches were easier for the classifiers to detect, thus resulting in a higher accuracy. High accuracies (above 0.94 for all thresholds) were observed from <xref ref-type="fig" rid="sensors-18-03299-f006">Figure 6</xref>, demonstrating that our algorithm is qualified for treatment area prediction. The treatment area and herbicide saving with different weed thresholds were calculated and are shown in <xref rid="sensors-18-03299-t005" ref-type="table">Table 5</xref>. From <xref rid="sensors-18-03299-t005" ref-type="table">Table 5</xref>, it can be seen that, with increasing the weed thresholds, the treatment area consistently decreases. The relevant herbicide saving ranges from 58.3% to 70.8%, demonstrating great potential to reduce the use of herbicides in SSWM applications. From a practical perspective, a threshold of 0.0 would be recommended as the optimal weed threshold for SSWM applications. There are two reasons for this choice, namely: (1) the accuracy (above 0.94) of this threshold is qualified and the relevant herbicide saving (58.3%) is acceptable, and (2) this threshold would minimize the risk of missing weed infestation, which may cause weed-crop competition.</p><p>The prescription maps generated with different thresholds are illustrated in <xref ref-type="fig" rid="sensors-18-03299-f007">Figure 7</xref>. From <xref ref-type="fig" rid="sensors-18-03299-f007">Figure 7</xref>, it can be seen that the changes of the threshold value have little influence on the areas with a high weed coverage (in blue dashed lines), as the weed coverage of these areas is higher than all of the threshold values. However, for the areas with a lower weed coverage (in yellow dashed lines), the weed threshold can effectively adjust the treatment areas, as the areas with lower weed coverage than the threshold will be ignored. The prescription maps generated by our method (with all thresholds) generally correspond to that generated by the GT label, thanks to the high accuracy of the output weed cover map. From <xref ref-type="fig" rid="sensors-18-03299-f007">Figure 7</xref>, it can also be seen that an overestimation for treatment areas was observed in the results of all of the thresholds. However, from an agronomic perspective, it is acceptable, as it can reduce the risk allowing the weeds to go untreated [<xref rid="B33-sensors-18-03299" ref-type="bibr">33</xref>].</p></sec></sec><sec id="sec5-sensors-18-03299"><title>5. Conclusions</title><p>Prescription maps can provide decision making support for the spraying machine, which may effectively reduce the use of herbicide while enhancing the chemical effects. In this paper, the study on weed mapping and prescription map generation was conducted using UAV imagery. (1) The UAV imagery over a rice field were captured at a high spatial resolution, and pre-processing was performed so as to generate our dataset. (2) Two different workflows (mosaicking-labeling and labeling-mosaicking) were applied in order to generate the weed cover maps. These workflows were evaluated and compared. The experimental results showed that the workflow of mosaicking-labeling outperformed the others in terms of efficiency with an approximate accuracy. (3) A modified FCN-4s introduced pixel-to-pixel translation from UAV imagery to weed cover maps. Theoretic analysis was conducted to seek for architecture improvement. The improved architecture was evaluated and compared with the classical FCN-8s and Deeplab. The experimental results showed that the modified FCN-4s outperformed others in both accuracy and efficiency. (4) A chessboard segmentation method was used to build the grid framework of the prescription map. Different weed thresholds were applied and evaluated. High accuracies (above 0.94) were observed for all of the thresholds, and the relevant herbicide savings ranged from 58.3% to 70.8%. The method applied in this paper was superior in efficiency, which may produce a prescription map for a rice field (50 &#x000d7; 60 m) within half an hour, demonstrating its rapid response capability to the emergency of weed infestation. </p><p>However, for the study of weed mapping and prescription map generation, more data is needed to extend and evaluate the generalization capability of the algorithm. Besides rough weed recognition, classification for specific weed species is also important for the SSWM applications, which can be extended based on our current work. All of these issues will be left as our future work.</p></sec></body><back><notes><title>Author Contributions</title><p>Conceptualization, J.D. and Y.L.; funding acquisition, Y.L.; methodology, S.W.; project administration, J.D.; software, H.H. and A.Y.; writing (original draft), H.H.; writing (review and editing), X.D., H.Z. and Y.Z.</p></notes><notes><title>Funding</title><p>This research was funded by the Educational Commission of Guangdong Province of China for Platform Construction: International Cooperation on R&#x00026;D of Key Technology of Precision Agricultural Aviation (grant no. 2015KGJHZ007), the Science and Technology Planning Project of Guangdong Province, China (grant no. 2017A020208046), the National Key Research and Development Plan, China (grant no. 2016YFD0200700), the National Natural Science Fund, China (grant no. 61675003), the Science and Technology Planning Project of Guangdong Province, China (grant no. 2016A020210100), the Science and Technology Planning Project of Guangdong Province, China (grant no. 2017B010117010), and the Science and Technology Planning Project of Guangzhou city, China (grant no. 201707010047).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-18-03299"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dass</surname><given-names>A.</given-names></name><name><surname>Shekhawat</surname><given-names>K.</given-names></name><name><surname>Choudhary</surname><given-names>A.K.</given-names></name><name><surname>Sepat</surname><given-names>S.</given-names></name><name><surname>Rathore</surname><given-names>S.S.</given-names></name><name><surname>Mahajan</surname><given-names>G.</given-names></name><name><surname>Chauhan</surname><given-names>B.S.</given-names></name></person-group><article-title>Weed management in rice using crop competition-a review</article-title><source>Crop Prot.</source><year>2017</year><volume>95</volume><fpage>45</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1016/j.cropro.2016.08.005</pub-id></element-citation></ref><ref id="B2-sensors-18-03299"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiu</surname><given-names>G.</given-names></name><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Shen</surname><given-names>S.</given-names></name><name><surname>Ming</surname><given-names>L.</given-names></name><name><surname>Lu</surname><given-names>Y.</given-names></name></person-group><article-title>Aplication Technology of Two Times of Closed Weed Control in Mechanical Transplanted Rice Field</article-title><source>J. Weed Sci.</source><year>2016</year><volume>4</volume><fpage>33</fpage><lpage>38</lpage></element-citation></ref><ref id="B3-sensors-18-03299"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>B.</given-names></name><name><surname>Liu</surname><given-names>D.</given-names></name><name><surname>Hui</surname><given-names>K.</given-names></name><name><surname>Wu</surname><given-names>X.</given-names></name></person-group><article-title>Weed control techniques in direct seeding rice field</article-title><source>Mod. Agric. Sci. Technol.</source><year>2007</year><volume>17</volume><fpage>114</fpage><lpage>117</lpage></element-citation></ref><ref id="B4-sensors-18-03299"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>L&#x000f3;pez-Granados</surname><given-names>F.</given-names></name><name><surname>Torres-S&#x000e1;nchez</surname><given-names>J.</given-names></name><name><surname>Serrano-P&#x000e9;rez</surname><given-names>A.</given-names></name><name><surname>de Castro</surname><given-names>A.I.</given-names></name><name><surname>Mesas-Carrascosa</surname><given-names>F.J.</given-names></name><name><surname>Pe&#x000f1;a</surname><given-names>J.</given-names></name></person-group><article-title>Early season weed mapping in sunflower using UAV technology: Variability of herbicide treatment maps against weed thresholds</article-title><source>Precis. Agric.</source><year>2016</year><volume>17</volume><fpage>183</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1007/s11119-015-9415-8</pub-id></element-citation></ref><ref id="B5-sensors-18-03299"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nordmeyer</surname><given-names>H.</given-names></name></person-group><article-title>Spatial and temporal dynamics of Apera spica-venti seedling populations</article-title><source>Crop Prot.</source><year>2009</year><volume>28</volume><fpage>831</fpage><lpage>837</lpage><pub-id pub-id-type="doi">10.1016/j.cropro.2009.06.006</pub-id></element-citation></ref><ref id="B6-sensors-18-03299"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pe&#x000f1;a</surname><given-names>J.</given-names></name><name><surname>Torress&#x000e1;nchez</surname><given-names>J.</given-names></name><name><surname>Serranop&#x000e9;rez</surname><given-names>A.</given-names></name><name><surname>L&#x000f3;pezgranados</surname><given-names>F.</given-names></name></person-group><article-title>Weed mapping in early-season maize fields using object-based analysis of unmanned aerial vehicle (UAV) images</article-title><source>PLoS One</source><year>2013</year><volume>8</volume><elocation-id>e77151</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0077151</pub-id><?supplied-pmid 24146963?><pub-id pub-id-type="pmid">24146963</pub-id></element-citation></ref><ref id="B7-sensors-18-03299"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lan</surname><given-names>Y.</given-names></name><name><surname>Thomson</surname><given-names>S.J.</given-names></name><name><surname>Huang</surname><given-names>Y.</given-names></name><name><surname>Hoffmann</surname><given-names>W.C.</given-names></name><name><surname>Zhang</surname><given-names>H.</given-names></name></person-group><article-title>Current status and future directions of precision aerial application for site-specific crop management in the USA</article-title><source>Comput. Electron. Agric.</source><year>2010</year><volume>74</volume><fpage>34</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2010.07.001</pub-id></element-citation></ref><ref id="B8-sensors-18-03299"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Castro</surname><given-names>A.</given-names></name><name><surname>Torres-S&#x000e1;nchez</surname><given-names>J.</given-names></name><name><surname>Pe&#x000f1;a</surname><given-names>J.</given-names></name><name><surname>Jim&#x000e9;nez-Brenes</surname><given-names>F.</given-names></name><name><surname>Csillik</surname><given-names>O.</given-names></name><name><surname>L&#x000f3;pez-Granados</surname><given-names>F.</given-names></name></person-group><article-title>An Automatic Random Forest-OBIA Algorithm for Early Weed Mapping between and within Crop Rows Using UAV Imagery</article-title><source>Remote Sens.</source><year>2018</year><volume>10</volume><elocation-id>285</elocation-id><pub-id pub-id-type="doi">10.3390/rs10020285</pub-id></element-citation></ref><ref id="B9-sensors-18-03299"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lan</surname><given-names>Y.</given-names></name><name><surname>Shengde</surname><given-names>C.</given-names></name><name><surname>Fritz</surname><given-names>B.K.</given-names></name></person-group><article-title>Current status and future trends of precision agricultural aviation technologies</article-title><source>Int. J. Agric. Biol. Eng.</source><year>2017</year><volume>10</volume><fpage>1</fpage><lpage>17</lpage></element-citation></ref><ref id="B10-sensors-18-03299"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castaldi</surname><given-names>F.</given-names></name><name><surname>Pelosi</surname><given-names>F.</given-names></name><name><surname>Pascucci</surname><given-names>S.</given-names></name><name><surname>Casa</surname><given-names>R.</given-names></name></person-group><article-title>Assessing the potential of images from unmanned aerial vehicles (UAV) to support herbicide patch spraying in maize</article-title><source>Precis. Agric.</source><year>2017</year><volume>18</volume><fpage>76</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1007/s11119-016-9468-3</pub-id></element-citation></ref><ref id="B11-sensors-18-03299"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ahonen</surname><given-names>T.</given-names></name><name><surname>Hadid</surname><given-names>A.</given-names></name><name><surname>Pietik&#x000e4;inen</surname><given-names>M.</given-names></name></person-group><article-title>Face Recognition with Local Binary Patterns</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Prague, The Czech Republic</conf-loc><conf-date>11&#x02013;14 May 2004</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2004</year><fpage>469</fpage><lpage>481</lpage></element-citation></ref><ref id="B12-sensors-18-03299"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DEFRIES</surname><given-names>R.S.</given-names></name><name><surname>TOWNSHEND</surname><given-names>J.R.G.</given-names></name></person-group><article-title>NDVI-derived land cover classifications at a global scale</article-title><source>Int. J. Remote Sens.</source><year>1994</year><volume>15</volume><fpage>3567</fpage><lpage>3586</lpage><pub-id pub-id-type="doi">10.1080/01431169408954345</pub-id></element-citation></ref><ref id="B13-sensors-18-03299"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gitelson</surname><given-names>A.A.</given-names></name><name><surname>Kaufman</surname><given-names>Y.J.</given-names></name><name><surname>Stark</surname><given-names>R.</given-names></name><name><surname>Rundquist</surname><given-names>D.</given-names></name></person-group><article-title>Novel algorithms for remote estimation of vegetation fraction</article-title><source>Remote Sens. Environ.</source><year>2002</year><volume>80</volume><fpage>76</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1016/S0034-4257(01)00289-9</pub-id></element-citation></ref><ref id="B14-sensors-18-03299"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hung</surname><given-names>C.</given-names></name><name><surname>Xu</surname><given-names>Z.</given-names></name><name><surname>Sukkarieh</surname><given-names>S.</given-names></name></person-group><article-title>Feature Learning Based Approach for Weed Classification Using High Resolution Aerial Images from a Digital Camera Mounted on a UAV</article-title><source>Remote Sens.</source><year>2014</year><volume>6</volume><fpage>12037</fpage><lpage>12054</lpage><pub-id pub-id-type="doi">10.3390/rs61212037</pub-id></element-citation></ref><ref id="B15-sensors-18-03299"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><?supplied-pmid 26017442?><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="B16-sensors-18-03299"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shelhamer</surname><given-names>E.</given-names></name><name><surname>Long</surname><given-names>J.</given-names></name><name><surname>Darrell</surname><given-names>T.</given-names></name></person-group><article-title>Fully Convolutional Networks for Semantic Segmentation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2014</year><volume>4</volume><fpage>640</fpage><lpage>651</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2572683</pub-id><?supplied-pmid 27244717?><pub-id pub-id-type="pmid">27244717</pub-id></element-citation></ref><ref id="B17-sensors-18-03299"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L.C.</given-names></name><name><surname>Papandreou</surname><given-names>G.</given-names></name><name><surname>Kokkinos</surname><given-names>I.</given-names></name><name><surname>Murphy</surname><given-names>K.</given-names></name><name><surname>Yuille</surname><given-names>A.L.</given-names></name></person-group><article-title>DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2018</year><volume>40</volume><fpage>834</fpage><lpage>848</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id><?supplied-pmid 28463186?><pub-id pub-id-type="pmid">28463186</pub-id></element-citation></ref><ref id="B18-sensors-18-03299"><label>18.</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Sherrah</surname><given-names>J.</given-names></name></person-group><article-title>Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution Aerial Imagery</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="arxiv">1606.02585</pub-id></element-citation></ref><ref id="B19-sensors-18-03299"><label>19.</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Maggiori</surname><given-names>E.</given-names></name><name><surname>Tarabalka</surname><given-names>Y.</given-names></name><name><surname>Charpiat</surname><given-names>G.</given-names></name><name><surname>Alliez</surname><given-names>P.</given-names></name></person-group><article-title>High-Resolution Semantic Labeling with Convolutional Neural Networks</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="arxiv">1611.01962</pub-id></element-citation></ref><ref id="B20-sensors-18-03299"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Maggiori</surname><given-names>E.</given-names></name><name><surname>Tarabalka</surname><given-names>Y.</given-names></name><name><surname>Charpiat</surname><given-names>G.</given-names></name><name><surname>Alliez</surname><given-names>P.</given-names></name></person-group><article-title>Fully Convolutional Neural Networks For Remote Sensing Image Classification</article-title><source>Proceedings of the 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)</source><conf-loc>Beijing, China</conf-loc><conf-date>10&#x02013;15 July 2016</conf-date><fpage>5071</fpage><lpage>5074</lpage></element-citation></ref><ref id="B21-sensors-18-03299"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>H.</given-names></name><name><surname>Deng</surname><given-names>J.</given-names></name><name><surname>Lan</surname><given-names>Y.</given-names></name><name><surname>Yang</surname><given-names>A.</given-names></name><name><surname>Deng</surname><given-names>X.</given-names></name><name><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>A fully convolutional network for weed mapping of unmanned aerial vehicle (UAV) imagery</article-title><source>PLoS One</source><year>2018</year><volume>13</volume><elocation-id>e196302</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0196302</pub-id><?supplied-pmid 29698500?><pub-id pub-id-type="pmid">29698500</pub-id></element-citation></ref><ref id="B22-sensors-18-03299"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>H.</given-names></name><name><surname>Lan</surname><given-names>Y.</given-names></name><name><surname>Deng</surname><given-names>J.</given-names></name><name><surname>Yang</surname><given-names>A.</given-names></name><name><surname>Deng</surname><given-names>X.</given-names></name><name><surname>Zhang</surname><given-names>L.</given-names></name><name><surname>Wen</surname><given-names>S.</given-names></name></person-group><article-title>A Semantic Labeling Approach for Accurate Weed Mapping of High Resolution UAV Imagery</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>2113</elocation-id><pub-id pub-id-type="doi">10.3390/s18072113</pub-id><?supplied-pmid 29966392?><pub-id pub-id-type="pmid">29966392</pub-id></element-citation></ref><ref id="B23-sensors-18-03299"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>A.M.</given-names></name><name><surname>Paskewitz</surname><given-names>S.M.</given-names></name><name><surname>Orth</surname><given-names>A.P.</given-names></name><name><surname>Tesch</surname><given-names>M.J.</given-names></name><name><surname>Toong</surname><given-names>Y.C.</given-names></name><name><surname>Goodman</surname><given-names>W.G.</given-names></name></person-group><article-title>The lethal effects of Cyperus iria on Aedes aegypti</article-title><source>J. Am. Mosq. Control Assoc.</source><year>1998</year><volume>14</volume><fpage>78</fpage><lpage>82</lpage><?supplied-pmid 9599328?><pub-id pub-id-type="pmid">9599328</pub-id></element-citation></ref><ref id="B24-sensors-18-03299"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>J.</given-names></name><name><surname>Gao</surname><given-names>H.</given-names></name><name><surname>Pan</surname><given-names>L.</given-names></name><name><surname>Yao</surname><given-names>Z.</given-names></name><name><surname>Dong</surname><given-names>L.</given-names></name></person-group><article-title>Mechanism of resistance to cyhalofop-butyl in Chinese sprangletop ( Leptochloa chinensis (L.) Nees)</article-title><source>Pestic. Biochem. Physiol.</source><year>2017</year><volume>143</volume><fpage>306</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1016/j.pestbp.2016.11.001</pub-id><?supplied-pmid 29183606?><pub-id pub-id-type="pmid">29183606</pub-id></element-citation></ref><ref id="B25-sensors-18-03299"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dandois</surname><given-names>J.P.</given-names></name><name><surname>Ellis</surname><given-names>E.C.</given-names></name></person-group><article-title>High spatial resolution three-dimensional mapping of vegetation spectral dynamics using computer vision</article-title><source>Remote Sens. Environ.</source><year>2013</year><volume>136</volume><fpage>259</fpage><lpage>276</lpage><pub-id pub-id-type="doi">10.1016/j.rse.2013.04.005</pub-id></element-citation></ref><ref id="B26-sensors-18-03299"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>W.</given-names></name><name><surname>Huang</surname><given-names>H.</given-names></name><name><surname>Schmitz</surname><given-names>M.</given-names></name><name><surname>Sun</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>H.</given-names></name><name><surname>Mayer</surname><given-names>H.</given-names></name></person-group><article-title>Effective Fusion of Multi-Modal Remote Sensing Data in a Fully Convolutional Network for Semantic Labeling</article-title><source>Remote Sens.</source><year>2018</year><volume>10</volume><elocation-id>52</elocation-id><pub-id pub-id-type="doi">10.3390/rs10010052</pub-id></element-citation></ref><ref id="B27-sensors-18-03299"><label>27.</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K.</given-names></name><name><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>Very Deep Convolutional Networks for Large-Scale Image Recognition</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="arxiv">1409.1556</pub-id></element-citation></ref><ref id="B28-sensors-18-03299"><label>28.</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Everingham</surname><given-names>M.</given-names></name><name><surname>Van Gool</surname><given-names>L.</given-names></name><name><surname>Williams</surname><given-names>C.</given-names></name><name><surname>Winn</surname><given-names>J.</given-names></name><name><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>PASCAL Visual Object Classes Recognition Challenge 2011 (VOC2011)&#x02013;Training &#x00026; Test Data</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.pascal-network.org/?q=node/598">http://www.pascal-network.org/?q=node/598</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2017-11-05">(accessed on 5 November 2017)</date-in-citation></element-citation></ref><ref id="B29-sensors-18-03299"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stathakis</surname><given-names>D.</given-names></name></person-group><article-title>How many hidden layers and nodes?</article-title><source>Int. J. Remote Sens.</source><year>2009</year><volume>30</volume><fpage>2133</fpage><lpage>2147</lpage><pub-id pub-id-type="doi">10.1080/01431160802549278</pub-id></element-citation></ref><ref id="B30-sensors-18-03299"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Deep Residual Learning for Image Recognition</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#x02013;30 June 2016</conf-date><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="B31-sensors-18-03299"><label>31.</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Kr Henb&#x000fc;hl</surname><given-names>P.</given-names></name><name><surname>Koltun</surname><given-names>V.</given-names></name></person-group><article-title>Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials</article-title><source>arXiv</source><year>2012</year><pub-id pub-id-type="arxiv">1210.5644</pub-id></element-citation></ref><ref id="B32-sensors-18-03299"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gonzalez-de-Santos</surname><given-names>P.</given-names></name><name><surname>Ribeiro</surname><given-names>A.</given-names></name><name><surname>Fernandez-Quintanilla</surname><given-names>C.</given-names></name><name><surname>Lopez-Granados</surname><given-names>F.</given-names></name><name><surname>Brandstoetter</surname><given-names>M.</given-names></name><name><surname>Tomic</surname><given-names>S.</given-names></name><name><surname>Pedrazzi</surname><given-names>S.</given-names></name><name><surname>Peruzzi</surname><given-names>A.</given-names></name><name><surname>Pajares</surname><given-names>G.</given-names></name><name><surname>Kaplanis</surname><given-names>G.</given-names></name><etal/></person-group><article-title>Fleets of robots for environmentally-safe pest control in agriculture</article-title><source>Precis. Agric.</source><year>2017</year><volume>18</volume><fpage>574</fpage><lpage>614</lpage><pub-id pub-id-type="doi">10.1007/s11119-016-9476-3</pub-id></element-citation></ref><ref id="B33-sensors-18-03299"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibson</surname><given-names>K.</given-names></name></person-group><article-title>Detection of Weed Species in Soybean Using Multispectral Digital Images</article-title><source>Weed Technol.</source><year>2004</year><volume>18</volume><fpage>742</fpage><lpage>749</lpage><pub-id pub-id-type="doi">10.1614/WT-03-170R1</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="sensors-18-03299-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>The photograph of the studied rice field.</p></caption><graphic xlink:href="sensors-18-03299-g001"/></fig><fig id="sensors-18-03299-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>Three image-ground truth (GT) label pairs in the dataset: (<bold>a</bold>) images in the dataset; (<bold>b</bold>) corresponding GT labels.</p></caption><graphic xlink:href="sensors-18-03299-g002"/></fig><fig id="sensors-18-03299-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>Two workflows to produce the weed cover map for the whole field. (<bold>a</bold>) The workflow of mosaicking-labeling; (<bold>b</bold>) the workflow of labeling-mosaicking.</p></caption><graphic xlink:href="sensors-18-03299-g003"/></fig><fig id="sensors-18-03299-f004" orientation="portrait" position="float"><label>Figure 4</label><caption><p>The illustration of the architecture of fully convolutional networks (FCN): (<bold>a</bold>) architecture of classical FCN-8s; (<bold>b</bold>) architecture of the modified FCN-4s.</p></caption><graphic xlink:href="sensors-18-03299-g004"/></fig><fig id="sensors-18-03299-f005" orientation="portrait" position="float"><label>Figure 5</label><caption><p>Weed cover maps output by different approaches. (<bold>a</bold>) Ortho-mosaicked imagery. (<bold>b</bold>) Corresponding GT-labels. The areas outside the studied plot were masked out (in black) and ignored in the training and evaluation. (<bold>c</bold>) Output by FCN-8s. (<bold>d</bold>) Output by Deeplab. (<bold>e</bold>) Output by FCN-4s.</p></caption><graphic xlink:href="sensors-18-03299-g005"/></fig><fig id="sensors-18-03299-f006" orientation="portrait" position="float"><label>Figure 6</label><caption><p>The accuracy curve with different weed thresholds.</p></caption><graphic xlink:href="sensors-18-03299-g006"/></fig><fig id="sensors-18-03299-f007" orientation="portrait" position="float"><label>Figure 7</label><caption><p>Prescription map generated with different weed thresholds. (<bold>a</bold>&#x02013;<bold>c</bold>) Prescription map generated from the GT label using the weed thresholds of 0.0, 0.1 and 0.2. (<bold>d</bold>&#x02013;<bold>f</bold>) Prescription map generated from the output weed cover map using the thresholds of 0.0, 0.1 and 0.2. From reference system datum WGS84.</p></caption><graphic xlink:href="sensors-18-03299-g007"/></fig><table-wrap id="sensors-18-03299-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03299-t001_Table 1</object-id><label>Table 1</label><caption><p>Specification for the dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Name</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Flight Date</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Number of Patches</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">D02-1 </td><td align="center" valign="middle" rowspan="1" colspan="1">2nd October 2017</td><td align="center" valign="middle" rowspan="1" colspan="1">182</td><td align="center" valign="middle" rowspan="1" colspan="1">Divided from the ortho-mosaic imagery</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">D10-1</td><td align="center" valign="middle" rowspan="1" colspan="1">10th October 2017</td><td align="center" valign="middle" rowspan="1" colspan="1">182</td><td align="center" valign="middle" rowspan="1" colspan="1">Divided from the ortho-mosaic imagery</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">D02-2</td><td align="center" valign="middle" rowspan="1" colspan="1">2nd October 2017</td><td align="center" valign="middle" rowspan="1" colspan="1">648</td><td align="center" valign="middle" rowspan="1" colspan="1">Divided from the collected imagery</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">D10-2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10th October 2017</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">600</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Divided from the collected imagery</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-03299-t002" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03299-t002_Table 2</object-id><label>Table 2</label><caption><p>Experimental results of different workflows. The speed was measured using the total time required to generate the weed cover map for the whole field, including data collection and data processing. Mean IU-mean intersection over union.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Workflow</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Overall Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mean IU</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Speed</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mosaicking&#x02013;labeling</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9096</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8303</td><td align="center" valign="middle" rowspan="1" colspan="1">24.8 min</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Labeling&#x02013;mosaicking</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9074</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8264</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32.5 min</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-03299-t003" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03299-t003_Table 3</object-id><label>Table 3</label><caption><p>Experimental results on different semantic labeling approaches. Speed-1 was measured using the inference time for a single imagery (1000 &#x000d7; 1000 pixels), and speed-2 was measured using the total time required to generate the weed cover map for the whole field, including data collection and data processing. FCN&#x02014;fully convolutional networks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Overall Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mean IU</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Speed-1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Speed-2</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">FCN-8s</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9096</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8303</td><td align="center" valign="middle" rowspan="1" colspan="1">0.413 s</td><td align="center" valign="middle" rowspan="1" colspan="1">24.8 min</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Deeplab</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9191</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8460</td><td align="center" valign="middle" rowspan="1" colspan="1">5.279 s</td><td align="center" valign="middle" rowspan="1" colspan="1">39.6 min</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FCN-4s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9196</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8473</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.356 s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.7 min</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-03299-t004" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03299-t004_Table 4</object-id><label>Table 4</label><caption><p>Confusion matrix by different semantic labeling approaches. GT&#x02014;ground truth.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GT/Predicted Category</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Others</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Rice</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Weeds</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">FCN-8s</td><td align="center" valign="middle" rowspan="1" colspan="1">others</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.939</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.042</td><td align="center" valign="middle" rowspan="1" colspan="1">0.018</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">rice</td><td align="center" valign="middle" rowspan="1" colspan="1">0.037</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.894</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.069</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">weeds</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.078</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.027</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.895</bold>
</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Deeplab</td><td align="center" valign="middle" rowspan="1" colspan="1">others</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.922</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.044</td><td align="center" valign="middle" rowspan="1" colspan="1">0.034</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">rice</td><td align="center" valign="middle" rowspan="1" colspan="1">0.023</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.924</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.052</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">weeds</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.056</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.036</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.907</bold>
</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">FCN-4s</td><td align="center" valign="middle" rowspan="1" colspan="1">others</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.938</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.030</td><td align="center" valign="middle" rowspan="1" colspan="1">0.031</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">rice</td><td align="center" valign="middle" rowspan="1" colspan="1">0.037</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.913</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.049</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">weeds</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.055</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.039</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.905</bold>
</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-03299-t005" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-03299-t005_Table 5</object-id><label>Table 5</label><caption><p>Herbicide saving with different weed thresholds.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Threshold</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Treatment Area</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Herbicide Saving</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.00</td><td align="center" valign="middle" rowspan="1" colspan="1">41.7%</td><td align="center" valign="middle" rowspan="1" colspan="1">58.3%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" rowspan="1" colspan="1">35.9%</td><td align="center" valign="middle" rowspan="1" colspan="1">64.1%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.10</td><td align="center" valign="middle" rowspan="1" colspan="1">33.6%</td><td align="center" valign="middle" rowspan="1" colspan="1">66.4%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.15</td><td align="center" valign="middle" rowspan="1" colspan="1">31.9%</td><td align="center" valign="middle" rowspan="1" colspan="1">68.1%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0.20</td><td align="center" valign="middle" rowspan="1" colspan="1">30.4%</td><td align="center" valign="middle" rowspan="1" colspan="1">69.6%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.2%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.8%</td></tr></tbody></table></table-wrap></floats-group></article>