<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6069478</article-id><article-id pub-id-type="doi">10.3390/s18072113</article-id><article-id pub-id-type="publisher-id">sensors-18-02113</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Semantic Labeling Approach for Accurate Weed Mapping of High Resolution UAV Imagery</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Huang</surname><given-names>Huasheng</given-names></name><xref ref-type="aff" rid="af1-sensors-18-02113">1</xref><xref ref-type="aff" rid="af2-sensors-18-02113">2</xref><xref ref-type="author-notes" rid="fn1-sensors-18-02113">&#x02020;</xref></contrib><contrib contrib-type="author"><name><surname>Lan</surname><given-names>Yubin</given-names></name><xref ref-type="aff" rid="af1-sensors-18-02113">1</xref><xref ref-type="aff" rid="af2-sensors-18-02113">2</xref><xref ref-type="author-notes" rid="fn1-sensors-18-02113">&#x02020;</xref></contrib><contrib contrib-type="author"><name><surname>Deng</surname><given-names>Jizhong</given-names></name><xref ref-type="aff" rid="af1-sensors-18-02113">1</xref><xref ref-type="aff" rid="af2-sensors-18-02113">2</xref><xref rid="c1-sensors-18-02113" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Aqing</given-names></name><xref ref-type="aff" rid="af3-sensors-18-02113">3</xref></contrib><contrib contrib-type="author"><name><surname>Deng</surname><given-names>Xiaoling</given-names></name><xref ref-type="aff" rid="af2-sensors-18-02113">2</xref><xref ref-type="aff" rid="af3-sensors-18-02113">3</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Lei</given-names></name><xref ref-type="aff" rid="af2-sensors-18-02113">2</xref><xref ref-type="aff" rid="af4-sensors-18-02113">4</xref></contrib><contrib contrib-type="author"><name><surname>Wen</surname><given-names>Sheng</given-names></name><xref ref-type="aff" rid="af2-sensors-18-02113">2</xref><xref ref-type="aff" rid="af5-sensors-18-02113">5</xref></contrib></contrib-group><aff id="af1-sensors-18-02113"><label>1</label>College of Engineering, South China Agricultural University, Wushan Road, Guangzhou 510642, China; <email>huanghsheng@stu.scau.edu.cn</email> (H.H.); <email>ylan@scau.edu.cn</email> (Y.L.)</aff><aff id="af2-sensors-18-02113"><label>2</label>National Center for International Collaboration Research on Precision Agricultural Aviation Pesticide Spraying Technology, Wushan Road, Guangzhou 510642, China; <email>dengxl@scau.edu.cn</email> (X.D.); <email>zhanglei@scau.edu.cn</email> (L.Z.); <email>vincen@scau.edu.cn</email> (S.W.)</aff><aff id="af3-sensors-18-02113"><label>3</label>College of Electronic Engineering, South China Agricultural University, Wushan Road, Guangzhou 516042, China; <email>yangaqing@stu.scau.edu.cn</email></aff><aff id="af4-sensors-18-02113"><label>4</label>College of Agriculture, South China Agricultural University, Wushan Road, Guangzhou 516042, China</aff><aff id="af5-sensors-18-02113"><label>5</label>Engineering Fundamental Teaching and Training Center, South China Agricultural University, Wushan Road, Guangzhou 510642, China</aff><author-notes><corresp id="c1-sensors-18-02113"><label>*</label>Correspondence: <email>jz-deng@scau.edu.cn</email>; Tel.: +86-20-8528-8201</corresp><fn id="fn1-sensors-18-02113"><label>&#x02020;</label><p>These authors contributed equally to this work and should be considered as co-first authors.</p></fn></author-notes><pub-date pub-type="epub"><day>01</day><month>7</month><year>2018</year></pub-date><pub-date pub-type="collection"><month>7</month><year>2018</year></pub-date><volume>18</volume><issue>7</issue><elocation-id>2113</elocation-id><history><date date-type="received"><day>13</day><month>5</month><year>2018</year></date><date date-type="accepted"><day>27</day><month>6</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; 2018 by the authors.</copyright-statement><copyright-year>2018</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Weed control is necessary in rice cultivation, but the excessive use of herbicide treatments has led to serious agronomic and environmental problems. Suitable site-specific weed management (SSWM) is a solution to address this problem while maintaining the rice production quality and quantity. In the context of SSWM, an accurate weed distribution map is needed to provide decision support information for herbicide treatment. UAV remote sensing offers an efficient and effective platform to monitor weeds thanks to its high spatial resolution. In this work, UAV imagery was captured in a rice field located in South China. A semantic labeling approach was adopted to generate the weed distribution maps of the UAV imagery. An ImageNet pre-trained CNN with residual framework was adapted in a fully convolutional form, and transferred to our dataset by fine-tuning. Atrous convolution was applied to extend the field of view of convolutional filters; the performance of multi-scale processing was evaluated; and a fully connected conditional random field (CRF) was applied after the CNN to further refine the spatial details. Finally, our approach was compared with the pixel-based-SVM and the classical FCN-8s. Experimental results demonstrated that our approach achieved the best performance in terms of accuracy. Especially for the detection of small weed patches in the imagery, our approach significantly outperformed other methods. The mean intersection over union (mean IU), overall accuracy, and Kappa coefficient of our method were 0.7751, 0.9445, and 0.9128, respectively. The experiments showed that our approach has high potential in accurate weed mapping of UAV imagery.</p></abstract><kwd-group><kwd>UAV</kwd><kwd>remote sensing</kwd><kwd>weed mapping</kwd><kwd>Deep Fully Convolutional Network</kwd><kwd>semantic labeling</kwd></kwd-group></article-meta></front><body><sec id="sec1-sensors-18-02113"><title>1. Introduction</title><p>Rice is the world&#x02019;s most important crop. Currently, more than one third of the world&#x02019;s population relies on rice as their principal food [<xref rid="B1-sensors-18-02113" ref-type="bibr">1</xref>]. However, weed infestations present great challenges for rice cultivation. Weedy rice populations have been reported in many rice growing areas in the world, from rice transplanting to direct seeding [<xref rid="B2-sensors-18-02113" ref-type="bibr">2</xref>]. The weeds compete with rice for light, water, and nutrients, which may cause serious yield losses [<xref rid="B3-sensors-18-02113" ref-type="bibr">3</xref>]. Weed control in rice fields is necessary, but inappropriate herbicide treatment has led to agronomic and environmental problems [<xref rid="B4-sensors-18-02113" ref-type="bibr">4</xref>]. Usually, herbicide treatments are applied at a constant dose by specific machinery (i.e., UAVs, tractors), ignoring the spatial distribution of weeds. Such operations may lead to excessive use of herbicide since somewhere it is needed less or it is not needed at all, increasing the risk for environmental pollution. To appropriately address this problem, it is necessary to integrate the weed control with Site-Specific Weed Management (SSWM). In the context of SSWM, the herbicide is only applied with the presence of weed infestation, and the dose should be adjusted according to weed densities.</p><p>However, before carrying out a SSWM task, an accurate weed distribution map is needed, which may provide decision support information for the spraying machinery [<xref rid="B5-sensors-18-02113" ref-type="bibr">5</xref>]. Usually, the optimal time for most herbicide treatment is at the early growth stages (i.e., seedling or tillering stages) of weeds and rice. Thus, weed mapping in this period is significant in real applications. UAVs are able to fly at a low altitude [<xref rid="B6-sensors-18-02113" ref-type="bibr">6</xref>], capturing imagery at a very high resolution [<xref rid="B7-sensors-18-02113" ref-type="bibr">7</xref>], which is suitable for mapping weeds during their early growth stages. Though UAV remote sensing was proven to be effective in weed mapping tasks [<xref rid="B8-sensors-18-02113" ref-type="bibr">8</xref>,<xref rid="B9-sensors-18-02113" ref-type="bibr">9</xref>], the conversion of UAV imagery into accurate weed distribution maps is still the main bottleneck in SSWM applications.</p><p>Several studies have employed machine learning methods [<xref rid="B8-sensors-18-02113" ref-type="bibr">8</xref>,<xref rid="B9-sensors-18-02113" ref-type="bibr">9</xref>] for UAV imagery weed mapping tasks. Alexandridis et al. [<xref rid="B8-sensors-18-02113" ref-type="bibr">8</xref>] applied four machine learning approaches to map the distribution of <italic>S. marianum</italic> in a field. The adopted architecture were One Class Support Vector Machine (OC-SVM), One Class Self-Organizing Maps (OC-SOM), Autoencoders and One Class Principal Component Analysis (OC-PCA). Experimental results demonstrated that the OC-SVM obtained best performance in <italic>S. marianum</italic> identification, and the overall accuracy was up to 96%. However, traditional machine learning approaches only involve low-level hand-engineered features (i.e., color or texture features) [<xref rid="B10-sensors-18-02113" ref-type="bibr">10</xref>] for classification, which tend to be less precise [<xref rid="B11-sensors-18-02113" ref-type="bibr">11</xref>] and hard to generalize.</p><p>Deep learning methods are automatic feature learning approaches which transform input data into representations at a higher and more abstract level [<xref rid="B12-sensors-18-02113" ref-type="bibr">12</xref>]. As one of the classical architectures of deep learning, fully convolutional network (FCN) has achieved state-of-art performance in semantic labeling tasks in computer vision [<xref rid="B13-sensors-18-02113" ref-type="bibr">13</xref>], which also shows great potential in remote sensing applications. Zhang et al. [<xref rid="B14-sensors-18-02113" ref-type="bibr">14</xref>] presented a detailed investigation on the sensitivities and contributions of each layer in FCN, and built an optimal layer fusion architecture. Experiments were performed on two public datasets (ISPRS Vaihingen 2D Semantic Labeling dataset and Potsdam dataset), and impressive results were observed. Volpi et al. [<xref rid="B15-sensors-18-02113" ref-type="bibr">15</xref>] presented a FCN-like network for dense semantic labeling task. The proposed network performed downsampling through convolutions and upsampled them to full resolution by deconvolutions. Experiments showed that the presented approach obtained results aligned with the state-of-art models on two challenging datasets (Vaihingen and Potsdam sub-decimeter resolution datasets). However, most of the previous studies mainly focused on the land cover classification tasks in remote sensing. To the best of our knowledge, related studies on weed mapping tasks in UAV imagery using semantic labeling approaches are not found, except for the previous work of our team [<xref rid="B16-sensors-18-02113" ref-type="bibr">16</xref>]. In our previous work, the FCN framework was applied for weed mapping of UAV imagery. We adapted the ImageNet pre-trained VGG-16 net and transferred their learned representations to our semantic labeling task. Skip architecture was used to improve the prediction accuracy. Though FCN was proven to be effective in [<xref rid="B16-sensors-18-02113" ref-type="bibr">16</xref>], the detection for small weed patches was still the bottleneck of generating an accurate weed distribution map. In this paper, we still follow the general idea of fully convolutional fashion. However, several new features are added to address the problems of previous work: (1) residual learning and deeper architecture are applied to further improve the accuracy; (2) a new type of convolution (atrous convolution) is adopted to extend the field of view of convolutional filters; (3) the impact of multi-scale processing (Atrous Spatial Pyramid Pooling) was evaluated; (4) the fully connected CRF was employed as the post processing to further refine the spatial details. All these issues will be addressed in the methodology section.</p><p>The objective of this work is to produce an accurate weed distribution map for the UAV imagery, and ultimately provide decision support information for herbicide treatment applications. The framework of this paper is arranged as follows: <xref ref-type="sec" rid="sec2-sensors-18-02113">Section 2</xref> introduces the process of collecting data, <xref ref-type="sec" rid="sec3-sensors-18-02113">Section 3</xref> shows the analyzed accurate weed mapping methodology, <xref ref-type="sec" rid="sec4-sensors-18-02113">Section 4</xref> presents the results and discussion, and <xref ref-type="sec" rid="sec5-sensors-18-02113">Section 5</xref> presents the conclusions and future work.</p></sec><sec id="sec2-sensors-18-02113"><title>2. Data Collection</title><sec id="sec2dot1-sensors-18-02113"><title>2.1. Study Site</title><p>The study site was located in a rice field in South China (113.636888 N, 23.240441 E), as shown in <xref ref-type="fig" rid="sensors-18-02113-f001">Figure 1</xref>. The rice field had an area of 0.54 ha (90 &#x000d7; 60 m<sup>2</sup>) and the ground was flat. The field was plowed and sown with Huahang No. 31 [<xref rid="B17-sensors-18-02113" ref-type="bibr">17</xref>] at a seeding rate of 60 kg&#x000b7;ha<sup>&#x02212;1</sup> on 21 August 2017, with a row spacing of 50 cm. N and P<sub>2</sub>O<sub>5</sub> were applied at the dose of 40 kg&#x000b7;ha<sup>&#x02212;1</sup> and 50 kg&#x000b7;ha<sup>&#x02212;1</sup>, respectively. No obvious presence of diseases or insect infestations was observed in this field during the growth stages. The rice field was naturally infested with <italic>Chinese sprangletop</italic> (<italic>L. chinensis</italic>) [<xref rid="B18-sensors-18-02113" ref-type="bibr">18</xref>] and <italic>Cyperus iric</italic> [<xref rid="B19-sensors-18-02113" ref-type="bibr">19</xref>], as shown in <xref ref-type="fig" rid="sensors-18-02113-f002">Figure 2</xref>. All these weeds can be treated with the same type of herbicide. The rice and weeds were both in the principal stage 2 (Early Tillering, 3&#x02013;5 tillers detectable, codes 23&#x02013;25) from the Biologische Bundesanstalt, Bundessortenamt und CHemische Industrie (BBCH) extended scale [<xref rid="B20-sensors-18-02113" ref-type="bibr">20</xref>].</p></sec><sec id="sec2dot2-sensors-18-02113"><title>2.2. Data Collection</title><sec id="sec2dot2dot1-sensors-18-02113"><title>2.2.1. Data Collection</title><p>UAV data was collected on 2 October 2018, when the weeds and crops were in their early tillering stages. Weed management (i.e., herbicide treatment) is usually recommended at this stage. A rectangle area of 90 &#x000d7; 60 m<sup>2</sup> was delimited for UAV data collection, and a quad-copter UAV (Phantom 4, SZ DJI Technology Co., Ltd., Shenzhen, China) was used to perform the flights. The typical technical characteristics of the Phantom 4 are listed in <xref rid="sensors-18-02113-t001" ref-type="table">Table 1</xref>. The coordinates of four corners were collected for automatic mission planning. The flight height was set to 6 m above the ground, and the side-lap and the end-lap of imagery were set to 50% and 60%, respectively. After that, the UAV was started to perform the flights and capture the imagery automatically, according to the planned mission. In this experiment, 91 UAV imagery were captured. <xref ref-type="fig" rid="sensors-18-02113-f003">Figure 3</xref> gave an example of the collected imagery. From <xref ref-type="fig" rid="sensors-18-02113-f003">Figure 3</xref> (the weed patches are indicated by red dashed lines) we can see that, the rice and weeds can be directly distinguished since the resolution (0.3 cm) was sufficient for visual discrimination.</p></sec><sec id="sec2dot2dot2-sensors-18-02113"><title>2.2.2. Dataset Description</title><p>In our work, ground truth (GT) maps with pixel correspondence were needed to evaluate the performance of the classifiers. We manually labeled the UAV imagery at the pixel level under the instruction of agronomic experts. The labeling was conducted by hand for the total of 91 UAV images, and labeling each image took 40 min on average. However, the resolution of the collected imagery was 4000 &#x000d7; 3000, making it a great challenge to train a deep neural network with limited GPU memory. In order to perform the training and inference on the UAV imagery at its original resolution, we followed the general idea of [<xref rid="B21-sensors-18-02113" ref-type="bibr">21</xref>] and split each image into non-overlapped regions of 1000 &#x000d7; 1000. Thus, our dataset contained 1092 UAV images (size 1000 &#x000d7; 1000), with 1092 GT maps (size 1000 &#x000d7; 1000). For each UAV image, there existed a GT map having a pixel-level correspondence with it. Three image-GT map samples are illustrated in <xref ref-type="fig" rid="sensors-18-02113-f004">Figure 4</xref>. For evaluation of the generalization capability and robustness of the classifiers, the dataset was randomly split into training set (892 samples) and validation set (200 samples) for training and validation, respectively.</p></sec></sec></sec><sec id="sec3-sensors-18-02113"><title>3. Methodology</title><p>Following the idea of [<xref rid="B22-sensors-18-02113" ref-type="bibr">22</xref>], we design a semantic labeling network for weed mapping. The workflow is shown in <xref ref-type="fig" rid="sensors-18-02113-f005">Figure 5</xref>. Firstly, the collected UAV imagery was imported into a Deep Fully Convolutional Network (DFCN), resulting in a coarse score map with reduced resolution. Secondly, the bilinear interpolation was applied to upsample the score map into full resolution. Lastly, the UAV imagery and upsampled score map were exported to a fully connected CRF to further refine the spatial details.</p><sec id="sec3dot1-sensors-18-02113"><title>3.1. Deep Fully Convolutional Network</title><p>The target of our research is to output an accurate weed distribution map, which belong to a semantic labeling task. In recent years, Deep Fully Convolutional Network (DFCN) has been proven effective for semantic labeling in computer vision [<xref rid="B13-sensors-18-02113" ref-type="bibr">13</xref>,<xref rid="B22-sensors-18-02113" ref-type="bibr">22</xref>] as well as remote sensing [<xref rid="B23-sensors-18-02113" ref-type="bibr">23</xref>,<xref rid="B24-sensors-18-02113" ref-type="bibr">24</xref>,<xref rid="B25-sensors-18-02113" ref-type="bibr">25</xref>] applications. The DFCN can output a dense class map for an input image, making it a potential approach to perform weed mapping tasks of UAV imagery.</p><sec id="sec3dot1dot1-sensors-18-02113"><title>3.1.1. Network Architecture</title><p>DFCN is a modified version of Deep Convolutional Neural Network (DCNN). In general, traditional DCNNs are composed of a few convolutional layers, pooling layers and fully connected layers [<xref rid="B12-sensors-18-02113" ref-type="bibr">12</xref>]. By transforming all the fully connected layers in convolutional forms, a DCNN can be converted into a DFCN, which will output a dense prediction map for the input image [<xref rid="B13-sensors-18-02113" ref-type="bibr">13</xref>]. We began our work by adapting proven classification architectures in fully convolutional fashion. In our work, the ResNet [<xref rid="B26-sensors-18-02113" ref-type="bibr">26</xref>] and VGG-16 [<xref rid="B27-sensors-18-02113" ref-type="bibr">27</xref>] net were considered as the baseline classification architectures.</p><p>ResNet was proposed by He et al. [<xref rid="B26-sensors-18-02113" ref-type="bibr">26</xref>] in 2015, and won the championship in the ILSVRC15. Compared with prior state-of-art classification architectures [<xref rid="B27-sensors-18-02113" ref-type="bibr">27</xref>,<xref rid="B28-sensors-18-02113" ref-type="bibr">28</xref>,<xref rid="B29-sensors-18-02113" ref-type="bibr">29</xref>], ResNet applied residual learning framework to the plain convolutional network, which well addressed the degradation problem of deep network. It was proven by He et al. [<xref rid="B26-sensors-18-02113" ref-type="bibr">26</xref>] that the 152-layer ResNet outperformed others (34-layer, 50-layer, and 101-layer) in terms of accuracy. However, the size of our imagery (1000 &#x000d7; 1000) is larger than that in the ImageNet (224 &#x000d7; 224), which may cause GPU exhaustion with 152-layer network, so the 101-layer ResNet was chosen to be the baseline architecture.</p><p>To fit the weed mapping task, the ResNet should be adapted in a fully convolutional fashion. The architectures of ResNet before and after adaption were shown in <xref rid="sensors-18-02113-t002" ref-type="table">Table 2</xref> (in the column of layer type, the architecture is shown in blocks, multiplied with the number of blocks). In the baseline architecture of ResNet, downsampling is performed by <italic>conv1</italic>, <italic>conv2_1</italic>, <italic>conv3_1</italic>, <italic>conv4_1</italic>, and <italic>conv5_1</italic> with a stride of 2, resulting in 1/32 downsampling feature maps (<italic>conv5_3</italic>). However, upsampling the feature maps to full resolution needed a 32 pixel stride, limiting the spatial precision of the output. Thus, in the modified architecture, the strides of <italic>conv4_1</italic> and <italic>conv5_1</italic> were set to 1, resulting in 1/8 downsampling feature maps. This change decreases the upsampling factor from 32 times to 8 times, improving the precision of details in the upsampled output. After that, the fully connected layer (<italic>fc6</italic>) was discarded and replaced with a 3 &#x000d7; 3 convolution layer with dimension 3 to predict scores for the whole classes (others, rice, and weeds). The final output of the ResNet was a coarse score map with reduced resolution (size 125 &#x000d7; 125), and was upsampled to full resolution (size 1000 &#x000d7; 1000) using a simple bilinear interpolation. Compared with the deconvolutional approach adopted in [<xref rid="B13-sensors-18-02113" ref-type="bibr">13</xref>], bilinear interpolation upsamples a signal without requiring learning any parameters, leading to faster training in practice.</p><p>Besides ResNet, VGG-16 net was also considered in this study. VGG-net was the runner-up in the ImageNet ILSVRC-2014, and secured the 1th and 2th places in the localization and classification tasks [<xref rid="B27-sensors-18-02113" ref-type="bibr">27</xref>]. As a classification network, VGG-net was popular to be the baseline architecture of the semantic labeling approaches [<xref rid="B13-sensors-18-02113" ref-type="bibr">13</xref>,<xref rid="B30-sensors-18-02113" ref-type="bibr">30</xref>]. In the baseline architecture of VGG-net, very small (3 &#x000d7; 3) convolution filters were used, and the depth of the network was pushed to 16&#x02013;19 layers, as shown in <xref rid="sensors-18-02113-t003" ref-type="table">Table 3</xref> (In the column of layer type, the architecture is shown in blocks, multiplied with the number of blocks). In our work, the VGG-16 net was adapted to fit our task. Similar to ResNet, the strides of the pool4 and pool5 were set to 1, reducing the degree of signal downsampling (from 1/32 to 1/8). After that, the fully connected layer (<italic>fc6</italic>) was discarded and replaced with a 3 &#x000d7; 3 convolution layer with dimension 3. The output of the modified VGG-16 net was a coarse score map (size 125 &#x000d7; 125), and upsampled to full resolution through a simple bilinear interpolation, same as the ResNet.</p></sec><sec id="sec3dot1dot2-sensors-18-02113"><title>3.1.2. Transfer Learning</title><p>DCNNs have shown astounding results in remote sensing applications [<xref rid="B31-sensors-18-02113" ref-type="bibr">31</xref>,<xref rid="B32-sensors-18-02113" ref-type="bibr">32</xref>]. However, with the limited data we had, the training of DCNNs will dramatically overfit the training data. In this work, transfer learning was applied to address this problem. The ImageNet pre-trained models (ResNet, VGG-16 net) were adapted to our task, and their representations were transfered to our dataset by fine-tuning technique.</p></sec><sec id="sec3dot1dot3-sensors-18-02113"><title>3.1.3. Atrous Convolution</title><p>Atrous convolution, proposed by Chen et al. [<xref rid="B33-sensors-18-02113" ref-type="bibr">33</xref>], was applied to DCNNs for generating dense feature maps. Since the feature maps were computed in 2-D forms, the application of atrous convolution in 2-D situations will be considered in the following. Assuming the input x as a 2-D signal, and the filter w (size K &#x000d7; K) as a 2-D matrix, then the standard convolution of x and w can be defined as:<disp-formula id="FD1-sensors-18-02113"><label>(1)</label><mml:math id="mm9"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">j</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mstyle mathsize="100%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi mathvariant="normal">k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:msubsup><mml:msubsup><mml:mstyle mathsize="100%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:msubsup><mml:mi mathvariant="normal">x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">k</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">j</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="normal">k</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
and the atrous convolution of x and w can be described in the following:<disp-formula id="FD2-sensors-18-02113"><label>(2)</label><mml:math id="mm10"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">j</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mstyle mathsize="100%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi mathvariant="normal">k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:msubsup><mml:msubsup><mml:mstyle mathsize="100%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:msubsup><mml:mi mathvariant="normal">x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">r</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="normal">k</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">j</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">r</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="normal">k</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where r denotes the parameter rate corresponding to the stride. From the Formulas (1) and (2), it can be seen that standard convolution is a special case of atrous convolution with rate = 1. In a 2-D case, the operation of standard convolution and atrous convolution with rate = 2 are illustrated in <xref ref-type="fig" rid="sensors-18-02113-f006">Figure 6</xref>.</p><p>In the implementation of the algorithm, the atrous convolution with rate = r inserts r&#x02212;1 zeros between two adjacent filter values, extending the field of view from K &#x000d7; K to K<sub>1</sub> &#x000d7; K<sub>1</sub>, where
<disp-formula id="FD3-sensors-18-02113"><label>(3)</label><mml:math id="mm11"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">K</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">K</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">K</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>However, during the computing process, only the nonzero filter values needs to be taken into account. Thus, atrous convolution extends the field of view of filters without extra parameters or computations.</p><p>In the setup of network architecture (<xref ref-type="sec" rid="sec3dot1dot1-sensors-18-02113">Section 3.1.1</xref>), the last two downsampling operations were removed to increase the spatial resolution of feature maps. However, the field of view of the filters was reduced (i.e., In the architecture of ResNet, the field of view of <italic>conv4_x</italic> layers was reduced by 1/2 &#x000d7; 1/2, and the field of view of <italic>conv5_x</italic> layers was reduced by 1/4 &#x000d7; 1/4), significantly weakening the invariance (to small shifts and distortions of previous layers) created by downsampling. In this work, atrous convolution was used to recover the field of view of the filters. Correspondingly, in the modified architecture of ResNet, the standard convolutional layers of <italic>conv4_x</italic> were replaced with atrous convolution with rate = 2, and the standard convolutional layers of <italic>conv5_x</italic> were replaced with atrous convolution of rate = 4. Similar changes were applied in the modified architecture of VGG-16 net.</p></sec><sec id="sec3dot1dot4-sensors-18-02113"><title>3.1.4. Multi-Scale Processing</title><p>In this work, the multi-scale processing simultaneously employs several braches of atrous convolutional layers to a feature map, which may improve the DCNN&#x02019;s capability to capture objects at different scales. In this scheme, the features are computed at different scales and fused to generate the output. The multiple atrous convolutional layers in multi-scale processing can be implemented in parallel, which significantly improve the efficiency during the network inference. As the setup of ASPP-S in [<xref rid="B22-sensors-18-02113" ref-type="bibr">22</xref>], four branches of atrous convolution (r = {2, 4, 8, 12}) was employed in the <italic>fc6</italic> layer (<xref rid="sensors-18-02113-t002" ref-type="table">Table 2</xref>), which is shown in <xref ref-type="fig" rid="sensors-18-02113-f007">Figure 7</xref>.</p></sec></sec><sec id="sec3dot2-sensors-18-02113"><title>3.2. Post Processing</title><p>The repeated max-pooling and downsampling (&#x02018;striding&#x02019;) in DCNNs significantly improves the invariance to small shifts and reduce the GPU memory involved in network inference. However, these operations cause loss in spatial precision, and generally result in excessive smoothing of spatial details. In this work, fully connected conditional random field (CRF) was employed to refine the spatial details. Fully connected CRF considers each pixel as a CRF node, and the image forms a graph on all nodes. The output labeling <inline-formula><mml:math id="mm12"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> of random field is determined by a distribution probability P(x). The distribution probability is related to an energy function E(x), which is designed to refine the spatial precision, as shown in the following:<disp-formula id="FD4-sensors-18-02113"><label>(4)</label><mml:math id="mm13"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>argmax</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="normal">x</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mtext>&#x000a0;</mml:mtext><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD5-sensors-18-02113"><label>(5)</label><mml:math id="mm14"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="normal">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD6-sensors-18-02113"><label>(6)</label><mml:math id="mm15"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mstyle mathsize="100%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="sans-serif">&#x003b8;</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mstyle mathsize="100%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:msub><mml:mstyle mathsize="100%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mi mathvariant="normal">a</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="sans-serif">&#x003b8;</mml:mi><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm16"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#x003b8;</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm17"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#x003b8;</mml:mi><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represent the unary and pairwise potential, respectively. The unary potential <inline-formula><mml:math id="mm18"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#x003b8;</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is defined as:<disp-formula id="FD7-sensors-18-02113"><label>(7)</label><mml:math id="mm19"><mml:mrow><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:msub><mml:mi mathvariant="sans-serif">&#x003b8;</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm20"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the probability of the pixel (at location a) exported by a DFCN. The pairwise potential <inline-formula><mml:math id="mm21"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#x003b8;</mml:mi><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is defined using the combination of two Gaussian kernels, as shown in the following:<disp-formula id="FD8-sensors-18-02113"><label>(8)</label><mml:math id="mm22"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#x003b8;</mml:mi><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="sans-serif">&#x003bc;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:msub><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub><mml:mo>&#x02016;</mml:mo><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi mathvariant="sans-serif">&#x003c3;</mml:mi><mml:mi mathvariant="sans-serif">&#x003b1;</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:msub><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub><mml:mo>&#x02016;</mml:mo><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi mathvariant="sans-serif">&#x003c3;</mml:mi><mml:mi mathvariant="sans-serif">&#x003b2;</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mi mathvariant="normal">w</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mi>exp</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:msub><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub><mml:mo>&#x02016;</mml:mo><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi mathvariant="sans-serif">&#x003c3;</mml:mi><mml:mi mathvariant="sans-serif">&#x003b3;</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The first kernel depends on the feature spaces of colors and positions, which was inspired that adjacent pixels with similar colors are likely to have the same type. The second kernel depends only on the feature space of positions, which intends to remove the noise from the output probabilities. The parameters <inline-formula><mml:math id="mm23"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#x003c3;</mml:mi><mml:mi mathvariant="sans-serif">&#x003b1;</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm24"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#x003c3;</mml:mi><mml:mi mathvariant="sans-serif">&#x003b2;</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm25"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#x003c3;</mml:mi><mml:mi mathvariant="sans-serif">&#x003b3;</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> control the shape of the Gaussian kernels. The value of compatibility label <inline-formula><mml:math id="mm26"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#x003bc;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is defined as:<disp-formula id="FD9-sensors-18-02113"><label>(9)</label><mml:math id="mm27"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#x003bc;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:mo>&#x02260;</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The compatibility label introduces a penalty for adjacent similar pixels with different classes, which significantly improve the precision in spatial details especially along the boundaries, as shown in <xref ref-type="fig" rid="sensors-18-02113-f008">Figure 8</xref>.</p></sec><sec id="sec3dot3-sensors-18-02113"><title>3.3. Method Comparisons</title><sec id="sec3dot3dot1-sensors-18-02113"><title>3.3.1. Pixel-Based-SVM</title><p>Following the idea of Alexandridis et al. [<xref rid="B8-sensors-18-02113" ref-type="bibr">8</xref>], we performed a per-pixel classification over the input image using the discriminative power of SVM. Different from the model in [<xref rid="B8-sensors-18-02113" ref-type="bibr">8</xref>], we used the C-SVC as the model type instead of One-Class-SVM, since there were three target classes (rice, weeds, and others) in our study. The three spectral bands (Red, Green, and Blue) were selected as inputs, and the corresponding classes were set as outputs.</p></sec><sec id="sec3dot3dot2-sensors-18-02113"><title>3.3.2. FCN-8s</title><p>In the previous work of our team [<xref rid="B16-sensors-18-02113" ref-type="bibr">16</xref>], it was proven that FCN-8s was effective in the weed mapping task of UAV imagery. Thus, in this study, we also compare our algorithm with the FCN-8s method. For FCN-8s, the setup of the network proposed in [<xref rid="B13-sensors-18-02113" ref-type="bibr">13</xref>] was used. We adapted the ImageNet pre-trained VGG-16 net and transferred their learned representations to our semantic labeling task. Skip architecture was used to improve the accuracy, as shown in <xref ref-type="fig" rid="sensors-18-02113-f009">Figure 9</xref>. From <xref ref-type="fig" rid="sensors-18-02113-f009">Figure 9</xref>, it can be seen that a 2 &#x000d7;upsampling operation was conducted to the last convolution layer (resulting in a score map fc_7 prediction), and a 1 &#x000d7; 1 convolutional layer was appended to the pool3 and pool4 (resulting in predictions from pool3 and pool4). The predictions from fc_7 and pool3 were fused with summation, and the result was later fused with the prediction from pool4. Finally, the fused result was upsampled to full resolution, building the final output of FCN-8s.</p><p>Although our current work (<xref ref-type="fig" rid="sensors-18-02113-f005">Figure 5</xref>) shared the same fully convolutional framework with previous FCN-8s (<xref ref-type="fig" rid="sensors-18-02113-f009">Figure 9</xref>), there were several new features added to the current approach: (1) residual learning was adopted to address the delegation problem of deep network; (2) atrous convolution was used to reduce the resolution downsampling, thus the skip architecture in FCN-8s was discarded, resulting in a simplified architecture; (3) simple bilinear interpolation was applied for signal upsampling; unlike deconvolutional operation in FCN-8s, parameters in bilinear interpolation does not require optimization, which may significantly accelerate the training process; (4) the fully connected CRF was used as post processing stage, which was not used in FCN-8s. All these newly added features built a significantly improved architecture, leading to better performance in terms of accuracy, which will be shown in <xref ref-type="sec" rid="sec4dot3-sensors-18-02113">Section 4.3</xref>.</p></sec></sec></sec><sec id="sec4-sensors-18-02113"><title>4. Results and Discussions</title><p>In the following, the experiments and comparisons will be presented to evaluate our semantic labeling approach. From <xref ref-type="sec" rid="sec2dot2dot2-sensors-18-02113">Section 2.2.2</xref>, it can be seen that our dataset was split into training set (892 samples) and validation set (200 samples). All the models were trained on the training set, and the results were reported on the evaluation on the validation set. All the experiments were conducted on a computer with an Intel i7-7700 processor clocked at 3.6G Hz and a NVIDIA GeForce GTX 1080 Ti graphic device.</p><p>The mean intersection over union (mean IU), overall accuracy, and Kappa coefficient were employed as the metrics for the experiments. The mean IU counts the mislabeled pixels, which is now the default standard for most semantic labeling competitions (i.e., PASCAL VOC). The Kappa coefficient is obtained by calculating from the confusion matrix, which is a measure that has been used in a variety of applications including semantic labeling tasks.</p><sec id="sec4dot1-sensors-18-02113"><title>4.1. Deep Fully Convolutional Network</title><p>In the experiments of DFCN, (1) the comparison on the performance of different baseline CNN architecture (ResNet-101 and VGG-16 net) was presented; (2) the impact of transfer learning on network training was evaluated; (3) the performance of multi-scale processing was tested. All the experiments in this section were conducted to seek for the optimal network architecture for the weed mapping task.</p><sec id="sec4dot1dot1-sensors-18-02113"><title>4.1.1. Network Architecture</title><p>In this section, we adapted two proven classification architectures (ResNet-101 and VGG-16 net) into fully convolutional fashion and transfer their learning representations from ImageNet to our weed mapping task (<xref ref-type="sec" rid="sec3dot1dot1-sensors-18-02113">Section 3.1.1</xref>). The atrous convolution was applied to the classification network for dense feature extraction (<xref ref-type="sec" rid="sec3dot1dot3-sensors-18-02113">Section 3.1.3</xref>). Follow the setup for <italic>fc6</italic> layer in [<xref rid="B22-sensors-18-02113" ref-type="bibr">22</xref>], we used the setting of ASPP-L (r = {6, 12, 18, 24}) to enhance the DFCN&#x02019;s capability to capture objects at different scales. Different from [<xref rid="B22-sensors-18-02113" ref-type="bibr">22</xref>], the 1 &#x000d7; 1 convolutional layers of fc_7 and fc_8 were removed, and good performance was observed.</p><p>The comparison of performance of ResNet-101 and VGG-16 net was listed in <xref rid="sensors-18-02113-t004" ref-type="table">Table 4</xref>. From <xref rid="sensors-18-02113-t004" ref-type="table">Table 4</xref>, it can be seen that the ResNet based DFCN outperforms the VGG-16 net in all terms of metrics. We owed this to the residual learning framework and the increased depth of ResNet. With residual learning, the ResNet enjoyed accuracy gains with increased depth of the network. For the experimental results in this section, the ResNet-101 was chosen as our baseline architecture in the following experiments.</p></sec><sec id="sec4dot1dot2-sensors-18-02113"><title>4.1.2. Transfer Learning</title><p>In this section, two strategies were applied to train the DFCN: (1) the ImageNet pre-trained ResNet was transferred to our dataset by fine-tuning; (2) the ResNet was adapted to our task and trained from scratch. Same as the <xref ref-type="sec" rid="sec4dot1dot1-sensors-18-02113">Section 4.1.1</xref>, ASPP-L (r = {6, 12, 18, 24}) was used as the setting for the <italic>fc6</italic> layer of ResNet. The training process was illustrated in <xref ref-type="fig" rid="sensors-18-02113-f010">Figure 10</xref>, and the experimental results were shown in <xref rid="sensors-18-02113-t005" ref-type="table">Table 5</xref>. From <xref ref-type="fig" rid="sensors-18-02113-f010">Figure 10</xref> and <xref rid="sensors-18-02113-t005" ref-type="table">Table 5</xref>, it can be seen that transfer learning significantly accelerates the training process and improves the prediction accuracy. One possible reason for this result was that, without transfer learning, the deep network overfit the limited training data, and this problem was well addressed by the transfer learning method.</p></sec><sec id="sec4dot1dot3-sensors-18-02113"><title>4.1.3. Multi-Scale Processing</title><p>As described in <xref ref-type="sec" rid="sec3dot1dot4-sensors-18-02113">Section 3.1.4</xref>, several branches of atrous convolutional layers with different rates were used in the fc6 layer in order to capture the objects at different scales. Following the setup in [<xref rid="B22-sensors-18-02113" ref-type="bibr">22</xref>,<xref rid="B33-sensors-18-02113" ref-type="bibr">33</xref>], three settings were adopted in this section: (1) ASPP-12, composed of a single atrous convolutional layer with rate = 12; (2) ASPP-S, composed of four parallel atrous convolutional layers with small rates (r = {2, 4, 8, 12}); (3) ASPP-L, composed of four parallel atrous convolutional layers with large rates (r = {6, 12, 18, 24}). Besides that, the performance of the standard convolution was also evaluated, which was denoted as: (4) ASPP-1, having a single branch with rate = 1.</p><p>The experimental results were shown in <xref rid="sensors-18-02113-t006" ref-type="table">Table 6</xref>. From <xref rid="sensors-18-02113-t006" ref-type="table">Table 6</xref>, it can be seen that, the performance of the standard convolution version (ASPP-1) outperforms other atrous combinations. One possible reason was that, the resolution of our UVA imagery was constant (0.3 cm) since they were all collected at the same altitude, and the sampling rate of 1 captured the features powerful for discrimination, so adding extra atrous convolutional layers conversely cause the accuracy decrease.</p></sec></sec><sec id="sec4dot2-sensors-18-02113"><title>4.2. Post Processsing</title><p>From Equation (4), it can be seen that the output of fully connected CRF is determined by the probability distribution P(x). However, the high computational cost is the bottleneck for the na&#x000ef;ve implementation. Instead, the mean field approximation was used to compute another distribution Q(x) which minimizes the K-L divergence with P(x). The refinement process of CRF generally employs several iterations of mean field approximations, and the number of iterations was set to 10 in our experiments. The value of <inline-formula><mml:math id="mm28"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">w</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm29"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#x003c3;</mml:mi><mml:mi mathvariant="sans-serif">&#x003b3;</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> were both set to 1, which was the default setup of [<xref rid="B34-sensors-18-02113" ref-type="bibr">34</xref>]. To seek for the optimal value of <inline-formula><mml:math id="mm30"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">w</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm31"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="sans-serif">&#x003c3;</mml:mi></mml:mrow><mml:mi mathvariant="sans-serif">&#x003b1;</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm32"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">&#x003c3;</mml:mi><mml:mi mathvariant="sans-serif">&#x003b2;</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the grid search strategy was applied on the training set, and this process took around 36 h.</p><p>Two samples on the CRF refinement are given in <xref ref-type="fig" rid="sensors-18-02113-f011">Figure 11</xref>, and the comparisons of quantitative statistics were presented in <xref rid="sensors-18-02113-t007" ref-type="table">Table 7</xref>. From <xref ref-type="fig" rid="sensors-18-02113-f011">Figure 11</xref>, it can be seen that the output after CRFs has a more clear boundaries, compared to that before CRFs. Especially for the small weed patches (in blue dashed lines), the output after CRFs better delineate the borders. From <xref rid="sensors-18-02113-t007" ref-type="table">Table 7</xref>, it is obvious that the CRF approach consistently boosts the performance for all models in all metrics. Though the increased margin of accuracy was not significant, the CRF approach can well address the problem of boundary blurring caused by the upsampling operation of our model. This improvement is especially important for the detection of small weed patches.</p></sec><sec id="sec4dot3-sensors-18-02113"><title>4.3. Comparison with Other Methods</title><p>For the setup of Pixel-based-SVM, the Radial Basis Function (RBF) was chosen as the kernel function, and the C-Support Vector Classification (C-SVC) was selected as the objective function. The best penalty factor C was chosen by using grid search strategy. For FCN-8s, the default configuration of [<xref rid="B16-sensors-18-02113" ref-type="bibr">16</xref>] was applied in our experiment. For our approach, the architectures of ASPP-1 (with and without CRFs) were adopted based on the experimental results in <xref rid="sensors-18-02113-t007" ref-type="table">Table 7</xref>.</p><p><xref ref-type="fig" rid="sensors-18-02113-f012">Figure 12</xref> illustrates the classification results by the involved methods. It can be seen from <xref ref-type="fig" rid="sensors-18-02113-f012">Figure 12</xref> that, semantic labeling approaches (FCN-8s and ours) significantly outperformed pixel-based-SVM in prediction accuracy. In the pixel-based-SVM approach, each pixel was input to the model independently, which ignored the correlations between pixels and resulted in pool performance in accuracy. With regard to the comparison with FCN-8s, our approach showed significant improvement in detecting small weed patches. From <xref ref-type="fig" rid="sensors-18-02113-f012">Figure 12</xref>, it is obvious that FCN-8s fails to detect the small weed patches distributed inside and outside the rice (in black and white dashed lines, respectively), while our model variants (ASPP-1 without and with CRFs) accurately locate the targets. We owed the improvement to the following reasons: (1) the residual learning framework in our network boosts the accuracy with increased depth; (2) the atrous convolution extends the field of view of convolutional filters, improving the network capability to capture the object at small scale, which leaded to a better performance in detection of small weed patches.</p><p><xref rid="sensors-18-02113-t008" ref-type="table">Table 8</xref> lists the experimental results in terms of accuracy and efficiency. From <xref rid="sensors-18-02113-t008" ref-type="table">Table 8</xref>, it can be seen that semantic labeling approaches (FCN-8s and ours) significantly outperformed the Pixel-based-SVM in terms of accuracy and efficiency. In the pixel-based approach, the labeling processing is performed pixel by pixel, which introduces abundant computation and slows down the inference speed. Compared with FCN-8s, our ASPP-1 model achieved better accuracy at an acceptable speed. However, the employment of CRF significantly slows down the inference, and the extra time for this step is up to 2.6255 s per image. One important reason for this is that the CRF algorithm is implemented in a single CPU thread, which is much slower than those driven by parallelism mechanism or hardware acceleration. Nevertheless, our approach obtained an acceptable accuracy even without the CRF. Thus, in some applications asking for fast image processing, the post processing of CRF can be discarded in order to speed up the network inference.</p><p><xref rid="sensors-18-02113-t009" ref-type="table">Table 9</xref> lists the confusion matrix of FCN-8s and our approach. In order to reduce the inference time, the ASPP-1 with CRF was not considered in this case, since the ASPP-1 without CRF has already achieved a competitive performance for weed mapping. From <xref rid="sensors-18-02113-t009" ref-type="table">Table 9</xref>, it can be seen that our approach obtained higher weed detection rate than Pixel-based-SVM and FCN-8s, which may provide more trustable decision support information for precise herbicide treatment in real applications.</p></sec></sec><sec id="sec5-sensors-18-02113"><title>5. Conclusions</title><p>In this work, high resolution UAV images were collected on a rice field. A semantic labeling approach was applied to automatically generate accurate weed distribution maps. An ImageNet pre-trained ResNet was employed and adapted to our weed mapping task in a fully convolutional fashion, and the learned representations of ResNet were transferred to our dataset using fine-tuning. Atrous convolution was used to extend the field of view of convolutional filters; the performance of multi-scale processing was evaluated; and a fully connected CRF was applied to improve the spatial precision. Our approach was compared with the Pixel-based-SVM and the classical FCN-8s. Comparison results showed that our approach achieved higher accuracy than Pixel-based-SVM and FCN-8s. Especially for the detection of small weed patches, our approach significantly outperformed other methods. All the experimental results demonstrated that our approach has potential to generate accurate weed distribution maps to provide decision support information for herbicide treatment applications. However, the increased complexity of the network leads to a decrease in inference speed, which may limit the applications of our approach. Especially in the post-processing stage, much time is needed to carry out the CRF process. Therefore, one of the future directions is to simplify the network architecture and accelerate the inference process, which will remain as the future work of our study.</p></sec></body><back><ack><title>Acknowledgments</title><p>Authors are grateful to Yuancheng Liang and Junhui Wen for providing agronomic advises during the field campaign and data labeling.</p></ack><notes><title>Author Contributions</title><p>Conceptualization, Y.L. and J.D.; Data curation, H.H., L.Z. and S.W.; Funding acquisition, Y.L., J.D., X.D. and S.W.; Methodology, H.H., Y.L., J.D., A.Y. and X.D.; Software, H.H. and A.Y.; Writing&#x02014;original draft, H.H.; Writing&#x02014;review &#x00026; editing, Y.L., J.D., X.D., L.Z. and S.W.</p></notes><notes><title>Funding</title><p>This research was funded by the Science and Technology Planning Project of Guangdong Province, China (Grant No. 2017A020208046), the National Key Research and Development Plan: High Efficient Ground and Aerial Spraying Technology and Intelligent Equipment, China (Grant No. 2016YFD0200700), the National Natural Science Fund, China (Grant No. 61675003), the Science and Technology Planning Project of Guangdong Province, China (Grant No. 2017B010117010), and the Science and Technology Planning Project of Guangzhou city, China (Grant No. 201707010047).</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-18-02113"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pacanoski</surname><given-names>Z.</given-names></name><name><surname>Glatkova</surname><given-names>G.</given-names></name></person-group><article-title>The Use of Herbicides for Weed Control in Direct Wet-Seeded Rice (<italic>Oryza sativa</italic> L.) in Rice Production Regions in the Republic of Macedonia</article-title><source>Plant Prot. Sci.</source><year>2009</year><volume>45</volume><fpage>113</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.17221/4/2008-PPS</pub-id></element-citation></ref><ref id="B2-sensors-18-02113"><label>2.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ferrero</surname><given-names>A.</given-names></name></person-group><article-title>Weedy rice, biological features and control</article-title><source>Fao Plant Production Protection Paper</source><publisher-name>Food and Agriculture Organization of the United Nations</publisher-name><publisher-loc>Rome, Italy</publisher-loc><year>2003</year><volume>Volume 120</volume><fpage>89</fpage><lpage>107</lpage></element-citation></ref><ref id="B3-sensors-18-02113"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castaldi</surname><given-names>F.</given-names></name><name><surname>Pelosi</surname><given-names>F.</given-names></name><name><surname>Pascucci</surname><given-names>S.</given-names></name><name><surname>Casa</surname><given-names>R.</given-names></name></person-group><article-title>Assessing the potential of images from unmanned aerial vehicles (UAV) to support herbicide patch spraying in maize</article-title><source>Precis. Agric.</source><year>2017</year><volume>18</volume><fpage>76</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1007/s11119-016-9468-3</pub-id></element-citation></ref><ref id="B4-sensors-18-02113"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zanin</surname><given-names>G.</given-names></name><name><surname>Berti</surname><given-names>A.</given-names></name><name><surname>Toniolo</surname><given-names>L.</given-names></name></person-group><article-title>Estimation of economic thresholds for weed control in winter wheat</article-title><source>Weed Res.</source><year>1993</year><volume>33</volume><fpage>459</fpage><lpage>467</lpage><pub-id pub-id-type="doi">10.1111/j.1365-3180.1993.tb01962.x</pub-id></element-citation></ref><ref id="B5-sensors-18-02113"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pena</surname><given-names>J.M.</given-names></name><name><surname>Torres-Sanchez</surname><given-names>J.</given-names></name><name><surname>de Castro</surname><given-names>A.I.</given-names></name><name><surname>Kelly</surname><given-names>M.</given-names></name><name><surname>Lopez-Granados</surname><given-names>F.</given-names></name></person-group><article-title>Weed mapping in early-season maize fields using object-based analysis of unmanned aerial vehicle (UAV) images</article-title><source>PLoS ONE</source><year>2013</year><volume>8</volume><elocation-id>e77151</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0077151</pub-id><?supplied-pmid 24146963?><pub-id pub-id-type="pmid">24146963</pub-id></element-citation></ref><ref id="B6-sensors-18-02113"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lan</surname><given-names>Y.</given-names></name><name><surname>Shengde</surname><given-names>C.</given-names></name><name><surname>Fritz</surname><given-names>B.K.</given-names></name></person-group><article-title>Current status and future trends of precision agricultural aviation technologies</article-title><source>Int. J. Agric. Biol. Eng.</source><year>2017</year><volume>10</volume><fpage>1</fpage><lpage>16</lpage></element-citation></ref><ref id="B7-sensors-18-02113"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Senthilnath</surname><given-names>J.</given-names></name><name><surname>Dokania</surname><given-names>A.</given-names></name><name><surname>Kandukuri</surname><given-names>M.</given-names></name><name><surname>Ramesh</surname><given-names>K.N.</given-names></name><name><surname>Anand</surname><given-names>G.</given-names></name><name><surname>Omkar</surname><given-names>S.N.</given-names></name></person-group><article-title>Detection of tomatoes using spectral-spatial methods in remotely sensed RGB images captured by UAV</article-title><source>Biosyst. Eng.</source><year>2016</year><volume>146</volume><fpage>16</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1016/j.biosystemseng.2015.12.003</pub-id></element-citation></ref><ref id="B8-sensors-18-02113"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexandridis</surname><given-names>T.K.</given-names></name><name><surname>Tamouridou</surname><given-names>A.A.</given-names></name><name><surname>Pantazi</surname><given-names>X.E.</given-names></name><name><surname>Lagopodi</surname><given-names>A.L.</given-names></name><name><surname>Kashefi</surname><given-names>J.</given-names></name><name><surname>Ovakoglou</surname><given-names>G.</given-names></name><name><surname>Polychronos</surname><given-names>V.</given-names></name><name><surname>Moshou</surname><given-names>D.</given-names></name></person-group><article-title>Novelty Detection Classifiers in Weed Mapping: <italic>Silybum marianum</italic> Detection on UAV Multispectral Images</article-title><source>Sensors</source><year>2017</year><volume>17</volume><elocation-id>2007</elocation-id><pub-id pub-id-type="doi">10.3390/s17092007</pub-id><?supplied-pmid 28862663?><pub-id pub-id-type="pmid">28862663</pub-id></element-citation></ref><ref id="B9-sensors-18-02113"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tamouridou</surname><given-names>A.</given-names></name><name><surname>Alexandridis</surname><given-names>T.</given-names></name><name><surname>Pantazi</surname><given-names>X.</given-names></name><name><surname>Lagopodi</surname><given-names>A.</given-names></name><name><surname>Kashefi</surname><given-names>J.</given-names></name><name><surname>Kasampalis</surname><given-names>D.</given-names></name><name><surname>Kontouris</surname><given-names>G.</given-names></name><name><surname>Moshou</surname><given-names>D.</given-names></name></person-group><article-title>Application of Multilayer Perceptron with Automatic Relevance Determination on Weed Mapping Using UAV Multispectral Imagery</article-title><source>Sensors</source><year>2017</year><volume>17</volume><elocation-id>2307</elocation-id><pub-id pub-id-type="doi">10.3390/s17102307</pub-id><?supplied-pmid 29019957?><pub-id pub-id-type="pmid">29019957</pub-id></element-citation></ref><ref id="B10-sensors-18-02113"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hung</surname><given-names>C.</given-names></name><name><surname>Xu</surname><given-names>Z.</given-names></name><name><surname>Sukkarieh</surname><given-names>S.</given-names></name></person-group><article-title>Feature Learning Based Approach for Weed Classification</article-title><source>Remote Sens.</source><year>2014</year><volume>6</volume><fpage>12037</fpage><lpage>12054</lpage><pub-id pub-id-type="doi">10.3390/rs61212037</pub-id></element-citation></ref><ref id="B11-sensors-18-02113"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Penatti</surname><given-names>O.</given-names></name><name><surname>Nogueira</surname><given-names>K.</given-names></name><name><surname>Santos</surname><given-names>J.</given-names></name></person-group><article-title>Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?</article-title><source>Proceedings of the 2015 IEEE Conference on Computer Vision &#x00026; Pattern Recognition Workshops</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#x02013;12 June 2015</conf-date><fpage>44</fpage><lpage>51</lpage></element-citation></ref><ref id="B12-sensors-18-02113"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><?supplied-pmid 26017442?><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="B13-sensors-18-02113"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shelhamer</surname><given-names>E.</given-names></name><name><surname>Long</surname><given-names>J.</given-names></name><name><surname>Darrell</surname><given-names>T.</given-names></name></person-group><article-title>Fully Convolutional Networks for Semantic Segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision &#x00026; Pattern Recognition</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#x02013;12 June 2015</conf-date><volume>Volume 79</volume><fpage>3431</fpage><lpage>3440</lpage></element-citation></ref><ref id="B14-sensors-18-02113"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>W.</given-names></name><name><surname>Huang</surname><given-names>H.</given-names></name><name><surname>Schmitz</surname><given-names>M.</given-names></name><name><surname>Sun</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>H.</given-names></name><name><surname>Mayer</surname><given-names>H.</given-names></name></person-group><article-title>Effective Fusion of Multi-Modal Remote Sensing Data in a Fully Convolutional Network for Semantic Labeling</article-title><source>Remote Sens.</source><year>2018</year><volume>10</volume><elocation-id>52</elocation-id><pub-id pub-id-type="doi">10.3390/rs10010052</pub-id></element-citation></ref><ref id="B15-sensors-18-02113"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Volpi</surname><given-names>M.</given-names></name><name><surname>Tuia</surname><given-names>D.</given-names></name></person-group><article-title>Dense semantic labeling of sub-decimeter resolution images with convolutional neural networks</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2017</year><volume>55</volume><fpage>881</fpage><lpage>893</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2016.2616585</pub-id></element-citation></ref><ref id="B16-sensors-18-02113"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>H.</given-names></name><name><surname>Deng</surname><given-names>J.</given-names></name><name><surname>Lan</surname><given-names>Y.</given-names></name><name><surname>Yang</surname><given-names>A.</given-names></name><name><surname>Deng</surname><given-names>X.</given-names></name><name><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>A fully convolutional network for weed mapping of unmanned aerial vehicle (UAV) imagery</article-title><source>PLoS ONE</source><year>2018</year><volume>13</volume><elocation-id>e196302</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0196302</pub-id><?supplied-pmid 29698500?><pub-id pub-id-type="pmid">29698500</pub-id></element-citation></ref><ref id="B17-sensors-18-02113"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>H.</given-names></name><name><surname>Guo</surname><given-names>T.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Tang</surname><given-names>X.</given-names></name><name><surname>Chen</surname><given-names>Z.</given-names></name></person-group><article-title>Breeding and application of high-quality and diseaseresistant rice variety, huahang No.31</article-title><source>Guangdong Agric. Sci.</source><year>2013</year><volume>10</volume><fpage>8</fpage><lpage>11</lpage></element-citation></ref><ref id="B18-sensors-18-02113"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>J.</given-names></name><name><surname>Gao</surname><given-names>H.</given-names></name><name><surname>Pan</surname><given-names>L.</given-names></name><name><surname>Yao</surname><given-names>Z.</given-names></name><name><surname>Dong</surname><given-names>L.</given-names></name></person-group><article-title>Mechanism of resistance to cyhalofop-butyl in Chinese sprangletop (<italic>Leptochloa chinensis</italic> (L.) Nees)</article-title><source>Pestic. Biochem. Physiol.</source><year>2017</year><volume>143</volume><fpage>306</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1016/j.pestbp.2016.11.001</pub-id><?supplied-pmid 29183606?><pub-id pub-id-type="pmid">29183606</pub-id></element-citation></ref><ref id="B19-sensors-18-02113"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>A.M.</given-names></name><name><surname>Paskewitz</surname><given-names>S.M.</given-names></name><name><surname>Orth</surname><given-names>A.P.</given-names></name><name><surname>Tesch</surname><given-names>M.J.</given-names></name><name><surname>Toong</surname><given-names>Y.C.</given-names></name></person-group><article-title>The lethal effects of <italic>Cyperus iria</italic> on <italic>Aedes aegypti</italic></article-title><source>J. Am. Mosq. Control Assoc.</source><year>1998</year><volume>1</volume><fpage>78</fpage><lpage>82</lpage></element-citation></ref><ref id="B20-sensors-18-02113"><label>20.</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Meier</surname><given-names>U.</given-names></name></person-group><article-title>Growth Stages of Mono-and Dicotyledonous Plants</article-title><year>2001</year><comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://www.reterurale.it/downloads/BBCH_engl_2001.pdf">https://www.reterurale.it/downloads/BBCH_engl_2001.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2018-04-21">(accessed on 21 April 2018)</date-in-citation></element-citation></ref><ref id="B21-sensors-18-02113"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cordts</surname><given-names>M.</given-names></name><name><surname>Omran</surname><given-names>M.</given-names></name><name><surname>Ramos</surname><given-names>S.</given-names></name><name><surname>Rehfeld</surname><given-names>T.</given-names></name><name><surname>Enzweiler</surname><given-names>M.</given-names></name><name><surname>Benenson</surname><given-names>R.</given-names></name><name><surname>Franke</surname><given-names>U.</given-names></name><name><surname>Roth</surname><given-names>S.</given-names></name><name><surname>Schiele</surname><given-names>B.</given-names></name></person-group><article-title>The Cityscapes Dataset for Semantic Urban Scene Understanding</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>26 June&#x02013;1 July 2016</conf-date><fpage>3213</fpage><lpage>3223</lpage></element-citation></ref><ref id="B22-sensors-18-02113"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L.C.</given-names></name><name><surname>Papandreou</surname><given-names>G.</given-names></name><name><surname>Kokkinos</surname><given-names>I.</given-names></name><name><surname>Murphy</surname><given-names>K.</given-names></name><name><surname>Yuille</surname><given-names>A.L.</given-names></name></person-group><article-title>DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</article-title><source>IEEE Trans Pattern Anal. Mach. Intell.</source><year>2018</year><volume>40</volume><fpage>834</fpage><lpage>848</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id><?supplied-pmid 28463186?><pub-id pub-id-type="pmid">28463186</pub-id></element-citation></ref><ref id="B23-sensors-18-02113"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Maggiori</surname><given-names>E.</given-names></name><name><surname>Tarabalka</surname><given-names>Y.</given-names></name><name><surname>Charpiat</surname><given-names>G.</given-names></name><name><surname>Alliez</surname><given-names>P.</given-names></name></person-group><article-title>Fully convolutional neural networks for remote sensing image classification</article-title><source>Proceedings of the IEEE International Geoscience and Remote Sensing Symposium</source><conf-loc>Beijing, China</conf-loc><conf-date>10&#x02013;15 July 2016</conf-date><pub-id pub-id-type="doi">10.1109/IGARSS.2016.7730322</pub-id></element-citation></ref><ref id="B24-sensors-18-02113"><label>24.</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Maggiori</surname><given-names>E.</given-names></name><name><surname>Tarabalka</surname><given-names>Y.</given-names></name><name><surname>Charpiat</surname><given-names>G.</given-names></name><name><surname>Alliez</surname><given-names>P.</given-names></name></person-group><article-title>High-Resolution Semantic Labeling with Convolutional Neural Networks</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="arxiv">1611.01962</pub-id></element-citation></ref><ref id="B25-sensors-18-02113"><label>25.</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Sherrah</surname><given-names>J.</given-names></name></person-group><article-title>Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution Aerial Imagery</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="arxiv">1606.02585</pub-id></element-citation></ref><ref id="B26-sensors-18-02113"><label>26.</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Deep Residual Learning for Image Recognition</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="arxiv">1512.03385v1</pub-id></element-citation></ref><ref id="B27-sensors-18-02113"><label>27.</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K.</given-names></name><name><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>Very Deep Convolutional Networks for Large-Scale Image Recognition</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="arxiv">1409.1556v6</pub-id></element-citation></ref><ref id="B28-sensors-18-02113"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Hinton</surname><given-names>G.E.</given-names></name></person-group><article-title>ImageNet classification with deep convolutional neural networks</article-title><source>Proceedings of the International Conference on Neural Information Processing Systems</source><conf-loc>Lake Tahoe, NV, USA</conf-loc><conf-date>3&#x02013;6 December 2012</conf-date><volume>Volume 25</volume><fpage>1097</fpage><lpage>1105</lpage></element-citation></ref><ref id="B29-sensors-18-02113"><label>29.</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C.</given-names></name><name><surname>Liu</surname><given-names>W.</given-names></name><name><surname>Jia</surname><given-names>Y.</given-names></name><name><surname>Sermanet</surname><given-names>P.</given-names></name><name><surname>Reed</surname><given-names>S.</given-names></name></person-group><article-title>Going deeper with convolutions</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="arxiv">1409.4842v1</pub-id></element-citation></ref><ref id="B30-sensors-18-02113"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>G.</given-names></name><name><surname>Liu</surname><given-names>C.</given-names></name><name><surname>Zhou</surname><given-names>R.</given-names></name><name><surname>Sun</surname><given-names>T.</given-names></name><name><surname>Zhang</surname><given-names>Q.</given-names></name></person-group><article-title>Classification for High Resolution Remote Sensing Imagery Using a Fully Convolutional Network</article-title><source>Remote Sens.</source><year>2017</year><volume>6</volume><elocation-id>498</elocation-id><pub-id pub-id-type="doi">10.3390/rs9050498</pub-id></element-citation></ref><ref id="B31-sensors-18-02113"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>F.</given-names></name><name><surname>Xia</surname><given-names>G.S.</given-names></name><name><surname>Hu</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Transferring Deep Convolutional Neural Networks for the Scene Classification of High-Resolution Remote Sensing Imagery</article-title><source>Remote Sens.</source><year>2015</year><volume>7</volume><fpage>14680</fpage><lpage>14707</lpage><pub-id pub-id-type="doi">10.3390/rs71114680</pub-id></element-citation></ref><ref id="B32-sensors-18-02113"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>L&#x000e4;ngkvist</surname><given-names>M.</given-names></name><name><surname>Kiselev</surname><given-names>A.</given-names></name><name><surname>Alirezaie</surname><given-names>M.</given-names></name><name><surname>Loutfi</surname><given-names>A.</given-names></name></person-group><article-title>Classification and Segmentation of Satellite Orthoimagery Using Convolutional Neural Networks</article-title><source>Remote Sens.</source><year>2016</year><volume>8</volume><elocation-id>329</elocation-id><pub-id pub-id-type="doi">10.3390/rs8040329</pub-id></element-citation></ref><ref id="B33-sensors-18-02113"><label>33.</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L.</given-names></name><name><surname>Papandreou</surname><given-names>G.</given-names></name><name><surname>Kokkinos</surname><given-names>I.</given-names></name><name><surname>Murphy</surname><given-names>K.</given-names></name><name><surname>Yuille</surname><given-names>A.L.</given-names></name></person-group><article-title>Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="arxiv">1412.7062</pub-id></element-citation></ref><ref id="B34-sensors-18-02113"><label>34.</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Kr Henb&#x000fc;hl</surname><given-names>P.</given-names></name><name><surname>Koltun</surname><given-names>V.</given-names></name></person-group><article-title>Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials</article-title><source>arXiv</source><year>2011</year><pub-id pub-id-type="arxiv">1210.5644</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="sensors-18-02113-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>The general location of the study site.</p></caption><graphic xlink:href="sensors-18-02113-g001"/></fig><fig id="sensors-18-02113-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>An overview of the weed patches in the field.</p></caption><graphic xlink:href="sensors-18-02113-g002"/></fig><fig id="sensors-18-02113-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>An example of the UAV imagery collected in the experiment.</p></caption><graphic xlink:href="sensors-18-02113-g003"/></fig><fig id="sensors-18-02113-f004" orientation="portrait" position="float"><label>Figure 4</label><caption><p>Three samples of our dataset: (<bold>a</bold>) aerial images; (<bold>b</bold>) corresponding GT labels.</p></caption><graphic xlink:href="sensors-18-02113-g004"/></fig><fig id="sensors-18-02113-f005" orientation="portrait" position="float"><label>Figure 5</label><caption><p>The work-flow of our methodology.</p></caption><graphic xlink:href="sensors-18-02113-g005"/></fig><fig id="sensors-18-02113-f006" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Illustration of two types of convolution. (<bold>a</bold>) Standard convolution. (<bold>b</bold>) Atrous convolution.</p></caption><graphic xlink:href="sensors-18-02113-g006"/></fig><fig id="sensors-18-02113-f007" orientation="portrait" position="float"><label>Figure 7</label><caption><p>Illustration of multi-scale processing.</p></caption><graphic xlink:href="sensors-18-02113-g007"/></fig><fig id="sensors-18-02113-f008" orientation="portrait" position="float"><label>Figure 8</label><caption><p>Illustration of the fully connected CRF.</p></caption><graphic xlink:href="sensors-18-02113-g008"/></fig><fig id="sensors-18-02113-f009" orientation="portrait" position="float"><label>Figure 9</label><caption><p>Illustration of skip architecture for FCN-8s. Only the pooling and prediction layers are shown, and other types of layers are ignored in this figure.</p></caption><graphic xlink:href="sensors-18-02113-g009"/></fig><fig id="sensors-18-02113-f010" orientation="portrait" position="float"><label>Figure 10</label><caption><p>The training process with and without transfer learning.</p></caption><graphic xlink:href="sensors-18-02113-g010"/></fig><fig id="sensors-18-02113-f011" orientation="portrait" position="float"><label>Figure 11</label><caption><p>The classification results obtained by ASPP-1 before and after CRFs. (<bold>a</bold>) aerial images; (<bold>b</bold>) corresponding GT labels; (<bold>c</bold>) output by ASPP-1 before CRFs; (<bold>d</bold>) output by ASPP-1 after CRFs.</p></caption><graphic xlink:href="sensors-18-02113-g011"/></fig><fig id="sensors-18-02113-f012" orientation="portrait" position="float"><label>Figure 12</label><caption><p>The classification results obtained by methods in comparison. (<bold>a</bold>) aerial images; (<bold>b</bold>) corresponding GT labels; (<bold>c</bold>) output by Pixel-based-SVM; (<bold>d</bold>) output by FCN-8s; (<bold>e</bold>) output by ASPP-1 without CRFs; (<bold>f</bold>) output by ASPP-1 with CRFs.</p></caption><graphic xlink:href="sensors-18-02113-g012"/></fig><table-wrap id="sensors-18-02113-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-02113-t001_Table 1</object-id><label>Table 1</label><caption><p>Technical characteristics of the UAV platform.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameters</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Specifications</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Weight (battery included)</td><td align="center" valign="middle" rowspan="1" colspan="1">1380 g</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Max Flight Time</td><td align="center" valign="middle" rowspan="1" colspan="1">28 min</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Max speed</td><td align="center" valign="middle" rowspan="1" colspan="1">20 m/s</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Typical operating altitude</td><td align="center" valign="middle" rowspan="1" colspan="1">10&#x02013;300 m</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Resolution</td><td align="center" valign="middle" rowspan="1" colspan="1">4000 &#x000d7; 3000 pixels</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Len</td><td align="center" valign="middle" rowspan="1" colspan="1">35 mm</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Typical spatial resolution (at 6 m altitude)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3 cm</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-02113-t002" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-02113-t002_Table 2</object-id><label>Table 2</label><caption><p>Architectures of ResNet before and after adaption.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Baseline Architecture (before Adaption)</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Modified Architecture (after Adaption)</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Layer Name</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Size of Output</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Layer Type</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Size of Output</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Layer Type</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
<italic>conv1</italic>
</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">500 &#x000d7; 500</td><td align="center" valign="middle" rowspan="1" colspan="1">7 &#x000d7; 7, 64 (stride 2)</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">500 &#x000d7; 500</td><td align="center" valign="middle" rowspan="1" colspan="1">7 &#x000d7; 7, 64 (stride 2)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">max-pooling (stride 2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">max pooling (stride 2)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic>conv2_x</italic>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">250 &#x000d7; 250</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm1"><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>64</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>64</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>256</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">250 &#x000d7; 250</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm2"><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>64</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>64</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>256</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic>conv3_x</italic>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">125 &#x000d7; 125</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm3"><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>128</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>128</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>512</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">125 &#x000d7; 125</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm4"><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>128</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>128</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>512</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic>conv4_x</italic>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64 &#x000d7; 64</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm5"><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>256</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>256</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>1024</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>23</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">125 &#x000d7; 125</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm6"><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>256</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>256</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>1024</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>23</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic>conv5_x</italic>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32 &#x000d7; 32</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm7"><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>512</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>512</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>2048</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">125 &#x000d7; 125</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm8"><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>512</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>512</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>2048</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic>fc6</italic>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 &#x000d7; 1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1000-d fc, softmax</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">125 &#x000d7; 125</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3 &#x000d7; 3, 3</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-02113-t003" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-02113-t003_Table 3</object-id><label>Table 3</label><caption><p>Architectures of VGG-16 net before and after adaption.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Baseline Architecture (Before Adaption)</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Modified Architecture (After Adaption)</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Layer Name</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Size of Output</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Layer Type</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Size of Output</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Layer Type</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic>conv1</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1000 &#x000d7; 1000</td><td align="center" valign="middle" rowspan="1" colspan="1">[3 &#x000d7; 3, 64] &#x000d7; 2</td><td align="center" valign="middle" rowspan="1" colspan="1">1000 &#x000d7; 1000</td><td align="center" valign="middle" rowspan="1" colspan="1">[3 &#x000d7; 3, 64] &#x000d7; 2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic>pool1</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">500 &#x000d7; 500</td><td align="center" valign="middle" rowspan="1" colspan="1">max-pooling (stride 2)</td><td align="center" valign="middle" rowspan="1" colspan="1">500 &#x000d7; 500</td><td align="center" valign="middle" rowspan="1" colspan="1">max-pooling (stride 2)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic>conv2_x</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">500 &#x000d7; 500</td><td align="center" valign="middle" rowspan="1" colspan="1">[3 &#x000d7; 3, 128] &#x000d7; 2</td><td align="center" valign="middle" rowspan="1" colspan="1">500 &#x000d7; 500</td><td align="center" valign="middle" rowspan="1" colspan="1">[3 &#x000d7; 3, 128] &#x000d7; 2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic>pool2</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">250 &#x000d7; 250</td><td align="center" valign="middle" rowspan="1" colspan="1">max-pooling (stride 2)</td><td align="center" valign="middle" rowspan="1" colspan="1">250 &#x000d7; 250</td><td align="center" valign="middle" rowspan="1" colspan="1">max-pooling (stride 2)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic>conv3_x</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">250 &#x000d7; 250</td><td align="center" valign="middle" rowspan="1" colspan="1">[3 &#x000d7; 3, 256] &#x000d7; 3</td><td align="center" valign="middle" rowspan="1" colspan="1">250 &#x000d7; 250</td><td align="center" valign="middle" rowspan="1" colspan="1">[3 &#x000d7; 3, 256] &#x000d7; 3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic>pool3</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">125 &#x000d7; 125</td><td align="center" valign="middle" rowspan="1" colspan="1">max-pooling (stride 2)</td><td align="center" valign="middle" rowspan="1" colspan="1">125 &#x000d7; 125</td><td align="center" valign="middle" rowspan="1" colspan="1">max-pooling (stride 2)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic>conv4_x</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">125 &#x000d7; 125</td><td align="center" valign="middle" rowspan="1" colspan="1">[3 &#x000d7; 3, 512] &#x000d7; 3</td><td align="center" valign="middle" rowspan="1" colspan="1">125 &#x000d7; 125</td><td align="center" valign="middle" rowspan="1" colspan="1">[3 &#x000d7; 3, 512] &#x000d7; 3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic>pool4</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">64 &#x000d7; 64</td><td align="center" valign="middle" rowspan="1" colspan="1">max-pooling (stride 2)</td><td align="center" valign="middle" rowspan="1" colspan="1">125 &#x000d7; 125</td><td align="center" valign="middle" rowspan="1" colspan="1">max-pooling (stride 1)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic>conv5_x</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">64 &#x000d7; 64</td><td align="center" valign="middle" rowspan="1" colspan="1">[3 &#x000d7; 3, 512] &#x000d7; 3</td><td align="center" valign="middle" rowspan="1" colspan="1">125 &#x000d7; 125</td><td align="center" valign="middle" rowspan="1" colspan="1">[3 &#x000d7; 3, 512] &#x000d7; 3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic>pool5</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">32 &#x000d7; 32</td><td align="center" valign="middle" rowspan="1" colspan="1">max-pooling (stride 2)</td><td align="center" valign="middle" rowspan="1" colspan="1">125 &#x000d7; 125</td><td align="center" valign="middle" rowspan="1" colspan="1">max-pooling (stride 1)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic>fc6</italic>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 &#x000d7; 1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1000-d fc, softmax</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">125 &#x000d7; 125</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3 &#x000d7; 3, 3</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-02113-t004" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-02113-t004_Table 4</object-id><label>Table 4</label><caption><p>Experimental results of different baseline architectures.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Approach</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mean IU</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Overall Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Kappa</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-101</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7668</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9409</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9076</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VGG-16 net</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7478</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9350</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8979</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-02113-t005" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-02113-t005_Table 5</object-id><label>Table 5</label><caption><p>Experimental results of transfer learning.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Approach</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mean IU</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Overall Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Kappa</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-101 with transfer learning</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7668</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9409</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9076</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet-101 without transfer learning</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6959</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8995</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8417</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-02113-t006" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-02113-t006_Table 6</object-id><label>Table 6</label><caption><p>Experimental results of ASPP.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Approach</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mean IU</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Overall Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Kappa</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">ASPP-12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7660</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9395</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9054</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ASPP-S</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7703</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9397</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9059</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ASPP-L</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7668</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9409</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9076</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ASPP-1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7721</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9423</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9094</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-02113-t007" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-02113-t007_Table 7</object-id><label>Table 7</label><caption><p>Experimental results of CRF.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Approach</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mean IU</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Overall Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Kappa</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">ASPP-12 before CRF</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7660</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9395</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9054</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ASPP-12 after CRF</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7690</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9415</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9084</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ASPP-S before CRF</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7703</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9397</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9059</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ASPP-S after CRF</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7731</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9417</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9088</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ASPP-L before CRF</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7668</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9409</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9076</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ASPP-L after CRF</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7674</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9433</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9112</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ASPP-1 before CRF</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7721</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9423</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9094</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ASPP-1 after CRF</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7751</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9445</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9128</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-02113-t008" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-02113-t008_Table 8</object-id><label>Table 8</label><caption><p>Classification results of FCN-8s and our approach. The metrics of speed is measured in seconds by the inference time of a single image.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Approach</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mean IU</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Overall Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Kappa</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Speed</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Pixel-based-SVM</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6549</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8513</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6451</td><td align="center" valign="middle" rowspan="1" colspan="1">233.7187 s</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FCN-8s</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7478</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9350</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8979</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1406 s</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ASPP-1 without CRF</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7721</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9423</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9094</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2916 s</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ASPP-1 with CRF</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7751</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9445</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9128</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.9171 s</td></tr></tbody></table></table-wrap><table-wrap id="sensors-18-02113-t009" orientation="portrait" position="float"><object-id pub-id-type="pii">sensors-18-02113-t009_Table 9</object-id><label>Table 9</label><caption><p>Confusion matrix of FCN-8s and our approach.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Approach</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GT/Predicted Class</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Others</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Rice</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Weeds</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Pixel-based-SVM</td><td align="center" valign="middle" rowspan="1" colspan="1">others</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.898</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.054</td><td align="center" valign="middle" rowspan="1" colspan="1">0.048</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">rice</td><td align="center" valign="middle" rowspan="1" colspan="1">0.037</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.865</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.098</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">weeds</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.141</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.128</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.731</bold>
</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">FCN-8s</td><td align="center" valign="middle" rowspan="1" colspan="1">others</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.940</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.050</td><td align="center" valign="middle" rowspan="1" colspan="1">0.010</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">rice</td><td align="center" valign="middle" rowspan="1" colspan="1">0.027</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.956</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.017</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">weeds</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.063</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.054</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.883</bold>
</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">ASPP-1 before CRF</td><td align="center" valign="middle" rowspan="1" colspan="1">others</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.950</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.034</td><td align="center" valign="middle" rowspan="1" colspan="1">0.016</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">rice</td><td align="center" valign="middle" rowspan="1" colspan="1">0.039</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.944</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.017</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">weeds</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.050</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.025</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.925</bold>
</td></tr></tbody></table></table-wrap></floats-group></article>